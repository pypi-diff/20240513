# Comparing `tmp/h2o_client-3.46.0.1-py2.py3-none-any.whl.zip` & `tmp/h2o_client-3.46.0.2-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,155 +1,155 @@
-Zip file size: 584961 bytes, number of entries: 153
--rw-r--r--  2.0 unx     3855 b- defN 24-Mar-13 15:24 h2o/__init__.py
--rw-r--r--  2.0 unx    17190 b- defN 24-Mar-13 15:24 h2o/assembly.py
--rw-r--r--  2.0 unx    20488 b- defN 24-Mar-13 15:24 h2o/astfun.py
--rw-r--r--  2.0 unx     2154 b- defN 24-Mar-13 15:24 h2o/auth.py
--rw-r--r--  2.0 unx      624 b- defN 24-Mar-13 15:24 h2o/base.py
--rw-r--r--  2.0 unx      233 b- defN 24-Mar-13 15:24 h2o/buildinfo.txt
--rw-r--r--  2.0 unx     1984 b- defN 24-Mar-13 15:24 h2o/cross_validation.py
--rw-r--r--  2.0 unx     1683 b- defN 24-Mar-13 15:24 h2o/debug.py
--rw-r--r--  2.0 unx    10834 b- defN 24-Mar-13 15:24 h2o/demos.py
--rw-r--r--  2.0 unx    27501 b- defN 24-Mar-13 15:24 h2o/display.py
--rw-r--r--  2.0 unx     6906 b- defN 24-Mar-13 15:24 h2o/exceptions.py
--rw-r--r--  2.0 unx    17499 b- defN 24-Mar-13 15:24 h2o/expr.py
--rw-r--r--  2.0 unx     4452 b- defN 24-Mar-13 15:24 h2o/expr_optimizer.py
--rw-r--r--  2.0 unx   226202 b- defN 24-Mar-13 15:24 h2o/frame.py
--rw-r--r--  2.0 unx    16866 b- defN 24-Mar-13 15:24 h2o/group_by.py
--rw-r--r--  2.0 unx   121518 b- defN 24-Mar-13 15:24 h2o/h2o.py
--rw-r--r--  2.0 unx     6702 b- defN 24-Mar-13 15:24 h2o/job.py
--rw-r--r--  2.0 unx     2837 b- defN 24-Mar-13 15:24 h2o/scoring.py
--rw-r--r--  2.0 unx     6967 b- defN 24-Mar-13 15:24 h2o/two_dim_table.py
--rw-r--r--  2.0 unx        8 b- defN 24-Mar-13 15:24 h2o/version.txt
--rw-r--r--  2.0 unx       23 b- defN 24-Mar-13 15:24 h2o/automl/__init__.py
--rw-r--r--  2.0 unx    15391 b- defN 24-Mar-13 15:24 h2o/automl/_base.py
--rw-r--r--  2.0 unx    40070 b- defN 24-Mar-13 15:24 h2o/automl/_estimator.py
--rw-r--r--  2.0 unx     1903 b- defN 24-Mar-13 15:24 h2o/automl/_output.py
--rw-r--r--  2.0 unx     2055 b- defN 24-Mar-13 15:24 h2o/automl/autoh2o.py
--rw-r--r--  2.0 unx     1800 b- defN 24-Mar-13 15:24 h2o/backend/__init__.py
--rw-r--r--  2.0 unx    13180 b- defN 24-Mar-13 15:24 h2o/backend/cluster.py
--rw-r--r--  2.0 unx    43495 b- defN 24-Mar-13 15:24 h2o/backend/connection.py
--rw-r--r--  2.0 unx    23979 b- defN 24-Mar-13 15:24 h2o/backend/server.py
--rw-r--r--  2.0 unx      859 b- defN 24-Mar-13 15:24 h2o/backend/bin/h2o.jar
--rw-r--r--  2.0 unx     3250 b- defN 24-Mar-13 15:24 h2o/estimators/__init__.py
--rw-r--r--  2.0 unx    10893 b- defN 24-Mar-13 15:24 h2o/estimators/adaboost.py
--rw-r--r--  2.0 unx    17449 b- defN 24-Mar-13 15:24 h2o/estimators/aggregator.py
--rw-r--r--  2.0 unx    35232 b- defN 24-Mar-13 15:24 h2o/estimators/anovaglm.py
--rw-r--r--  2.0 unx    23722 b- defN 24-Mar-13 15:24 h2o/estimators/coxph.py
--rw-r--r--  2.0 unx     6486 b- defN 24-Mar-13 15:24 h2o/estimators/decision_tree.py
--rw-r--r--  2.0 unx   147616 b- defN 24-Mar-13 15:24 h2o/estimators/deeplearning.py
--rw-r--r--  2.0 unx    27107 b- defN 24-Mar-13 15:24 h2o/estimators/estimator_base.py
--rw-r--r--  2.0 unx    14400 b- defN 24-Mar-13 15:24 h2o/estimators/extended_isolation_forest.py
--rw-r--r--  2.0 unx    72166 b- defN 24-Mar-13 15:24 h2o/estimators/gam.py
--rw-r--r--  2.0 unx   104156 b- defN 24-Mar-13 15:24 h2o/estimators/gbm.py
--rw-r--r--  2.0 unx     4975 b- defN 24-Mar-13 15:24 h2o/estimators/generic.py
--rw-r--r--  2.0 unx   118492 b- defN 24-Mar-13 15:24 h2o/estimators/glm.py
--rw-r--r--  2.0 unx    45803 b- defN 24-Mar-13 15:24 h2o/estimators/glrm.py
--rw-r--r--  2.0 unx    57630 b- defN 24-Mar-13 15:24 h2o/estimators/infogram.py
--rw-r--r--  2.0 unx    32729 b- defN 24-Mar-13 15:24 h2o/estimators/isolation_forest.py
--rw-r--r--  2.0 unx    12390 b- defN 24-Mar-13 15:24 h2o/estimators/isotonicregression.py
--rw-r--r--  2.0 unx    31417 b- defN 24-Mar-13 15:24 h2o/estimators/kmeans.py
--rw-r--r--  2.0 unx    69714 b- defN 24-Mar-13 15:24 h2o/estimators/model_selection.py
--rw-r--r--  2.0 unx    37967 b- defN 24-Mar-13 15:24 h2o/estimators/naive_bayes.py
--rw-r--r--  2.0 unx    23684 b- defN 24-Mar-13 15:24 h2o/estimators/pca.py
--rw-r--r--  2.0 unx    21460 b- defN 24-Mar-13 15:24 h2o/estimators/psvm.py
--rw-r--r--  2.0 unx    84526 b- defN 24-Mar-13 15:24 h2o/estimators/random_forest.py
--rw-r--r--  2.0 unx    16262 b- defN 24-Mar-13 15:24 h2o/estimators/rulefit.py
--rw-r--r--  2.0 unx     5433 b- defN 24-Mar-13 15:24 h2o/estimators/sdt.py
--rw-r--r--  2.0 unx     5466 b- defN 24-Mar-13 15:24 h2o/estimators/single_decision_tree.py
--rw-r--r--  2.0 unx    53237 b- defN 24-Mar-13 15:24 h2o/estimators/stackedensemble.py
--rw-r--r--  2.0 unx    17771 b- defN 24-Mar-13 15:24 h2o/estimators/svd.py
--rw-r--r--  2.0 unx    20604 b- defN 24-Mar-13 15:24 h2o/estimators/targetencoder.py
--rw-r--r--  2.0 unx    26454 b- defN 24-Mar-13 15:24 h2o/estimators/uplift_random_forest.py
--rw-r--r--  2.0 unx    18694 b- defN 24-Mar-13 15:24 h2o/estimators/word2vec.py
--rw-r--r--  2.0 unx   113260 b- defN 24-Mar-13 15:24 h2o/estimators/xgboost.py
--rw-r--r--  2.0 unx     2863 b- defN 24-Mar-13 15:24 h2o/explanation/__init__.py
--rw-r--r--  2.0 unx   162225 b- defN 24-Mar-13 15:24 h2o/explanation/_explain.py
--rw-r--r--  2.0 unx      550 b- defN 24-Mar-13 15:24 h2o/grid/__init__.py
--rw-r--r--  2.0 unx    78425 b- defN 24-Mar-13 15:24 h2o/grid/grid_search.py
--rw-r--r--  2.0 unx    67575 b- defN 24-Mar-13 15:24 h2o/grid/metrics.py
--rw-rw-r--  2.0 unx     4608 b- defN 24-Mar-13 15:24 h2o/h2o_data/iris.csv
--rw-rw-r--  2.0 unx     9254 b- defN 24-Mar-13 15:24 h2o/h2o_data/prostate.csv
--rw-r--r--  2.0 unx       49 b- defN 24-Mar-13 15:24 h2o/information_retrieval/__init__.py
--rw-r--r--  2.0 unx     1877 b- defN 24-Mar-13 15:24 h2o/information_retrieval/tf_idf.py
--rw-r--r--  2.0 unx     1064 b- defN 24-Mar-13 15:24 h2o/model/__init__.py
--rw-r--r--  2.0 unx     3068 b- defN 24-Mar-13 15:24 h2o/model/confusion_matrix.py
--rw-r--r--  2.0 unx    27066 b- defN 24-Mar-13 15:24 h2o/model/metrics_base.py
--rw-r--r--  2.0 unx   101654 b- defN 24-Mar-13 15:24 h2o/model/model_base.py
--rw-r--r--  2.0 unx     3658 b- defN 24-Mar-13 15:24 h2o/model/model_builder.py
--rw-r--r--  2.0 unx      326 b- defN 24-Mar-13 15:24 h2o/model/model_future.py
--rw-r--r--  2.0 unx     1505 b- defN 24-Mar-13 15:24 h2o/model/segment_models.py
--rw-r--r--  2.0 unx     2133 b- defN 24-Mar-13 15:24 h2o/model/extensions/__init__.py
--rw-r--r--  2.0 unx     4443 b- defN 24-Mar-13 15:24 h2o/model/extensions/contributions.py
--rw-r--r--  2.0 unx    28961 b- defN 24-Mar-13 15:24 h2o/model/extensions/fairness.py
--rw-r--r--  2.0 unx     2636 b- defN 24-Mar-13 15:24 h2o/model/extensions/feature_interaction.py
--rw-r--r--  2.0 unx     1961 b- defN 24-Mar-13 15:24 h2o/model/extensions/h_statistic.py
--rw-r--r--  2.0 unx      535 b- defN 24-Mar-13 15:24 h2o/model/extensions/row_to_tree_assignment.py
--rw-r--r--  2.0 unx     6596 b- defN 24-Mar-13 15:24 h2o/model/extensions/scoring_history.py
--rw-r--r--  2.0 unx     5815 b- defN 24-Mar-13 15:24 h2o/model/extensions/std_coef.py
--rw-r--r--  2.0 unx     2355 b- defN 24-Mar-13 15:24 h2o/model/extensions/supervised_trees.py
--rw-r--r--  2.0 unx      395 b- defN 24-Mar-13 15:24 h2o/model/extensions/trees.py
--rw-r--r--  2.0 unx     3812 b- defN 24-Mar-13 15:24 h2o/model/extensions/varimp.py
--rw-r--r--  2.0 unx     1513 b- defN 24-Mar-13 15:24 h2o/model/metrics/__init__.py
--rw-r--r--  2.0 unx     2055 b- defN 24-Mar-13 15:24 h2o/model/metrics/anomaly_detection.py
--rw-r--r--  2.0 unx    45988 b- defN 24-Mar-13 15:24 h2o/model/metrics/binomial.py
--rw-r--r--  2.0 unx     2277 b- defN 24-Mar-13 15:24 h2o/model/metrics/clustering.py
--rw-r--r--  2.0 unx     1731 b- defN 24-Mar-13 15:24 h2o/model/metrics/coxph.py
--rw-r--r--  2.0 unx      804 b- defN 24-Mar-13 15:24 h2o/model/metrics/dim_reduction.py
--rw-r--r--  2.0 unx       88 b- defN 24-Mar-13 15:24 h2o/model/metrics/generic.py
--rw-r--r--  2.0 unx     4940 b- defN 24-Mar-13 15:24 h2o/model/metrics/multinomial.py
--rw-r--r--  2.0 unx     2313 b- defN 24-Mar-13 15:24 h2o/model/metrics/ordinal.py
--rw-r--r--  2.0 unx     1032 b- defN 24-Mar-13 15:24 h2o/model/metrics/regression.py
--rw-r--r--  2.0 unx    28636 b- defN 24-Mar-13 15:24 h2o/model/metrics/uplift.py
--rw-r--r--  2.0 unx      706 b- defN 24-Mar-13 15:24 h2o/model/models/__init__.py
--rw-r--r--  2.0 unx     2268 b- defN 24-Mar-13 15:24 h2o/model/models/anomaly_detection.py
--rw-r--r--  2.0 unx     2024 b- defN 24-Mar-13 15:24 h2o/model/models/autoencoder.py
--rw-r--r--  2.0 unx    54349 b- defN 24-Mar-13 15:24 h2o/model/models/binomial.py
--rw-r--r--  2.0 unx    11130 b- defN 24-Mar-13 15:24 h2o/model/models/clustering.py
--rw-r--r--  2.0 unx     1113 b- defN 24-Mar-13 15:24 h2o/model/models/coxph.py
--rw-r--r--  2.0 unx     5257 b- defN 24-Mar-13 15:24 h2o/model/models/dim_reduction.py
--rw-r--r--  2.0 unx    12333 b- defN 24-Mar-13 15:24 h2o/model/models/multinomial.py
--rw-r--r--  2.0 unx     3729 b- defN 24-Mar-13 15:24 h2o/model/models/ordinal.py
--rw-r--r--  2.0 unx     5287 b- defN 24-Mar-13 15:24 h2o/model/models/regression.py
--rw-r--r--  2.0 unx    26918 b- defN 24-Mar-13 15:24 h2o/model/models/uplift.py
--rw-r--r--  2.0 unx     4382 b- defN 24-Mar-13 15:24 h2o/model/models/word_embedding.py
--rw-r--r--  2.0 unx      142 b- defN 24-Mar-13 15:24 h2o/persist/__init__.py
--rw-r--r--  2.0 unx     1624 b- defN 24-Mar-13 15:24 h2o/persist/persist.py
--rw-r--r--  2.0 unx      281 b- defN 24-Mar-13 15:24 h2o/pipeline/__init__.py
--rw-r--r--  2.0 unx     2232 b- defN 24-Mar-13 15:24 h2o/pipeline/mojo_pipeline.py
--rw-r--r--  2.0 unx      110 b- defN 24-Mar-13 15:24 h2o/plot/__init__.py
--rw-r--r--  2.0 unx     2105 b- defN 24-Mar-13 15:24 h2o/plot/_matplotlib.py
--rw-r--r--  2.0 unx     1256 b- defN 24-Mar-13 15:24 h2o/plot/_plot_result.py
--rw-r--r--  2.0 unx      271 b- defN 24-Mar-13 15:24 h2o/schemas/__init__.py
--rw-r--r--  2.0 unx     2253 b- defN 24-Mar-13 15:24 h2o/schemas/error.py
--rw-r--r--  2.0 unx     2142 b- defN 24-Mar-13 15:24 h2o/schemas/metadata.py
--rw-r--r--  2.0 unx     2596 b- defN 24-Mar-13 15:24 h2o/schemas/schema.py
--rw-r--r--  2.0 unx    11212 b- defN 24-Mar-13 15:24 h2o/sklearn/__init__.py
--rw-r--r--  2.0 unx    36832 b- defN 24-Mar-13 15:24 h2o/sklearn/wrapper.py
--rw-r--r--  2.0 unx      380 b- defN 24-Mar-13 15:24 h2o/transforms/__init__.py
--rw-r--r--  2.0 unx     8715 b- defN 24-Mar-13 15:24 h2o/transforms/decomposition.py
--rw-r--r--  2.0 unx     6802 b- defN 24-Mar-13 15:24 h2o/transforms/preprocessing.py
--rw-r--r--  2.0 unx     2158 b- defN 24-Mar-13 15:24 h2o/transforms/transform_base.py
--rw-r--r--  2.0 unx      178 b- defN 24-Mar-13 15:24 h2o/tree/__init__.py
--rw-r--r--  2.0 unx    45271 b- defN 24-Mar-13 15:24 h2o/tree/tree.py
--rw-r--r--  2.0 unx      488 b- defN 24-Mar-13 15:24 h2o/utils/__init__.py
--rw-r--r--  2.0 unx     7007 b- defN 24-Mar-13 15:24 h2o/utils/compatibility.py
--rw-r--r--  2.0 unx     4542 b- defN 24-Mar-13 15:24 h2o/utils/config.py
--rw-r--r--  2.0 unx    14261 b- defN 24-Mar-13 15:24 h2o/utils/debugging.py
--rw-r--r--  2.0 unx     4064 b- defN 24-Mar-13 15:24 h2o/utils/distributions.py
--rw-r--r--  2.0 unx    14510 b- defN 24-Mar-13 15:24 h2o/utils/metaclass.py
--rw-r--r--  2.0 unx     5158 b- defN 24-Mar-13 15:24 h2o/utils/mixin.py
--rw-r--r--  2.0 unx      603 b- defN 24-Mar-13 15:24 h2o/utils/model_utils.py
--rw-r--r--  2.0 unx    31851 b- defN 24-Mar-13 15:24 h2o/utils/progressbar.py
--rw-r--r--  2.0 unx    20134 b- defN 24-Mar-13 15:24 h2o/utils/shared_utils.py
--rw-r--r--  2.0 unx     3255 b- defN 24-Mar-13 15:24 h2o/utils/threading.py
--rw-r--r--  2.0 unx    28251 b- defN 24-Mar-13 15:24 h2o/utils/typechecks.py
--rw-r--r--  2.0 unx      423 b- defN 24-Mar-13 15:24 h2o/utils/csv/__init__.py
--rw-r--r--  2.0 unx     1879 b- defN 24-Mar-13 15:24 h2o/utils/csv/_common.py
--rw-r--r--  2.0 unx     1091 b- defN 24-Mar-13 15:24 h2o/utils/csv/_dispatch.py
--rw-r--r--  2.0 unx     1101 b- defN 24-Mar-13 15:24 h2o/utils/csv/_workarounds.py
--rw-r--r--  2.0 unx      111 b- defN 24-Mar-13 15:24 h2o/utils/csv/dialects.py
--rw-r--r--  2.0 unx     3447 b- defN 24-Mar-13 15:24 h2o/utils/csv/readers.py
--rw-r--r--  2.0 unx     2122 b- defN 24-Mar-13 15:24 h2o_client-3.46.0.1.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 24-Mar-13 15:24 h2o_client-3.46.0.1.dist-info/WHEEL
--rw-r--r--  2.0 unx        4 b- defN 24-Mar-13 15:24 h2o_client-3.46.0.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    12629 b- defN 24-Mar-13 15:24 h2o_client-3.46.0.1.dist-info/RECORD
-153 files, 2949586 bytes uncompressed, 565459 bytes compressed:  80.8%
+Zip file size: 587607 bytes, number of entries: 153
+-rw-r--r--  2.0 unx     3855 b- defN 24-May-13 15:30 h2o/__init__.py
+-rw-r--r--  2.0 unx    17190 b- defN 24-May-13 15:30 h2o/assembly.py
+-rw-r--r--  2.0 unx    20488 b- defN 24-May-13 15:30 h2o/astfun.py
+-rw-r--r--  2.0 unx     2154 b- defN 24-May-13 15:30 h2o/auth.py
+-rw-r--r--  2.0 unx      624 b- defN 24-May-13 15:30 h2o/base.py
+-rw-r--r--  2.0 unx      231 b- defN 24-May-13 15:30 h2o/buildinfo.txt
+-rw-r--r--  2.0 unx     1984 b- defN 24-May-13 15:30 h2o/cross_validation.py
+-rw-r--r--  2.0 unx     1683 b- defN 24-May-13 15:30 h2o/debug.py
+-rw-r--r--  2.0 unx    10834 b- defN 24-May-13 15:30 h2o/demos.py
+-rw-r--r--  2.0 unx    27501 b- defN 24-May-13 15:30 h2o/display.py
+-rw-r--r--  2.0 unx     6906 b- defN 24-May-13 15:30 h2o/exceptions.py
+-rw-r--r--  2.0 unx    17499 b- defN 24-May-13 15:30 h2o/expr.py
+-rw-r--r--  2.0 unx     4452 b- defN 24-May-13 15:30 h2o/expr_optimizer.py
+-rw-r--r--  2.0 unx   226202 b- defN 24-May-13 15:30 h2o/frame.py
+-rw-r--r--  2.0 unx    16866 b- defN 24-May-13 15:30 h2o/group_by.py
+-rw-r--r--  2.0 unx   121518 b- defN 24-May-13 15:30 h2o/h2o.py
+-rw-r--r--  2.0 unx     6702 b- defN 24-May-13 15:30 h2o/job.py
+-rw-r--r--  2.0 unx     2837 b- defN 24-May-13 15:30 h2o/scoring.py
+-rw-r--r--  2.0 unx     6967 b- defN 24-May-13 15:30 h2o/two_dim_table.py
+-rw-r--r--  2.0 unx        8 b- defN 24-May-13 15:30 h2o/version.txt
+-rw-r--r--  2.0 unx       23 b- defN 24-May-13 15:30 h2o/automl/__init__.py
+-rw-r--r--  2.0 unx    15391 b- defN 24-May-13 15:30 h2o/automl/_base.py
+-rw-r--r--  2.0 unx    40070 b- defN 24-May-13 15:30 h2o/automl/_estimator.py
+-rw-r--r--  2.0 unx     1903 b- defN 24-May-13 15:30 h2o/automl/_output.py
+-rw-r--r--  2.0 unx     2055 b- defN 24-May-13 15:30 h2o/automl/autoh2o.py
+-rw-r--r--  2.0 unx     1756 b- defN 24-May-13 15:30 h2o/backend/__init__.py
+-rw-r--r--  2.0 unx    13180 b- defN 24-May-13 15:30 h2o/backend/cluster.py
+-rw-r--r--  2.0 unx    43495 b- defN 24-May-13 15:30 h2o/backend/connection.py
+-rw-r--r--  2.0 unx    23979 b- defN 24-May-13 15:30 h2o/backend/server.py
+-rw-r--r--  2.0 unx      859 b- defN 24-May-13 15:30 h2o/backend/bin/h2o.jar
+-rw-r--r--  2.0 unx     3250 b- defN 24-May-13 15:30 h2o/estimators/__init__.py
+-rw-r--r--  2.0 unx    10893 b- defN 24-May-13 15:30 h2o/estimators/adaboost.py
+-rw-r--r--  2.0 unx    17449 b- defN 24-May-13 15:30 h2o/estimators/aggregator.py
+-rw-r--r--  2.0 unx    35232 b- defN 24-May-13 15:30 h2o/estimators/anovaglm.py
+-rw-r--r--  2.0 unx    23722 b- defN 24-May-13 15:30 h2o/estimators/coxph.py
+-rw-r--r--  2.0 unx     6486 b- defN 24-May-13 15:30 h2o/estimators/decision_tree.py
+-rw-r--r--  2.0 unx   147616 b- defN 24-May-13 15:30 h2o/estimators/deeplearning.py
+-rw-r--r--  2.0 unx    27107 b- defN 24-May-13 15:30 h2o/estimators/estimator_base.py
+-rw-r--r--  2.0 unx    14400 b- defN 24-May-13 15:30 h2o/estimators/extended_isolation_forest.py
+-rw-r--r--  2.0 unx    72166 b- defN 24-May-13 15:30 h2o/estimators/gam.py
+-rw-r--r--  2.0 unx   104156 b- defN 24-May-13 15:30 h2o/estimators/gbm.py
+-rw-r--r--  2.0 unx     4975 b- defN 24-May-13 15:30 h2o/estimators/generic.py
+-rw-r--r--  2.0 unx   131178 b- defN 24-May-13 15:30 h2o/estimators/glm.py
+-rw-r--r--  2.0 unx    45803 b- defN 24-May-13 15:30 h2o/estimators/glrm.py
+-rw-r--r--  2.0 unx    57630 b- defN 24-May-13 15:30 h2o/estimators/infogram.py
+-rw-r--r--  2.0 unx    32729 b- defN 24-May-13 15:30 h2o/estimators/isolation_forest.py
+-rw-r--r--  2.0 unx    13229 b- defN 24-May-13 15:30 h2o/estimators/isotonicregression.py
+-rw-r--r--  2.0 unx    31417 b- defN 24-May-13 15:30 h2o/estimators/kmeans.py
+-rw-r--r--  2.0 unx    68961 b- defN 24-May-13 15:30 h2o/estimators/model_selection.py
+-rw-r--r--  2.0 unx    37967 b- defN 24-May-13 15:30 h2o/estimators/naive_bayes.py
+-rw-r--r--  2.0 unx    23684 b- defN 24-May-13 15:30 h2o/estimators/pca.py
+-rw-r--r--  2.0 unx    21460 b- defN 24-May-13 15:30 h2o/estimators/psvm.py
+-rw-r--r--  2.0 unx    84526 b- defN 24-May-13 15:30 h2o/estimators/random_forest.py
+-rw-r--r--  2.0 unx    16262 b- defN 24-May-13 15:30 h2o/estimators/rulefit.py
+-rw-r--r--  2.0 unx     5433 b- defN 24-May-13 15:30 h2o/estimators/sdt.py
+-rw-r--r--  2.0 unx     5466 b- defN 24-May-13 15:30 h2o/estimators/single_decision_tree.py
+-rw-r--r--  2.0 unx    53237 b- defN 24-May-13 15:30 h2o/estimators/stackedensemble.py
+-rw-r--r--  2.0 unx    17771 b- defN 24-May-13 15:30 h2o/estimators/svd.py
+-rw-r--r--  2.0 unx    20604 b- defN 24-May-13 15:30 h2o/estimators/targetencoder.py
+-rw-r--r--  2.0 unx    32031 b- defN 24-May-13 15:30 h2o/estimators/uplift_random_forest.py
+-rw-r--r--  2.0 unx    18694 b- defN 24-May-13 15:30 h2o/estimators/word2vec.py
+-rw-r--r--  2.0 unx   113260 b- defN 24-May-13 15:30 h2o/estimators/xgboost.py
+-rw-r--r--  2.0 unx     2863 b- defN 24-May-13 15:30 h2o/explanation/__init__.py
+-rw-r--r--  2.0 unx   162225 b- defN 24-May-13 15:30 h2o/explanation/_explain.py
+-rw-r--r--  2.0 unx      550 b- defN 24-May-13 15:30 h2o/grid/__init__.py
+-rw-r--r--  2.0 unx    78425 b- defN 24-May-13 15:30 h2o/grid/grid_search.py
+-rw-r--r--  2.0 unx    67575 b- defN 24-May-13 15:30 h2o/grid/metrics.py
+-rw-rw-r--  2.0 unx     4608 b- defN 24-May-13 15:30 h2o/h2o_data/iris.csv
+-rw-rw-r--  2.0 unx     9254 b- defN 24-May-13 15:30 h2o/h2o_data/prostate.csv
+-rw-r--r--  2.0 unx       49 b- defN 24-May-13 15:30 h2o/information_retrieval/__init__.py
+-rw-r--r--  2.0 unx     1877 b- defN 24-May-13 15:30 h2o/information_retrieval/tf_idf.py
+-rw-r--r--  2.0 unx     1064 b- defN 24-May-13 15:30 h2o/model/__init__.py
+-rw-r--r--  2.0 unx     3068 b- defN 24-May-13 15:30 h2o/model/confusion_matrix.py
+-rw-r--r--  2.0 unx    27066 b- defN 24-May-13 15:30 h2o/model/metrics_base.py
+-rw-r--r--  2.0 unx   101921 b- defN 24-May-13 15:30 h2o/model/model_base.py
+-rw-r--r--  2.0 unx     3658 b- defN 24-May-13 15:30 h2o/model/model_builder.py
+-rw-r--r--  2.0 unx      326 b- defN 24-May-13 15:30 h2o/model/model_future.py
+-rw-r--r--  2.0 unx     1505 b- defN 24-May-13 15:30 h2o/model/segment_models.py
+-rw-r--r--  2.0 unx     2133 b- defN 24-May-13 15:30 h2o/model/extensions/__init__.py
+-rw-r--r--  2.0 unx     4443 b- defN 24-May-13 15:30 h2o/model/extensions/contributions.py
+-rw-r--r--  2.0 unx    28961 b- defN 24-May-13 15:30 h2o/model/extensions/fairness.py
+-rw-r--r--  2.0 unx     2636 b- defN 24-May-13 15:30 h2o/model/extensions/feature_interaction.py
+-rw-r--r--  2.0 unx     1961 b- defN 24-May-13 15:30 h2o/model/extensions/h_statistic.py
+-rw-r--r--  2.0 unx      535 b- defN 24-May-13 15:30 h2o/model/extensions/row_to_tree_assignment.py
+-rw-r--r--  2.0 unx     6596 b- defN 24-May-13 15:30 h2o/model/extensions/scoring_history.py
+-rw-r--r--  2.0 unx     5815 b- defN 24-May-13 15:30 h2o/model/extensions/std_coef.py
+-rw-r--r--  2.0 unx     2355 b- defN 24-May-13 15:30 h2o/model/extensions/supervised_trees.py
+-rw-r--r--  2.0 unx      395 b- defN 24-May-13 15:30 h2o/model/extensions/trees.py
+-rw-r--r--  2.0 unx     3812 b- defN 24-May-13 15:30 h2o/model/extensions/varimp.py
+-rw-r--r--  2.0 unx     1513 b- defN 24-May-13 15:30 h2o/model/metrics/__init__.py
+-rw-r--r--  2.0 unx     2055 b- defN 24-May-13 15:30 h2o/model/metrics/anomaly_detection.py
+-rw-r--r--  2.0 unx    45988 b- defN 24-May-13 15:30 h2o/model/metrics/binomial.py
+-rw-r--r--  2.0 unx     2277 b- defN 24-May-13 15:30 h2o/model/metrics/clustering.py
+-rw-r--r--  2.0 unx     1731 b- defN 24-May-13 15:30 h2o/model/metrics/coxph.py
+-rw-r--r--  2.0 unx      804 b- defN 24-May-13 15:30 h2o/model/metrics/dim_reduction.py
+-rw-r--r--  2.0 unx       88 b- defN 24-May-13 15:30 h2o/model/metrics/generic.py
+-rw-r--r--  2.0 unx     4940 b- defN 24-May-13 15:30 h2o/model/metrics/multinomial.py
+-rw-r--r--  2.0 unx     2313 b- defN 24-May-13 15:30 h2o/model/metrics/ordinal.py
+-rw-r--r--  2.0 unx     1032 b- defN 24-May-13 15:30 h2o/model/metrics/regression.py
+-rw-r--r--  2.0 unx    28636 b- defN 24-May-13 15:30 h2o/model/metrics/uplift.py
+-rw-r--r--  2.0 unx      706 b- defN 24-May-13 15:30 h2o/model/models/__init__.py
+-rw-r--r--  2.0 unx     2268 b- defN 24-May-13 15:30 h2o/model/models/anomaly_detection.py
+-rw-r--r--  2.0 unx     2024 b- defN 24-May-13 15:30 h2o/model/models/autoencoder.py
+-rw-r--r--  2.0 unx    54349 b- defN 24-May-13 15:30 h2o/model/models/binomial.py
+-rw-r--r--  2.0 unx    11130 b- defN 24-May-13 15:30 h2o/model/models/clustering.py
+-rw-r--r--  2.0 unx     1113 b- defN 24-May-13 15:30 h2o/model/models/coxph.py
+-rw-r--r--  2.0 unx     5257 b- defN 24-May-13 15:30 h2o/model/models/dim_reduction.py
+-rw-r--r--  2.0 unx    12333 b- defN 24-May-13 15:30 h2o/model/models/multinomial.py
+-rw-r--r--  2.0 unx     3729 b- defN 24-May-13 15:30 h2o/model/models/ordinal.py
+-rw-r--r--  2.0 unx     5287 b- defN 24-May-13 15:30 h2o/model/models/regression.py
+-rw-r--r--  2.0 unx    26918 b- defN 24-May-13 15:30 h2o/model/models/uplift.py
+-rw-r--r--  2.0 unx     4382 b- defN 24-May-13 15:30 h2o/model/models/word_embedding.py
+-rw-r--r--  2.0 unx      142 b- defN 24-May-13 15:30 h2o/persist/__init__.py
+-rw-r--r--  2.0 unx     1624 b- defN 24-May-13 15:30 h2o/persist/persist.py
+-rw-r--r--  2.0 unx      281 b- defN 24-May-13 15:30 h2o/pipeline/__init__.py
+-rw-r--r--  2.0 unx     2232 b- defN 24-May-13 15:30 h2o/pipeline/mojo_pipeline.py
+-rw-r--r--  2.0 unx      110 b- defN 24-May-13 15:30 h2o/plot/__init__.py
+-rw-r--r--  2.0 unx     1839 b- defN 24-May-13 15:30 h2o/plot/_matplotlib.py
+-rw-r--r--  2.0 unx     1256 b- defN 24-May-13 15:30 h2o/plot/_plot_result.py
+-rw-r--r--  2.0 unx      271 b- defN 24-May-13 15:30 h2o/schemas/__init__.py
+-rw-r--r--  2.0 unx     2253 b- defN 24-May-13 15:30 h2o/schemas/error.py
+-rw-r--r--  2.0 unx     2142 b- defN 24-May-13 15:30 h2o/schemas/metadata.py
+-rw-r--r--  2.0 unx     2596 b- defN 24-May-13 15:30 h2o/schemas/schema.py
+-rw-r--r--  2.0 unx    11212 b- defN 24-May-13 15:30 h2o/sklearn/__init__.py
+-rw-r--r--  2.0 unx    36832 b- defN 24-May-13 15:30 h2o/sklearn/wrapper.py
+-rw-r--r--  2.0 unx      380 b- defN 24-May-13 15:30 h2o/transforms/__init__.py
+-rw-r--r--  2.0 unx     8715 b- defN 24-May-13 15:30 h2o/transforms/decomposition.py
+-rw-r--r--  2.0 unx     6802 b- defN 24-May-13 15:30 h2o/transforms/preprocessing.py
+-rw-r--r--  2.0 unx     2158 b- defN 24-May-13 15:30 h2o/transforms/transform_base.py
+-rw-r--r--  2.0 unx      178 b- defN 24-May-13 15:30 h2o/tree/__init__.py
+-rw-r--r--  2.0 unx    45271 b- defN 24-May-13 15:30 h2o/tree/tree.py
+-rw-r--r--  2.0 unx      488 b- defN 24-May-13 15:30 h2o/utils/__init__.py
+-rw-r--r--  2.0 unx     7007 b- defN 24-May-13 15:30 h2o/utils/compatibility.py
+-rw-r--r--  2.0 unx     4542 b- defN 24-May-13 15:30 h2o/utils/config.py
+-rw-r--r--  2.0 unx    14261 b- defN 24-May-13 15:30 h2o/utils/debugging.py
+-rw-r--r--  2.0 unx     4064 b- defN 24-May-13 15:30 h2o/utils/distributions.py
+-rw-r--r--  2.0 unx    14510 b- defN 24-May-13 15:30 h2o/utils/metaclass.py
+-rw-r--r--  2.0 unx     5158 b- defN 24-May-13 15:30 h2o/utils/mixin.py
+-rw-r--r--  2.0 unx      603 b- defN 24-May-13 15:30 h2o/utils/model_utils.py
+-rw-r--r--  2.0 unx    31851 b- defN 24-May-13 15:30 h2o/utils/progressbar.py
+-rw-r--r--  2.0 unx    20134 b- defN 24-May-13 15:30 h2o/utils/shared_utils.py
+-rw-r--r--  2.0 unx     3255 b- defN 24-May-13 15:30 h2o/utils/threading.py
+-rw-r--r--  2.0 unx    28251 b- defN 24-May-13 15:30 h2o/utils/typechecks.py
+-rw-r--r--  2.0 unx      423 b- defN 24-May-13 15:30 h2o/utils/csv/__init__.py
+-rw-r--r--  2.0 unx     1879 b- defN 24-May-13 15:30 h2o/utils/csv/_common.py
+-rw-r--r--  2.0 unx     1091 b- defN 24-May-13 15:30 h2o/utils/csv/_dispatch.py
+-rw-r--r--  2.0 unx     1101 b- defN 24-May-13 15:30 h2o/utils/csv/_workarounds.py
+-rw-r--r--  2.0 unx      111 b- defN 24-May-13 15:30 h2o/utils/csv/dialects.py
+-rw-r--r--  2.0 unx     3447 b- defN 24-May-13 15:30 h2o/utils/csv/readers.py
+-rw-r--r--  2.0 unx     2122 b- defN 24-May-13 15:30 h2o_client-3.46.0.2.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 24-May-13 15:30 h2o_client-3.46.0.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx        4 b- defN 24-May-13 15:30 h2o_client-3.46.0.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    12629 b- defN 24-May-13 15:30 h2o_client-3.46.0.2.dist-info/RECORD
+153 files, 2967890 bytes uncompressed, 568105 bytes compressed:  80.9%
```

## zipnote {}

```diff
@@ -441,20 +441,20 @@
 
 Filename: h2o/utils/csv/dialects.py
 Comment: 
 
 Filename: h2o/utils/csv/readers.py
 Comment: 
 
-Filename: h2o_client-3.46.0.1.dist-info/METADATA
+Filename: h2o_client-3.46.0.2.dist-info/METADATA
 Comment: 
 
-Filename: h2o_client-3.46.0.1.dist-info/WHEEL
+Filename: h2o_client-3.46.0.2.dist-info/WHEEL
 Comment: 
 
-Filename: h2o_client-3.46.0.1.dist-info/top_level.txt
+Filename: h2o_client-3.46.0.2.dist-info/top_level.txt
 Comment: 
 
-Filename: h2o_client-3.46.0.1.dist-info/RECORD
+Filename: h2o_client-3.46.0.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## h2o/buildinfo.txt

```diff
@@ -1 +1 @@
-versionFromGradle='3.46.0',projectVersion='3.46.0.1',branch='rel-3.46.0',lastCommitHash='a35cd8a449055c283049e880e77b52bbd7e976e5',gitDescribe='jenkins-master-6501-9-ga35cd8a4490',compiledOn='2024-03-13 15:24:19',compiledBy='jenkins'
+versionFromGradle='3.46.0',projectVersion='3.46.0.2',branch='rel-3.46.0',lastCommitHash='a72a974cfbfeb3560a1a2dfb4c9b30993374fee0',gitDescribe='jenkins-3.46.0.1-36-ga72a974cfbf',compiledOn='2024-05-13 15:30:48',compiledBy='jenkins'
```

## h2o/version.txt

```diff
@@ -1 +1 @@
-3.46.0.1
+3.46.0.2
```

## h2o/backend/__init__.py

```diff
@@ -32,15 +32,14 @@
         h2o.connect(server=hs)
 
 Functions :func:`h2o.connect` and :func:`h2o.init` take many parameters that allow you to fine-tune the connection
 settings. When used, they will create a new :class:`H2OConnection` object and store it in a global variable -- this
 connection will be used by all subsequent calls to ``h2o.`` functions. Currently, there is no effective way to
 have multiple connections to separate H2O servers open at the same time. Such facility may be added in the future.
 """
-from distutils.version import StrictVersion
 import sys
 
 
 from .cluster import H2OCluster
 from .server import H2OLocalServer
 from .connection import H2OConnection
 from .connection import H2OConnectionConf
```

## h2o/backend/bin/h2o.jar

### zipinfo {}

```diff
@@ -1,6 +1,6 @@
 Zip file size: 859 bytes, number of entries: 4
-drwxr-xr-x  2.0 unx        0 b- defN 24-Mar-13 15:24 META-INF/
--rw-r--r--  2.0 unx       25 b- defN 24-Mar-13 15:24 META-INF/MANIFEST.MF
-drwxr-xr-x  2.0 unx        0 b- defN 24-Mar-13 15:24 water/
--rw-r--r--  2.0 unx      587 b- defN 24-Mar-13 15:24 water/H2OApp.class
+drwxr-xr-x  2.0 unx        0 b- defN 24-May-13 15:30 META-INF/
+-rw-r--r--  2.0 unx       25 b- defN 24-May-13 15:30 META-INF/MANIFEST.MF
+drwxr-xr-x  2.0 unx        0 b- defN 24-May-13 15:30 water/
+-rw-r--r--  2.0 unx      587 b- defN 24-May-13 15:30 water/H2OApp.class
 4 files, 612 bytes uncompressed, 427 bytes compressed:  30.2%
```

## h2o/estimators/glm.py

```diff
@@ -112,14 +112,22 @@
                  build_null_model=False,  # type: bool
                  fix_dispersion_parameter=False,  # type: bool
                  generate_variable_inflation_factors=False,  # type: bool
                  fix_tweedie_variance_power=True,  # type: bool
                  dispersion_learning_rate=0.5,  # type: float
                  influence=None,  # type: Optional[Literal["dfbetas"]]
                  gainslift_bins=-1,  # type: int
+                 linear_constraints=None,  # type: Optional[Union[None, str, H2OFrame]]
+                 init_optimal_glm=False,  # type: bool
+                 separate_linear_beta=False,  # type: bool
+                 constraint_eta0=0.1258925,  # type: float
+                 constraint_tau=10.0,  # type: float
+                 constraint_alpha=0.1,  # type: float
+                 constraint_beta=0.9,  # type: float
+                 constraint_c0=10.0,  # type: float
                  ):
         """
         :param model_id: Destination id for this model; auto-generated if not specified.
                Defaults to ``None``.
         :type model_id: Union[None, str, H2OEstimator], optional
         :param training_frame: Id of the training data frame.
                Defaults to ``None``.
@@ -132,15 +140,15 @@
         :type nfolds: int
         :param checkpoint: Model checkpoint to resume training with.
                Defaults to ``None``.
         :type checkpoint: Union[None, str, H2OEstimator], optional
         :param export_checkpoints_dir: Automatically export generated models to this directory.
                Defaults to ``None``.
         :type export_checkpoints_dir: str, optional
-        :param seed: Seed for pseudo random number generator (if applicable)
+        :param seed: Seed for pseudo random number generator (if applicable).
                Defaults to ``-1``.
         :type seed: int
         :param keep_cross_validation_models: Whether to keep the cross-validation models.
                Defaults to ``True``.
         :type keep_cross_validation_models: bool
         :param keep_cross_validation_predictions: Whether to keep the predictions of the cross-validation models.
                Defaults to ``False``.
@@ -166,15 +174,15 @@
         :type random_columns: List[int], optional
         :param ignore_const_cols: Ignore constant columns.
                Defaults to ``True``.
         :type ignore_const_cols: bool
         :param score_each_iteration: Whether to score during each iteration of model training.
                Defaults to ``False``.
         :type score_each_iteration: bool
-        :param score_iteration_interval: Perform scoring for every score_iteration_interval iterations
+        :param score_iteration_interval: Perform scoring for every score_iteration_interval iterations.
                Defaults to ``-1``.
         :type score_iteration_interval: int
         :param offset_column: Offset column. This will be added to the combination of columns before applying the link
                function.
                Defaults to ``None``.
         :type offset_column: str, optional
         :param weights_column: Column with observation weights. Giving some observation a weight of zero is equivalent
@@ -194,15 +202,15 @@
         :param rand_family: Random Component Family array.  One for each random component. Only support gaussian for
                now.
                Defaults to ``None``.
         :type rand_family: List[Literal["[gaussian]"]], optional
         :param tweedie_variance_power: Tweedie variance power
                Defaults to ``0.0``.
         :type tweedie_variance_power: float
-        :param tweedie_link_power: Tweedie link power
+        :param tweedie_link_power: Tweedie link power.
                Defaults to ``1.0``.
         :type tweedie_link_power: float
         :param theta: Theta
                Defaults to ``1e-10``.
         :type theta: float
         :param solver: AUTO will set the solver based on given data and the other parameters. IRLSM is fast on on
                problems with small number of predictors and for lambda-search with L1 penalty, L_BFGS scales better for
@@ -215,105 +223,107 @@
                specifies the amount of mixing between the two. Default value of alpha is 0 when SOLVER = 'L-BFGS'; 0.5
                otherwise.
                Defaults to ``None``.
         :type alpha: List[float], optional
         :param lambda_: Regularization strength
                Defaults to ``None``.
         :type lambda_: List[float], optional
-        :param lambda_search: Use lambda search starting at lambda max, given lambda is then interpreted as lambda min
+        :param lambda_search: Use lambda search starting at lambda max, given lambda is then interpreted as lambda min.
                Defaults to ``False``.
         :type lambda_search: bool
         :param early_stopping: Stop early when there is no more relative improvement on train or validation (if
-               provided)
+               provided).
                Defaults to ``True``.
         :type early_stopping: bool
         :param nlambdas: Number of lambdas to be used in a search. Default indicates: If alpha is zero, with lambda
                search set to True, the value of nlamdas is set to 30 (fewer lambdas are needed for ridge regression)
                otherwise it is set to 100.
                Defaults to ``-1``.
         :type nlambdas: int
-        :param standardize: Standardize numeric columns to have zero mean and unit variance
+        :param standardize: Standardize numeric columns to have zero mean and unit variance.
                Defaults to ``True``.
         :type standardize: bool
         :param missing_values_handling: Handling of missing values. Either MeanImputation, Skip or PlugValues.
                Defaults to ``"mean_imputation"``.
         :type missing_values_handling: Literal["mean_imputation", "skip", "plug_values"]
         :param plug_values: Plug Values (a single row frame containing values that will be used to impute missing values
-               of the training/validation frame, use with conjunction missing_values_handling = PlugValues)
+               of the training/validation frame, use with conjunction missing_values_handling = PlugValues).
                Defaults to ``None``.
         :type plug_values: Union[None, str, H2OFrame], optional
-        :param compute_p_values: Request p-values computation, p-values work only with IRLSM solver and no
-               regularization
+        :param compute_p_values: Request p-values computation, p-values work only with IRLSM solver.
                Defaults to ``False``.
         :type compute_p_values: bool
         :param dispersion_parameter_method: Method used to estimate the dispersion parameter for Tweedie, Gamma and
                Negative Binomial only.
                Defaults to ``"pearson"``.
         :type dispersion_parameter_method: Literal["deviance", "pearson", "ml"]
         :param init_dispersion_parameter: Only used for Tweedie, Gamma and Negative Binomial GLM.  Store the initial
                value of dispersion parameter.  If fix_dispersion_parameter is set, this value will be used in the
-               calculation of p-values.Default to 1.0.
+               calculation of p-values.
                Defaults to ``1.0``.
         :type init_dispersion_parameter: float
-        :param remove_collinear_columns: In case of linearly dependent columns, remove some of the dependent columns
+        :param remove_collinear_columns: In case of linearly dependent columns, remove the dependent columns.
                Defaults to ``False``.
         :type remove_collinear_columns: bool
         :param intercept: Include constant term in the model
                Defaults to ``True``.
         :type intercept: bool
-        :param non_negative: Restrict coefficients (not intercept) to be non-negative
+        :param non_negative: Restrict coefficients (not intercept) to be non-negative.
                Defaults to ``False``.
         :type non_negative: bool
-        :param max_iterations: Maximum number of iterations
+        :param max_iterations: Maximum number of iterations.  Value should >=1.  A value of 0 is only set when only the
+               model coefficient names and model coefficient dimensions are needed.
                Defaults to ``-1``.
         :type max_iterations: int
         :param objective_epsilon: Converge if  objective value changes less than this. Default (of -1.0) indicates: If
                lambda_search is set to True the value of objective_epsilon is set to .0001. If the lambda_search is set
                to False and lambda is equal to zero, the value of objective_epsilon is set to .000001, for any other
                value of lambda the default value of objective_epsilon is set to .0001.
                Defaults to ``-1.0``.
         :type objective_epsilon: float
-        :param beta_epsilon: Converge if  beta changes less (using L-infinity norm) than beta esilon, ONLY applies to
-               IRLSM solver
+        :param beta_epsilon: Converge if beta changes less (using L-infinity norm) than beta esilon. ONLY applies to
+               IRLSM solver.
                Defaults to ``0.0001``.
         :type beta_epsilon: float
         :param gradient_epsilon: Converge if  objective changes less (using L-infinity norm) than this, ONLY applies to
                L-BFGS solver. Default (of -1.0) indicates: If lambda_search is set to False and lambda is equal to zero,
                the default value of gradient_epsilon is equal to .000001, otherwise the default value is .0001. If
                lambda_search is set to True, the conditional values above are 1E-8 and 1E-6 respectively.
                Defaults to ``-1.0``.
         :type gradient_epsilon: float
         :param link: Link function.
                Defaults to ``"family_default"``.
         :type link: Literal["family_default", "identity", "logit", "log", "inverse", "tweedie", "ologit"]
         :param rand_link: Link function array for random component in HGLM.
                Defaults to ``None``.
         :type rand_link: List[Literal["[identity]", "[family_default]"]], optional
-        :param startval: double array to initialize fixed and random coefficients for HGLM, coefficients for GLM.
+        :param startval: double array to initialize fixed and random coefficients for HGLM, coefficients for GLM.  If
+               standardize is true, the standardized coefficients should be used.  Otherwise, use the regular
+               coefficients.
                Defaults to ``None``.
         :type startval: List[float], optional
         :param calc_like: if true, will return likelihood function value.
                Defaults to ``False``.
         :type calc_like: bool
-        :param HGLM: If set to true, will return HGLM model.  Otherwise, normal GLM model will be returned
+        :param HGLM: If set to true, will return HGLM model.  Otherwise, normal GLM model will be returned.
                Defaults to ``False``.
         :type HGLM: bool
         :param prior: Prior probability for y==1. To be used only for logistic regression iff the data has been sampled
                and the mean of response does not reflect reality.
                Defaults to ``-1.0``.
         :type prior: float
         :param cold_start: Only applicable to multiple alpha/lambda values.  If false, build the next model for next set
                of alpha/lambda values starting from the values provided by current model.  If true will start GLM model
                from scratch.
                Defaults to ``False``.
         :type cold_start: bool
         :param lambda_min_ratio: Minimum lambda used in lambda search, specified as a ratio of lambda_max (the smallest
-               lambda that drives all coefficients to zero). Default indicates: if the number of observations is greater
-               than the number of variables, then lambda_min_ratio is set to 0.0001; if the number of observations is
-               less than the number of variables, then lambda_min_ratio is set to 0.01.
+               lambda that drives all coefficients to zero).  Default indicates: if the number of observations is
+               greater than the number of variables, then lambda_min_ratio is set to 0.0001; if the number of
+               observations is less than the number of variables, then lambda_min_ratio is set to 0.01.
                Defaults to ``-1.0``.
         :type lambda_min_ratio: float
         :param beta_constraints: Beta constraints
                Defaults to ``None``.
         :type beta_constraints: Union[None, str, H2OFrame], optional
         :param max_active_predictors: Maximum number of active predictors during computation. Use as a stopping
                criterion to prevent expensive model building with many predictors. Default indicates: If the IRLSM
@@ -323,15 +333,15 @@
         :param interactions: A list of predictor column indices to interact. All pairwise combinations will be computed
                for the list.
                Defaults to ``None``.
         :type interactions: List[str], optional
         :param interaction_pairs: A list of pairwise (first order) column interactions.
                Defaults to ``None``.
         :type interaction_pairs: List[tuple], optional
-        :param obj_reg: Likelihood divider in objective value computation, default (of -1.0) will set it to 1/nobs
+        :param obj_reg: Likelihood divider in objective value computation, default (of -1.0) will set it to 1/nobs.
                Defaults to ``-1.0``.
         :type obj_reg: float
         :param stopping_rounds: Early stopping based on convergence of stopping_metric. Stop if simple moving average of
                length k of the stopping_metric does not improve for k:=stopping_rounds scoring events (0 to disable)
                Defaults to ``0``.
         :type stopping_rounds: int
         :param stopping_metric: Metric to use for early stopping (AUTO: logloss for classification, deviance for
@@ -353,15 +363,15 @@
                Defaults to ``None``.
         :type class_sampling_factors: List[float], optional
         :param max_after_balance_size: Maximum relative size of the training data after balancing class counts (can be
                less than 1.0). Requires balance_classes.
                Defaults to ``5.0``.
         :type max_after_balance_size: float
         :param max_confusion_matrix_size: [Deprecated] Maximum size (# classes) for confusion matrices to be printed in
-               the Logs
+               the Logs.
                Defaults to ``20``.
         :type max_confusion_matrix_size: int
         :param max_runtime_secs: Maximum allowed runtime in seconds for model training. Use 0 to disable.
                Defaults to ``0.0``.
         :type max_runtime_secs: float
         :param custom_metric_func: Reference to custom evaluation function, format: `language:keyName=funcName`
                Defaults to ``None``.
@@ -412,14 +422,45 @@
                excluded in the dataset.
                Defaults to ``None``.
         :type influence: Literal["dfbetas"], optional
         :param gainslift_bins: Gains/Lift table number of bins. 0 means disabled.. Default value -1 means automatic
                binning.
                Defaults to ``-1``.
         :type gainslift_bins: int
+        :param linear_constraints: Linear constraints: used to specify linear constraints involving more than one
+               coefficients in standard form.  It is only supported for solver IRLSM.  It contains four columns: names
+               (strings for coefficient names or constant), values, types ( strings of 'Equal' or 'LessThanEqual'),
+               constraint_numbers (0 for first linear constraint, 1 for second linear constraint, ...).
+               Defaults to ``None``.
+        :type linear_constraints: Union[None, str, H2OFrame], optional
+        :param init_optimal_glm: If true, will initialize coefficients with values derived from GLM runs without linear
+               constraints.  Only available for linear constraints.
+               Defaults to ``False``.
+        :type init_optimal_glm: bool
+        :param separate_linear_beta: If true, will keep the beta constraints and linear constraints separate.  After new
+               coefficients are found, first beta constraints will be applied followed by the application of linear
+               constraints.  Note that the beta constraints in this case will not be part of the objective function.  If
+               false, will combine the beta and linear constraints.
+               Defaults to ``False``.
+        :type separate_linear_beta: bool
+        :param constraint_eta0: For constrained GLM only.  It affects the setting of eta_k+1=eta_0/power(ck+1, alpha).
+               Defaults to ``0.1258925``.
+        :type constraint_eta0: float
+        :param constraint_tau: For constrained GLM only.  It affects the setting of c_k+1=tau*c_k.
+               Defaults to ``10.0``.
+        :type constraint_tau: float
+        :param constraint_alpha: For constrained GLM only.  It affects the setting of  eta_k = eta_0/pow(c_0, alpha).
+               Defaults to ``0.1``.
+        :type constraint_alpha: float
+        :param constraint_beta: For constrained GLM only.  It affects the setting of eta_k+1 = eta_k/pow(c_k, beta).
+               Defaults to ``0.9``.
+        :type constraint_beta: float
+        :param constraint_c0: For constrained GLM only.  It affects the initial setting of epsilon_k = 1/c_0.
+               Defaults to ``10.0``.
+        :type constraint_c0: float
         """
         super(H2OGeneralizedLinearEstimator, self).__init__()
         self._parms = {}
         self._id = self._parms['model_id'] = model_id
         self.training_frame = training_frame
         self.validation_frame = validation_frame
         self.nfolds = nfolds
@@ -493,14 +534,22 @@
         self.build_null_model = build_null_model
         self.fix_dispersion_parameter = fix_dispersion_parameter
         self.generate_variable_inflation_factors = generate_variable_inflation_factors
         self.fix_tweedie_variance_power = fix_tweedie_variance_power
         self.dispersion_learning_rate = dispersion_learning_rate
         self.influence = influence
         self.gainslift_bins = gainslift_bins
+        self.linear_constraints = linear_constraints
+        self.init_optimal_glm = init_optimal_glm
+        self.separate_linear_beta = separate_linear_beta
+        self.constraint_eta0 = constraint_eta0
+        self.constraint_tau = constraint_tau
+        self.constraint_alpha = constraint_alpha
+        self.constraint_beta = constraint_beta
+        self.constraint_c0 = constraint_c0
 
     @property
     def training_frame(self):
         """
         Id of the training data frame.
 
         Type: ``Union[None, str, H2OFrame]``.
@@ -629,15 +678,15 @@
     def export_checkpoints_dir(self, export_checkpoints_dir):
         assert_is_type(export_checkpoints_dir, None, str)
         self._parms["export_checkpoints_dir"] = export_checkpoints_dir
 
     @property
     def seed(self):
         """
-        Seed for pseudo random number generator (if applicable)
+        Seed for pseudo random number generator (if applicable).
 
         Type: ``int``, defaults to ``-1``.
 
         :examples:
 
         >>> airlines= h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/airlines/allyears2k_headers.zip")
         >>> airlines["Year"] = airlines["Year"].asfactor()
@@ -921,15 +970,15 @@
     def score_each_iteration(self, score_each_iteration):
         assert_is_type(score_each_iteration, None, bool)
         self._parms["score_each_iteration"] = score_each_iteration
 
     @property
     def score_iteration_interval(self):
         """
-        Perform scoring for every score_iteration_interval iterations
+        Perform scoring for every score_iteration_interval iterations.
 
         Type: ``int``, defaults to ``-1``.
         """
         return self._parms.get("score_iteration_interval")
 
     @score_iteration_interval.setter
     def score_iteration_interval(self, score_iteration_interval):
@@ -1073,15 +1122,15 @@
     def tweedie_variance_power(self, tweedie_variance_power):
         assert_is_type(tweedie_variance_power, None, numeric)
         self._parms["tweedie_variance_power"] = tweedie_variance_power
 
     @property
     def tweedie_link_power(self):
         """
-        Tweedie link power
+        Tweedie link power.
 
         Type: ``float``, defaults to ``1.0``.
 
         :examples:
 
         >>> auto = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/glm_test/auto.csv")
         >>> predictors = auto.names
@@ -1224,15 +1273,15 @@
     def lambda_(self, lambda_):
         assert_is_type(lambda_, None, numeric, [numeric])
         self._parms["lambda"] = lambda_
 
     @property
     def lambda_search(self):
         """
-        Use lambda search starting at lambda max, given lambda is then interpreted as lambda min
+        Use lambda search starting at lambda max, given lambda is then interpreted as lambda min.
 
         Type: ``bool``, defaults to ``False``.
 
         :examples:
 
         >>> boston = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/BostonHousing.csv")
         >>> predictors = boston.columns[:-1]
@@ -1252,15 +1301,15 @@
     def lambda_search(self, lambda_search):
         assert_is_type(lambda_search, None, bool)
         self._parms["lambda_search"] = lambda_search
 
     @property
     def early_stopping(self):
         """
-        Stop early when there is no more relative improvement on train or validation (if provided)
+        Stop early when there is no more relative improvement on train or validation (if provided).
 
         Type: ``bool``, defaults to ``True``.
 
         :examples:
 
         >>> cars = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/junit/cars_20mpg.csv")
         >>> cars["economy_20mpg"] = cars["economy_20mpg"].asfactor()
@@ -1311,15 +1360,15 @@
     def nlambdas(self, nlambdas):
         assert_is_type(nlambdas, None, int)
         self._parms["nlambdas"] = nlambdas
 
     @property
     def standardize(self):
         """
-        Standardize numeric columns to have zero mean and unit variance
+        Standardize numeric columns to have zero mean and unit variance.
 
         Type: ``bool``, defaults to ``True``.
 
         :examples:
 
         >>> boston = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/BostonHousing.csv")
         >>> predictors = boston.columns[:-1]
@@ -1369,15 +1418,15 @@
         assert_is_type(missing_values_handling, None, Enum("mean_imputation", "skip", "plug_values"))
         self._parms["missing_values_handling"] = missing_values_handling
 
     @property
     def plug_values(self):
         """
         Plug Values (a single row frame containing values that will be used to impute missing values of the
-        training/validation frame, use with conjunction missing_values_handling = PlugValues)
+        training/validation frame, use with conjunction missing_values_handling = PlugValues).
 
         Type: ``Union[None, str, H2OFrame]``.
 
         :examples:
 
         >>> cars = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/junit/cars_20mpg.csv")
         >>> cars = cars.drop(0)
@@ -1402,15 +1451,15 @@
     @plug_values.setter
     def plug_values(self, plug_values):
         self._parms["plug_values"] = H2OFrame._validate(plug_values, 'plug_values')
 
     @property
     def compute_p_values(self):
         """
-        Request p-values computation, p-values work only with IRLSM solver and no regularization
+        Request p-values computation, p-values work only with IRLSM solver.
 
         Type: ``bool``, defaults to ``False``.
 
         :examples:
 
         >>> airlines= h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/airlines/allyears2k_headers.zip")
         >>> airlines["Year"] = airlines["Year"].asfactor()
@@ -1453,29 +1502,29 @@
         assert_is_type(dispersion_parameter_method, None, Enum("deviance", "pearson", "ml"))
         self._parms["dispersion_parameter_method"] = dispersion_parameter_method
 
     @property
     def init_dispersion_parameter(self):
         """
         Only used for Tweedie, Gamma and Negative Binomial GLM.  Store the initial value of dispersion parameter.  If
-        fix_dispersion_parameter is set, this value will be used in the calculation of p-values.Default to 1.0.
+        fix_dispersion_parameter is set, this value will be used in the calculation of p-values.
 
         Type: ``float``, defaults to ``1.0``.
         """
         return self._parms.get("init_dispersion_parameter")
 
     @init_dispersion_parameter.setter
     def init_dispersion_parameter(self, init_dispersion_parameter):
         assert_is_type(init_dispersion_parameter, None, numeric)
         self._parms["init_dispersion_parameter"] = init_dispersion_parameter
 
     @property
     def remove_collinear_columns(self):
         """
-        In case of linearly dependent columns, remove some of the dependent columns
+        In case of linearly dependent columns, remove the dependent columns.
 
         Type: ``bool``, defaults to ``False``.
 
         :examples:
 
         >>> airlines= h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/airlines/allyears2k_headers.zip")
         >>> airlines["Year"] = airlines["Year"].asfactor()
@@ -1531,15 +1580,15 @@
     def intercept(self, intercept):
         assert_is_type(intercept, None, bool)
         self._parms["intercept"] = intercept
 
     @property
     def non_negative(self):
         """
-        Restrict coefficients (not intercept) to be non-negative
+        Restrict coefficients (not intercept) to be non-negative.
 
         Type: ``bool``, defaults to ``False``.
 
         :examples:
 
         >>> airlines = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/airlines/allyears2k_headers.zip")
         >>> airlines["Year"] = airlines["Year"].asfactor()
@@ -1565,15 +1614,16 @@
     def non_negative(self, non_negative):
         assert_is_type(non_negative, None, bool)
         self._parms["non_negative"] = non_negative
 
     @property
     def max_iterations(self):
         """
-        Maximum number of iterations
+        Maximum number of iterations.  Value should >=1.  A value of 0 is only set when only the model coefficient names
+        and model coefficient dimensions are needed.
 
         Type: ``int``, defaults to ``-1``.
 
         :examples:
 
         >>> cars = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/junit/cars_20mpg.csv")
         >>> cars["economy_20mpg"] = cars["economy_20mpg"].asfactor()
@@ -1625,15 +1675,15 @@
     def objective_epsilon(self, objective_epsilon):
         assert_is_type(objective_epsilon, None, numeric)
         self._parms["objective_epsilon"] = objective_epsilon
 
     @property
     def beta_epsilon(self):
         """
-        Converge if  beta changes less (using L-infinity norm) than beta esilon, ONLY applies to IRLSM solver
+        Converge if beta changes less (using L-infinity norm) than beta esilon. ONLY applies to IRLSM solver.
 
         Type: ``float``, defaults to ``0.0001``.
 
         :examples:
 
         >>> cars = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/junit/cars_20mpg.csv")
         >>> predictors = ["displacement","power","weight","year"]
@@ -1727,15 +1777,16 @@
     def rand_link(self, rand_link):
         assert_is_type(rand_link, None, [Enum("[identity]", "[family_default]")])
         self._parms["rand_link"] = rand_link
 
     @property
     def startval(self):
         """
-        double array to initialize fixed and random coefficients for HGLM, coefficients for GLM.
+        double array to initialize fixed and random coefficients for HGLM, coefficients for GLM.  If standardize is
+        true, the standardized coefficients should be used.  Otherwise, use the regular coefficients.
 
         Type: ``List[float]``.
         """
         return self._parms.get("startval")
 
     @startval.setter
     def startval(self, startval):
@@ -1755,15 +1806,15 @@
     def calc_like(self, calc_like):
         assert_is_type(calc_like, None, bool)
         self._parms["calc_like"] = calc_like
 
     @property
     def HGLM(self):
         """
-        If set to true, will return HGLM model.  Otherwise, normal GLM model will be returned
+        If set to true, will return HGLM model.  Otherwise, normal GLM model will be returned.
 
         Type: ``bool``, defaults to ``False``.
         """
         return self._parms.get("HGLM")
 
     @HGLM.setter
     def HGLM(self, HGLM):
@@ -1814,17 +1865,17 @@
         assert_is_type(cold_start, None, bool)
         self._parms["cold_start"] = cold_start
 
     @property
     def lambda_min_ratio(self):
         """
         Minimum lambda used in lambda search, specified as a ratio of lambda_max (the smallest lambda that drives all
-        coefficients to zero). Default indicates: if the number of observations is greater than the number of variables,
-        then lambda_min_ratio is set to 0.0001; if the number of observations is less than the number of variables, then
-        lambda_min_ratio is set to 0.01.
+        coefficients to zero).  Default indicates: if the number of observations is greater than the number of
+        variables, then lambda_min_ratio is set to 0.0001; if the number of observations is less than the number of
+        variables, then lambda_min_ratio is set to 0.01.
 
         Type: ``float``, defaults to ``-1.0``.
 
         :examples:
 
         >>> boston = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/BostonHousing.csv")
         >>> predictors = boston.columns[:-1]
@@ -1986,15 +2037,15 @@
     def interaction_pairs(self, interaction_pairs):
         assert_is_type(interaction_pairs, None, [tuple])
         self._parms["interaction_pairs"] = interaction_pairs
 
     @property
     def obj_reg(self):
         """
-        Likelihood divider in objective value computation, default (of -1.0) will set it to 1/nobs
+        Likelihood divider in objective value computation, default (of -1.0) will set it to 1/nobs.
 
         Type: ``float``, defaults to ``-1.0``.
 
         :examples:
 
         >>> df = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/bigdata/laptop/glm_ordinal_logit/ordinal_multinomial_training_set.csv")
         >>> df["C11"] = df["C11"].asfactor()
@@ -2152,15 +2203,15 @@
     def max_after_balance_size(self, max_after_balance_size):
         assert_is_type(max_after_balance_size, None, float)
         self._parms["max_after_balance_size"] = max_after_balance_size
 
     @property
     def max_confusion_matrix_size(self):
         """
-        [Deprecated] Maximum size (# classes) for confusion matrices to be printed in the Logs
+        [Deprecated] Maximum size (# classes) for confusion matrices to be printed in the Logs.
 
         Type: ``int``, defaults to ``20``.
         """
         return self._parms.get("max_confusion_matrix_size")
 
     @max_confusion_matrix_size.setter
     def max_confusion_matrix_size(self, max_confusion_matrix_size):
@@ -2395,14 +2446,132 @@
         return self._parms.get("gainslift_bins")
 
     @gainslift_bins.setter
     def gainslift_bins(self, gainslift_bins):
         assert_is_type(gainslift_bins, None, int)
         self._parms["gainslift_bins"] = gainslift_bins
 
+    @property
+    def linear_constraints(self):
+        """
+        Linear constraints: used to specify linear constraints involving more than one coefficients in standard form.
+        It is only supported for solver IRLSM.  It contains four columns: names (strings for coefficient names or
+        constant), values, types ( strings of 'Equal' or 'LessThanEqual'), constraint_numbers (0 for first linear
+        constraint, 1 for second linear constraint, ...).
+
+        Type: ``Union[None, str, H2OFrame]``.
+        """
+        return self._parms.get("linear_constraints")
+
+    @linear_constraints.setter
+    def linear_constraints(self, linear_constraints):
+        self._parms["linear_constraints"] = H2OFrame._validate(linear_constraints, 'linear_constraints')
+
+    @property
+    def init_optimal_glm(self):
+        """
+        If true, will initialize coefficients with values derived from GLM runs without linear constraints.  Only
+        available for linear constraints.
+
+        Type: ``bool``, defaults to ``False``.
+        """
+        return self._parms.get("init_optimal_glm")
+
+    @init_optimal_glm.setter
+    def init_optimal_glm(self, init_optimal_glm):
+        assert_is_type(init_optimal_glm, None, bool)
+        self._parms["init_optimal_glm"] = init_optimal_glm
+
+    @property
+    def separate_linear_beta(self):
+        """
+        If true, will keep the beta constraints and linear constraints separate.  After new coefficients are found,
+        first beta constraints will be applied followed by the application of linear constraints.  Note that the beta
+        constraints in this case will not be part of the objective function.  If false, will combine the beta and linear
+        constraints.
+
+        Type: ``bool``, defaults to ``False``.
+        """
+        return self._parms.get("separate_linear_beta")
+
+    @separate_linear_beta.setter
+    def separate_linear_beta(self, separate_linear_beta):
+        assert_is_type(separate_linear_beta, None, bool)
+        self._parms["separate_linear_beta"] = separate_linear_beta
+
+    @property
+    def constraint_eta0(self):
+        """
+        For constrained GLM only.  It affects the setting of eta_k+1=eta_0/power(ck+1, alpha).
+
+        Type: ``float``, defaults to ``0.1258925``.
+        """
+        return self._parms.get("constraint_eta0")
+
+    @constraint_eta0.setter
+    def constraint_eta0(self, constraint_eta0):
+        assert_is_type(constraint_eta0, None, numeric)
+        self._parms["constraint_eta0"] = constraint_eta0
+
+    @property
+    def constraint_tau(self):
+        """
+        For constrained GLM only.  It affects the setting of c_k+1=tau*c_k.
+
+        Type: ``float``, defaults to ``10.0``.
+        """
+        return self._parms.get("constraint_tau")
+
+    @constraint_tau.setter
+    def constraint_tau(self, constraint_tau):
+        assert_is_type(constraint_tau, None, numeric)
+        self._parms["constraint_tau"] = constraint_tau
+
+    @property
+    def constraint_alpha(self):
+        """
+        For constrained GLM only.  It affects the setting of  eta_k = eta_0/pow(c_0, alpha).
+
+        Type: ``float``, defaults to ``0.1``.
+        """
+        return self._parms.get("constraint_alpha")
+
+    @constraint_alpha.setter
+    def constraint_alpha(self, constraint_alpha):
+        assert_is_type(constraint_alpha, None, numeric)
+        self._parms["constraint_alpha"] = constraint_alpha
+
+    @property
+    def constraint_beta(self):
+        """
+        For constrained GLM only.  It affects the setting of eta_k+1 = eta_k/pow(c_k, beta).
+
+        Type: ``float``, defaults to ``0.9``.
+        """
+        return self._parms.get("constraint_beta")
+
+    @constraint_beta.setter
+    def constraint_beta(self, constraint_beta):
+        assert_is_type(constraint_beta, None, numeric)
+        self._parms["constraint_beta"] = constraint_beta
+
+    @property
+    def constraint_c0(self):
+        """
+        For constrained GLM only.  It affects the initial setting of epsilon_k = 1/c_0.
+
+        Type: ``float``, defaults to ``10.0``.
+        """
+        return self._parms.get("constraint_c0")
+
+    @constraint_c0.setter
+    def constraint_c0(self, constraint_c0):
+        assert_is_type(constraint_c0, None, numeric)
+        self._parms["constraint_c0"] = constraint_c0
+
     Lambda = deprecated_property('Lambda', lambda_)
 
     def get_regression_influence_diagnostics(self):
         """
         For GLM model, if influence is set to dfbetas, a frame containing the original predictors, response
         and DFBETA_ for each predictors that are used in building the model is returned.
 
@@ -2595,7 +2764,98 @@
                   "names": list(coefs.keys()),
                   "beta": list(coefs.values()),
                   "threshold": threshold}
         )
         m = H2OGeneralizedLinearEstimator()
         m._resolve_model(model_json["model_id"]["name"], model_json)
         return m
+
+    @staticmethod
+    def getConstraintsInfo(model):
+        """
+
+        Given a constrained GLM model, the constraints descriptions, constraints values, constraints conditions and 
+        whether the constraints are satisfied (true) or not (false) are returned.
+
+        :param model: GLM model with linear and beta (if applicable)  constraints
+        :return: H2OTwoDimTable containing the above constraints information.
+
+        :example:
+        >>> train = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/glm_test/binomial_20_cols_10KRows.csv")
+        >>> response = "C21"
+        >>> predictors = list(range(0,20))
+        >>> loose_init_const = [] # this constraint is satisfied by default coefficient initialization
+        >>> # add loose constraints
+        >>> name = "C19"
+        >>> values = 0.5
+        >>> types = "LessThanEqual"
+        >>> contraint_numbers = 0
+        >>> loose_init_const.append([name, values, types, contraint_numbers])
+        >>> name = "C20"
+        >>> values = -0.8
+        >>> types = "LessThanEqual"
+        >>> contraint_numbers = 0
+        >>> loose_init_const.append([name, values, types, contraint_numbers])
+        >>> name = "constant"
+        >>> values = -1000
+        >>> types = "LessThanEqual"
+        >>> contraint_numbers = 0
+        >>> loose_init_const.append([name, values, types, contraint_numbers])
+        >>> linear_constraints2 = h2o.H2OFrame(loose_init_const)
+        >>> linear_constraints2.set_names(["names", "values", "types", "constraint_numbers"])    
+        >>> # GLM model with GLM coefficients with default initialization
+        >>> h2o_glm = H2OGeneralizedLinearEstimator(family="binomial", compute_p_values=True, remove_collinear_columns=True, 
+        ...                                         lambda_=0.0, solver="irlsm", linear_constraints=linear_constraints2,
+        ...                                         init_optimal_glm = False, seed=12345)
+        >>> h2o_glm.train(x=predictors, y=response, training_frame=train)
+        >>> print(H2OGeneralizedLinearEstimator.getConstraintsInfo(h2o_glm))
+        """
+        if model.actual_params["linear_constraints"] is not None:
+            return model._model_json["output"]["linear_constraints_table"]
+        else:
+            raise H2OValueError("getConstraintsInfo can only be called when there are linear constraints.")
+
+    @staticmethod
+    def allConstraintsPassed(model):
+        """
+
+        Given a constrainted GLM model, this will return true  if all beta (if exists) and linear constraints are
+         satified.  It will return false even if one constraint is not satisfied.  To see which ones failed, use
+         getConstraintsInfo function.
+
+        :param model:  GLM model with linear and beta (if applicable)  constraints
+        :return: boolean True or False
+
+        :example:
+        >>> train = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/glm_test/binomial_20_cols_10KRows.csv")
+        >>> response = "C21"
+        >>> predictors = list(range(0,20))
+        >>> loose_init_const = [] # this constraint is satisfied by default coefficient initialization
+        >>> # add loose constraints
+        >>> name = "C19"
+        >>> values = 0.5
+        >>> types = "LessThanEqual"
+        >>> contraint_numbers = 0
+        >>> loose_init_const.append([name, values, types, contraint_numbers])
+        >>> name = "C20"
+        >>> values = -0.8
+        >>> types = "LessThanEqual"
+        >>> contraint_numbers = 0
+        >>> loose_init_const.append([name, values, types, contraint_numbers])
+        >>> name = "constant"
+        >>> values = -1000
+        >>> types = "LessThanEqual"
+        >>> contraint_numbers = 0
+        >>> loose_init_const.append([name, values, types, contraint_numbers])
+        >>> linear_constraints2 = h2o.H2OFrame(loose_init_const)
+        >>> linear_constraints2.set_names(["names", "values", "types", "constraint_numbers"])    
+        >>> # GLM model with GLM coefficients with default initialization
+        >>> h2o_glm = H2OGeneralizedLinearEstimator(family="binomial", compute_p_values=True, remove_collinear_columns=True, 
+        ...                                         lambda_=0.0, solver="irlsm", linear_constraints=linear_constraints2,
+        ...                                         init_optimal_glm = False, seed=12345)
+        >>> h2o_glm.train(x=predictors, y=response, training_frame=train)
+        >>> print(H2OGeneralizedLinearEstimator.allConstraintsPassed(h2o_glm))
+        """
+        if model.actual_params["linear_constraints"] is not None:
+            return model._model_json["output"]["all_constraints_satisfied"]
+        else:
+            raise H2OValueError("allConstraintsPassed can only be called when there are linear constraints.")
```

## h2o/estimators/isotonicregression.py

```diff
@@ -180,14 +180,32 @@
 
     @property
     def out_of_bounds(self):
         """
         Method of handling values of X predictor that are outside of the bounds seen in training.
 
         Type: ``Literal["na", "clip"]``, defaults to ``"na"``.
+
+        :examples:
+
+        >>> import h2o
+        >>> from h2o import H2OFrame
+        >>> from h2o.estimators.isotonicregression import H2OIsotonicRegressionEstimator
+        >>> from sklearn.datasets import make_regression
+        >>> import numpy as np
+        >>> h2o.init()
+        >>> X, y = make_regression(n_samples=10000, n_features=1, random_state=41, noise=0.8)
+        >>> X = X.reshape(-1)
+        >>> train = H2OFrame(np.column_stack((y, X)), column_names=["y", "X"])
+        >>> w_values = np.random.rand(train.shape[0])
+        >>> w_frame = H2OFrame(w_values.reshape(-1, 1), column_names=["w"])
+        >>> train = train.cbind(w_frame)
+        >>> h2o_iso_reg = H2OIsotonicRegressionEstimator(out_of_bounds="clip")
+        >>> h2o_iso_reg.train(training_frame=train, x="X", y="y")
+        >>> h2o_iso_reg.predict(train)
         """
         return self._parms.get("out_of_bounds")
 
     @out_of_bounds.setter
     def out_of_bounds(self, out_of_bounds):
         assert_is_type(out_of_bounds, None, Enum("na", "clip"))
         self._parms["out_of_bounds"] = out_of_bounds
```

## h2o/estimators/model_selection.py

```diff
@@ -77,15 +77,14 @@
                  stopping_metric="auto",  # type: Literal["auto", "deviance", "logloss", "mse", "rmse", "mae", "rmsle", "auc", "aucpr", "lift_top_group", "misclassification", "mean_per_class_error", "custom", "custom_increasing"]
                  stopping_tolerance=0.001,  # type: float
                  balance_classes=False,  # type: bool
                  class_sampling_factors=None,  # type: Optional[List[float]]
                  max_after_balance_size=5.0,  # type: float
                  max_confusion_matrix_size=20,  # type: int
                  max_runtime_secs=0.0,  # type: float
-                 custom_metric_func=None,  # type: Optional[str]
                  nparallelism=0,  # type: int
                  max_predictor_number=1,  # type: int
                  min_predictor_number=1,  # type: int
                  mode="maxr",  # type: Literal["allsubsets", "maxr", "maxrsweep", "backward"]
                  build_glm_model=False,  # type: bool
                  p_values_threshold=0.0,  # type: float
                  influence=None,  # type: Optional[Literal["dfbetas"]]
@@ -286,17 +285,14 @@
         :param max_confusion_matrix_size: [Deprecated] Maximum size (# classes) for confusion matrices to be printed in
                the Logs
                Defaults to ``20``.
         :type max_confusion_matrix_size: int
         :param max_runtime_secs: Maximum allowed runtime in seconds for model training. Use 0 to disable.
                Defaults to ``0.0``.
         :type max_runtime_secs: float
-        :param custom_metric_func: Reference to custom evaluation function, format: `language:keyName=funcName`
-               Defaults to ``None``.
-        :type custom_metric_func: str, optional
         :param nparallelism: number of models to build in parallel.  Defaults to 0.0 which is adaptive to the system
                capability
                Defaults to ``0``.
         :type nparallelism: int
         :param max_predictor_number: Maximum number of predictors to be considered when building GLM models.  Defaults
                to 1.
                Defaults to ``1``.
@@ -379,15 +375,14 @@
         self.stopping_metric = stopping_metric
         self.stopping_tolerance = stopping_tolerance
         self.balance_classes = balance_classes
         self.class_sampling_factors = class_sampling_factors
         self.max_after_balance_size = max_after_balance_size
         self.max_confusion_matrix_size = max_confusion_matrix_size
         self.max_runtime_secs = max_runtime_secs
-        self.custom_metric_func = custom_metric_func
         self.nparallelism = nparallelism
         self.max_predictor_number = max_predictor_number
         self.min_predictor_number = min_predictor_number
         self.mode = mode
         self.build_glm_model = build_glm_model
         self.p_values_threshold = p_values_threshold
         self.influence = influence
@@ -1121,28 +1116,14 @@
 
     @max_runtime_secs.setter
     def max_runtime_secs(self, max_runtime_secs):
         assert_is_type(max_runtime_secs, None, numeric)
         self._parms["max_runtime_secs"] = max_runtime_secs
 
     @property
-    def custom_metric_func(self):
-        """
-        Reference to custom evaluation function, format: `language:keyName=funcName`
-
-        Type: ``str``.
-        """
-        return self._parms.get("custom_metric_func")
-
-    @custom_metric_func.setter
-    def custom_metric_func(self, custom_metric_func):
-        assert_is_type(custom_metric_func, None, str)
-        self._parms["custom_metric_func"] = custom_metric_func
-
-    @property
     def nparallelism(self):
         """
         number of models to build in parallel.  Defaults to 0.0 which is adaptive to the system capability
 
         Type: ``int``, defaults to ``0``.
         """
         return self._parms.get("nparallelism")
```

## h2o/estimators/uplift_random_forest.py

```diff
@@ -569,56 +569,157 @@
     @property
     def treatment_column(self):
         """
         Define the column which will be used for computing uplift gain to select best split for a tree. The column has
         to divide the dataset into treatment (value 1) and control (value 0) groups.
 
         Type: ``str``, defaults to ``"treatment"``.
+
+        :examples:
+
+        >>> import h2o
+        >>> from h2o.estimators import H2OUpliftRandomForestEstimator
+        >>> h2o.init()
+        >>> data = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/uplift/criteo_uplift_13k.csv")
+        >>> predictors = ["f1", "f2", "f3", "f4", "f5", "f6","f7", "f8"]
+        >>> response = "conversion"
+        >>> data[response] = data[response].asfactor()
+        >>> treatment_column = "treatment"
+        >>> data[treatment_column] = data[treatment_column].asfactor()
+        >>> train, valid = data.split_frame(ratios=[.8], seed=1234)
+        >>> uplift_model = H2OUpliftRandomForestEstimator(ntrees=10,
+        ...                                               max_depth=5,
+        ...                                               uplift_metric="KL",
+        ...                                               min_rows=10,
+        ...                                               seed=1234,
+        ...                                               auuc_type="qini",
+        ...                                               treatment_column=treatment_column)
+        >>> uplift_model.train(x=predictors,
+        ...                    y=response,
+        ...                    training_frame=train,
+        ...                    validation_frame=valid)
+        >>> uplift_model.model_performance()
         """
         return self._parms.get("treatment_column")
 
     @treatment_column.setter
     def treatment_column(self, treatment_column):
         assert_is_type(treatment_column, None, str)
         self._parms["treatment_column"] = treatment_column
 
     @property
     def uplift_metric(self):
         """
         Divergence metric used to find best split when building an uplift tree.
 
         Type: ``Literal["auto", "kl", "euclidean", "chi_squared"]``, defaults to ``"auto"``.
+
+        :examples:
+
+        >>> import h2o
+        >>> from h2o.estimators import H2OUpliftRandomForestEstimator
+        >>> h2o.init()
+        >>> data = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/uplift/criteo_uplift_13k.csv")
+        >>> predictors = ["f1", "f2", "f3", "f4", "f5", "f6","f7", "f8"]
+        >>> response = "conversion"
+        >>> data[response] = data[response].asfactor()
+        >>> treatment_column = "treatment"
+        >>> data[treatment_column] = data[treatment_column].asfactor()
+        >>> train, valid = data.split_frame(ratios=[.8], seed=1234)
+        >>> uplift_model = H2OUpliftRandomForestEstimator(ntrees=10,
+        ...                                               max_depth=5,
+        ...                                               min_rows=10,
+        ...                                               seed=1234,
+        ...                                               auuc_type="qini",
+        ...                                               treatment_column=treatment_column,
+        ...                                               uplift_metric="euclidean")
+        >>> uplift_model.train(x=predictors,
+        ...                    y=response,
+        ...                    training_frame=train,
+        ...                    validation_frame=valid)
+        >>> uplift_model.model_performance()
         """
         return self._parms.get("uplift_metric")
 
     @uplift_metric.setter
     def uplift_metric(self, uplift_metric):
         assert_is_type(uplift_metric, None, Enum("auto", "kl", "euclidean", "chi_squared"))
         self._parms["uplift_metric"] = uplift_metric
 
     @property
     def auuc_type(self):
         """
         Metric used to calculate Area Under Uplift Curve.
 
         Type: ``Literal["auto", "qini", "lift", "gain"]``, defaults to ``"auto"``.
+
+        :examples:
+
+        >>> import h2o
+        >>> from h2o.estimators import H2OUpliftRandomForestEstimator
+        >>> h2o.init()
+        >>> data = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/uplift/criteo_uplift_13k.csv")
+        >>> predictors = ["f1", "f2", "f3", "f4", "f5", "f6","f7", "f8"]
+        >>> response = "conversion"
+        >>> data[response] = data[response].asfactor()
+        >>> treatment_column = "treatment"
+        >>> data[treatment_column] = data[treatment_column].asfactor()
+        >>> train, valid = data.split_frame(ratios=[.8], seed=1234)
+        >>> uplift_model = H2OUpliftRandomForestEstimator(ntrees=10,
+        ...                                               max_depth=5,
+        ...                                               treatment_column=treatment_column,
+        ...                                               uplift_metric="KL",
+        ...                                               min_rows=10,
+        ...                                               seed=1234,
+        ...                                               auuc_type="gain")
+        >>> uplift_model.train(x=predictors,
+        ...                    y=response,
+        ...                    training_frame=train,
+        ...                    validation_frame=valid)
+        >>> uplift_model.model_performance()
         """
         return self._parms.get("auuc_type")
 
     @auuc_type.setter
     def auuc_type(self, auuc_type):
         assert_is_type(auuc_type, None, Enum("auto", "qini", "lift", "gain"))
         self._parms["auuc_type"] = auuc_type
 
     @property
     def auuc_nbins(self):
         """
         Number of bins to calculate Area Under Uplift Curve.
 
         Type: ``int``, defaults to ``-1``.
+
+        :examples:
+
+        >>> import h2o
+        >>> from h2o.estimators import H2OUpliftRandomForestEstimator
+        >>> h2o.init()
+        >>> data = h2o.import_file("https://s3.amazonaws.com/h2o-public-test-data/smalldata/uplift/criteo_uplift_13k.csv")
+        >>> predictors = ["f1", "f2", "f3", "f4", "f5", "f6","f7", "f8"]
+        >>> response = "conversion"
+        >>> data[response] = data[response].asfactor()
+        >>> treatment_column = "treatment"
+        >>> data[treatment_column] = data[treatment_column].asfactor()
+        >>> train, valid = data.split_frame(ratios=[.8], seed=1234)
+        >>> uplift_model = H2OUpliftRandomForestEstimator(ntrees=10,
+        ...                                               max_depth=5,
+        ...                                               treatment_column=treatment_column,
+        ...                                               uplift_metric="KL",
+        ...                                               min_rows=10,
+        ...                                               seed=1234,
+        ...                                               auuc_type="qini",
+        ...                                               auuc_nbins=100)
+        >>> uplift_model.train(x=predictors,
+        ...                    y=response,
+        ...                    training_frame=train,
+        ...                    validation_frame=valid)
+        >>> uplift_model.model_performance()
         """
         return self._parms.get("auuc_nbins")
 
     @auuc_nbins.setter
     def auuc_nbins(self, auuc_nbins):
         assert_is_type(auuc_nbins, None, int)
         self._parms["auuc_nbins"] = auuc_nbins
```

## h2o/model/model_base.py

```diff
@@ -829,14 +829,23 @@
                     return None
                 return {name: vif for name, vif in zip(tbl["names"], tbl["variable_inflation_factor"])}
             else:
                 raise ValueError("variable inflation factors are generated only when "
                                  "generate_variable_inflation_factors is enabled.")
         else:
             raise ValueError("variable inflation factors are only found in GLM models for numerical predictors.")
+    
+    def coef_names(self):
+        """
+        Return the coefficient names of glm model
+        """
+        if self.algo == 'glm':
+            coefs = self._model_json['output']['coefficient_names']
+            coefs.remove('Intercept')
+            return coefs
         
     def coef(self):
         """
         Return the coefficients which can be applied to the non-standardized data.
 
         **Note**: ``standardize=True`` by default; when ``standardize=False``, then ``coef()`` will return the coefficients which are fit directly.
         """
```

## h2o/plot/_matplotlib.py

```diff
@@ -1,18 +1,13 @@
 
 def get_matplotlib_pyplot(server, raise_if_not_available=False):
     try:
         # noinspection PyUnresolvedReferences
         import matplotlib
-        from distutils.version import LooseVersion
-        if server:
-            if LooseVersion(matplotlib.__version__) <= LooseVersion("3.1"):
-                matplotlib.use("Agg", warn=False)
-            else:  # Versions >= 3.2 don't have warn argument
-                matplotlib.use("Agg")
+        matplotlib.use("Agg")
         try:
             # noinspection PyUnresolvedReferences
             import matplotlib.pyplot as plt
         except ImportError as e:
             if server:
                 raise e
             import warnings
```

## Comparing `h2o_client-3.46.0.1.dist-info/METADATA` & `h2o_client-3.46.0.2.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: h2o-client
-Version: 3.46.0.1
+Version: 3.46.0.2
 Summary: H2O, Fast Scalable Machine Learning, for python 
 Home-page: https://github.com/h2oai/h2o-3.git
 Author: H2O.ai
 Author-email: support@h2o.ai
 License: Apache v2
 Keywords: machine learning,data mining,statistical analysis,modeling,big data,distributed,parallel
 Classifier: Development Status :: 5 - Production/Stable
```

## Comparing `h2o_client-3.46.0.1.dist-info/RECORD` & `h2o_client-3.46.0.2.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,82 +1,82 @@
 h2o/__init__.py,sha256=6jpb6mF83uvnX4F8HGxpjTKcggQOewFs8g34QZSpsBI,3855
 h2o/assembly.py,sha256=vg6BXqeBR4m2pD7HD4vfZb4U6v68mL_TZFfrKqO7VSk,17190
 h2o/astfun.py,sha256=To0rP-4XzkWeTmLKrf7jgesLHupzN4U4GBJ7wNHDNVo,20488
 h2o/auth.py,sha256=n1waB_fAplL_-DdY-dRdveWnTg5XDVcH12Bvw6cY6hA,2154
 h2o/base.py,sha256=61pf1VqBAwY_9z5st29TczRqMrFXIBdg83YZoK2WXXA,624
-h2o/buildinfo.txt,sha256=hmup6N-HmhFQI99jQo6CQvwouYentqrZ0SfsqEF5VD8,233
+h2o/buildinfo.txt,sha256=F66HiFIlOfTIuataU89jAudc1JKO4Vk86dGWPSmq5ng,231
 h2o/cross_validation.py,sha256=3xcNxFGBI33mSq6tWuf35MV4atNzuU-pMYL2KPg9Qw8,1984
 h2o/debug.py,sha256=G-bPRjaCL_mkkt2mo1yYrShK2T2fOutlRZ9IjinZaq4,1683
 h2o/demos.py,sha256=xp8JWzHTgISQ7NpnQfcJ_rHOShLHlCrAAIEQZ-_SLHk,10834
 h2o/display.py,sha256=WYw9pKWNk0BXee03qthQWcgqr4lTE234MIdoDuIuTZc,27501
 h2o/exceptions.py,sha256=_IYtVJcqE64EWQd0rserTAfZbQavB4Ng8_kuMmvR0zE,6906
 h2o/expr.py,sha256=MgpLhiF1q82Zo_7fggxreMw-Vg2k0FwhLtDGDtswTHo,17499
 h2o/expr_optimizer.py,sha256=sKFQj1I_zllcu6fcBj_tg8e6sWnxQoETpmR7H3DyD0E,4452
 h2o/frame.py,sha256=DGbTIzBzvKtmO6idvagQwdEwPk0pqlGHtPFCp4RsmXA,226202
 h2o/group_by.py,sha256=ClRTTyD9T9WblMUDBZvauh3RTESRj82PKmgLLBbNSRo,16866
 h2o/h2o.py,sha256=by1Q5fLoxTi1w9DO_l3aA0rf5H4Vr2IXJ8gVFEsWXeM,121518
 h2o/job.py,sha256=kvtIKnRQ2LfeB85qnrwHo45eZrEP-sllyTbPr9N0X-I,6702
 h2o/scoring.py,sha256=e3xQ0ByXavMgiPjbaxx4OeSGqpgngpfx1nrAb7qDZ5k,2837
 h2o/two_dim_table.py,sha256=-I96dXJvcJLME1wTeGRpwbKvfTXSgX49Q4CAMLR7uw0,6967
-h2o/version.txt,sha256=Iu3LboSBWMGxdkE_ZaCuCN0Xgjp9DN9pfqjWG07Jd5w,8
+h2o/version.txt,sha256=LwvZLJNjLXo0M7XDlLPdup98EuDygwKRwqn0bP7dI7Q,8
 h2o/automl/__init__.py,sha256=qOd02u4pQJgBOEaFKob8QkuNOXrB4K0YmarWvMnJCAU,23
 h2o/automl/_base.py,sha256=V0fS-XNHYWZ1Xq2h3STL3A1ruNMqOotWmwET_LCV7pw,15391
 h2o/automl/_estimator.py,sha256=ssRg-aFswFB5--On1HThDY9DQpUOvIHYD38gEEtvSsw,40070
 h2o/automl/_output.py,sha256=WD4oaO2yTRD8IVY9W_u6VWsKL9MmFBA7f3vIZWU95v4,1903
 h2o/automl/autoh2o.py,sha256=uyU7rdHpnjTUbmHAc61ZtsS-Sa_7fmZ891npOxEbiNk,2055
-h2o/backend/__init__.py,sha256=ADyBpuQ1DJxS6DfGVRdY_0mUeJ6maB9X0ncfcZ8iJP8,1800
+h2o/backend/__init__.py,sha256=XFXOax5EEusWV73Vjm7Gidr5jXV-umP5T13K1YJKtMU,1756
 h2o/backend/cluster.py,sha256=qFAnaT7uS4BKwu59zT5pnnXtQzEC9zcDz77EokeNujU,13180
 h2o/backend/connection.py,sha256=zwG0q4butw_2IKRZVR2_I5FWpaIVZ9NAnves_TYoRkY,43495
 h2o/backend/server.py,sha256=ObbVBMcM70yAGsZwsvaYEzSNtjliS4YH33hekdkx61g,23979
-h2o/backend/bin/h2o.jar,sha256=rUGIPwkWJNDB2EVuRyC6Rw12M6ygMcPqq2jVtUr7154,859
+h2o/backend/bin/h2o.jar,sha256=C824MoD16hlM0Idiq-KEY10P5yko-u5lWei1AVv1d-o,859
 h2o/estimators/__init__.py,sha256=P0uIJSx1bbPMUwz2sfjms_UwLtDIeyPrPcTfJAydmGo,3250
 h2o/estimators/adaboost.py,sha256=DULoStXl3uYOIrcCUizueVT-JQRCJwkR6nPqf8Lyif0,10893
 h2o/estimators/aggregator.py,sha256=ZTajKU-Op6O8TxQ3L31X2vYmZPSGCQxRE1lWW1M_-8s,17449
 h2o/estimators/anovaglm.py,sha256=biQYjDbig_GOuyMbQzrajJu8Br9J0JxZqUfPBD76aZw,35232
 h2o/estimators/coxph.py,sha256=s2wdtwaNvwwXKPdydUVkWKwPx2v4X_YESarQQuBd9v8,23722
 h2o/estimators/decision_tree.py,sha256=JpmSqIm4fOj7FLmgpeyJPpozWtLh5qA_o-BLRp_reGg,6486
 h2o/estimators/deeplearning.py,sha256=s3hpdu32yfQv0lUUk3-fX17FzuAU0QGckWMAOmg6Rz0,147616
 h2o/estimators/estimator_base.py,sha256=1QiNSz0c_BX-kzdm6P9DkmF4g6VxZPcl0SnIHOLwttU,27107
 h2o/estimators/extended_isolation_forest.py,sha256=2VSS_YjExsJxD9RXsb73miGQ4YVHMTbwq5QFD7gFZwU,14400
 h2o/estimators/gam.py,sha256=cMCQw2LwiDV9wQdl0OJH-5CXmsmRRcsVtYUJ3_16gEc,72166
 h2o/estimators/gbm.py,sha256=9l1DHbsZU9eDNbcXkJDNg4uL7JBb81ii6qIavHsMSmg,104156
 h2o/estimators/generic.py,sha256=P9FQ3gAnGT54yPmIOtCXIT_2f34uJwEWhH2uJHgCzTk,4975
-h2o/estimators/glm.py,sha256=FTvusk9Y2tYIIwg4odr-hNMVOujFQPemvUrl1XuxIvI,118492
+h2o/estimators/glm.py,sha256=-SnNYzmgDBLqyF80_lUTyqCL-PbEv65l6Ti33UCUdBg,131178
 h2o/estimators/glrm.py,sha256=Ih2XRqpCVPJl6LtnOaBDZc80m6Hb9KBwzRhVKRhbLyU,45803
 h2o/estimators/infogram.py,sha256=Ph8ULG8wYyDSy0cZmkFE2tSjPOwf2FMiCdKoq07819I,57630
 h2o/estimators/isolation_forest.py,sha256=IbmSfmZcxvfX5I3dLGm03u7guIkr6bOHV2kgrn42v44,32729
-h2o/estimators/isotonicregression.py,sha256=n-xtZdE8g3nrZa2mUsRZ0tJQ6TdSymVveNKu0798K6s,12390
+h2o/estimators/isotonicregression.py,sha256=Pc3PMqGwnGo4rSuQArUeIjpuWZwlkWjyRG7AO4LapLI,13229
 h2o/estimators/kmeans.py,sha256=RYxuq5X6zZ9xpC1VhXFjccwH_FJtijfPZzCYxvz_ZHg,31417
-h2o/estimators/model_selection.py,sha256=7Pc-lYCnX08hzFMulsn75tB30Q2TMzqtvSVLxZRJWkI,69714
+h2o/estimators/model_selection.py,sha256=25qWKMYcHOmzmp99Em5sBxTWbT8m9wYpQd94goHIbhY,68961
 h2o/estimators/naive_bayes.py,sha256=nXuuqEH6U1keRYvc6B9FQ8rXEzGmbHKZUMKvBIroqL0,37967
 h2o/estimators/pca.py,sha256=gRZWeAYAbVikD9k7ITIQ23MXfA44x1ol_k5AoWoe0xs,23684
 h2o/estimators/psvm.py,sha256=Nn-Tyk8lLToValL_oNr9nUxQHFfzokR5znIeFf8Iy6A,21460
 h2o/estimators/random_forest.py,sha256=U0ORtS_gnhFQdiFlnwzgxGPB1wB4Mvrsv1oUwA0ja-U,84526
 h2o/estimators/rulefit.py,sha256=cBKf_g2uKoHBarUNOqiNu3HukUHsizSXdxMC-AIQXTA,16262
 h2o/estimators/sdt.py,sha256=c75_OhDNQRfBRvJbLZfxCkX4AZnTnywa-ezcfu8ojQQ,5433
 h2o/estimators/single_decision_tree.py,sha256=T1VdNPycyCl0kbz-2uw4Y2loeZj8Ju6BEx4yREFbqlI,5466
 h2o/estimators/stackedensemble.py,sha256=Z0FHNPHVYhRkn5dxGI8vqEcE81Y4uT0ZvJBWrFR-9Bw,53237
 h2o/estimators/svd.py,sha256=bAPlcu_DjCn9KTo2vaEKX4y4iReUsAcIDAMl060e52A,17771
 h2o/estimators/targetencoder.py,sha256=KGzdPsMRyWnjYFkZVfFQG7-B9Huo6fQe9dQoXM9J8Cw,20604
-h2o/estimators/uplift_random_forest.py,sha256=UEVI4ilNH0El5v-t5vJ9SZF50-3FlYQ8oGoaxDrANMQ,26454
+h2o/estimators/uplift_random_forest.py,sha256=WzkaqEDHxLKf9aGGB7zsK257QeJZkyPNz-OedJzQCpk,32031
 h2o/estimators/word2vec.py,sha256=hCrXWz-4ZwdlXtXRsruB8n9-FOkMKnZX6pCR1vhdZHM,18694
 h2o/estimators/xgboost.py,sha256=8ePeEE8v629HWNxFI79DYv0-Y27ZAhVypYfUzUoRgvs,113260
 h2o/explanation/__init__.py,sha256=fNW2KrqG1GSrUvGvrlR0baFwhD0SyODmJfYdXtg9Kso,2863
 h2o/explanation/_explain.py,sha256=nSC1CIVShY-HYC-PhVpzO_yo--G1RJpqF_O3Rz3z9wE,162225
 h2o/grid/__init__.py,sha256=OFbyNeerfJ-jHmp--MydzwyAjQ5P54IbvZu55gzW3_c,550
 h2o/grid/grid_search.py,sha256=orjq049rS4hr1jeneHOi6ZXU6b7T_3WsJB-PIoyD1hg,78425
 h2o/grid/metrics.py,sha256=JUPDxuNypIxH8aLHf5_Lm6vml0OJ9k60F5qPmKb28FQ,67575
 h2o/h2o_data/iris.csv,sha256=A9LOdT9EPtDPble1oFWlVGwVH6v9cKdcF59r5x6tINk,4608
 h2o/h2o_data/prostate.csv,sha256=N3k68I2Gfw3km_34bAWlvkMGkp_motxxlQCYP8QV56I,9254
 h2o/information_retrieval/__init__.py,sha256=mxblMHbNwb0OfB0ID8JOvsC5JXG7E8FgNG2Gbni_IIs,49
 h2o/information_retrieval/tf_idf.py,sha256=qaiu3842SDr7lgJlbBZVEvV4OnD6jKYLD9yEjJwLKBQ,1877
 h2o/model/__init__.py,sha256=KnJ-0PYFEF4PU4U0dLwRqNqtY3-sKI0EuT7z3tCagcM,1064
 h2o/model/confusion_matrix.py,sha256=P_dlF8IDTqxfnP13FP5eWi8uyi4P8G7kfZMWe_kWiCk,3068
 h2o/model/metrics_base.py,sha256=B_qjNWymmUKwzd6RhPyOsw35TSoe5_1HqKVdjQwljKE,27066
-h2o/model/model_base.py,sha256=IhDuaBLLHDrJa8ZHZpnl4-UHcVbjFnt6mTl8Oi9QXN0,101654
+h2o/model/model_base.py,sha256=uA1HQQceRN3isf1GmJprHWHfbGdJsXWmc_GaOIRNMgA,101921
 h2o/model/model_builder.py,sha256=7-QIi2Z_YVK90QyA4vuoElzM4hblYxtxY58Wtmf4Irk,3658
 h2o/model/model_future.py,sha256=GvW0RyzdSUEnaAmNQ-H_x8w_9oSoB3lCW0gj5McLpZg,326
 h2o/model/segment_models.py,sha256=mLzTzKrw5MJIcrqzRXClyGFLXQzItVFORIjOj1ju3pg,1505
 h2o/model/extensions/__init__.py,sha256=o7rFhfsag7SGez1Ihd7rnGltkSh3S_7R6dpy0pI8pvw,2133
 h2o/model/extensions/contributions.py,sha256=KO-OvrBHiHLljfSjfLWjgvfPpOSDkyg_kYph2ns0ark,4443
 h2o/model/extensions/fairness.py,sha256=AI-b3xGzc1ZcFP-RwRZgDVR7CPVaiXS6b5zGOULp50Y,28961
 h2o/model/extensions/feature_interaction.py,sha256=YWBRgRXNmAwbSoIDJIg_Yl4HLGaUh8zljmxZryKiO6o,2636
@@ -111,15 +111,15 @@
 h2o/model/models/uplift.py,sha256=R1jB_hDYjvO4UY5-66wEai5SgIhxDzspn6VEs6pKXew,26918
 h2o/model/models/word_embedding.py,sha256=yzNvk5dX1IioQrICMm7rk14vtBA7n3NxyF-qmZQw9lY,4382
 h2o/persist/__init__.py,sha256=kuxm7nmyWdSd21_H4aOagAKS9MGJTxcHlXyGHG9cAUY,142
 h2o/persist/persist.py,sha256=5jANXg_Z6VAbIN8Gddj1zUxZxlornUHMzafkIZR6lpI,1624
 h2o/pipeline/__init__.py,sha256=jYO-tldwN0Depr780bCUzViYAKa9m2dpy5uKxojip_A,281
 h2o/pipeline/mojo_pipeline.py,sha256=PAq7qvSZSoNfOj43eIRu9v4W6NLIkQ8K7obEPVP3MJQ,2232
 h2o/plot/__init__.py,sha256=pow_fOrwoS4iZ1MCWvLtpqcPHmW7Vrj27zpwKCUc84w,110
-h2o/plot/_matplotlib.py,sha256=1IZc2Cjy4BsE2yTsUvL7I51G2i9g4lgAx3kdgF3KLRc,2105
+h2o/plot/_matplotlib.py,sha256=k1HduOD8Iqo3Kq3FJGWbIJm4nlDenUTmlJhNVrgQwq4,1839
 h2o/plot/_plot_result.py,sha256=EnwmT-Ns-Swj8XEZ64o0d8Y4zrpU5ZSuRBPAnfYrwYI,1256
 h2o/schemas/__init__.py,sha256=_xlH5ADvDEc4Ko-BTPgGyx9hdKt1wi0-2WS1gU5vOuY,271
 h2o/schemas/error.py,sha256=6cX2oAms-6pScXR3v0mfQFmPTqKAUEUxUhbMdUg3G70,2253
 h2o/schemas/metadata.py,sha256=ARnZqfUqTbFNhD6WOlwhMUUAQ-ATZLyxWz1zdf0rKvc,2142
 h2o/schemas/schema.py,sha256=R7Yvd_c_pTN824Uoc-G-qL8Y4mKyGBoWlUL1UIr0i44,2596
 h2o/sklearn/__init__.py,sha256=2K0fiWxbDUVXk1zw2_PPuzkKBG54KJ_hvE96e0F2bU4,11212
 h2o/sklearn/wrapper.py,sha256=BQfTlQbuyuD9Dfobu-3YReDSilLTFEr9P_oUVwT55-s,36832
@@ -143,11 +143,11 @@
 h2o/utils/typechecks.py,sha256=L5ZM6wVBGR_27uYZNHJ2aRmlKw7m8_OLas_E_9uZ7b8,28251
 h2o/utils/csv/__init__.py,sha256=DJNCai91YzBYLZz5wt-2vvS5f0ft0Of_Cheb-WUwOMI,423
 h2o/utils/csv/_common.py,sha256=4q6Nf-98PMYnFDPrYOhEWGhtZA0UTC37tfNBF_x87Vo,1879
 h2o/utils/csv/_dispatch.py,sha256=hIIt6FWe9JxqXM_lw1JaDswlvmjm6Q_ru1fTycJugQg,1091
 h2o/utils/csv/_workarounds.py,sha256=YrJehz4Z1_Q1SzSbw2yOEUtnWkmL7ak9TodUMc718Ss,1101
 h2o/utils/csv/dialects.py,sha256=U_ddP2qXAxvY-aS0zyeSRsmjNmOL1ux-ftafLUmmUO4,111
 h2o/utils/csv/readers.py,sha256=l3aTdLfcEYm0p7Z5ncLP7MIbBWLnWayIL4RMELjZbTo,3447
-h2o_client-3.46.0.1.dist-info/METADATA,sha256=GkE1JFCvBcVGMC7TuQoOGjauAh4e_o9bjNTxHKtsLQU,2122
-h2o_client-3.46.0.1.dist-info/WHEEL,sha256=-G_t0oGuE7UD0DrSpVZnq1hHMBV9DD2XkS5v7XpmTnk,110
-h2o_client-3.46.0.1.dist-info/top_level.txt,sha256=2gqHe7a_EcOMpPKAJE2dfWDr5axH3iLNPymQBGLPHOY,4
-h2o_client-3.46.0.1.dist-info/RECORD,,
+h2o_client-3.46.0.2.dist-info/METADATA,sha256=wZXkUMzQ6UTz4xlXBl60SgbYlsnVqgThbsOYCS7GweU,2122
+h2o_client-3.46.0.2.dist-info/WHEEL,sha256=-G_t0oGuE7UD0DrSpVZnq1hHMBV9DD2XkS5v7XpmTnk,110
+h2o_client-3.46.0.2.dist-info/top_level.txt,sha256=2gqHe7a_EcOMpPKAJE2dfWDr5axH3iLNPymQBGLPHOY,4
+h2o_client-3.46.0.2.dist-info/RECORD,,
```

