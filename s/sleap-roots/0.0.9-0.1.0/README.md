# Comparing `tmp/sleap_roots-0.0.9-py3-none-any.whl.zip` & `tmp/sleap_roots-0.1.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,20 +1,20 @@
-Zip file size: 43644 bytes, number of entries: 18
--rw-r--r--  2.0 unx      713 b- defN 24-Apr-23 18:33 sleap_roots/__init__.py
--rw-r--r--  2.0 unx     4213 b- defN 24-Apr-23 18:33 sleap_roots/angle.py
--rw-r--r--  2.0 unx    16062 b- defN 24-Apr-23 18:33 sleap_roots/bases.py
--rw-r--r--  2.0 unx    22852 b- defN 24-Apr-23 18:33 sleap_roots/convhull.py
--rw-r--r--  2.0 unx     3436 b- defN 24-Apr-23 18:33 sleap_roots/ellipse.py
--rw-r--r--  2.0 unx     6175 b- defN 24-Apr-23 18:33 sleap_roots/lengths.py
--rw-r--r--  2.0 unx     6711 b- defN 24-Apr-23 18:33 sleap_roots/networklength.py
--rw-r--r--  2.0 unx    25061 b- defN 24-Apr-23 18:33 sleap_roots/points.py
--rw-r--r--  2.0 unx     3217 b- defN 24-Apr-23 18:33 sleap_roots/scanline.py
--rw-r--r--  2.0 unx    20331 b- defN 24-Apr-23 18:33 sleap_roots/series.py
--rw-r--r--  2.0 unx     2165 b- defN 24-Apr-23 18:33 sleap_roots/summary.py
--rw-r--r--  2.0 unx     2504 b- defN 24-Apr-23 18:33 sleap_roots/tips.py
--rw-r--r--  2.0 unx    87334 b- defN 24-Apr-23 18:33 sleap_roots/trait_pipelines.py
--rw-r--r--  2.0 unx     1518 b- defN 24-Apr-23 18:33 sleap_roots-0.0.9.dist-info/LICENSE
--rw-r--r--  2.0 unx     7029 b- defN 24-Apr-23 18:33 sleap_roots-0.0.9.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-23 18:33 sleap_roots-0.0.9.dist-info/WHEEL
--rw-r--r--  2.0 unx       12 b- defN 24-Apr-23 18:33 sleap_roots-0.0.9.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1445 b- defN 24-Apr-23 18:33 sleap_roots-0.0.9.dist-info/RECORD
-18 files, 210870 bytes uncompressed, 41308 bytes compressed:  80.4%
+Zip file size: 44666 bytes, number of entries: 18
+-rw-r--r--  2.0 unx      805 b- defN 24-May-13 03:55 sleap_roots/__init__.py
+-rw-r--r--  2.0 unx     4213 b- defN 24-May-13 03:55 sleap_roots/angle.py
+-rw-r--r--  2.0 unx    16062 b- defN 24-May-13 03:55 sleap_roots/bases.py
+-rw-r--r--  2.0 unx    22877 b- defN 24-May-13 03:55 sleap_roots/convhull.py
+-rw-r--r--  2.0 unx     3436 b- defN 24-May-13 03:55 sleap_roots/ellipse.py
+-rw-r--r--  2.0 unx     6175 b- defN 24-May-13 03:55 sleap_roots/lengths.py
+-rw-r--r--  2.0 unx     6711 b- defN 24-May-13 03:55 sleap_roots/networklength.py
+-rw-r--r--  2.0 unx    25081 b- defN 24-May-13 03:55 sleap_roots/points.py
+-rw-r--r--  2.0 unx     3217 b- defN 24-May-13 03:55 sleap_roots/scanline.py
+-rw-r--r--  2.0 unx    26685 b- defN 24-May-13 03:55 sleap_roots/series.py
+-rw-r--r--  2.0 unx     2165 b- defN 24-May-13 03:55 sleap_roots/summary.py
+-rw-r--r--  2.0 unx     2504 b- defN 24-May-13 03:55 sleap_roots/tips.py
+-rw-r--r--  2.0 unx    87440 b- defN 24-May-13 03:55 sleap_roots/trait_pipelines.py
+-rw-r--r--  2.0 unx     1518 b- defN 24-May-13 03:55 sleap_roots-0.1.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     7132 b- defN 24-May-13 03:55 sleap_roots-0.1.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-13 03:55 sleap_roots-0.1.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       12 b- defN 24-May-13 03:55 sleap_roots-0.1.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1445 b- defN 24-May-13 03:55 sleap_roots-0.1.0.dist-info/RECORD
+18 files, 217570 bytes uncompressed, 42330 bytes compressed:  80.5%
```

## zipnote {}

```diff
@@ -33,23 +33,23 @@
 
 Filename: sleap_roots/tips.py
 Comment: 
 
 Filename: sleap_roots/trait_pipelines.py
 Comment: 
 
-Filename: sleap_roots-0.0.9.dist-info/LICENSE
+Filename: sleap_roots-0.1.0.dist-info/LICENSE
 Comment: 
 
-Filename: sleap_roots-0.0.9.dist-info/METADATA
+Filename: sleap_roots-0.1.0.dist-info/METADATA
 Comment: 
 
-Filename: sleap_roots-0.0.9.dist-info/WHEEL
+Filename: sleap_roots-0.1.0.dist-info/WHEEL
 Comment: 
 
-Filename: sleap_roots-0.0.9.dist-info/top_level.txt
+Filename: sleap_roots-0.1.0.dist-info/top_level.txt
 Comment: 
 
-Filename: sleap_roots-0.0.9.dist-info/RECORD
+Filename: sleap_roots-0.1.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## sleap_roots/__init__.py

```diff
@@ -15,12 +15,18 @@
 from sleap_roots.trait_pipelines import (
     DicotPipeline,
     TraitDef,
     YoungerMonocotPipeline,
     OlderMonocotPipeline,
     MultipleDicotPipeline,
 )
-from sleap_roots.series import Series, find_all_series
+from sleap_roots.series import (
+    Series,
+    find_all_h5_paths,
+    find_all_slp_paths,
+    load_series_from_h5s,
+    load_series_from_slps,
+)
 
 # Define package version.
 # This is read dynamically by setuptools in pyproject.toml to determine the release version.
-__version__ = "0.0.9"
+__version__ = "0.1.0"
```

## sleap_roots/convhull.py

```diff
@@ -544,26 +544,29 @@
     )
 
     # Find the intersection between the hull perimeter and the extended line
     intersection = extended_line.intersection(hull_perimeter)
 
     # Get the intersection points
     if not intersection.is_empty:
-        intersect_points = (
-            np.array([[point.x, point.y] for point in intersection.geoms])
-            if intersection.geom_type == "MultiPoint"
-            else np.array([[intersection.x, intersection.y]])
-        )
+        intersect_points = extract_points_from_geometry(intersection)
     else:
         # Return two vectors of NaNs if there is no intersection
         return leftmost_vector, rightmost_vector
 
-    # Get the leftmost and rightmost intersection points
-    leftmost_intersect = intersect_points[np.argmin(intersect_points[:, 0])]
-    rightmost_intersect = intersect_points[np.argmax(intersect_points[:, 0])]
+    # Convert the list of NumPy arrays to a 2D NumPy array
+    intersection_points_array = np.vstack(intersect_points)
+
+    # Find the leftmost and rightmost intersection points
+    leftmost_intersect = intersection_points_array[
+        np.argmin(intersection_points_array[:, 0])
+    ]
+    rightmost_intersect = intersection_points_array[
+        np.argmax(intersection_points_array[:, 0])
+    ]
 
     # Make a vector from the leftmost r0 point to the leftmost intersection point
     leftmost_vector = (leftmost_intersect - leftmost_r0).reshape(1, -1)
 
     # Make a vector from the rightmost r0 point to the rightmost intersection point
     rightmost_vector = (rightmost_intersect - rightmost_r0).reshape(1, -1)
```

## sleap_roots/points.py

```diff
@@ -4,15 +4,15 @@
 from matplotlib import pyplot as plt
 from matplotlib.lines import Line2D
 from shapely.geometry import Point, MultiPoint, LineString, GeometryCollection
 from shapely.ops import nearest_points
 from typing import List, Optional, Tuple
 
 
-def extract_points_from_geometry(geometry):
+def extract_points_from_geometry(geometry) -> List[np.ndarray]:
     """Extracts coordinates as a list of numpy arrays from any given Shapely geometry object.
 
     This function supports Point, MultiPoint, LineString, and GeometryCollection types.
     It recursively extracts coordinates from complex geometries and aggregates them into a single list.
     For unsupported geometry types, it returns an empty list.
 
     Args:
```

## sleap_roots/series.py

```diff
@@ -13,15 +13,22 @@
 
 
 @attrs.define
 class Series:
     """Data and predictions for a single image series.
 
     Attributes:
+        series_name: Unique identifier for the series.
         h5_path: Optional path to the HDF5-formatted image series.
+        primary_path: Optional path to the primary root predictions file. At least one
+            of the primary, lateral, or crown paths must be provided.
+        lateral_path: Optional path to the lateral root predictions file. At least one
+            of the primary, lateral, or crown paths must be provided.
+        crown_path: Optional path to the crown predictions file. At least one of the
+            primary, lateral, or crown paths must be provided.
         primary_labels: Optional `sio.Labels` corresponding to the primary root predictions.
         lateral_labels: Optional `sio.Labels` corresponding to the lateral root predictions.
         crown_labels: Optional `sio.Labels` corresponding to the crown predictions.
         video: Optional `sio.Video` corresponding to the image series.
         csv_path: Optional path to the CSV file containing the expected plant count.
 
     Methods:
@@ -32,119 +39,143 @@
         get_frame: Return labeled frames for predictions.
         plot: Plot predictions on top of the image.
         get_primary_points: Get primary root points.
         get_lateral_points: Get lateral root points.
         get_crown_points: Get crown root points.
 
     Properties:
-        series_name: Name of the series derived from the HDF5 filename.
         expected_count: Fetch the expected plant count for this series from the CSV.
         group: Group name for the series from the CSV.
         qc_fail: Flag to indicate if the series failed QC from the CSV.
     """
 
+    series_name: str
     h5_path: Optional[str] = None
+    primary_path: Optional[str] = None
+    lateral_path: Optional[str] = None
+    crown_path: Optional[str] = None
     primary_labels: Optional[sio.Labels] = None
     lateral_labels: Optional[sio.Labels] = None
     crown_labels: Optional[sio.Labels] = None
     video: Optional[sio.Video] = None
     csv_path: Optional[str] = None
 
     @classmethod
     def load(
         cls,
-        h5_path: str,
-        primary_name: Optional[str] = None,
-        lateral_name: Optional[str] = None,
-        crown_name: Optional[str] = None,
+        series_name: str,
+        h5_path: Optional[str] = None,
+        primary_path: Optional[str] = None,
+        lateral_path: Optional[str] = None,
+        crown_path: Optional[str] = None,
         csv_path: Optional[str] = None,
     ) -> "Series":
         """Load a set of predictions for this series.
 
         Args:
-            h5_path: Path to the HDF5-formatted image series.
-            primary_name: Optional name of the primary root predictions file. If provided,
-                the file is expected to be named "{h5_path}.{primary_name}.predictions.slp".
-            lateral_name: Optional name of the lateral root predictions file. If provided,
-                the file is expected to be named "{h5_path}.{lateral_name}.predictions.slp".
-            crown_name: Optional name of the crown predictions file. If provided,
-                the file is expected to be named "{h5_path}.{crown_name}.predictions.slp".
+            series_name: Unique identifier for the series.
+            h5_path: Optional path to the HDF5-formatted image series, which will be
+                used to load the video.
+            primary_path: Optional path to the primary root '.slp' predictions file.
+            lateral_path: Optional path to the lateral root '.slp' predictions file.
+            crown_path: Optional path to the crown '.slp' predictions file.
             csv_path: Optional path to the CSV file containing the expected plant count.
 
         Returns:
             An instance of Series loaded with the specified predictions.
         """
         # Initialize the labels as None
         primary_labels, lateral_labels, crown_labels = None, None, None
 
         # Attempt to load the predictions, with error handling
         try:
-            if primary_name:
-                primary_path = (
-                    Path(h5_path)
-                    .with_suffix(f".{primary_name}.predictions.slp")
-                    .as_posix()
-                )
-                if Path(primary_path).exists():
+            if primary_path:
+                # Make path object
+                primary_path = Path(primary_path)
+                # Check if the file exists
+                if primary_path.exists():
+                    # Make the primary_path POSIX-compliant
+                    primary_path = primary_path.as_posix()
+                    # Load the primary predictions
                     primary_labels = sio.load_slp(primary_path)
                 else:
                     print(f"Primary prediction file not found: {primary_path}")
-            if lateral_name:
-                lateral_path = (
-                    Path(h5_path)
-                    .with_suffix(f".{lateral_name}.predictions.slp")
-                    .as_posix()
-                )
-                if Path(lateral_path).exists():
+            if lateral_path:
+                # Make path object
+                lateral_path = Path(lateral_path)
+                # Check if the file exists
+                if lateral_path.exists():
+                    # Make the lateral_path POSIX-compliant
+                    lateral_path = lateral_path.as_posix()
+                    # Load the lateral predictions
                     lateral_labels = sio.load_slp(lateral_path)
                 else:
                     print(f"Lateral prediction file not found: {lateral_path}")
-            if crown_name:
-                crown_path = (
-                    Path(h5_path)
-                    .with_suffix(f".{crown_name}.predictions.slp")
-                    .as_posix()
-                )
-                if Path(crown_path).exists():
+            if crown_path:
+                # Make path object
+                crown_path = Path(crown_path)
+                # Check if the file exists
+                if crown_path.exists():
+                    # Make the crown_path POSIX-compliant
+                    crown_path = crown_path.as_posix()
+                    # Load the crown predictions
                     crown_labels = sio.load_slp(crown_path)
                 else:
                     print(f"Crown prediction file not found: {crown_path}")
         except Exception as e:
             print(f"Error loading prediction files: {e}")
 
         # Attempt to load the video, with error handling
         video = None
         try:
-            video = sio.Video.from_filename(h5_path) if Path(h5_path).exists() else None
+            if h5_path:
+                # Make path object
+                h5_path = Path(h5_path)
+                # Check if the file exists
+                if h5_path.exists():
+                    # Make the h5_path POSIX-compliant
+                    h5_path = h5_path.as_posix()
+                    # Load the video
+                    video = sio.Video.from_filename(h5_path)
+                    # Replace the filename in the labels with the h5_path
+                    for labels in [primary_labels, lateral_labels, crown_labels]:
+                        if labels is not None:
+                            labels.video.replace_filename(h5_path)
+                else:
+                    print(f"Video file not found: {h5_path}")
         except Exception as e:
             print(f"Error loading video file {h5_path}: {e}")
 
+        # Make the csv path POSIX-compliant
+        if csv_path:
+            csv_path = Path(csv_path).as_posix()
+
         return cls(
+            series_name=series_name,
             h5_path=h5_path,
+            primary_path=primary_path,
+            lateral_path=lateral_path,
+            crown_path=crown_path,
             primary_labels=primary_labels,
             lateral_labels=lateral_labels,
             crown_labels=crown_labels,
             video=video,
             csv_path=csv_path,
         )
 
     @property
-    def series_name(self) -> str:
-        """Name of the series derived from the HDF5 filename."""
-        return Path(self.h5_path).name.split(".")[0]
-
-    @property
     def expected_count(self) -> Union[float, int]:
         """Fetch the expected plant count for this series from the CSV."""
         if not self.csv_path or not Path(self.csv_path).exists():
             print("CSV path is not set or the file does not exist.")
             return np.nan
         df = pd.read_csv(self.csv_path)
         try:
-            # Match the series_name (or plant_qr_code in the CSV) to fetch the expected count
+            # Match the series_name (or plant_qr_code in the CSV) to fetch the expected
+            # count
             return df[df["plant_qr_code"] == self.series_name][
                 "number_of_plants_cylinder"
             ].iloc[0]
         except IndexError:
             print(f"No expected count found for series {self.series_name} in CSV.")
             return np.nan
 
@@ -174,15 +205,23 @@
             return df[df["plant_qr_code"] == self.series_name]["qc_cylinder"].iloc[0]
         except IndexError:
             print(f"No QC flag found for series {self.series_name} in CSV.")
             return np.nan
 
     def __len__(self) -> int:
         """Length of the series (number of images)."""
-        return len(self.video)
+        if self.video is not None:
+            return len(self.video)
+        else:
+            # Check all labels if video is None
+            for labels in [self.primary_labels, self.lateral_labels, self.crown_labels]:
+                if labels is not None:
+                    return len(labels)
+            # If all labels are None, return 0
+            return 0
 
     def __getitem__(self, idx: int) -> Dict[str, Optional[sio.LabeledFrame]]:
         """Return labeled frames for primary and/or lateral and/or crown predictions."""
         return self.get_frame(idx)
 
     def __iter__(self):
         """Iterator for looping through predictions."""
@@ -232,14 +271,18 @@
         """Plot predictions on top of the image.
 
         Args:
             frame_idx: Frame index to visualize.
             scale: Relative size of the visualized image. Useful for plotting smaller
                 images within notebooks.
         """
+        # Check if the video is available
+        if self.video is None:
+            raise ValueError("Video is not available. Specify the h5_path to load it.")
+
         # Retrieve all available frames
         frames = self.get_frame(frame_idx)
 
         # Generate the color palette from seaborn
         cmap = sns.color_palette("tab10")
 
         # Define the order of preference for the predictions for plotting the image
@@ -343,30 +386,139 @@
             crown_pts = np.array([[(np.nan, np.nan), (np.nan, np.nan)]])
         # Otherwise, stack the instances into an array
         else:
             crown_pts = np.stack([inst.numpy() for inst in gt_instances_cr], axis=0)
         return crown_pts
 
 
-def find_all_series(data_folders: Union[str, List[str]]) -> List[str]:
-    """Find all .h5 series from a list of folders.
+def find_all_h5_paths(data_folders: Union[str, List[str]]) -> List[str]:
+    """Find all .h5 paths from a list of folders.
 
     Args:
-        data_folders: Path or list of paths to folders containing .h5 series.
+        data_folders: Path or list of paths to folders containing .h5 paths.
 
     Returns:
-        A list of filenames to .h5 series.
+        A list of filenames to .h5 paths.
     """
     if type(data_folders) != list:
         data_folders = [data_folders]
 
-    h5_series = []
+    h5_paths = []
     for data_folder in data_folders:
-        h5_series.extend([Path(p).as_posix() for p in Path(data_folder).glob("*.h5")])
-    return h5_series
+        h5_paths.extend([Path(p).as_posix() for p in Path(data_folder).glob("*.h5")])
+    return h5_paths
+
+
+def find_all_slp_paths(data_folders: Union[str, List[str]]) -> List[str]:
+    """Find all .slp paths from a list of folders.
+
+    Args:
+        data_folders: Path or list of paths to folders containing .slp paths.
+
+    Returns:
+        A list of filenames to .slp paths.
+    """
+    if type(data_folders) != list:
+        data_folders = [data_folders]
+
+    slp_paths = []
+    for data_folder in data_folders:
+        slp_paths.extend([Path(p).as_posix() for p in Path(data_folder).glob("*.slp")])
+    return slp_paths
+
+
+def load_series_from_h5s(
+    h5_paths: List[str], model_id: str, csv_path: Optional[str] = None
+) -> List[Series]:
+    """Load a list of Series from a list of .h5 paths.
+
+    To load the `Series`, the files must be named with the following convention:
+    h5_path: '/path/to/scan/series_name.h5'
+    primary_path: '/path/to/scan/series_name.model{model_id}.rootprimary.slp'
+    lateral_path: '/path/to/scan/series_name.model{model_id}.rootlateral.slp'
+    crown_path: '/path/to/scan/series_name.model{model_id}.rootcrown.slp'
+
+    Our pipeline outputs prediction files with this format:
+    /<output_folder>/scan{scan_id}.model{model_id}.root{model_type}.slp
+
+    Args:
+        h5_paths: List of paths to .h5 files.
+        csv_path: Optional path to the CSV file containing the expected plant count.
+
+    Returns:
+        A list of Series loaded with the specified .h5 files.
+    """
+    series_list = []
+    for h5_path in h5_paths:
+        # Extract the series name from the h5 path
+        series_name = Path(h5_path).name.split(".")[0]
+        # Generate the paths for the primary, lateral, and crown predictions
+        primary_path = h5_path.replace(".h5", f".model{model_id}.rootprimary.slp")
+        lateral_path = h5_path.replace(".h5", f".model{model_id}.rootlateral.slp")
+        crown_path = h5_path.replace(".h5", f".model{model_id}.rootcrown.slp")
+        # Load the Series
+        series = Series.load(
+            series_name,
+            h5_path=h5_path,
+            primary_path=primary_path,
+            lateral_path=lateral_path,
+            crown_path=crown_path,
+            csv_path=csv_path,
+        )
+        series_list.append(series)
+    return series_list
+
+
+def load_series_from_slps(
+    slp_paths: List[str], h5s: bool = False, csv_path: Optional[str] = None
+) -> List[Series]:
+    """Load a list of Series from a list of .slp paths.
+
+    To load the `Series`, the files must be named with the following convention:
+    h5_path: '/path/to/scan/series_name.h5'
+    primary_path: '/path/to/scan/series_name.model{model_id}.rootprimary.slp'
+    lateral_path: '/path/to/scan/series_name.model{model_id}.rootlateral.slp'
+    crown_path: '/path/to/scan/series_name.model{model_id}.rootcrown.slp'
+    Note that everything is expected to be in the same folder.
+
+    Our pipeline outputs prediction files with this format:
+    /<output_folder>/scan{scan_id}.model{model_id}.root{model_type}.slp
+
+
+    Args:
+        slp_paths: List of paths to .slp files.
+        h5s: Boolean flag to indicate if the .h5 files are available. Default is False.
+        csv_path: Optional path to the CSV file containing the expected plant count.
+    """
+    series_list = []
+    series_names = list(set([Path(p).name.split(".")[0] for p in slp_paths]))
+    for series_name in series_names:
+        # Generate the paths for the primary, lateral, and crown predictions
+        primary_path = [p for p in slp_paths if series_name in p and "primary" in p]
+        lateral_path = [p for p in slp_paths if series_name in p and "lateral" in p]
+        crown_path = [p for p in slp_paths if series_name in p and "crown" in p]
+        # Check if the .h5 files are available
+        if h5s:
+            # Get directory of the h5s
+            h5_dir = Path(slp_paths[0]).parent
+            # Generate the path to the .h5 file
+            h5_path = h5_dir / f"{series_name}.h5"
+        else:
+            h5_path = None
+        # Load the Series
+        series = Series.load(
+            series_name,
+            primary_path=primary_path[0] if primary_path else None,
+            lateral_path=lateral_path[0] if lateral_path else None,
+            crown_path=crown_path[0] if crown_path else None,
+            h5_path=h5_path,
+            csv_path=csv_path,
+        )
+        series_list.append(series)
+    return series_list
 
 
 def imgfig(
     size: Union[float, Tuple] = 6, dpi: int = 72, scale: float = 1.0
 ) -> matplotlib.figure.Figure:
     """Create a tight figure for image plotting.
```

## sleap_roots/trait_pipelines.py

```diff
@@ -325,23 +325,25 @@
         """
         raise NotImplementedError
 
     def compute_plant_traits(
         self,
         plant: Series,
         write_csv: bool = False,
+        output_dir: str = ".",
         csv_suffix: str = ".traits.csv",
         return_non_scalar: bool = False,
     ) -> pd.DataFrame:
         """Compute traits for a plant.
 
         Args:
             plant: The plant image series as a `Series` object.
             write_csv: A boolean value. If True, it writes per plant detailed
                 CSVs with traits for every instance on every frame.
+            output_dir: The directory to write the CSV files to.
             csv_suffix: If `write_csv` is `True`, a CSV file will be saved with the same
                 name as the plant's `{plant.series_name}{csv_suffix}`.
             return_non_scalar: If `True`, return all non-scalar traits as well as the
                 summarized traits.
 
         Returns:
             The computed traits as a pandas DataFrame.
@@ -369,15 +371,15 @@
 
         # Move metadata columns to the front.
         plant_name = traits.pop("plant_name")
         frame_idx = traits.pop("frame_idx")
         traits = pd.concat([plant_name, frame_idx, traits], axis=1)
 
         if write_csv:
-            csv_name = Path(plant.h5_path).with_suffix(csv_suffix)
+            csv_name = Path(output_dir) / f"{plant.series_name}{csv_suffix}"
             traits[["plant_name", "frame_idx"] + self.csv_traits].to_csv(
                 csv_name, index=False
             )
 
         if return_non_scalar:
             return traits
         else:
```

## Comparing `sleap_roots-0.0.9.dist-info/LICENSE` & `sleap_roots-0.1.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `sleap_roots-0.0.9.dist-info/METADATA` & `sleap_roots-0.1.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,21 @@
 Metadata-Version: 2.1
 Name: sleap-roots
-Version: 0.0.9
+Version: 0.1.0
 Summary: Analysis tools for SLEAP-based plant root phenotyping.
 Author-email: Elizabeth Berrigan <eberrigan@salk.edu>, Lin Wang <linwang@salk.edu>, Talmo Pereira <talmo@salk.edu>
 License: BSD-3-Clause
 Project-URL: Homepage, https://github.com/talmolab/sleap-roots
 Project-URL: Repository, https://github.com/talmolab/sleap-roots
 Keywords: sleap,plants,roots,phenotyping
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
 Requires-Python: >=3.7
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: numpy
 Requires-Dist: h5py
 Requires-Dist: attrs
 Requires-Dist: pandas
@@ -43,15 +45,15 @@
 ## Installation
 ```
 pip install sleap-roots
 ```
 
 If you are using conda (recommended):
 ```
-conda create -n sleap-roots python=3.9
+conda create -n sleap-roots python=3.11
 conda activate sleap-roots
 pip install sleap-roots
 ```
 
 ## Usage
 
 Detailed trait documentation per pipeline is available here:
```

