# Comparing `tmp/asyncdb-2.7.0-pp39-pypy39_pp73-win_amd64.whl.zip` & `tmp/asyncdb-2.7.1-pp39-pypy39_pp73-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,71 +1,71 @@
-Zip file size: 215432 bytes, number of entries: 69
--rw-rw-rw-  2.0 fat      411 b- defN 24-May-10 01:18 asyncdb/__init__.py
--rw-rw-rw-  2.0 fat     1297 b- defN 24-May-10 01:18 asyncdb/connections.py
--rw-rw-rw-  2.0 fat    25523 b- defN 24-May-10 01:18 asyncdb/interfaces.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-May-10 01:18 asyncdb/py.typed
--rw-rw-rw-  2.0 fat      280 b- defN 24-May-10 01:18 asyncdb/version.py
--rw-rw-rw-  2.0 fat       28 b- defN 24-May-10 01:18 asyncdb/drivers/__init__.py
--rw-rw-rw-  2.0 fat     4529 b- defN 24-May-10 01:18 asyncdb/drivers/abstract.py
--rw-rw-rw-  2.0 fat    12373 b- defN 24-May-10 01:18 asyncdb/drivers/bigquery.py
--rw-rw-rw-  2.0 fat    17209 b- defN 24-May-10 01:18 asyncdb/drivers/cassandra.py
--rw-rw-rw-  2.0 fat    10172 b- defN 24-May-10 01:18 asyncdb/drivers/delta.py
--rw-rw-rw-  2.0 fat    10193 b- defN 24-May-10 01:18 asyncdb/drivers/duckdb.py
--rw-rw-rw-  2.0 fat     2110 b- defN 24-May-10 01:18 asyncdb/drivers/dummy.py
--rw-rw-rw-  2.0 fat    13068 b- defN 24-May-10 01:18 asyncdb/drivers/hazel.py
--rw-rw-rw-  2.0 fat    20827 b- defN 24-May-10 01:18 asyncdb/drivers/influx.py
--rw-rw-rw-  2.0 fat    29764 b- defN 24-May-10 01:18 asyncdb/drivers/jdbc.py
--rw-rw-rw-  2.0 fat     6638 b- defN 24-May-10 01:18 asyncdb/drivers/mcache.py
--rw-rw-rw-  2.0 fat     9336 b- defN 24-May-10 01:18 asyncdb/drivers/memcache.py
--rw-rw-rw-  2.0 fat     3316 b- defN 24-May-10 01:18 asyncdb/drivers/mongo.py
--rw-rw-rw-  2.0 fat    19474 b- defN 24-May-10 01:18 asyncdb/drivers/mredis.py
--rw-rw-rw-  2.0 fat    15824 b- defN 24-May-10 01:18 asyncdb/drivers/mssql.py
--rw-rw-rw-  2.0 fat    16072 b- defN 24-May-10 01:18 asyncdb/drivers/mysql.py
--rw-rw-rw-  2.0 fat    18635 b- defN 24-May-10 01:18 asyncdb/drivers/mysqlclient.py
--rw-rw-rw-  2.0 fat     8036 b- defN 24-May-10 01:18 asyncdb/drivers/odbc.py
--rw-rw-rw-  2.0 fat     4206 b- defN 24-May-10 01:18 asyncdb/drivers/oracle.py
--rw-rw-rw-  2.0 fat    61614 b- defN 24-May-10 01:18 asyncdb/drivers/pg.py
--rw-rw-rw-  2.0 fat    22590 b- defN 24-May-10 01:18 asyncdb/drivers/postgres.py
--rw-rw-rw-  2.0 fat    19814 b- defN 24-May-10 01:18 asyncdb/drivers/redis.py
--rw-rw-rw-  2.0 fat    35185 b- defN 24-May-10 01:18 asyncdb/drivers/rethink.py
--rw-rw-rw-  2.0 fat    16815 b- defN 24-May-10 01:18 asyncdb/drivers/sa.py
--rw-rw-rw-  2.0 fat    56434 b- defN 24-May-10 01:18 asyncdb/drivers/scylladb.py
--rw-rw-rw-  2.0 fat     2071 b- defN 24-May-10 01:18 asyncdb/drivers/sql.py
--rw-rw-rw-  2.0 fat    25972 b- defN 24-May-10 01:18 asyncdb/drivers/sqlite.py
--rw-rw-rw-  2.0 fat    14374 b- defN 24-May-10 01:18 asyncdb/drivers/sqlserver.py
--rw-rw-rw-  2.0 fat      513 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/__init__.py
--rw-rw-rw-  2.0 fat      848 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/arrow.py
--rw-rw-rw-  2.0 fat      429 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/base.py
--rw-rw-rw-  2.0 fat      997 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/csv.py
--rw-rw-rw-  2.0 fat      693 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/dataclass.py
--rw-rw-rw-  2.0 fat      827 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/dt.py
--rw-rw-rw-  2.0 fat      408 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/generator.py
--rw-rw-rw-  2.0 fat      543 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/iter.py
--rw-rw-rw-  2.0 fat      418 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/json.py
--rw-rw-rw-  2.0 fat      986 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/output.py
--rw-rw-rw-  2.0 fat     1021 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/pandas.py
--rw-rw-rw-  2.0 fat      864 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/polars.py
--rw-rw-rw-  2.0 fat     1236 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/pyspark.py
--rw-rw-rw-  2.0 fat     1038 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/record.py
--rw-rw-rw-  2.0 fat      770 b- defN 24-May-10 01:18 asyncdb/drivers/outputs/recordset.py
--rw-rw-rw-  2.0 fat      867 b- defN 24-May-10 01:18 asyncdb/exceptions/__init__.py
--rw-rw-rw-  2.0 fat   158720 b- defN 24-May-10 01:25 asyncdb/exceptions/exceptions.pypy39-pp73-win_amd64.pyd
--rw-rw-rw-  2.0 fat     2539 b- defN 24-May-10 01:18 asyncdb/exceptions/handlers.py
--rw-rw-rw-  2.0 fat      176 b- defN 24-May-10 01:18 asyncdb/meta/__init__.py
--rw-rw-rw-  2.0 fat     3057 b- defN 24-May-10 01:18 asyncdb/meta/record.py
--rw-rw-rw-  2.0 fat     2221 b- defN 24-May-10 01:18 asyncdb/meta/recordset.py
--rw-rw-rw-  2.0 fat      542 b- defN 24-May-10 01:18 asyncdb/models/__init__.py
--rw-rw-rw-  2.0 fat    17784 b- defN 24-May-10 01:18 asyncdb/models/model.py
--rw-rw-rw-  2.0 fat      136 b- defN 24-May-10 01:18 asyncdb/utils/__init__.py
--rw-rw-rw-  2.0 fat     2484 b- defN 24-May-10 01:18 asyncdb/utils/functions.py
--rw-rw-rw-  2.0 fat      655 b- defN 24-May-10 01:18 asyncdb/utils/modules.py
--rw-rw-rw-  2.0 fat   111616 b- defN 24-May-10 01:25 asyncdb/utils/types.pypy39-pp73-win_amd64.pyd
--rw-rw-rw-  2.0 fat      319 b- defN 24-May-10 01:18 asyncdb/utils/uv.py
--rw-rw-rw-  2.0 fat      234 b- defN 24-May-10 01:18 asyncdb/utils/encoders/__init__.py
--rw-rw-rw-  2.0 fat      487 b- defN 24-May-10 01:18 asyncdb/utils/encoders/numpy.py
--rw-rw-rw-  2.0 fat     1195 b- defN 24-May-10 01:18 asyncdb/utils/encoders/pg.py
--rw-rw-rw-  2.0 fat     1538 b- defN 24-May-10 01:25 asyncdb-2.7.0.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    12674 b- defN 24-May-10 01:25 asyncdb-2.7.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat      107 b- defN 24-May-10 01:25 asyncdb-2.7.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 24-May-10 01:25 asyncdb-2.7.0.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     5791 b- defN 24-May-10 01:25 asyncdb-2.7.0.dist-info/RECORD
-69 files, 838931 bytes uncompressed, 206332 bytes compressed:  75.4%
+Zip file size: 216340 bytes, number of entries: 69
+-rw-rw-rw-  2.0 fat      411 b- defN 24-May-13 15:28 asyncdb/__init__.py
+-rw-rw-rw-  2.0 fat     1297 b- defN 24-May-13 15:28 asyncdb/connections.py
+-rw-rw-rw-  2.0 fat    25523 b- defN 24-May-13 15:28 asyncdb/interfaces.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-May-13 15:28 asyncdb/py.typed
+-rw-rw-rw-  2.0 fat      280 b- defN 24-May-13 15:28 asyncdb/version.py
+-rw-rw-rw-  2.0 fat       28 b- defN 24-May-13 15:28 asyncdb/drivers/__init__.py
+-rw-rw-rw-  2.0 fat     4529 b- defN 24-May-13 15:28 asyncdb/drivers/abstract.py
+-rw-rw-rw-  2.0 fat    17731 b- defN 24-May-13 15:28 asyncdb/drivers/bigquery.py
+-rw-rw-rw-  2.0 fat    17209 b- defN 24-May-13 15:28 asyncdb/drivers/cassandra.py
+-rw-rw-rw-  2.0 fat    10172 b- defN 24-May-13 15:28 asyncdb/drivers/delta.py
+-rw-rw-rw-  2.0 fat    10193 b- defN 24-May-13 15:28 asyncdb/drivers/duckdb.py
+-rw-rw-rw-  2.0 fat     2110 b- defN 24-May-13 15:28 asyncdb/drivers/dummy.py
+-rw-rw-rw-  2.0 fat    13068 b- defN 24-May-13 15:28 asyncdb/drivers/hazel.py
+-rw-rw-rw-  2.0 fat    20827 b- defN 24-May-13 15:28 asyncdb/drivers/influx.py
+-rw-rw-rw-  2.0 fat    29764 b- defN 24-May-13 15:28 asyncdb/drivers/jdbc.py
+-rw-rw-rw-  2.0 fat     6638 b- defN 24-May-13 15:28 asyncdb/drivers/mcache.py
+-rw-rw-rw-  2.0 fat     9336 b- defN 24-May-13 15:28 asyncdb/drivers/memcache.py
+-rw-rw-rw-  2.0 fat     3316 b- defN 24-May-13 15:28 asyncdb/drivers/mongo.py
+-rw-rw-rw-  2.0 fat    19474 b- defN 24-May-13 15:28 asyncdb/drivers/mredis.py
+-rw-rw-rw-  2.0 fat    15824 b- defN 24-May-13 15:28 asyncdb/drivers/mssql.py
+-rw-rw-rw-  2.0 fat    16072 b- defN 24-May-13 15:28 asyncdb/drivers/mysql.py
+-rw-rw-rw-  2.0 fat    18635 b- defN 24-May-13 15:28 asyncdb/drivers/mysqlclient.py
+-rw-rw-rw-  2.0 fat     8036 b- defN 24-May-13 15:28 asyncdb/drivers/odbc.py
+-rw-rw-rw-  2.0 fat     4206 b- defN 24-May-13 15:28 asyncdb/drivers/oracle.py
+-rw-rw-rw-  2.0 fat    61614 b- defN 24-May-13 15:28 asyncdb/drivers/pg.py
+-rw-rw-rw-  2.0 fat    22590 b- defN 24-May-13 15:28 asyncdb/drivers/postgres.py
+-rw-rw-rw-  2.0 fat    19814 b- defN 24-May-13 15:28 asyncdb/drivers/redis.py
+-rw-rw-rw-  2.0 fat    35185 b- defN 24-May-13 15:28 asyncdb/drivers/rethink.py
+-rw-rw-rw-  2.0 fat    16815 b- defN 24-May-13 15:28 asyncdb/drivers/sa.py
+-rw-rw-rw-  2.0 fat    56513 b- defN 24-May-13 15:28 asyncdb/drivers/scylladb.py
+-rw-rw-rw-  2.0 fat     2071 b- defN 24-May-13 15:28 asyncdb/drivers/sql.py
+-rw-rw-rw-  2.0 fat    25972 b- defN 24-May-13 15:28 asyncdb/drivers/sqlite.py
+-rw-rw-rw-  2.0 fat    14374 b- defN 24-May-13 15:28 asyncdb/drivers/sqlserver.py
+-rw-rw-rw-  2.0 fat      513 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/__init__.py
+-rw-rw-rw-  2.0 fat      848 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/arrow.py
+-rw-rw-rw-  2.0 fat      429 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/base.py
+-rw-rw-rw-  2.0 fat      997 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/csv.py
+-rw-rw-rw-  2.0 fat      693 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/dataclass.py
+-rw-rw-rw-  2.0 fat      827 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/dt.py
+-rw-rw-rw-  2.0 fat      408 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/generator.py
+-rw-rw-rw-  2.0 fat      543 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/iter.py
+-rw-rw-rw-  2.0 fat      418 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/json.py
+-rw-rw-rw-  2.0 fat      986 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/output.py
+-rw-rw-rw-  2.0 fat     1021 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/pandas.py
+-rw-rw-rw-  2.0 fat      864 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/polars.py
+-rw-rw-rw-  2.0 fat     1236 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/pyspark.py
+-rw-rw-rw-  2.0 fat     1038 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/record.py
+-rw-rw-rw-  2.0 fat      770 b- defN 24-May-13 15:28 asyncdb/drivers/outputs/recordset.py
+-rw-rw-rw-  2.0 fat      867 b- defN 24-May-13 15:28 asyncdb/exceptions/__init__.py
+-rw-rw-rw-  2.0 fat   158720 b- defN 24-May-13 15:35 asyncdb/exceptions/exceptions.pypy39-pp73-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     2539 b- defN 24-May-13 15:28 asyncdb/exceptions/handlers.py
+-rw-rw-rw-  2.0 fat      176 b- defN 24-May-13 15:28 asyncdb/meta/__init__.py
+-rw-rw-rw-  2.0 fat     3057 b- defN 24-May-13 15:28 asyncdb/meta/record.py
+-rw-rw-rw-  2.0 fat     2221 b- defN 24-May-13 15:28 asyncdb/meta/recordset.py
+-rw-rw-rw-  2.0 fat      542 b- defN 24-May-13 15:28 asyncdb/models/__init__.py
+-rw-rw-rw-  2.0 fat    17784 b- defN 24-May-13 15:28 asyncdb/models/model.py
+-rw-rw-rw-  2.0 fat      136 b- defN 24-May-13 15:28 asyncdb/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2484 b- defN 24-May-13 15:28 asyncdb/utils/functions.py
+-rw-rw-rw-  2.0 fat      655 b- defN 24-May-13 15:28 asyncdb/utils/modules.py
+-rw-rw-rw-  2.0 fat   111616 b- defN 24-May-13 15:35 asyncdb/utils/types.pypy39-pp73-win_amd64.pyd
+-rw-rw-rw-  2.0 fat      319 b- defN 24-May-13 15:28 asyncdb/utils/uv.py
+-rw-rw-rw-  2.0 fat      234 b- defN 24-May-13 15:28 asyncdb/utils/encoders/__init__.py
+-rw-rw-rw-  2.0 fat      487 b- defN 24-May-13 15:28 asyncdb/utils/encoders/numpy.py
+-rw-rw-rw-  2.0 fat     1195 b- defN 24-May-13 15:28 asyncdb/utils/encoders/pg.py
+-rw-rw-rw-  2.0 fat     1538 b- defN 24-May-13 15:35 asyncdb-2.7.1.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    12805 b- defN 24-May-13 15:35 asyncdb-2.7.1.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      107 b- defN 24-May-13 15:35 asyncdb-2.7.1.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 24-May-13 15:35 asyncdb-2.7.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     5791 b- defN 24-May-13 15:35 asyncdb-2.7.1.dist-info/RECORD
+69 files, 844499 bytes uncompressed, 207240 bytes compressed:  75.5%
```

## zipnote {}

```diff
@@ -186,23 +186,23 @@
 
 Filename: asyncdb/utils/encoders/numpy.py
 Comment: 
 
 Filename: asyncdb/utils/encoders/pg.py
 Comment: 
 
-Filename: asyncdb-2.7.0.dist-info/LICENSE
+Filename: asyncdb-2.7.1.dist-info/LICENSE
 Comment: 
 
-Filename: asyncdb-2.7.0.dist-info/METADATA
+Filename: asyncdb-2.7.1.dist-info/METADATA
 Comment: 
 
-Filename: asyncdb-2.7.0.dist-info/WHEEL
+Filename: asyncdb-2.7.1.dist-info/WHEEL
 Comment: 
 
-Filename: asyncdb-2.7.0.dist-info/top_level.txt
+Filename: asyncdb-2.7.1.dist-info/top_level.txt
 Comment: 
 
-Filename: asyncdb-2.7.0.dist-info/RECORD
+Filename: asyncdb-2.7.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## asyncdb/version.py

```diff
@@ -1,9 +1,9 @@
 """AsyncDB Meta information."""
 
 __title__ = "asyncdb"
 __description__ = "Library for Asynchronous data source connections \
     Collection of asyncio drivers."
-__version__ = "2.7.0"
+__version__ = "2.7.1"
 __author__ = "Jesus Lara"
 __author_email__ = "jesuslarag@gmail.com"
 __license__ = "BSD"
```

## asyncdb/drivers/bigquery.py

```diff
@@ -1,16 +1,26 @@
 import os
-from typing import Any
+from typing import Any, Union
 from collections.abc import Iterable
-from pathlib import Path
+import io
+from pathlib import Path, PurePath
 import asyncio
+import aiofiles
 import pandas_gbq
 import pandas as pd
+try:
+    from google.cloud import storage
+except ImportError:
+    raise ImportError(
+        "BigQuery: google-cloud-storage library not found. Hint: Please install it using 'pip install google-cloud-storage'"
+    )
+
 from google.cloud import bigquery as bq
 from google.cloud.exceptions import Conflict
+from google.cloud.bigquery import LoadJobConfig, SourceFormat
 from google.oauth2 import service_account
 from ..exceptions import DriverError
 from .sql import SQLDriver
 
 
 class bigquery(SQLDriver):
     _provider = "bigquery"
@@ -42,15 +52,17 @@
                     self._project_id = self.credentials.project_id
                 self._connection = bq.Client(credentials=self.credentials, project=self._project_id)
                 self._connected = True
             else:
                 self.credentials = self._account
                 self._connection = bq.Client(project=self._project_id)
         except Exception as e:
-            raise DriverError(f"BigQuery: Error initializing client: {e}")
+            raise DriverError(
+                f"BigQuery: Error initializing client: {e}"
+            )
         return self
 
     async def close(self):
         # BigQuery client does not maintain persistent connections, so nothing to close here.
         self._connected = False
 
     disconnect = close
@@ -62,15 +74,17 @@
         if not self._connection:
             await self.connection()
         try:
             job = self._connection.query(query, **kwargs)
             result = job.result()  # Waits for the query to finish
             return result
         except Exception as e:
-            raise DriverError(f"BigQuery: Error executing query: {e}")
+            raise DriverError(
+                f"BigQuery: Error executing query: {e}"
+            )
 
     async def execute_many(self, query, **kwargs):
         """
         Execute a BigQuery query
         """
         if not self._connection:
             await self.connection()
@@ -87,26 +101,31 @@
     def get_query_config(self, **kwargs):
         return bq.QueryJobConfig(**kwargs)
 
     def get_load_config(self, **kwargs):
         args = {}
         _type = kwargs.pop("type", "json")
         if _type == "json":
-            args = {"source_format": bq.SourceFormat.NEWLINE_DELIMITED_JSON, "autodetect": True}
+            args = {
+                "source_format": bq.SourceFormat.NEWLINE_DELIMITED_JSON,
+                "autodetect": True
+            }
         args = {**kwargs, **args}
         return bq.LoadJobConfig(**args)
 
     async def create_dataset(self, dataset_id: str):
         try:
             dataset_ref = bq.DatasetReference(self._connection.project, dataset_id)
             dataset_obj = bq.Dataset(dataset_ref)
             dataset_obj = self._connection.create_dataset(dataset_obj)
             return dataset_obj
         except Conflict:
-            self._logger.warning(f"Dataset {self._connection.project}.{dataset_obj.dataset_id} already exists")
+            self._logger.warning(
+                f"Dataset {self._connection.project}.{dataset_obj.dataset_id} already exists"
+            )
             return dataset_obj
         except Exception as exc:
             self._logger.error(f"Error creating Dataset: {exc}")
             raise DriverError(f"Error creating Dataset: {exc}")
 
     async def create_table(self, dataset_id, table_id, schema):
         """
@@ -119,18 +138,22 @@
             await self.connection()
 
         dataset_ref = bq.DatasetReference(self._connection.project, dataset_id)
         table_ref = dataset_ref.table(table_id)
         table = bq.Table(table_ref, schema=schema)
         try:
             table = self._connection.create_table(table)  # API request
-            self._logger.info(f"Created table {table.project}.{table.dataset_id}.{table.table_id}")
+            self._logger.info(
+                f"Created table {table.project}.{table.dataset_id}.{table.table_id}"
+            )
             return table
         except Conflict:
-            self._logger.warning(f"Table {table.project}.{table.dataset_id}.{table.table_id} already exists")
+            self._logger.warning(
+                f"Table {table.project}.{table.dataset_id}.{table.table_id} already exists"
+            )
             return table
         except Exception as e:
             raise DriverError(f"BigQuery: Error creating table: {e}")
 
     async def truncate_table(self, table_id: str, dataset_id: str):
         """
         Truncate a BigQuery table by overwriting with an empty table.
@@ -144,19 +167,26 @@
         table = self._connection.get_table(table_ref)  # API request to fetch the table schema
 
         # Create an empty table with the same schema
         job_config = bq.QueryJobConfig(destination=table_ref)
         job_config.write_disposition = bq.WriteDisposition.WRITE_TRUNCATE
 
         try:
-            job = self._connection.query(f"SELECT * FROM `{table_ref}` WHERE FALSE", job_config=job_config)
+            job = self._connection.query(
+                f"SELECT * FROM `{table_ref}` WHERE FALSE",
+                job_config=job_config
+            )
             job.result()  # Wait for the job to finish
-            self._logger.info(f"Truncated table {dataset_id}.{table_id}")
+            self._logger.info(
+                f"Truncated table {dataset_id}.{table_id}"
+            )
         except Exception as e:
-            raise DriverError(f"BigQuery: Error truncating table: {e}")
+            raise DriverError(
+                f"BigQuery: Error truncating table: {e}"
+            )
 
     async def query(self, sentence: str, **kwargs):
         if not self._connection:
             await self.connection()
         await self.valid_operation(sentence)
         self.start_timing()
         error = None
@@ -237,40 +267,63 @@
         """
         if not self._connection:
             await self.connection()
         try:
             if isinstance(data, pd.DataFrame):
                 if use_pandas is True:
                     job = await self._thread_func(
-                        self._connection.load_table_from_dataframe, data, table_id, if_exists=if_exists, **kwargs
+                        self._connection.load_table_from_dataframe,
+                        data,
+                        table_id,
+                        if_exists=if_exists,
+                        **kwargs
                     )
                 else:
                     job = await self._thread_func(
-                        data.to_gbq, table_id, project_id=self._project_id, if_exists=if_exists
+                        data.to_gbq,
+                        table_id,
+                        project_id=self._project_id,
+                        if_exists=if_exists
                     )
             elif isinstance(data, list):
                 dataset_ref = self._connection.dataset(dataset_id)
                 table_ref = dataset_ref.table(table_id)
                 table = bq.Table(table_ref)
                 if use_streams is True:
-                    errors = await self._thread_func(self._connection.insert_rows_json, table, data, **kwargs)
+                    errors = await self._thread_func(
+                        self._connection.insert_rows_json,
+                        table,
+                        data,
+                        **kwargs
+                    )
                     if errors:
-                        raise RuntimeError(f"Errors occurred while inserting rows: {errors}")
+                        raise RuntimeError(
+                            f"Errors occurred while inserting rows: {errors}"
+                        )
                 else:
-                    job = await self._thread_func(self._connection.load_table_from_json, table, data, **kwargs)
+                    job = await self._thread_func(
+                        self._connection.load_table_from_json,
+                        table,
+                        data,
+                        **kwargs
+                    )
                     loop = asyncio.get_event_loop()
                     await loop.run_in_executor(None, job.result)
                     if job.errors and len(job.errors) > 0:
                         raise RuntimeError(f"Job failed with errors: {job.errors}")
                     else:
                         self._logger.info(f"Loaded {len(data)} rows into {table_id}")
 
-            self._logger.info(f"Inserted rows into {table_id}")
+            self._logger.info(
+                f"Inserted rows into {table_id}"
+            )
         except Exception as e:
-            raise DriverError(f"BigQuery: Error writing to table: {e}")
+            raise DriverError(
+                f"BigQuery: Error writing to table: {e}"
+            )
 
     async def load_table_from_uri(
         self,
         source_uri: str,
         table: Any = None,
         job_config=None,
         dataset_id: str = None,
@@ -283,21 +336,28 @@
             await self.connection()
         if not table:
             dataset_ref = self._connection.dataset(dataset_id)
             table_ref = dataset_ref.table(table_id)
             table = bq.Table(table_ref)
         try:
             job = await self._thread_func(
-                self._connection.load_table_from_uri, source_uri, table, job_config=job_config
+                self._connection.load_table_from_uri,
+                source_uri,
+                table,
+                job_config=job_config
             )
             job.result()  # Waits for table load to complete.
-            self._logger.info(f"Loaded {job.output_rows} rows into {table.project}.{table.dataset_id}.{table.table_id}")
+            self._logger.info(
+                f"Loaded {job.output_rows} rows into {table.project}.{table.dataset_id}.{table.table_id}"
+            )
             return job
         except Exception as e:
-            raise DriverError(f"BigQuery: Error loading table from URI: {e}")
+            raise DriverError(
+                f"BigQuery: Error loading table from URI: {e}"
+            )
 
     @property
     def connected(self):
         return self._connection is not None
 
     def is_connected(self):
         return self._connected
@@ -306,7 +366,116 @@
         raise NotImplementedError
 
     def table(self, tablename: str = "") -> Iterable[Any]:
         raise NotImplementedError
 
     async def use(self, database: str):
         raise NotImplementedError  # pragma: no cover
+
+    async def create_gcs_from_csv(
+        self,
+        bucket_name: str,
+        object_name: str,
+        csv_data: Union[bytes, PurePath, pd.DataFrame],
+        **kwargs
+    ):
+        """Creates a GCS object from CSV data."""
+        if isinstance(csv_data, PurePath) and csv_data.is_file():
+            async with aiofiles.open(csv_data, mode="rb") as file:
+                csv_data = await file.read()
+        elif isinstance(csv_data, pd.DataFrame):
+            csv_data = csv_data.to_csv(index=False)
+        elif not isinstance(csv_data, bytes):
+            raise DriverError("BigQuery: Invalid file object")
+        try:
+            storage_client = storage.Client()
+            bucket = storage_client.bucket(bucket_name)
+            blob = bucket.blob(object_name)
+            # Option 1: Upload from a string
+            blob.upload_from_string(csv_data, content_type='text/csv')
+            # If successful, return the GCS URI
+            gcs_uri = f"gs://{bucket_name}/{object_name}"
+            return gcs_uri
+        except Exception as e:
+            raise DriverError(
+                f"BigQuery: Error creating GCS object: {e}"
+            )
+
+    async def read_csv_from_gcs(
+        self,
+        table_id: str,
+        dataset_id: str,
+        bucket_name: str,
+        object_name: str,
+        **kwargs
+    ):
+        """Load data into a BigQuery table from a CSV file in GCS."""
+        try:
+            gcs_uri = f"gs://{bucket_name}/{object_name}"
+            job_config = LoadJobConfig(
+                source_format=SourceFormat.CSV,
+                autodetect=True,
+                **kwargs
+            )
+            job = self._connection.load_table_from_uri(
+                gcs_uri,
+                table_id,
+                job_config=job_config
+            )
+            job.result()  # Wait for the job to complete
+            return job
+        except Exception as e:
+            raise DriverError(
+                f"BigQuery: Error loading from CSV in GCS: {e}"
+            )
+
+    async def read_csv(self, table_id, dataset_id, file_obj: Union[bytes, PurePath], **kwargs):
+        """Load data into a BigQuery table from a CSV file object."""
+        job_config = LoadJobConfig(
+            source_format=SourceFormat.CSV,
+            autodetect=True,
+            **kwargs
+        )
+        if isinstance(file_obj, PurePath) and file_obj.is_file():
+            async with aiofiles.open(file_obj, mode="rb") as file:
+                file_obj = await file.read()
+        elif not isinstance(file_obj, bytes):
+            raise DriverError("BigQuery: Invalid file object")
+        try:
+            job = self._connection.load_table_from_file(
+                file_obj,
+                table_id,
+                job_config=job_config
+            )
+            job.result()  # Wait for the job to complete
+            return job
+        except Exception as e:
+            raise DriverError(
+                f"BigQuery: Error loading from CSV: {e}"
+            )
+
+    async def read_excel(self, table_id, dataset_id, file_obj, **kwargs):
+        """Load data into a BigQuery table from an Excel file object."""
+        try:
+            df = pd.read_excel(file_obj)
+            return await self.write(
+                table_id,
+                df,
+                dataset_id,
+                use_pandas=True,
+                **kwargs
+            )
+        except Exception as e:
+            raise DriverError(
+                f"BigQuery: Error loading from Excel: {e}"
+            )
+
+    async def multi_query(self, queries: list):
+        """Execute multiple BigQuery queries in parallel."""
+        tasks = []
+        for query in queries:
+            tasks.append(
+                asyncio.create_task(
+                    self.execute(query))
+                )  # Create async tasks
+        results = await asyncio.gather(*tasks)  # Execute tasks concurrently and gather results
+        return results
```

## asyncdb/drivers/scylladb.py

```diff
@@ -966,17 +966,19 @@
             columns.append(column)
             # validating required field
             try:
                 required = field.required()
             except AttributeError:
                 required = False
             pk = self._get_attribute(field, value, attr="primary_key")
-            if pk is True and value is None:
-                if "db_default" in field.metadata:
-                    continue
+            # if pk is True and value is None:
+            #     if "db_default" in field.metadata:
+            #         continue
+            if pk is True and value is None and "db_default" in field.metadata:
+                continue
             if required is False and value is None or value == "None":
                 if "db_default" in field.metadata:
                     continue
                 else:
                     # get default value
                     default = field.default
                     if callable(default):
@@ -1375,17 +1377,16 @@
             sc = model.Meta.schema
             if sc:
                 schema = f"{sc}."
             table = f"{schema}{model.Meta.name}"
         except AttributeError:
             table = model.__name__
         fields = model.columns(model)
-        if _filter is None:
-            if args:
-                _filter = args[0]
+        if _filter is None and args:
+            _filter = args[0]
         cols = []
         source = []
         new_cond = {}
         n = 1
         for name, field in fields.items():
             try:
                 val = kwargs[name]
@@ -1432,17 +1433,16 @@
             sc = model.Meta.schema
             if sc:
                 schema = f"{sc}."
             table = f"{schema}{model.Meta.name}"
         except AttributeError:
             table = model.__name__
         fields = model.columns(model)
-        if _filter is None:
-            if args:
-                _filter = args[0]
+        if _filter is None and args:
+            _filter = args[0]
         cols = []
         source = []
         new_cond = {}
         n = 1
         for name, field in fields.items():
             try:
                 val = kwargs[name]
```

## Comparing `asyncdb-2.7.0.dist-info/LICENSE` & `asyncdb-2.7.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `asyncdb-2.7.0.dist-info/METADATA` & `asyncdb-2.7.1.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: asyncdb
-Version: 2.7.0
+Version: 2.7.1
 Summary: Library for Asynchronous data source connections     Collection of asyncio drivers.
 Home-page: https://github.com/phenobarbital/asyncdb
 Author: Jesus Lara
 Author-email: jesuslarag@gmail.com
 License: BSD
 Project-URL: Source, https://github.com/phenobarbital/asyncdb
 Project-URL: Funding, https://paypal.me/phenobarbital
@@ -69,34 +69,36 @@
 Requires-Dist: influxdb-client ==1.39.0 ; extra == 'all'
 Requires-Dist: aioodbc ==0.5.0 ; extra == 'all'
 Requires-Dist: JayDeBeApi ==1.2.3 ; extra == 'all'
 Requires-Dist: pyodbc ==5.1.0 ; extra == 'all'
 Requires-Dist: sqlalchemy[asyncio] ==2.0.23 ; extra == 'all'
 Requires-Dist: elasticsearch[async] ==8.13.0 ; extra == 'all'
 Requires-Dist: pymongo ==4.6.1 ; extra == 'all'
-Requires-Dist: motor ==3.3.2 ; extra == 'all'
+Requires-Dist: motor ==3.4.0 ; extra == 'all'
 Requires-Dist: pymssql ==2.2.11 ; extra == 'all'
 Requires-Dist: aiocouch ==3.0.0 ; extra == 'all'
 Requires-Dist: asyncmy ==0.2.9 ; extra == 'all'
 Requires-Dist: mysqlclient ==2.2.0 ; extra == 'all'
 Requires-Dist: aiomysql ==0.2.0 ; extra == 'all'
 Requires-Dist: pyspark ==3.5.0 ; extra == 'all'
 Requires-Dist: oracledb ==2.1.1 ; extra == 'all'
 Requires-Dist: hazelcast-python-client ==5.3.0 ; extra == 'all'
-Requires-Dist: duckdb ==0.10.1 ; extra == 'all'
+Requires-Dist: duckdb ==0.10.2 ; extra == 'all'
 Requires-Dist: deltalake ==0.13.0 ; extra == 'all'
 Requires-Dist: botocore ==1.31.64 ; extra == 'all'
 Requires-Dist: aiobotocore ==2.7.0 ; extra == 'all'
 Requires-Dist: aioboto3 ==12.0.0 ; extra == 'all'
 Requires-Dist: google-cloud-bigquery ==3.13.0 ; extra == 'all'
+Requires-Dist: google-cloud-storage ==2.16.0 ; extra == 'all'
 Requires-Dist: pandas-gbq ==0.19.2 ; extra == 'all'
 Requires-Dist: tqdm ==4.66.1 ; extra == 'all'
 Provides-Extra: bigquery
 Requires-Dist: google-cloud-bigquery ==3.13.0 ; extra == 'bigquery'
 Requires-Dist: pandas-gbq ==0.19.2 ; extra == 'bigquery'
+Requires-Dist: google-cloud-storage ==2.16.0 ; extra == 'bigquery'
 Provides-Extra: boto3
 Requires-Dist: botocore ==1.31.64 ; extra == 'boto3'
 Requires-Dist: aiobotocore ==2.7.0 ; extra == 'boto3'
 Requires-Dist: aioboto3 ==12.0.0 ; extra == 'boto3'
 Provides-Extra: cassandra
 Requires-Dist: cassandra-driver ==3.29.1 ; extra == 'cassandra'
 Provides-Extra: couchdb
@@ -116,15 +118,15 @@
 Requires-Dist: aiosqlite >=0.18.0 ; extra == 'default'
 Requires-Dist: cassandra-driver ==3.29.1 ; extra == 'default'
 Requires-Dist: rethinkdb ==2.4.10.post1 ; extra == 'default'
 Requires-Dist: influxdb ==5.3.1 ; extra == 'default'
 Requires-Dist: influxdb-client[async] ==1.39.0 ; extra == 'default'
 Requires-Dist: pymssql ==2.2.11 ; extra == 'default'
 Requires-Dist: redis ==5.0.1 ; extra == 'default'
-Requires-Dist: duckdb ==0.10.1 ; extra == 'default'
+Requires-Dist: duckdb ==0.10.2 ; extra == 'default'
 Requires-Dist: deltalake ==0.13.0 ; extra == 'default'
 Provides-Extra: elasticsearch
 Requires-Dist: elasticsearch[async] ==8.13.0 ; extra == 'elasticsearch'
 Provides-Extra: hazelcast
 Requires-Dist: hazelcast-python-client ==5.3.0 ; extra == 'hazelcast'
 Provides-Extra: influxdb
 Requires-Dist: influxdb ==5.3.1 ; extra == 'influxdb'
@@ -134,15 +136,15 @@
 Provides-Extra: mariadb
 Requires-Dist: aiomysql ==0.2.0 ; extra == 'mariadb'
 Provides-Extra: memcache
 Requires-Dist: pylibmc ==1.6.3 ; extra == 'memcache'
 Requires-Dist: aiomcache ==0.8.1 ; extra == 'memcache'
 Provides-Extra: mongodb
 Requires-Dist: pymongo ==4.6.1 ; extra == 'mongodb'
-Requires-Dist: motor ==3.3.2 ; extra == 'mongodb'
+Requires-Dist: motor ==3.4.0 ; extra == 'mongodb'
 Provides-Extra: msqlserver
 Requires-Dist: pymssql ==2.2.11 ; extra == 'msqlserver'
 Provides-Extra: mysql
 Requires-Dist: asyncmy ==0.2.9 ; extra == 'mysql'
 Requires-Dist: mysqlclient ==2.2.0 ; extra == 'mysql'
 Provides-Extra: odbc
 Requires-Dist: aioodbc ==0.5.0 ; extra == 'odbc'
```

## Comparing `asyncdb-2.7.0.dist-info/RECORD` & `asyncdb-2.7.1.dist-info/RECORD`

 * *Files 3% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 asyncdb/__init__.py,sha256=0udlaJRz2alek7XfmdLNCM_kLB6NPY9szC5j4nsu3CU,411
 asyncdb/connections.py,sha256=MN_pNS66VqA3E_1gwa-U7eRwgbgmAOwYIVW7eZnFdtw,1297
 asyncdb/interfaces.py,sha256=neAlRetp-gjwSSktJF-v3Z5vbJdgtODpTXgb1EKDIYk,25523
 asyncdb/py.typed,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-asyncdb/version.py,sha256=u8T9I4a2qeDzLfKasOVT4cjgRpgh-q1vGZhDbGzbW1k,280
+asyncdb/version.py,sha256=RO-dkXHIk7qPglTDz1TsU9seiqc5oUfz0MAGhlQ9Wko,280
 asyncdb/drivers/__init__.py,sha256=okzf2s2YG3qjE5Co21sdcsRPHsCedCc7lmhQjqT3OCY,28
 asyncdb/drivers/abstract.py,sha256=DFTeAdUC98t5n-1ASIrFnZQFr0kD3HMVC5SFc9r--6U,4529
-asyncdb/drivers/bigquery.py,sha256=obTW_XoZzQnIhzC9CkoOUpcMKQgqe959OjD_CIyJZTE,12373
+asyncdb/drivers/bigquery.py,sha256=9Q4KgZtE6GHuEjEFgIeX9Q66HehBr-IkNYTymPhV8AY,17731
 asyncdb/drivers/cassandra.py,sha256=IVLklNnZ5IoZygL99zXyHLLEvfMkX_G6CQU1U3VaLtY,17209
 asyncdb/drivers/delta.py,sha256=laJLvumooUBlRRmobtS_TQLOI6lPhMGOopWX-v48s94,10172
 asyncdb/drivers/duckdb.py,sha256=Mp7pcCSpHcDaEUhYOaOQsovmaj2TjIXJVi1CFqujScg,10193
 asyncdb/drivers/dummy.py,sha256=hv4-_xDGc0vmYupoezxoA-afCJmTaSEn5jSTo12hGT8,2110
 asyncdb/drivers/hazel.py,sha256=aEXztGMbRJivEnIMeOn47KN1N5JLuVkbtUy9YE4q-Xg,13068
 asyncdb/drivers/influx.py,sha256=ag6hiy6cZP6qbXDWA3ax1_hjc4WYchvsKe5rYdaPLHU,20827
 asyncdb/drivers/jdbc.py,sha256=iUh4koHoEkK4nn2iE90Gqe4eRq-PtJo2TvXSZpdUFIQ,29764
@@ -23,15 +23,15 @@
 asyncdb/drivers/odbc.py,sha256=xeGoxj7IsexhrV-J1tlKr5PYnIHHM-w6N5BIHES7g-M,8036
 asyncdb/drivers/oracle.py,sha256=x8NF4Obj021SUvYbNJdVHCxPTAHJ0EcfYrG_r4Fnaa8,4206
 asyncdb/drivers/pg.py,sha256=D-imbS02AGWjUp_RaaAQ4sP9-e9bLylsN94vmBmUXpQ,61614
 asyncdb/drivers/postgres.py,sha256=T2JXgMRHXeS11M7HhSP-xkyyg0U06sUcfu_a86qwTW8,22590
 asyncdb/drivers/redis.py,sha256=vqdJ4NCOkaZWtpDXz5hJA8Eu_hC90HFKa30aT80XV5U,19814
 asyncdb/drivers/rethink.py,sha256=Pnin62i2a46SnM0kyYz_lyGGAb434PkZ68WU4gqMG4U,35185
 asyncdb/drivers/sa.py,sha256=fBOgeYSkzPTjcAsOkvZVrvtpb_bDy1yYZpw5oIBkzV0,16815
-asyncdb/drivers/scylladb.py,sha256=nMNTZDJqd0uaj-YMK40Nwtc7TUZJzS0ivDegNM-yH60,56434
+asyncdb/drivers/scylladb.py,sha256=BIXlRnuh_VUzWoK_Fbovzfy9NIffn3lUyZ-Exa-aUT8,56513
 asyncdb/drivers/sql.py,sha256=YxUQb61MJiALin-NsYgxcQgkg-WqXekAVfMiii0-M3E,2071
 asyncdb/drivers/sqlite.py,sha256=HtyIutxNF6pUqwaS6NezmQ-HMKoPoUT4ds-vRjzOcwA,25972
 asyncdb/drivers/sqlserver.py,sha256=xbjv-Rafm1aWcbfp2N8lqCXcY2Xb2tJr8eTZry1b4qI,14374
 asyncdb/drivers/outputs/__init__.py,sha256=jRYvaCZZQWmWkbf5sJATxLmsmoSA0cgJQrX6ZlNuNOo,513
 asyncdb/drivers/outputs/arrow.py,sha256=1FKK52vDM3ZzZViCO476mOkiM-QGrxFgFusUJrCZYEc,848
 asyncdb/drivers/outputs/base.py,sha256=UmePeAfWIt1XE_de9HdJFcKNFtPMw0eewVYQCpbhLQs,429
 asyncdb/drivers/outputs/csv.py,sha256=n-9-QWA-VmldleXmGCdCtStBdMN9Y3TuH70sHMcpp-o,997
@@ -43,27 +43,27 @@
 asyncdb/drivers/outputs/output.py,sha256=WgcgqrqwN0WSHpf4jTuTKftyH7Ov-KQjaWzAspR2TOc,986
 asyncdb/drivers/outputs/pandas.py,sha256=T94VGwsfQhq5P_o46TXTq6Av7E3vjPkxoBA4H4T4Z8k,1021
 asyncdb/drivers/outputs/polars.py,sha256=qJNPIu6AsBzSkSjMicLUwp20PnpvPA_Gx7fkHoPHtU4,864
 asyncdb/drivers/outputs/pyspark.py,sha256=ZoLiKGAcqO96NU2oapshn6rkxt-MKppeLlVWIsYq9IQ,1236
 asyncdb/drivers/outputs/record.py,sha256=sPFwhGgM9M1APINMzyQQJ6312MYqjNnauCVJWAbzC-A,1038
 asyncdb/drivers/outputs/recordset.py,sha256=oJpN0h2AaXmj6CbsOdN8uyUo3AC9YnD77GUqfAUOZsE,770
 asyncdb/exceptions/__init__.py,sha256=c6HhQu9AQcSn85Dp-jyvqiUUUVMK2gYJalN7gYdS008,867
-asyncdb/exceptions/exceptions.pypy39-pp73-win_amd64.pyd,sha256=JBJhZoL6KklwF3TjHU95li7NqnUcrG4nPwftWJPlyNA,158720
+asyncdb/exceptions/exceptions.pypy39-pp73-win_amd64.pyd,sha256=oI3HE3q8smcig5Pj3ti0jYBke_XLNPSvSnXNw23aGDE,158720
 asyncdb/exceptions/handlers.py,sha256=GEFCoNMkyXTBz9-tuJ3Y1Gsq2gcgAZcrQnzFZ4k_LVM,2539
 asyncdb/meta/__init__.py,sha256=Wl5uJSXic-gh52UKleUdEqdVaS2LTsSPsUOlh2nO3P8,176
 asyncdb/meta/record.py,sha256=bdKiYrlmX0FtBShvVw1rscMGfQq_CgDZwp194hHrCGg,3057
 asyncdb/meta/recordset.py,sha256=yRyKbCIg5kxDWgqsddMCTKQJctBfgvs6W9bOecaZg3k,2221
 asyncdb/models/__init__.py,sha256=I_v3ttQX7tdF6V2WlBh5bf6YUAxaukfHgiQmsE59cj8,542
 asyncdb/models/model.py,sha256=e02FQHXf2vEFOeMGpqO86mmzzNzHWOYMT6B2oXZP1No,17784
 asyncdb/utils/__init__.py,sha256=Bx7Kk7jHsVfVsNchJH5BdA4zDQFmvRxt9TnY0fd9qWo,136
 asyncdb/utils/functions.py,sha256=Pr5wkjacMvuG7vOSGGLF_CfKkVqTRZpnbJZcl6T2IpE,2484
 asyncdb/utils/modules.py,sha256=6tUulpoEUpitmoOd972JLdj_XiYx0xbHcAimZRZ81yE,655
-asyncdb/utils/types.pypy39-pp73-win_amd64.pyd,sha256=QSq3O7Hz01bP5zbyaRBvX80jJTyaUsJeBRa5vZW6GR0,111616
+asyncdb/utils/types.pypy39-pp73-win_amd64.pyd,sha256=PidyGU4ghRt3ZVgJJKUhkBqA_Ncr1B_QMYR0QfltqQo,111616
 asyncdb/utils/uv.py,sha256=r3HYjQmAAl4yOYQwvi2ghUGGJVWPqUFNqRWcjKWCNxs,319
 asyncdb/utils/encoders/__init__.py,sha256=YwtnLtxCmPm21EALjW1Y2upbyIHJiNWHjrchuetp4Fc,234
 asyncdb/utils/encoders/numpy.py,sha256=DkNoKLLA2haOG0qOm9J5RmRBkgiEZbcl9do5mBvIQbY,487
 asyncdb/utils/encoders/pg.py,sha256=i70nydH2S4XxAZUJdoJjXYsU_Ju6hAJbynLsztZSP10,1195
-asyncdb-2.7.0.dist-info/LICENSE,sha256=mThyqULh5pc9N8g3nfITEVMYcHOxzkPyJHQGWwFuQ_c,1538
-asyncdb-2.7.0.dist-info/METADATA,sha256=jvhYMIHS33yCUQ-tbfR3QcsL33J71R0doav-i21D9tQ,12674
-asyncdb-2.7.0.dist-info/WHEEL,sha256=1cqEyrKxUA_c-f0mOszCh_7q-AnzL9K6iMxWION_yb8,107
-asyncdb-2.7.0.dist-info/top_level.txt,sha256=vMLK2jQFYyNY3ta7idY3krhSki0RPpQ8yVZCVa724JI,8
-asyncdb-2.7.0.dist-info/RECORD,,
+asyncdb-2.7.1.dist-info/LICENSE,sha256=mThyqULh5pc9N8g3nfITEVMYcHOxzkPyJHQGWwFuQ_c,1538
+asyncdb-2.7.1.dist-info/METADATA,sha256=0X3UMD24035gkd1kgpSgTy2MufBTEvk_OtCKwKFoGF0,12805
+asyncdb-2.7.1.dist-info/WHEEL,sha256=1cqEyrKxUA_c-f0mOszCh_7q-AnzL9K6iMxWION_yb8,107
+asyncdb-2.7.1.dist-info/top_level.txt,sha256=vMLK2jQFYyNY3ta7idY3krhSki0RPpQ8yVZCVa724JI,8
+asyncdb-2.7.1.dist-info/RECORD,,
```

