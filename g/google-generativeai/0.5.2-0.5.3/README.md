# Comparing `tmp/google_generativeai-0.5.2-py3-none-any.whl.zip` & `tmp/google_generativeai-0.5.3-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,75 +1,75 @@
-Zip file size: 146781 bytes, number of entries: 73
--rw-r--r--  2.0 unx      467 b- defN 24-Apr-18 21:09 google_generativeai-0.5.2-py3.11-nspkg.pth
--rw-r--r--  2.0 unx     2806 b- defN 24-Apr-18 21:09 google/generativeai/__init__.py
--rw-r--r--  2.0 unx    13383 b- defN 24-Apr-18 21:09 google/generativeai/answer.py
--rw-r--r--  2.0 unx    12609 b- defN 24-Apr-18 21:09 google/generativeai/client.py
--rw-r--r--  2.0 unx    20937 b- defN 24-Apr-18 21:09 google/generativeai/discuss.py
--rw-r--r--  2.0 unx    11583 b- defN 24-Apr-18 21:09 google/generativeai/embedding.py
--rw-r--r--  2.0 unx     2226 b- defN 24-Apr-18 21:09 google/generativeai/files.py
--rw-r--r--  2.0 unx    28672 b- defN 24-Apr-18 21:09 google/generativeai/generative_models.py
--rw-r--r--  2.0 unx    14685 b- defN 24-Apr-18 21:09 google/generativeai/models.py
--rw-r--r--  2.0 unx     4900 b- defN 24-Apr-18 21:09 google/generativeai/operations.py
--rw-r--r--  2.0 unx     1410 b- defN 24-Apr-18 21:09 google/generativeai/permission.py
--rw-r--r--  2.0 unx    15386 b- defN 24-Apr-18 21:09 google/generativeai/responder.py
--rw-r--r--  2.0 unx     8542 b- defN 24-Apr-18 21:09 google/generativeai/retriever.py
--rw-r--r--  2.0 unx     2441 b- defN 24-Apr-18 21:09 google/generativeai/string_utils.py
--rw-r--r--  2.0 unx    13190 b- defN 24-Apr-18 21:09 google/generativeai/text.py
--rw-r--r--  2.0 unx      976 b- defN 24-Apr-18 21:09 google/generativeai/utils.py
--rw-r--r--  2.0 unx      656 b- defN 24-Apr-18 21:09 google/generativeai/version.py
--rw-r--r--  2.0 unx     1169 b- defN 24-Apr-18 21:09 google/generativeai/notebook/__init__.py
--rw-r--r--  2.0 unx     3961 b- defN 24-Apr-18 21:09 google/generativeai/notebook/argument_parser.py
--rw-r--r--  2.0 unx    20532 b- defN 24-Apr-18 21:09 google/generativeai/notebook/cmd_line_parser.py
--rw-r--r--  2.0 unx     1535 b- defN 24-Apr-18 21:09 google/generativeai/notebook/command.py
--rw-r--r--  2.0 unx     6270 b- defN 24-Apr-18 21:09 google/generativeai/notebook/command_utils.py
--rw-r--r--  2.0 unx     2566 b- defN 24-Apr-18 21:09 google/generativeai/notebook/compare_cmd.py
--rw-r--r--  2.0 unx     2365 b- defN 24-Apr-18 21:09 google/generativeai/notebook/compile_cmd.py
--rw-r--r--  2.0 unx     2756 b- defN 24-Apr-18 21:09 google/generativeai/notebook/eval_cmd.py
--rw-r--r--  2.0 unx    17279 b- defN 24-Apr-18 21:09 google/generativeai/notebook/flag_def.py
--rw-r--r--  2.0 unx     7711 b- defN 24-Apr-18 21:09 google/generativeai/notebook/gspread_client.py
--rw-r--r--  2.0 unx     1555 b- defN 24-Apr-18 21:09 google/generativeai/notebook/html_utils.py
--rw-r--r--  2.0 unx     2819 b- defN 24-Apr-18 21:09 google/generativeai/notebook/input_utils.py
--rw-r--r--  2.0 unx     1686 b- defN 24-Apr-18 21:09 google/generativeai/notebook/ipython_env.py
--rw-r--r--  2.0 unx     1058 b- defN 24-Apr-18 21:09 google/generativeai/notebook/ipython_env_impl.py
--rw-r--r--  2.0 unx     4616 b- defN 24-Apr-18 21:09 google/generativeai/notebook/magics.py
--rw-r--r--  2.0 unx     5468 b- defN 24-Apr-18 21:09 google/generativeai/notebook/magics_engine.py
--rw-r--r--  2.0 unx     1919 b- defN 24-Apr-18 21:09 google/generativeai/notebook/model_registry.py
--rw-r--r--  2.0 unx     2085 b- defN 24-Apr-18 21:09 google/generativeai/notebook/output_utils.py
--rw-r--r--  2.0 unx     3084 b- defN 24-Apr-18 21:09 google/generativeai/notebook/parsed_args_lib.py
--rw-r--r--  2.0 unx     5803 b- defN 24-Apr-18 21:09 google/generativeai/notebook/post_process_utils.py
--rw-r--r--  2.0 unx      991 b- defN 24-Apr-18 21:09 google/generativeai/notebook/post_process_utils_test_helper.py
--rw-r--r--  2.0 unx     2073 b- defN 24-Apr-18 21:09 google/generativeai/notebook/py_utils.py
--rw-r--r--  2.0 unx     2744 b- defN 24-Apr-18 21:09 google/generativeai/notebook/run_cmd.py
--rw-r--r--  2.0 unx     3071 b- defN 24-Apr-18 21:09 google/generativeai/notebook/sheets_id.py
--rw-r--r--  2.0 unx     2977 b- defN 24-Apr-18 21:09 google/generativeai/notebook/sheets_sanitize_url.py
--rw-r--r--  2.0 unx     3994 b- defN 24-Apr-18 21:09 google/generativeai/notebook/sheets_utils.py
--rw-r--r--  2.0 unx     2634 b- defN 24-Apr-18 21:09 google/generativeai/notebook/text_model.py
--rw-r--r--  2.0 unx      598 b- defN 24-Apr-18 21:09 google/generativeai/notebook/lib/__init__.py
--rw-r--r--  2.0 unx    18315 b- defN 24-Apr-18 21:09 google/generativeai/notebook/lib/llm_function.py
--rw-r--r--  2.0 unx     2969 b- defN 24-Apr-18 21:09 google/generativeai/notebook/lib/llmfn_input_utils.py
--rw-r--r--  2.0 unx     2443 b- defN 24-Apr-18 21:09 google/generativeai/notebook/lib/llmfn_inputs_source.py
--rw-r--r--  2.0 unx     5917 b- defN 24-Apr-18 21:09 google/generativeai/notebook/lib/llmfn_output_row.py
--rw-r--r--  2.0 unx     8532 b- defN 24-Apr-18 21:09 google/generativeai/notebook/lib/llmfn_outputs.py
--rw-r--r--  2.0 unx     2470 b- defN 24-Apr-18 21:09 google/generativeai/notebook/lib/llmfn_post_process.py
--rw-r--r--  2.0 unx     8569 b- defN 24-Apr-18 21:09 google/generativeai/notebook/lib/llmfn_post_process_cmds.py
--rw-r--r--  2.0 unx     2055 b- defN 24-Apr-18 21:09 google/generativeai/notebook/lib/model.py
--rw-r--r--  2.0 unx     1264 b- defN 24-Apr-18 21:09 google/generativeai/notebook/lib/prompt_utils.py
--rw-r--r--  2.0 unx     1487 b- defN 24-Apr-18 21:09 google/generativeai/notebook/lib/unique_fn.py
--rw-r--r--  2.0 unx     1181 b- defN 24-Apr-18 21:09 google/generativeai/types/__init__.py
--rw-r--r--  2.0 unx     2116 b- defN 24-Apr-18 21:09 google/generativeai/types/answer_types.py
--rw-r--r--  2.0 unx     1232 b- defN 24-Apr-18 21:09 google/generativeai/types/citation_types.py
--rw-r--r--  2.0 unx    22835 b- defN 24-Apr-18 21:09 google/generativeai/types/content_types.py
--rw-r--r--  2.0 unx     6661 b- defN 24-Apr-18 21:09 google/generativeai/types/discuss_types.py
--rw-r--r--  2.0 unx     2011 b- defN 24-Apr-18 21:09 google/generativeai/types/file_types.py
--rw-r--r--  2.0 unx    18640 b- defN 24-Apr-18 21:09 google/generativeai/types/generation_types.py
--rw-r--r--  2.0 unx    11638 b- defN 24-Apr-18 21:09 google/generativeai/types/model_types.py
--rw-r--r--  2.0 unx     7599 b- defN 24-Apr-18 21:09 google/generativeai/types/permission_types.py
--rw-r--r--  2.0 unx    61705 b- defN 24-Apr-18 21:09 google/generativeai/types/retriever_types.py
--rw-r--r--  2.0 unx    10362 b- defN 24-Apr-18 21:09 google/generativeai/types/safety_types.py
--rw-r--r--  2.0 unx     2319 b- defN 24-Apr-18 21:09 google/generativeai/types/text_types.py
--rw-r--r--  2.0 unx    11358 b- defN 24-Apr-18 21:09 google_generativeai-0.5.2.dist-info/LICENSE
--rw-r--r--  2.0 unx     3921 b- defN 24-Apr-18 21:09 google_generativeai-0.5.2.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-18 21:09 google_generativeai-0.5.2.dist-info/WHEEL
--rw-r--r--  2.0 unx        7 b- defN 24-Apr-18 21:09 google_generativeai-0.5.2.dist-info/namespace_packages.txt
--rw-r--r--  2.0 unx        7 b- defN 24-Apr-18 21:09 google_generativeai-0.5.2.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     7169 b- defN 24-Apr-18 21:09 google_generativeai-0.5.2.dist-info/RECORD
-73 files, 495062 bytes uncompressed, 135095 bytes compressed:  72.7%
+Zip file size: 150714 bytes, number of entries: 73
+-rw-r--r--  2.0 unx      467 b- defN 24-May-13 20:54 google_generativeai-0.5.3-py3.11-nspkg.pth
+-rw-r--r--  2.0 unx     2806 b- defN 24-Apr-30 00:03 google/generativeai/__init__.py
+-rw-r--r--  2.0 unx    13510 b- defN 24-May-10 18:13 google/generativeai/answer.py
+-rw-r--r--  2.0 unx    12452 b- defN 24-May-13 20:54 google/generativeai/client.py
+-rw-r--r--  2.0 unx    20937 b- defN 24-May-10 18:13 google/generativeai/discuss.py
+-rw-r--r--  2.0 unx    11583 b- defN 24-May-08 16:55 google/generativeai/embedding.py
+-rw-r--r--  2.0 unx     3056 b- defN 24-May-13 20:54 google/generativeai/files.py
+-rw-r--r--  2.0 unx    29684 b- defN 24-May-10 18:13 google/generativeai/generative_models.py
+-rw-r--r--  2.0 unx    14685 b- defN 24-May-08 16:55 google/generativeai/models.py
+-rw-r--r--  2.0 unx     4900 b- defN 23-Oct-19 13:31 google/generativeai/operations.py
+-rw-r--r--  2.0 unx     6141 b- defN 24-May-09 21:10 google/generativeai/permission.py
+-rw-r--r--  2.0 unx    16475 b- defN 24-May-09 21:10 google/generativeai/responder.py
+-rw-r--r--  2.0 unx     8542 b- defN 24-May-08 16:55 google/generativeai/retriever.py
+-rw-r--r--  2.0 unx     2441 b- defN 24-Jan-08 18:13 google/generativeai/string_utils.py
+-rw-r--r--  2.0 unx    13190 b- defN 24-May-10 18:13 google/generativeai/text.py
+-rw-r--r--  2.0 unx      976 b- defN 24-Jan-31 13:04 google/generativeai/utils.py
+-rw-r--r--  2.0 unx      656 b- defN 24-May-13 20:54 google/generativeai/version.py
+-rw-r--r--  2.0 unx     1169 b- defN 23-Jun-09 16:06 google/generativeai/notebook/__init__.py
+-rw-r--r--  2.0 unx     3961 b- defN 23-Oct-17 17:28 google/generativeai/notebook/argument_parser.py
+-rw-r--r--  2.0 unx    20532 b- defN 24-Jan-31 13:04 google/generativeai/notebook/cmd_line_parser.py
+-rw-r--r--  2.0 unx     1535 b- defN 23-Oct-17 17:28 google/generativeai/notebook/command.py
+-rw-r--r--  2.0 unx     6270 b- defN 23-Oct-19 13:31 google/generativeai/notebook/command_utils.py
+-rw-r--r--  2.0 unx     2566 b- defN 23-Oct-17 17:28 google/generativeai/notebook/compare_cmd.py
+-rw-r--r--  2.0 unx     2365 b- defN 23-Oct-19 13:31 google/generativeai/notebook/compile_cmd.py
+-rw-r--r--  2.0 unx     2756 b- defN 23-Oct-17 17:28 google/generativeai/notebook/eval_cmd.py
+-rw-r--r--  2.0 unx    17279 b- defN 23-Oct-19 13:31 google/generativeai/notebook/flag_def.py
+-rw-r--r--  2.0 unx     7711 b- defN 23-Oct-19 13:31 google/generativeai/notebook/gspread_client.py
+-rw-r--r--  2.0 unx     1555 b- defN 23-Oct-17 17:28 google/generativeai/notebook/html_utils.py
+-rw-r--r--  2.0 unx     2819 b- defN 23-Oct-17 17:29 google/generativeai/notebook/input_utils.py
+-rw-r--r--  2.0 unx     1686 b- defN 23-Jun-01 19:13 google/generativeai/notebook/ipython_env.py
+-rw-r--r--  2.0 unx     1058 b- defN 23-Jun-01 19:13 google/generativeai/notebook/ipython_env_impl.py
+-rw-r--r--  2.0 unx     4616 b- defN 24-Jan-31 13:04 google/generativeai/notebook/magics.py
+-rw-r--r--  2.0 unx     5468 b- defN 24-Feb-01 20:54 google/generativeai/notebook/magics_engine.py
+-rw-r--r--  2.0 unx     1919 b- defN 23-Oct-19 13:31 google/generativeai/notebook/model_registry.py
+-rw-r--r--  2.0 unx     2085 b- defN 23-Oct-17 17:28 google/generativeai/notebook/output_utils.py
+-rw-r--r--  2.0 unx     3084 b- defN 23-Oct-19 13:31 google/generativeai/notebook/parsed_args_lib.py
+-rw-r--r--  2.0 unx     5803 b- defN 23-Oct-19 13:31 google/generativeai/notebook/post_process_utils.py
+-rw-r--r--  2.0 unx      991 b- defN 23-Jun-01 19:13 google/generativeai/notebook/post_process_utils_test_helper.py
+-rw-r--r--  2.0 unx     2073 b- defN 23-Oct-17 17:28 google/generativeai/notebook/py_utils.py
+-rw-r--r--  2.0 unx     2744 b- defN 23-Oct-17 17:28 google/generativeai/notebook/run_cmd.py
+-rw-r--r--  2.0 unx     3071 b- defN 23-Oct-17 17:28 google/generativeai/notebook/sheets_id.py
+-rw-r--r--  2.0 unx     2977 b- defN 23-Oct-19 13:31 google/generativeai/notebook/sheets_sanitize_url.py
+-rw-r--r--  2.0 unx     3994 b- defN 23-Oct-17 17:29 google/generativeai/notebook/sheets_utils.py
+-rw-r--r--  2.0 unx     2634 b- defN 24-Apr-30 00:03 google/generativeai/notebook/text_model.py
+-rw-r--r--  2.0 unx      598 b- defN 23-Jun-01 19:13 google/generativeai/notebook/lib/__init__.py
+-rw-r--r--  2.0 unx    18315 b- defN 23-Oct-19 13:31 google/generativeai/notebook/lib/llm_function.py
+-rw-r--r--  2.0 unx     2969 b- defN 23-Oct-17 17:29 google/generativeai/notebook/lib/llmfn_input_utils.py
+-rw-r--r--  2.0 unx     2443 b- defN 23-Oct-19 13:31 google/generativeai/notebook/lib/llmfn_inputs_source.py
+-rw-r--r--  2.0 unx     5917 b- defN 23-Oct-17 17:29 google/generativeai/notebook/lib/llmfn_output_row.py
+-rw-r--r--  2.0 unx     8532 b- defN 24-Feb-01 20:54 google/generativeai/notebook/lib/llmfn_outputs.py
+-rw-r--r--  2.0 unx     2470 b- defN 23-Oct-17 17:29 google/generativeai/notebook/lib/llmfn_post_process.py
+-rw-r--r--  2.0 unx     8569 b- defN 23-Oct-19 13:31 google/generativeai/notebook/lib/llmfn_post_process_cmds.py
+-rw-r--r--  2.0 unx     2055 b- defN 23-Oct-19 13:31 google/generativeai/notebook/lib/model.py
+-rw-r--r--  2.0 unx     1264 b- defN 23-Oct-17 17:28 google/generativeai/notebook/lib/prompt_utils.py
+-rw-r--r--  2.0 unx     1487 b- defN 23-Oct-17 17:29 google/generativeai/notebook/lib/unique_fn.py
+-rw-r--r--  2.0 unx     1181 b- defN 24-May-08 16:55 google/generativeai/types/__init__.py
+-rw-r--r--  2.0 unx     2116 b- defN 24-Apr-30 00:03 google/generativeai/types/answer_types.py
+-rw-r--r--  2.0 unx     1232 b- defN 24-Jan-08 18:13 google/generativeai/types/citation_types.py
+-rw-r--r--  2.0 unx    25991 b- defN 24-May-09 21:10 google/generativeai/types/content_types.py
+-rw-r--r--  2.0 unx     6661 b- defN 24-May-10 18:13 google/generativeai/types/discuss_types.py
+-rw-r--r--  2.0 unx     1996 b- defN 24-May-09 21:10 google/generativeai/types/file_types.py
+-rw-r--r--  2.0 unx    20692 b- defN 24-May-13 16:26 google/generativeai/types/generation_types.py
+-rw-r--r--  2.0 unx    12179 b- defN 24-May-09 21:10 google/generativeai/types/model_types.py
+-rw-r--r--  2.0 unx    14632 b- defN 24-May-09 21:10 google/generativeai/types/permission_types.py
+-rw-r--r--  2.0 unx    59036 b- defN 24-May-09 21:10 google/generativeai/types/retriever_types.py
+-rw-r--r--  2.0 unx    10362 b- defN 24-May-10 23:14 google/generativeai/types/safety_types.py
+-rw-r--r--  2.0 unx     2319 b- defN 24-May-10 18:13 google/generativeai/types/text_types.py
+-rw-r--r--  2.0 unx    11358 b- defN 24-May-13 20:54 google_generativeai-0.5.3.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3921 b- defN 24-May-13 20:54 google_generativeai-0.5.3.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-13 20:54 google_generativeai-0.5.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx        7 b- defN 24-May-13 20:54 google_generativeai-0.5.3.dist-info/namespace_packages.txt
+-rw-r--r--  2.0 unx        7 b- defN 24-May-13 20:54 google_generativeai-0.5.3.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7170 b- defN 24-May-13 20:54 google_generativeai-0.5.3.dist-info/RECORD
+73 files, 512793 bytes uncompressed, 139028 bytes compressed:  72.9%
```

## zipnote {}

```diff
@@ -1,8 +1,8 @@
-Filename: google_generativeai-0.5.2-py3.11-nspkg.pth
+Filename: google_generativeai-0.5.3-py3.11-nspkg.pth
 Comment: 
 
 Filename: google/generativeai/__init__.py
 Comment: 
 
 Filename: google/generativeai/answer.py
 Comment: 
@@ -195,26 +195,26 @@
 
 Filename: google/generativeai/types/safety_types.py
 Comment: 
 
 Filename: google/generativeai/types/text_types.py
 Comment: 
 
-Filename: google_generativeai-0.5.2.dist-info/LICENSE
+Filename: google_generativeai-0.5.3.dist-info/LICENSE
 Comment: 
 
-Filename: google_generativeai-0.5.2.dist-info/METADATA
+Filename: google_generativeai-0.5.3.dist-info/METADATA
 Comment: 
 
-Filename: google_generativeai-0.5.2.dist-info/WHEEL
+Filename: google_generativeai-0.5.3.dist-info/WHEEL
 Comment: 
 
-Filename: google_generativeai-0.5.2.dist-info/namespace_packages.txt
+Filename: google_generativeai-0.5.3.dist-info/namespace_packages.txt
 Comment: 
 
-Filename: google_generativeai-0.5.2.dist-info/top_level.txt
+Filename: google_generativeai-0.5.3.dist-info/top_level.txt
 Comment: 
 
-Filename: google_generativeai-0.5.2.dist-info/RECORD
+Filename: google_generativeai-0.5.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## google/generativeai/answer.py

```diff
@@ -245,17 +245,17 @@
     semantic_retriever: SemanticRetrieverConfigOptions | None = None,
     answer_style: AnswerStyle | None = None,
     safety_settings: safety_types.SafetySettingOptions | None = None,
     temperature: float | None = None,
     client: glm.GenerativeServiceClient | None = None,
     request_options: dict[str, Any] | None = None,
 ):
-    f"""
+    """
     Calls the GenerateAnswer API and returns a `types.Answer` containing the response.
-    
+
     You can pass a literal list of text chunks:
 
     >>> from google.generativeai import answer
     >>> answer.generate_answer(
     ...     content=question,
     ...     inline_passages=splitter.split(document)
     ... )
@@ -316,14 +316,15 @@
     contents: content_types.ContentsType,
     inline_passages: GroundingPassagesOptions | None = None,
     semantic_retriever: SemanticRetrieverConfigOptions | None = None,
     answer_style: AnswerStyle | None = None,
     safety_settings: safety_types.SafetySettingOptions | None = None,
     temperature: float | None = None,
     client: glm.GenerativeServiceClient | None = None,
+    request_options: dict[str, Any] | None = None,
 ):
     """
     Calls the API and returns a `types.Answer` containing the answer.
 
     Args:
         model: Which model to call, as a string or a `types.Model`.
         contents: The question to be answered by the model, grounded in the
@@ -337,23 +338,26 @@
         safety_settings: Safety settings for generated output. Defaults to None.
         temperature: Controls the randomness of the output.
         client: If you're not relying on a default client, you pass a `glm.TextServiceClient` instead.
 
     Returns:
         A `types.Answer` containing the model's text answer response.
     """
+    if request_options is None:
+        request_options = {}
+
     if client is None:
         client = get_default_generative_async_client()
 
     request = _make_generate_answer_request(
         model=model,
         contents=contents,
         inline_passages=inline_passages,
         semantic_retriever=semantic_retriever,
         safety_settings=safety_settings,
         temperature=temperature,
         answer_style=answer_style,
     )
 
-    response = await client.generate_answer(request)
+    response = await client.generate_answer(request, **request_options)
 
     return response
```

## google/generativeai/client.py

```diff
@@ -55,37 +55,32 @@
     def create_file(
         self,
         path: str | pathlib.Path | os.PathLike,
         *,
         mime_type: str | None = None,
         name: str | None = None,
         display_name: str | None = None,
+        resumable: bool = True,
     ) -> glm.File:
         if self._discovery_api is None:
             self._setup_discovery_api()
 
         file = {}
         if name is not None:
             file["name"] = name
         if display_name is not None:
             file["displayName"] = display_name
 
-        media = googleapiclient.http.MediaFileUpload(filename=path, mimetype=mime_type)
+        media = googleapiclient.http.MediaFileUpload(
+            filename=path, mimetype=mime_type, resumable=resumable
+        )
         request = self._discovery_api.media().upload(body={"file": file}, media_body=media)
         result = request.execute()
 
-        allowed_keys = set(glm.File.__annotations__)
-
-        return glm.File(
-            {
-                re.sub("[A-Z]", lambda ch: f"_{ch.group(0).lower()}", key): value
-                for key, value in result["file"].items()
-                if key in allowed_keys
-            }
-        )
+        return self.get_file({"name": result["file"]["name"]})
 
 
 class FileServiceAsyncClient(glm.FileServiceAsyncClient):
     async def create_file(self, *args, **kwargs):
         raise NotImplementedError("`create_file` is not yet implemented for the async client.")
```

## google/generativeai/files.py

```diff
@@ -31,30 +31,47 @@
 
 def upload_file(
     path: str | pathlib.Path | os.PathLike,
     *,
     mime_type: str | None = None,
     name: str | None = None,
     display_name: str | None = None,
+    resumable: bool = True,
 ) -> file_types.File:
+    """Uploads a file using a supported file service.
+
+    Args:
+        path: The path to the file to be uploaded.
+        mime_type: The MIME type of the file. If not provided, it will be
+            inferred from the file extension.
+        name: The name of the file in the destination (e.g., 'files/sample-image').
+            If not provided, a system generated ID will be created.
+        display_name: Optional display name of the file.
+        resumable: Whether to use the resumable upload protocol. By default, this is enabled.
+            See details at
+            https://googleapis.github.io/google-api-python-client/docs/epy/googleapiclient.http.MediaFileUpload-class.html#resumable
+
+    Returns:
+        file_types.File: The response of the uploaded file.
+    """
     client = get_default_file_client()
 
     path = pathlib.Path(os.fspath(path))
 
     if mime_type is None:
         mime_type, _ = mimetypes.guess_type(path)
 
     if name is not None and "/" not in name:
         name = f"files/{name}"
 
     if display_name is None:
         display_name = path.name
 
     response = client.create_file(
-        path=path, mime_type=mime_type, name=name, display_name=display_name
+        path=path, mime_type=mime_type, name=name, display_name=display_name, resumable=resumable
     )
     return file_types.File(response)
 
 
 def list_files(page_size=100) -> Iterable[file_types.File]:
     client = get_default_file_client()
```

## google/generativeai/generative_models.py

```diff
@@ -318,43 +318,65 @@
                     "`f = genai.upload_file(path); m.generate_content(['tell me about this file:', f])`"
                 )
             raise
 
     # fmt: off
     def count_tokens(
         self,
-        contents: content_types.ContentsType,
+        contents: content_types.ContentsType = None,
+        *,
+        generation_config: generation_types.GenerationConfigType | None = None,
+        safety_settings: safety_types.SafetySettingOptions | None = None,
+        tools: content_types.FunctionLibraryType | None = None,
+        tool_config: content_types.ToolConfigType | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> glm.CountTokensResponse:
         if request_options is None:
             request_options = {}
 
         if self._client is None:
             self._client = client.get_default_generative_client()
-        contents = content_types.to_contents(contents)
-        return self._client.count_tokens(
-            glm.CountTokensRequest(model=self.model_name, contents=contents),
-                **request_options,
-        )
+
+        request = glm.CountTokensRequest(
+            model=self.model_name,
+            generate_content_request=self._prepare_request(
+                contents=contents,
+                generation_config=generation_config,
+                safety_settings=safety_settings,
+                tools=tools,
+                tool_config=tool_config,
+        ))
+        return self._client.count_tokens(request, **request_options)
 
     async def count_tokens_async(
         self,
-        contents: content_types.ContentsType,
+        contents: content_types.ContentsType = None,
+        *,
+        generation_config: generation_types.GenerationConfigType | None = None,
+        safety_settings: safety_types.SafetySettingOptions | None = None,
+        tools: content_types.FunctionLibraryType | None = None,
+        tool_config: content_types.ToolConfigType | None = None,
         request_options: dict[str, Any] | None = None,
     ) -> glm.CountTokensResponse:
         if request_options is None:
             request_options = {}
 
         if self._async_client is None:
             self._async_client = client.get_default_generative_async_client()
-        contents = content_types.to_contents(contents)
-        return await self._async_client.count_tokens(
-            glm.CountTokensRequest(model=self.model_name, contents=contents),
-                **request_options,
-        )
+
+        request = glm.CountTokensRequest(
+            model=self.model_name,
+            generate_content_request=self._prepare_request(
+                contents=contents,
+                generation_config=generation_config,
+                safety_settings=safety_settings,
+                tools=tools,
+                tool_config=tool_config,
+        ))
+        return await self._async_client.count_tokens(request, **request_options)
 
     # fmt: on
 
     def start_chat(
         self,
         *,
         history: Iterable[content_types.StrictContentType] | None = None,
@@ -680,15 +702,15 @@
             glm.Candidate.FinishReason.MAX_TOKENS,
         ):
             error = generation_types.StopCandidateException(last.candidates[0])
             last._error = error
 
         if last._error is not None:
             raise generation_types.BrokenResponseError(
-                "Can not build a coherent char history after a broken "
+                "Can not build a coherent chat history after a broken "
                 "streaming response "
                 "(See the previous Exception fro details). "
                 "To inspect the last response object, use `chat.last`."
                 "To remove the last request/response `Content` objects from the chat "
                 "call `last_send, last_received = chat.rewind()` and continue "
                 "without it."
             ) from last._error
```

## google/generativeai/permission.py

```diff
@@ -10,35 +10,160 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
+from typing import Callable
+
 import google.ai.generativelanguage as glm
 
 from google.generativeai.types import permission_types
+from google.generativeai.types import retriever_types
+from google.generativeai.types import model_types
+
+
+_RESOURCE_TYPE: dict[str, str] = {
+    "corpus": "corpora",
+    "corpora": "corpora",
+    "tunedmodel": "tunedModels",
+    "tunedmodels": "tunedModels",
+}
+
+
+def _to_resource_type(x: str) -> str:
+    if isinstance(x, str):
+        x = x.lower()
+    resource_type = _RESOURCE_TYPE.get(x, None)
+    if not resource_type:
+        raise ValueError(f"Unsupported resource type. Got: `{x}` instead.")
+
+    return resource_type
+
+
+def _validate_resource_name(x: str, resource_type: str) -> None:
+    if resource_type == "corpora":
+        if not retriever_types.valid_name(x):
+            raise ValueError(retriever_types.NAME_ERROR_MSG.format(length=len(x), name=x))
+
+    elif resource_type == "tunedModels":
+        if not model_types.valid_tuned_model_name(x):
+            raise ValueError(model_types.TUNED_MODEL_NAME_ERROR_MSG.format(length=len(x), name=x))
+
+    else:
+        raise ValueError(f"Unsupported resource type: {resource_type}")
+
+
+def _validate_permission_id(x: str) -> None:
+    if not permission_types.valid_id(x):
+        raise ValueError(permission_types.INVALID_PERMISSION_ID_MSG.format(permission_id=x))
+
+
+def _get_valid_name_components(name: str) -> str:
+    # name is of the format: resource_type/resource_name/permissions/permission_id
+    name_path_components = name.split("/")
+    if len(name_path_components) != 4:
+        raise ValueError(
+            f"Invalid name format. Expected format: \
+                `resource_type/<resource_name>/permissions/<permission_id>`. Got: `{name}` instead."
+        )
+
+    resource_type, resource_name, permission_placeholder, permission_id = name_path_components
+    resource_type = _to_resource_type(resource_type)
+
+    permission_id = "/".join([permission_placeholder, permission_id])
+
+    _validate_resource_name(resource_name, resource_type)
+    _validate_permission_id(permission_id)
+
+    return "/".join([resource_type, resource_name, permission_id])
+
+
+def _construct_name(
+    name: str | None = None,
+    resource_name: str | None = None,
+    permission_id: str | int | None = None,
+    resource_type: str | None = None,
+) -> str:
+    # resource_name is the name of the supported resource (corpus or tunedModel as of now) for which the permission is being created.
+    if not name:
+        # if name is not provided, then try to construct name via provided resource_name and permission_id.
+        if not (resource_name and permission_id):
+            raise ValueError(
+                "Either `name` or (`resource_name` and `permission_id`) must be provided."
+            )
+
+        if resource_type:
+            resource_type = _to_resource_type(resource_type)
+        else:
+            # if resource_type is not provided, then try to infer it from resource_name.
+            resource_path_components = resource_name.split("/")
+            if len(resource_path_components) != 2:
+                raise ValueError(
+                    f"Invalid `resource_name` format. Expected format: \
+                        `resource_type/resource_name`. Got: `{resource_name}` instead."
+                )
+            resource_type = _to_resource_type(resource_path_components[0])
+
+        if f"{resource_type}/" in resource_name:
+            name = f"{resource_name}/"
+        else:
+            name = f"{resource_type}/{resource_name}/"
+
+        if isinstance(permission_id, int) or "permissions/" not in permission_id:
+            name += f"permissions/{permission_id}"
+        else:
+            name += permission_id
+
+    # if name is provided, override resource_name and permission_id
+    name = _get_valid_name_components(name)
+    return name
 
 
 def get_permission(
-    name: str,
+    name: str | None = None,
+    *,
     client: glm.PermissionServiceClient | None = None,
+    resource_name: str | None = None,
+    permission_id: str | int | None = None,
+    resource_type: str | None = None,
 ) -> permission_types.Permission:
-    """Get a permission by name.
+    """Get information about a permission by name.
 
     Args:
         name: The name of the permission.
+        resource_name: The name of the supported resource for which the permission details are needed.
+        permission_id: The name of the permission.
+        resource_type: The type of the resource (corpus or tunedModel as of now) for which the permission details are needed.
+                        If not provided, it will be inferred from `resource_name`.
 
     Returns:
         The permission as an instance of `permission_types.Permission`.
     """
+    name = _construct_name(
+        name=name,
+        resource_name=resource_name,
+        permission_id=permission_id,
+        resource_type=resource_type,
+    )
     return permission_types.Permission.get(name=name, client=client)
 
 
 async def get_permission_async(
-    name: str,
+    name: str | None = None,
+    *,
     client: glm.PermissionServiceAsyncClient | None = None,
+    resource_name: str | None = None,
+    permission_id: str | int | None = None,
+    resource_type: str | None = None,
 ) -> permission_types.Permission:
     """
     This is the async version of `permission.get_permission`.
     """
+    name = _construct_name(
+        name=name,
+        resource_name=resource_name,
+        permission_id=permission_id,
+        resource_type=resource_type,
+    )
     return await permission_types.Permission.get_async(name=name, client=client)
```

## google/generativeai/responder.py

```diff
@@ -20,14 +20,55 @@
 from typing import Any, Callable, Union
 from typing_extensions import TypedDict
 
 import pydantic
 
 from google.ai import generativelanguage as glm
 
+Type = glm.Type
+
+TypeOptions = Union[int, str, Type]
+
+_TYPE_TYPE: dict[TypeOptions, Type] = {
+    Type.TYPE_UNSPECIFIED: Type.TYPE_UNSPECIFIED,
+    0: Type.TYPE_UNSPECIFIED,
+    "type_unspecified": Type.TYPE_UNSPECIFIED,
+    "unspecified": Type.TYPE_UNSPECIFIED,
+    Type.STRING: Type.STRING,
+    1: Type.STRING,
+    "type_string": Type.STRING,
+    "string": Type.STRING,
+    Type.NUMBER: Type.NUMBER,
+    2: Type.NUMBER,
+    "type_number": Type.NUMBER,
+    "number": Type.NUMBER,
+    Type.INTEGER: Type.INTEGER,
+    3: Type.INTEGER,
+    "type_integer": Type.INTEGER,
+    "integer": Type.INTEGER,
+    Type.BOOLEAN: Type.BOOLEAN,
+    4: Type.INTEGER,
+    "type_boolean": Type.BOOLEAN,
+    "boolean": Type.BOOLEAN,
+    Type.ARRAY: Type.ARRAY,
+    5: Type.ARRAY,
+    "type_array": Type.ARRAY,
+    "array": Type.ARRAY,
+    Type.OBJECT: Type.OBJECT,
+    6: Type.OBJECT,
+    "type_object": Type.OBJECT,
+    "object": Type.OBJECT,
+}
+
+
+def to_type(x: TypeOptions) -> Type:
+    if isinstance(x, str):
+        x = x.lower()
+    return _TYPE_TYPE[x]
+
 
 def _generate_schema(
     f: Callable[..., Any],
     *,
     descriptions: Mapping[str, str] | None = None,
     required: Sequence[str] | None = None,
 ) -> dict[str, Any]:
@@ -111,23 +152,26 @@
                 )
             )
         ]
     schema = dict(name=f.__name__, description=f.__doc__, parameters=parameters)
     return schema
 
 
-def _rename_schema_fields(schema):
+def _rename_schema_fields(schema: dict[str, Any]):
     if schema is None:
         return schema
 
     schema = schema.copy()
 
     type_ = schema.pop("type", None)
     if type_ is not None:
-        schema["type_"] = type_.upper()
+        schema["type_"] = type_
+    type_ = schema.get("type_", None)
+    if type_ is not None:
+        schema["type_"] = to_type(type_)
 
     format_ = schema.pop("format", None)
     if format_ is not None:
         schema["format_"] = format_
 
     items = schema.pop("items", None)
     if items is not None:
@@ -143,34 +187,34 @@
 class FunctionDeclaration:
     def __init__(self, *, name: str, description: str, parameters: dict[str, Any] | None = None):
         """A  class wrapping a `glm.FunctionDeclaration`, describes a function for `genai.GenerativeModel`'s `tools`."""
         self._proto = glm.FunctionDeclaration(
             name=name, description=description, parameters=_rename_schema_fields(parameters)
         )
 
-        @property
-        def name(self) -> str:
-            return self._proto.name
-
-        @property
-        def description(self) -> str:
-            return self._proto.description
-
-        @property
-        def parameters(self) -> glm.Schema:
-            return self._proto.parameters
-
-        @classmethod
-        def from_proto(cls, proto) -> FunctionDeclaration:
-            self = cls(name="", description="", parameters={})
-            self._proto = proto
-            return self
+    @property
+    def name(self) -> str:
+        return self._proto.name
+
+    @property
+    def description(self) -> str:
+        return self._proto.description
+
+    @property
+    def parameters(self) -> glm.Schema:
+        return self._proto.parameters
+
+    @classmethod
+    def from_proto(cls, proto) -> FunctionDeclaration:
+        self = cls(name="", description="", parameters={})
+        self._proto = proto
+        return self
 
-        def to_proto(self) -> glm.FunctionDeclaration:
-            return self._proto
+    def to_proto(self) -> glm.FunctionDeclaration:
+        return self._proto
 
     @staticmethod
     def from_function(function: Callable[..., Any], descriptions: dict[str, str] | None = None):
         """Builds a `CallableFunctionDeclaration` from a python function.
 
         The function should have type annotations.
```

## google/generativeai/version.py

```diff
@@ -10,8 +10,8 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
-__version__ = "0.5.2"
+__version__ = "0.5.3"
```

## google/generativeai/types/content_types.py

```diff
@@ -1,7 +1,22 @@
+# Copyright 2024 Google LLC
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
 from __future__ import annotations
 
 from collections.abc import Iterable, Mapping, Sequence
 import io
 import inspect
 import mimetypes
 import typing
@@ -296,15 +311,20 @@
             # of parts, not a list of contents, so fall back to `to_content`.
             pass
 
     contents = [to_content(contents)]
     return contents
 
 
-def _generate_schema(
+def _schema_for_class(cls: TypedDict) -> dict[str, Any]:
+    schema = _build_schema("dummy", {"dummy": (cls, pydantic.Field())})
+    return schema["properties"]["dummy"]
+
+
+def _schema_for_function(
     f: Callable[..., Any],
     *,
     descriptions: Mapping[str, str] | None = None,
     required: Sequence[str] | None = None,
 ) -> dict[str, Any]:
     """Generates the OpenAPI Schema for a python function.
 
@@ -319,60 +339,44 @@
             inferred from `f`.
 
     Returns:
         dict[str, Any]: The OpenAPI Schema for the function `f` in JSON format.
     """
     if descriptions is None:
         descriptions = {}
-    if required is None:
-        required = []
     defaults = dict(inspect.signature(f).parameters)
-    fields_dict = {
-        name: (
-            # 1. We infer the argument type here: use Any rather than None so
-            # it will not try to auto-infer the type based on the default value.
-            (param.annotation if param.annotation != inspect.Parameter.empty else Any),
-            pydantic.Field(
-                # 2. We do not support default values for now.
-                # default=(
-                #     param.default if param.default != inspect.Parameter.empty
-                #     else None
-                # ),
-                # 3. We support user-provided descriptions.
-                description=descriptions.get(name, None),
-            ),
-        )
-        for name, param in defaults.items()
-        # We do not support *args or **kwargs
-        if param.kind
-        in (
+
+    fields_dict = {}
+    for name, param in defaults.items():
+        if param.kind in (
             inspect.Parameter.POSITIONAL_OR_KEYWORD,
             inspect.Parameter.KEYWORD_ONLY,
             inspect.Parameter.POSITIONAL_ONLY,
-        )
-    }
-    parameters = pydantic.create_model(f.__name__, **fields_dict).schema()
-    # Postprocessing
-    # 4. Suppress unnecessary title generation:
-    #    * https://github.com/pydantic/pydantic/issues/1051
-    #    * http://cl/586221780
-    parameters.pop("title", None)
-    for name, function_arg in parameters.get("properties", {}).items():
-        function_arg.pop("title", None)
-        annotation = defaults[name].annotation
-        # 5. Nullable fields:
-        #     * https://github.com/pydantic/pydantic/issues/1270
-        #     * https://stackoverflow.com/a/58841311
-        #     * https://github.com/pydantic/pydantic/discussions/4872
-        if typing.get_origin(annotation) is typing.Union and type(None) in typing.get_args(
-            annotation
         ):
-            function_arg["nullable"] = True
+            # We do not support default values for now.
+            # default=(
+            #     param.default if param.default != inspect.Parameter.empty
+            #     else None
+            # ),
+            field = pydantic.Field(
+                # We support user-provided descriptions.
+                description=descriptions.get(name, None)
+            )
+
+            # 1. We infer the argument type here: use Any rather than None so
+            # it will not try to auto-infer the type based on the default value.
+            if param.annotation != inspect.Parameter.empty:
+                fields_dict[name] = param.annotation, field
+            else:
+                fields_dict[name] = Any, field
+
+    parameters = _build_schema(f.__name__, fields_dict)
+
     # 6. Annotate required fields.
-    if required:
+    if required is not None:
         # We use the user-provided "required" fields if specified.
         parameters["required"] = required
     else:
         # Otherwise we infer it from the function signature.
         parameters["required"] = [
             k
             for k in defaults
@@ -383,17 +387,120 @@
                     inspect.Parameter.POSITIONAL_OR_KEYWORD,
                     inspect.Parameter.KEYWORD_ONLY,
                     inspect.Parameter.POSITIONAL_ONLY,
                 )
             )
         ]
     schema = dict(name=f.__name__, description=f.__doc__, parameters=parameters)
+
     return schema
 
 
+def _build_schema(fname, fields_dict):
+    parameters = pydantic.create_model(fname, **fields_dict).schema()
+    defs = parameters.pop("$defs", {})
+    # flatten the defs
+    for name, value in defs.items():
+        unpack_defs(value, defs)
+    unpack_defs(parameters, defs)
+
+    # 5. Nullable fields:
+    #     * https://github.com/pydantic/pydantic/issues/1270
+    #     * https://stackoverflow.com/a/58841311
+    #     * https://github.com/pydantic/pydantic/discussions/4872
+    convert_to_nullable(parameters)
+    add_object_type(parameters)
+    # Postprocessing
+    # 4. Suppress unnecessary title generation:
+    #    * https://github.com/pydantic/pydantic/issues/1051
+    #    * http://cl/586221780
+    strip_titles(parameters)
+    return parameters
+
+
+def unpack_defs(schema, defs):
+    properties = schema["properties"]
+    for name, value in properties.items():
+        ref_key = value.get("$ref", None)
+        if ref_key is not None:
+            ref = defs[ref_key.split("defs/")[-1]]
+            unpack_defs(ref, defs)
+            properties[name] = ref
+            continue
+
+        anyof = value.get("anyOf", None)
+        if anyof is not None:
+            for i, atype in enumerate(anyof):
+                ref_key = atype.get("$ref", None)
+                if ref_key is not None:
+                    ref = defs[ref_key.split("defs/")[-1]]
+                    unpack_defs(ref, defs)
+                    anyof[i] = ref
+            continue
+
+        items = value.get("items", None)
+        if items is not None:
+            ref_key = items.get("$ref", None)
+            if ref_key is not None:
+                ref = defs[ref_key.split("defs/")[-1]]
+                unpack_defs(ref, defs)
+                value["items"] = ref
+                continue
+
+
+def strip_titles(schema):
+    title = schema.pop("title", None)
+
+    properties = schema.get("properties", None)
+    if properties is not None:
+        for name, value in properties.items():
+            strip_titles(value)
+
+    items = schema.get("items", None)
+    if items is not None:
+        strip_titles(items)
+
+
+def add_object_type(schema):
+    properties = schema.get("properties", None)
+    if properties is not None:
+        schema.pop("required", None)
+        schema["type"] = "object"
+        for name, value in properties.items():
+            add_object_type(value)
+
+    items = schema.get("items", None)
+    if items is not None:
+        add_object_type(items)
+
+
+def convert_to_nullable(schema):
+    anyof = schema.pop("anyOf", None)
+    if anyof is not None:
+        if len(anyof) != 2:
+            raise ValueError("Type Unions are not supported (except for Optional)")
+        a, b = anyof
+        if a == {"type": "null"}:
+            schema.update(b)
+        elif b == {"type": "null"}:
+            schema.update(a)
+        else:
+            raise ValueError("Type Unions are not supported (except for Optional)")
+        schema["nullable"] = True
+
+    properties = schema.get("properties", None)
+    if properties is not None:
+        for name, value in properties.items():
+            convert_to_nullable(value)
+
+    items = schema.get("items", None)
+    if items is not None:
+        convert_to_nullable(items)
+
+
 def _rename_schema_fields(schema):
     if schema is None:
         return schema
 
     schema = schema.copy()
 
     type_ = schema.pop("type", None)
@@ -456,15 +563,15 @@
         This method does not yet build a schema for `TypedDict`, that would allow you to specify the dictionary
         contents. But you can build these manually.
         """
 
         if descriptions is None:
             descriptions = {}
 
-        schema = _generate_schema(function, descriptions=descriptions)
+        schema = _schema_for_function(function, descriptions=descriptions)
 
         return CallableFunctionDeclaration(**schema, function=function)
 
 
 StructType = dict[str, "ValueType"]
 ValueType = Union[float, str, bool, StructType, list["ValueType"], None]
```

## google/generativeai/types/file_types.py

```diff
@@ -55,21 +55,21 @@
         return self._proto.update_time
 
     @property
     def expiration_time(self) -> datetime.datetime:
         return self._proto.expiration_time
 
     @property
-    def update_time(self) -> datetime.datetime:
-        return self._proto.update_time
-
-    @property
     def sha256_hash(self) -> bytes:
         return self._proto.sha256_hash
 
     @property
     def uri(self) -> str:
         return self._proto.uri
 
+    @property
+    def state(self) -> glm.File.State:
+        return self._proto.state
+
     def delete(self):
         client = get_default_file_client()
         client.delete_file(name=self.name)
```

## google/generativeai/types/generation_types.py

```diff
@@ -13,26 +13,31 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
 import collections
 import contextlib
 import sys
-from collections.abc import Iterable, AsyncIterable
+from collections.abc import Iterable, AsyncIterable, Mapping
 import dataclasses
 import itertools
+import json
+import sys
 import textwrap
-from typing import Union
+from typing import Union, Any
 from typing_extensions import TypedDict
+import types
 
 import google.protobuf.json_format
 import google.api_core.exceptions
 
 from google.ai import generativelanguage as glm
 from google.generativeai import string_utils
+from google.generativeai.types import content_types
+from google.generativeai.responder import _rename_schema_fields
 
 __all__ = [
     "AsyncGenerateContentResponse",
     "BlockedPromptException",
     "StopCandidateException",
     "IncompleteIterationError",
     "BrokenResponseError",
@@ -76,14 +81,15 @@
 class GenerationConfigDict(TypedDict, total=False):
     # TODO(markdaoust): Python 3.11+ use `NotRequired`, ref: https://peps.python.org/pep-0655/
     candidate_count: int
     stop_sequences: Iterable[str]
     max_output_tokens: int
     temperature: float
     response_mime_type: str
+    response_schema: glm.Schema | Mapping[str, Any]  # fmt: off
 
 
 @dataclasses.dataclass
 class GenerationConfig:
     """A simple dataclass used to configure the generation parameters of `GenerativeModel.generate_content`.
 
     Attributes:
@@ -142,38 +148,74 @@
 
         response_mime_type:
             Optional. Output response mimetype of the generated candidate text.
 
             Supported mimetype:
                 `text/plain`: (default) Text output.
                 `application/json`: JSON response in the candidates.
+
+        response_schema:
+            Optional. Specifies the format of the JSON requested if response_mime_type is
+            `application/json`.
     """
 
     candidate_count: int | None = None
     stop_sequences: Iterable[str] | None = None
     max_output_tokens: int | None = None
     temperature: float | None = None
     top_p: float | None = None
     top_k: int | None = None
     response_mime_type: str | None = None
+    response_schema: glm.Schema | Mapping[str, Any] | None = None
 
 
 GenerationConfigType = Union[glm.GenerationConfig, GenerationConfigDict, GenerationConfig]
 
 
+def _normalize_schema(generation_config):
+    # Convert response_schema to glm.Schema for request
+    response_schema = generation_config.get("response_schema", None)
+    if response_schema is None:
+        return
+
+    if isinstance(response_schema, glm.Schema):
+        return
+
+    if isinstance(response_schema, type):
+        response_schema = content_types._schema_for_class(response_schema)
+    elif isinstance(response_schema, types.GenericAlias):
+        if not str(response_schema).startswith("list["):
+            raise ValueError(
+                f"Could not understand {response_schema}, expected: `int`, `float`, `str`, `bool`, "
+                "`typing_extensions.TypedDict`, `dataclass`, or `list[...]`"
+            )
+        response_schema = content_types._schema_for_class(response_schema)
+
+    response_schema = _rename_schema_fields(response_schema)
+    generation_config["response_schema"] = glm.Schema(response_schema)
+
+
 def to_generation_config_dict(generation_config: GenerationConfigType):
     if generation_config is None:
         return {}
     elif isinstance(generation_config, glm.GenerationConfig):
-        return type(generation_config).to_dict(generation_config)  # pytype: disable=attribute-error
+        schema = generation_config.response_schema
+        generation_config = type(generation_config).to_dict(
+            generation_config
+        )  # pytype: disable=attribute-error
+        generation_config["response_schema"] = schema
+        return generation_config
     elif isinstance(generation_config, GenerationConfig):
         generation_config = dataclasses.asdict(generation_config)
+        _normalize_schema(generation_config)
         return {key: value for key, value in generation_config.items() if value is not None}
     elif hasattr(generation_config, "keys"):
-        return dict(generation_config)
+        generation_config = dict(generation_config)
+        _normalize_schema(generation_config)
+        return generation_config
     else:
         raise TypeError(
             "Did not understand `generation_config`, expected a `dict` or"
             f" `GenerationConfig`\nGot type: {type(generation_config)}\nValue:"
             f" {generation_config}"
         )
 
@@ -246,14 +288,15 @@
 
     return glm.Candidate(
         index=index,
         content=_join_contents([c.content for c in candidates]),
         finish_reason=candidates[-1].finish_reason,
         safety_ratings=_join_safety_ratings_lists([c.safety_ratings for c in candidates]),
         citation_metadata=_join_citation_metadatas([c.citation_metadata for c in candidates]),
+        token_count=candidates[-1].token_count,
     )
 
 
 def _join_candidate_lists(candidate_lists: Iterable[list[glm.Candidate]]):
     # Assuming that is a candidate ends, it is no longer returned in the list of
     # candidates and that's why candidates have an index
     candidates = collections.defaultdict(list)
@@ -272,17 +315,19 @@
     prompt_feedbacks: Iterable[glm.GenerateContentResponse.PromptFeedback],
 ):
     # Always return the first prompt feedback.
     return next(iter(prompt_feedbacks))
 
 
 def _join_chunks(chunks: Iterable[glm.GenerateContentResponse]):
+    chunks = tuple(chunks)
     return glm.GenerateContentResponse(
         candidates=_join_candidate_lists(c.candidates for c in chunks),
         prompt_feedback=_join_prompt_feedbacks(c.prompt_feedback for c in chunks),
+        usage_metadata=chunks[-1].usage_metadata,
     )
 
 
 _INCOMPLETE_ITERATION_MESSAGE = """\
 Please let the response complete iteration before accessing the final accumulated
 attributes (or call `response.resolve()`)"""
 
@@ -369,21 +414,29 @@
             )
         return parts[0].text
 
     @property
     def prompt_feedback(self):
         return self._result.prompt_feedback
 
+    @property
+    def usage_metadata(self):
+        return self._result.usage_metadata
+
     def __str__(self) -> str:
         if self._done:
             _iterator = "None"
         else:
             _iterator = f"<{self._iterator.__class__.__name__}>"
 
-        _result = f"glm.GenerateContentResponse({type(self._result).to_dict(self._result)})"
+        as_dict = type(self._result).to_dict(self._result)
+        json_str = json.dumps(as_dict, indent=2)
+
+        _result = f"glm.GenerateContentResponse({json_str})"
+        _result = _result.replace("\n", "\n                    ")
 
         if self._error:
             _error = f",\nerror=<{self._error.__class__.__name__}> {self._error}"
         else:
             _error = ""
 
         return (
```

## google/generativeai/types/model_types.py

```diff
@@ -25,14 +25,15 @@
 
 from typing import Any, Iterable, Union
 
 import urllib.request
 from typing_extensions import TypedDict
 
 import google.ai.generativelanguage as glm
+from google.generativeai.types import permission_types
 from google.generativeai import string_utils
 
 
 __all__ = [
     "Model",
     "ModelNameOptions",
     "AnyModelNameOptions",
@@ -43,14 +44,25 @@
     "TunedModelState",
 ]
 
 TunedModelState = glm.TunedModel.State
 
 TunedModelStateOptions = Union[None, str, int, TunedModelState]
 
+_TUNED_MODEL_VALID_NAME = r"[a-z](([a-z0-9-]{0,61}[a-z0-9])?)$"
+TUNED_MODEL_NAME_ERROR_MSG = """The `name` must consist of alphanumeric characters (or -) and be at most 63 characters; The name you entered:
+\tlen(name)== {length}
+\tname={name}
+"""
+
+
+def valid_tuned_model_name(name: str) -> bool:
+    return re.match(_TUNED_MODEL_VALID_NAME, name) is not None
+
+
 # fmt: off
 _TUNED_MODEL_STATES: dict[TunedModelStateOptions, TunedModelState] = {
     TunedModelState.ACTIVE: TunedModelState.ACTIVE,
     int(TunedModelState.ACTIVE): TunedModelState.ACTIVE,
     "active": TunedModelState.ACTIVE,
 
     TunedModelState.CREATING: TunedModelState.CREATING,
@@ -179,14 +191,18 @@
     top_p: float | None = None
     top_k: float | None = None
     state: TunedModelState = TunedModelState.STATE_UNSPECIFIED
     create_time: datetime.datetime | None = None
     update_time: datetime.datetime | None = None
     tuning_task: TuningTask | None = None
 
+    @property
+    def permissions(self) -> permission_types.Permissions:
+        return permission_types.Permissions(self)
+
 
 @string_utils.prettyprint
 @dataclasses.dataclass
 class TuningTask:
     start_time: datetime.datetime | None = None
     complete_time: datetime.datetime | None = None
     snapshots: list[TuningSnapshot] = dataclasses.field(default_factory=list)
@@ -330,15 +346,15 @@
         name = name.name  # pytype: disable=attribute-error
     elif isinstance(name, str):
         name = name
     else:
         raise TypeError("Expected: str, Model, or TunedModel")
 
     if not (name.startswith("models/") or name.startswith("tunedModels/")):
-        raise ValueError("Model names should start with `models/` or `tunedModels/`, got: {name}")
+        raise ValueError(f"Model names should start with `models/` or `tunedModels/`, got: {name}")
 
     return name
 
 
 ModelsIterable = Iterable[Model]
 TunedModelsIterable = Iterable[TunedModel]
```

## google/generativeai/types/permission_types.py

```diff
@@ -11,15 +11,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
 import dataclasses
-from typing import Optional, Union, Any
+from typing import Optional, Union, Any, Iterable, AsyncIterable
+import re
 
 import google.ai.generativelanguage as glm
 
 from google.protobuf import field_mask_pb2
 
 from google.generativeai.client import get_dafault_permission_client
 from google.generativeai.client import get_dafault_permission_async_client
@@ -61,27 +62,35 @@
     2: Role.WRITER,
     "writer": Role.WRITER,
     Role.READER: Role.READER,
     3: Role.READER,
     "reader": Role.READER,
 }
 
+_VALID_PERMISSION_ID = r"permissions/([a-z0-9]+)$"
+INVALID_PERMISSION_ID_MSG = """`permission_id` must follow the pattern: `permissions/<id>` and must \
+consist of only alphanumeric characters. Got: `{permission_id}` instead."""
+
 
 def to_grantee_type(x: GranteeTypeOptions) -> GranteeType:
     if isinstance(x, str):
         x = x.lower()
     return _GRANTEE_TYPE[x]
 
 
 def to_role(x: RoleOptions) -> Role:
     if isinstance(x, str):
         x = x.lower()
     return _ROLE[x]
 
 
+def valid_id(name: str) -> bool:
+    return re.match(_VALID_PERMISSION_ID, name) is not None
+
+
 @string_utils.prettyprint
 @dataclasses.dataclass
 class Permission:
     """
     A permission to access a resource.
     """
 
@@ -149,15 +158,15 @@
 
         for path in updates.keys():
             field_mask.paths.append(path)
         for path, value in updates.items():
             self._apply_update(path, value)
 
         update_request = glm.UpdatePermissionRequest(
-            permission=self.to_dict(), update_mask=field_mask
+            permission=self._to_proto(), update_mask=field_mask
         )
         client.update_permission(request=update_request)
         return self
 
     async def update_async(
         self,
         updates: dict[str, Any],
@@ -179,26 +188,29 @@
 
         for path in updates.keys():
             field_mask.paths.append(path)
         for path, value in updates.items():
             self._apply_update(path, value)
 
         update_request = glm.UpdatePermissionRequest(
-            permission=self.to_dict(), update_mask=field_mask
+            permission=self._to_proto(), update_mask=field_mask
         )
         await client.update_permission(request=update_request)
         return self
 
+    def _to_proto(self) -> glm.Permission:
+        return glm.Permission(
+            name=self.name,
+            role=self.role,
+            grantee_type=self.grantee_type,
+            email_address=self.email_address,
+        )
+
     def to_dict(self) -> dict[str, Any]:
-        return {
-            "name": self.name,
-            "role": self.role,
-            "grantee_type": self.grantee_type,
-            "email_address": self.email_address,
-        }
+        return dataclasses.asdict(self)
 
     @classmethod
     def get(
         cls,
         name: str,
         client: glm.PermissionServiceClient | None = None,
     ) -> Permission:
@@ -229,7 +241,182 @@
         """
         if client is None:
             client = get_dafault_permission_async_client()
         get_perm_request = glm.GetPermissionRequest(name=name)
         get_perm_response = await client.get_permission(request=get_perm_request)
         get_perm_response = type(get_perm_response).to_dict(get_perm_response)
         return cls(**get_perm_response)
+
+
+class Permissions:
+    def __init__(self, parent):
+        if isinstance(parent, str):
+            self._parent = parent
+        else:
+            self._parent = parent.name
+
+    @property
+    def parent(self):
+        return self._parent
+
+    def _make_create_permission_request(
+        self,
+        role: RoleOptions,
+        grantee_type: Optional[GranteeTypeOptions] = None,
+        email_address: Optional[str] = None,
+    ) -> glm.CreatePermissionRequest:
+        role = to_role(role)
+
+        if grantee_type:
+            grantee_type = to_grantee_type(grantee_type)
+
+        if email_address and grantee_type == GranteeType.EVERYONE:
+            raise ValueError(
+                f"Cannot limit access for: `{email_address}` when `grantee_type` is set to `EVERYONE`."
+            )
+
+        if not email_address and grantee_type != GranteeType.EVERYONE:
+            raise ValueError(
+                f"`email_address` must be specified unless `grantee_type` is set to `EVERYONE`."
+            )
+
+        permission = glm.Permission(
+            role=role,
+            grantee_type=grantee_type,
+            email_address=email_address,
+        )
+        return glm.CreatePermissionRequest(
+            parent=self.parent,
+            permission=permission,
+        )
+
+    def create(
+        self,
+        role: RoleOptions,
+        grantee_type: Optional[GranteeTypeOptions] = None,
+        email_address: Optional[str] = None,
+        client: glm.PermissionServiceClient | None = None,
+    ) -> Permission:
+        """
+        Create a new permission on a resource (self).
+
+        Args:
+            parent: The resource name of the parent resource in which the permission will be listed.
+            role: role that will be granted by the permission.
+            grantee_type: The type of the grantee for the permission.
+            email_address: The email address of the grantee.
+
+        Returns:
+            `Permission` object with specified parent, role, grantee type, and email address.
+
+        Raises:
+            ValueError: When email_address is specified and grantee_type is set to EVERYONE.
+            ValueError: When email_address is not specified and grantee_type is not set to EVERYONE.
+        """
+        if client is None:
+            client = get_dafault_permission_client()
+
+        request = self._make_create_permission_request(
+            role=role, grantee_type=grantee_type, email_address=email_address
+        )
+        permission_response = client.create_permission(request=request)
+        permission_response = type(permission_response).to_dict(permission_response)
+        return Permission(**permission_response)
+
+    async def create_async(
+        self,
+        role: RoleOptions,
+        grantee_type: Optional[GranteeTypeOptions] = None,
+        email_address: Optional[str] = None,
+        client: glm.PermissionServiceAsyncClient | None = None,
+    ) -> Permission:
+        """
+        This is the async version of `PermissionAdapter.create_permission`.
+        """
+        if client is None:
+            client = get_dafault_permission_async_client()
+
+        request = self._make_create_permission_request(
+            role=role, grantee_type=grantee_type, email_address=email_address
+        )
+        permission_response = await client.create_permission(request=request)
+        permission_response = type(permission_response).to_dict(permission_response)
+        return Permission(**permission_response)
+
+    def list(
+        self,
+        page_size: Optional[int] = None,
+        client: glm.PermissionServiceClient | None = None,
+    ) -> Iterable[Permission]:
+        """
+        List `Permission`s enforced on a resource (self).
+
+        Args:
+            parent: The resource name of the parent resource in which the permission will be listed.
+            page_size: The maximum number of permissions to return (per page). The service may return fewer permissions.
+
+        Returns:
+            Paginated list of `Permission` objects.
+        """
+        if client is None:
+            client = get_dafault_permission_client()
+
+        request = glm.ListPermissionsRequest(
+            parent=self.parent, page_size=page_size  # pytype: disable=attribute-error
+        )
+        for permission in client.list_permissions(request):
+            permission = type(permission).to_dict(permission)
+            yield Permission(**permission)
+
+    async def list_async(
+        self,
+        page_size: Optional[int] = None,
+        client: glm.PermissionServiceAsyncClient | None = None,
+    ) -> AsyncIterable[Permission]:
+        """
+        This is the async version of `PermissionAdapter.list_permissions`.
+        """
+        if client is None:
+            client = get_dafault_permission_async_client()
+
+        request = glm.ListPermissionsRequest(
+            parent=self.parent, page_size=page_size  # pytype: disable=attribute-error
+        )
+        async for permission in await client.list_permissions(request):
+            permission = type(permission).to_dict(permission)
+            yield Permission(**permission)
+
+    def transfer_ownership(
+        self,
+        email_address: str,
+        client: glm.PermissionServiceClient | None = None,
+    ) -> None:
+        """
+        Transfer ownership of a resource (self) to a new owner.
+
+        Args:
+            name: Name of the resource to transfer ownership.
+            email_address: Email address of the new owner.
+        """
+        if self.parent.startswith("corpora"):
+            raise NotImplementedError("Can'/t transfer_ownership for a Corpus")
+        if client is None:
+            client = get_dafault_permission_client()
+        transfer_request = glm.TransferOwnershipRequest(
+            name=self.parent, email_address=email_address  # pytype: disable=attribute-error
+        )
+        return client.transfer_ownership(request=transfer_request)
+
+    async def transfer_ownership_async(
+        self,
+        email_address: str,
+        client: glm.PermissionServiceAsyncClient | None = None,
+    ) -> None:
+        """This is the async version of `PermissionAdapter.transfer_ownership`."""
+        if self.parent.startswith("corpora"):
+            raise NotImplementedError("Can'/t transfer_ownership for a Corpus")
+        if client is None:
+            client = get_dafault_permission_async_client()
+        transfer_request = glm.TransferOwnershipRequest(
+            name=self.parent, email_address=email_address  # pytype: disable=attribute-error
+        )
+        return await client.transfer_ownership(request=transfer_request)
```

## google/generativeai/types/retriever_types.py

```diff
@@ -12,29 +12,25 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from __future__ import annotations
 
 import datetime
 import re
-import string
 import abc
 import dataclasses
 from typing import Any, AsyncIterable, Optional, Union, Iterable, Mapping
+from typing_extensions import deprecated  # type: ignore
 
 import google.ai.generativelanguage as glm
 
 from google.protobuf import field_mask_pb2
 from google.generativeai.client import get_default_retriever_client
 from google.generativeai.client import get_default_retriever_async_client
-from google.generativeai.client import get_dafault_permission_client
-from google.generativeai.client import get_dafault_permission_async_client
 from google.generativeai import string_utils
-from google.generativeai.types import safety_types
-from google.generativeai.types import citation_types
 from google.generativeai.types import permission_types
 from google.generativeai.types.model_types import idecode_time
 from google.generativeai.utils import flatten_update_paths
 
 _VALID_NAME = r"[a-z0-9]([a-z0-9-]{0,38}[a-z0-9])$"
 NAME_ERROR_MSG = """The `name` must consist of alphanumeric characters (or -) and be 40 or fewer characters; or be empty. The name you entered:
     len(name)== {length}
@@ -251,14 +247,18 @@
     """
 
     name: str
     display_name: str
     create_time: datetime.datetime
     update_time: datetime.datetime
 
+    @property
+    def permissions(self) -> permission_types.Permissions:
+        return permission_types.Permissions(self)
+
     def create_document(
         self,
         name: str | None = None,
         display_name: str | None = None,
         custom_metadata: Iterable[CustomMetadata] | None = None,
         client: glm.RetrieverServiceClient | None = None,
         request_options: dict[str, Any] | None = None,
@@ -654,136 +654,68 @@
         request = glm.ListDocumentsRequest(
             parent=self.name,
             page_size=page_size,
         )
         async for doc in await client.list_documents(request, **request_options):
             yield decode_document(doc)
 
-    def _make_create_permission_request(
-        self,
-        role: permission_types.RoleOptions,
-        grantee_type: Optional[permission_types.GranteeTypeOptions] = None,
-        email_address: Optional[str] = None,
-    ) -> glm.CreatePermissionRequest:
-        role = permission_types.to_role(role)
-
-        if grantee_type:
-            grantee_type = permission_types.to_grantee_type(grantee_type)
-
-        if email_address and grantee_type == permission_types.GranteeType.EVERYONE:
-            raise ValueError(
-                f"Cannot limit access for: `{email_address}` when `grantee_type` is set to `EVERYONE`."
-            )
-
-        if not email_address and grantee_type != permission_types.GranteeType.EVERYONE:
-            raise ValueError(
-                f"`email_address` must be specified unless `grantee_type` is set to `EVERYONE`."
-            )
-
-        permission = glm.Permission(
-            role=role,
-            grantee_type=grantee_type,
-            email_address=email_address,
-        )
-        return glm.CreatePermissionRequest(
-            parent=self.name,
-            permission=permission,
-        )
-
+    # PERMISSIONS STUBS: ..deprecated:: >0.5.2
+    @deprecated(
+        "`Corpus.create_permission` is deprecated and will be removed in a future release. \
+            Corpus permissions are now managed using the `permissions` property. Use `Corpus.permissions.create` instead."
+    )
     def create_permission(
         self,
         role: permission_types.RoleOptions,
         grantee_type: Optional[permission_types.GranteeTypeOptions] = None,
         email_address: Optional[str] = None,
         client: glm.PermissionServiceClient | None = None,
     ) -> permission_types.Permission:
-        """
-        Create a new permission on a resource (self).
-
-        Args:
-            parent: The resource name of the parent resource in which the permission will be listed.
-            role: role that will be granted by the permission.
-            grantee_type: The type of the grantee for the permission.
-            email_address: The email address of the grantee.
-
-        Returns:
-            `permission_types.Permission` object with specified parent, role, grantee type, and email address.
-
-        Raises:
-            ValueError: When email_address is specified and grantee_type is set to EVERYONE.
-            ValueError: When email_address is not specified and grantee_type is not set to EVERYONE.
-        """
-        if client is None:
-            client = get_dafault_permission_client()
-
-        request = self._make_create_permission_request(
-            role=role, grantee_type=grantee_type, email_address=email_address
+        return self.permissions.create(
+            role=role, grantee_type=grantee_type, email_address=email_address, client=client
         )
-        permission_response = client.create_permission(request=request)
-        permission_response = type(permission_response).to_dict(permission_response)
-        return permission_types.Permission(**permission_response)
 
+    @deprecated(
+        "`Corpus.create_permission_async` is deprecated and will be removed in a future release. \
+            Corpus permissions are now managed using the `permissions` property. Use `Corpus.permissions.create_async` instead."
+    )
     async def create_permission_async(
         self,
         role: permission_types.RoleOptions,
         grantee_type: Optional[permission_types.GranteeTypeOptions] = None,
         email_address: Optional[str] = None,
         client: glm.PermissionServiceAsyncClient | None = None,
     ) -> permission_types.Permission:
-        """
-        This is the async version of `Corpus.create_permission`.
-        """
-        if client is None:
-            client = get_dafault_permission_async_client()
-
-        request = self._make_create_permission_request(
-            role=role, grantee_type=grantee_type, email_address=email_address
+        return await self.permissions.create_async(
+            role=role, grantee_type=grantee_type, email_address=email_address, client=client
         )
-        permission_response = await client.create_permission(request=request)
-        permission_response = type(permission_response).to_dict(permission_response)
-        return permission_types.Permission(**permission_response)
 
+    @deprecated(
+        "`Corpus.list_permission` is deprecated and will be removed in a future release. \
+            Corpus permissions are now managed using the `permissions` property. Use `Corpus.permissions.list` instead."
+    )
     def list_permissions(
         self,
         page_size: Optional[int] = None,
         client: glm.PermissionServiceClient | None = None,
     ) -> Iterable[permission_types.Permission]:
-        """
-        List `permission_types.Permission`s enforced on a resource (self).
-
-        Args:
-            parent: The resource name of the parent resource in which the permission will be listed.
-            page_size: The maximum number of permissions to return (per page). The service may return fewer permissions.
-
-        Returns:
-            Paginated list of `permission_types.Permission` objects.
-        """
-        if client is None:
-            client = get_dafault_permission_client()
-
-        request = glm.ListPermissionsRequest(parent=self.name, page_size=page_size)
-        for permission in client.list_permissions(request):
-            permission = type(permission).to_dict(permission)
-            yield permission_types.Permission(**permission)
+        return self.permissions.list(page_size=page_size, client=client)
 
+    @deprecated(
+        "`Corpus.list_permission_async` is deprecated and will be removed in a future release. \
+            Corpus permissions are now managed using the `permissions` property. Use `Corpus.permissions.list_async` instead."
+    )
     async def list_permissions_async(
         self,
         page_size: Optional[int] = None,
         client: glm.PermissionServiceAsyncClient | None = None,
     ) -> AsyncIterable[permission_types.Permission]:
-        """
-        This is the async version of `Corpus.list_permissions`.
-        """
-        if client is None:
-            client = get_dafault_permission_async_client()
+        return self.permissions.list_async(page_size=page_size, client=client)
 
-        request = glm.ListPermissionsRequest(parent=self.name, page_size=page_size)
-        async for permission in await client.list_permissions(request):
-            permission = type(permission).to_dict(permission)
-            yield permission_types.Permission(**permission)
+    # PERMISSIONS STUBS END
 
     def to_dict(self) -> dict[str, Any]:
         result = {"name": self.name, "display_name": self.display_name}
         return result
 
 
 def decode_document(document):
```

## Comparing `google_generativeai-0.5.2.dist-info/LICENSE` & `google_generativeai-0.5.3.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `google_generativeai-0.5.2.dist-info/METADATA` & `google_generativeai-0.5.3.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: google-generativeai
-Version: 0.5.2
+Version: 0.5.3
 Summary: Google Generative AI High level API client library and tools.
 Home-page: https://github.com/google/generative-ai-python
 Author: Google LLC
 Author-email: googleapis-packages@google.com
 License: Apache 2.0
 Platform: Posix; MacOS X; Windows
 Classifier: Development Status :: 4 - Beta
@@ -17,15 +17,15 @@
 Classifier: Programming Language :: Python :: 3.11
 Classifier: Programming Language :: Python :: 3.12
 Classifier: Operating System :: OS Independent
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Requires-Python: >=3.9
 Description-Content-Type: text/markdown
 License-File: LICENSE
-Requires-Dist: google-ai-generativelanguage ==0.6.2
+Requires-Dist: google-ai-generativelanguage ==0.6.3
 Requires-Dist: google-api-core
 Requires-Dist: google-api-python-client
 Requires-Dist: google-auth >=2.15.0
 Requires-Dist: protobuf
 Requires-Dist: pydantic
 Requires-Dist: tqdm
 Requires-Dist: typing-extensions
```

## Comparing `google_generativeai-0.5.2.dist-info/RECORD` & `google_generativeai-0.5.3.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,24 +1,24 @@
-google_generativeai-0.5.2-py3.11-nspkg.pth,sha256=SSGHNp7YlGakPsphxkWm0fKg3pzrfQYRQ42ZPtiPF-4,467
+google_generativeai-0.5.3-py3.11-nspkg.pth,sha256=SSGHNp7YlGakPsphxkWm0fKg3pzrfQYRQ42ZPtiPF-4,467
 google/generativeai/__init__.py,sha256=F-xuWzgEZq1aOXnYZJ7pby8qZfistnRKLVOStvXmJR0,2806
-google/generativeai/answer.py,sha256=r7q3Q7tVAJPPxJThUtZXMU_csT2mp-y1ZlPdINbwf7k,13383
-google/generativeai/client.py,sha256=A9jIlT11R8czQCx0l42MHRibS1DY4dEcFHV-4dsPgA4,12609
+google/generativeai/answer.py,sha256=nXPSDmWuMwjujHSyZlg2e18jZvJbhNClqUgBaXuONEg,13510
+google/generativeai/client.py,sha256=mD0ouy5940GMlyYnVyKxuf-m8zaLhJfOuSNp4lXKJOQ,12452
 google/generativeai/discuss.py,sha256=FYPRK8WRRryjSzXm0xRmPEGa7F0KxWX624aSAcXrowA,20937
 google/generativeai/embedding.py,sha256=4GlNXOXJQ_evVSrla2qoWA8KuaJRaOb5Kt_99lRXgDo,11583
-google/generativeai/files.py,sha256=MTWTnQszUsDxtMOsQ9qliS_oG8oKglgxrDqIrvUzvlA,2226
-google/generativeai/generative_models.py,sha256=EqpAQVjjEdfZ9mCdrZgP55b6InPw32tqDubdiaSwYRM,28672
+google/generativeai/files.py,sha256=ATXXauw3V1nUuKocVpED1h_U56Ydj1NUV9kbQdQk-K0,3056
+google/generativeai/generative_models.py,sha256=o1NkgtMk8wARpwa79Byl1JuwJvJvepwkG0MX6RMFjsc,29684
 google/generativeai/models.py,sha256=dV10pXwUUAojlwA6O5LiYcUdG1tRvL9gv8fGmeAftsg,14685
 google/generativeai/operations.py,sha256=9c0BRjEwQ8sWUTs1ehJwhbn1UfsYvJvU4FiqF0Zo8HY,4900
-google/generativeai/permission.py,sha256=9iiIaVmPcFFG6dWF-tob1wx4jOCDZe7x4UoEDkp-Rn8,1410
-google/generativeai/responder.py,sha256=rgbgpsYWCg_Melc0wAbBMWIzRDc1i-VykxofC4ojoKw,15386
+google/generativeai/permission.py,sha256=rW_D2NIqgsF87bYIxQKyKpOExQjJyN9G1pjQoGh7YuQ,6141
+google/generativeai/responder.py,sha256=MzTuDA8aAmbQ8V2WTdaGowh4hEjRRyHnPqcL9G5jbZY,16475
 google/generativeai/retriever.py,sha256=FoDJhO8_oo5-8hTkLwyU_xrm97mtrFrKgGwLzsKPDPA,8542
 google/generativeai/string_utils.py,sha256=vOU4fHULo9O69vtwsZ5DNQGndEV3ZDizX3sytv0ixZg,2441
 google/generativeai/text.py,sha256=Lr08glxMDT4_6YJCZ4zOtrD6CdRnvWOGjg5z_qSrayI,13190
 google/generativeai/utils.py,sha256=d33Ia_WnNoHaCqUfq7uRZsUKuCxMAtIa1wC-P0jlbpM,976
-google/generativeai/version.py,sha256=95tlkAKMja1th4Zk83zOK9xhx724ywl8jQaa4JMAU4k,656
+google/generativeai/version.py,sha256=46zqkH1iVcs6BAwBnuieM04spkr_84-IUBYviO2yKA0,656
 google/generativeai/notebook/__init__.py,sha256=CVZrwI1B6hcVoGkVXk5M1UPPF7mpH7PfSAjjyAnVW4o,1169
 google/generativeai/notebook/argument_parser.py,sha256=akmRzKaDm-_aTGe8cy7a6zYlJpo7Cf2nMZ0FtPf_BwE,3961
 google/generativeai/notebook/cmd_line_parser.py,sha256=F2hXljhbSZCm4s13gS32_1TStZyOi8cAwKs1nmK0dvI,20532
 google/generativeai/notebook/command.py,sha256=BwpHMSSORuZfyCqVeJ9q694dPpmfL2jJrLhnbhWqAHw,1535
 google/generativeai/notebook/command_utils.py,sha256=u9F-OrVHHjjQWbCdeHY4TAIMdgseluynDWUK-CA92Ug,6270
 google/generativeai/notebook/compare_cmd.py,sha256=_S3yv9DrLicCspBlBYC4JuSCIT4vWDqf1PvQ_8O7PPI,2566
 google/generativeai/notebook/compile_cmd.py,sha256=LLGrGYCsG8PFj1DQ1Nu2NK3ihWxVeJ-ClVIM6mrRX3w,2365
@@ -52,22 +52,22 @@
 google/generativeai/notebook/lib/llmfn_post_process_cmds.py,sha256=IRZfkhzgPyN9P13zI9EmeELyF_PyJ8PAA6AwuA15MyU,8569
 google/generativeai/notebook/lib/model.py,sha256=LlgdUxiY6FwoUkNxAfwFMdkqxBvr4A4bo0YRiP5wOFA,2055
 google/generativeai/notebook/lib/prompt_utils.py,sha256=7lVWJjn0eqzhhux70XiXokqsQo7RbOVUkuGN6hepJ38,1264
 google/generativeai/notebook/lib/unique_fn.py,sha256=yo1rucNEWEtrJ2rxbAa5HCLr9XF92sHwb7EB8NXYP6Y,1487
 google/generativeai/types/__init__.py,sha256=HZJVO_R7Fz_mNFkb0yEolOThgKx3gABN3b9Os3QYuFo,1181
 google/generativeai/types/answer_types.py,sha256=u2HwQYilcXlZp2gEc0PPtScH8La67EgRDK8MlkuR_Ow,2116
 google/generativeai/types/citation_types.py,sha256=M3Nljz1VG_ortaRTlTWjDFbRPlxx1BoHm4UTmKsFhrg,1232
-google/generativeai/types/content_types.py,sha256=DPus-565BnHa0VUOoq-GMcLc47v_ERfjOPWkfghkdPs,22835
+google/generativeai/types/content_types.py,sha256=yQI_vBdHHY0bt4uCczOqqlPrjXwAQKaqiFOymW5WPeE,25991
 google/generativeai/types/discuss_types.py,sha256=dE3O2OL7Xdkk-u6Rf6iC9bITPXNgABuBj4j1V5_OsCM,6661
-google/generativeai/types/file_types.py,sha256=rbAZzvpDdd4cjWGOhTK7iEuGPDq_X5HH4Bt1epnTbq0,2011
-google/generativeai/types/generation_types.py,sha256=abryvGm9CiEDnFg8CXhzu0xtjmL-zOyt0XSu6x24PrY,18640
-google/generativeai/types/model_types.py,sha256=9riBRp_bqh0I3Ev3A0kKxYN6kjQpJrrWyME9u21S96A,11638
-google/generativeai/types/permission_types.py,sha256=-I2M4NRFKiX9xVgEIUiPni7KQBSP1uI59GOSvth1Bb4,7599
-google/generativeai/types/retriever_types.py,sha256=lVQqqIFSWwGHWLcRQ5779yM6HLoaHaQuOI3MsBZ7lSI,61705
+google/generativeai/types/file_types.py,sha256=PH079Ud6g1pfHHysMsfpgA_6fgfwf83jaGmxMb9I1a0,1996
+google/generativeai/types/generation_types.py,sha256=nfywbJDbmuiJ5pXbGYYoq3_0KkRH7of0iH-ytvlKmXE,20692
+google/generativeai/types/model_types.py,sha256=d4ecCKhpobIoSsJYh8scum3Todnubz8usHYNbJ4lZ4A,12179
+google/generativeai/types/permission_types.py,sha256=jjW5b49_9z6XMUqU5463nq2vcfDh5J8D98ZlCJNI49k,14632
+google/generativeai/types/retriever_types.py,sha256=amGsz5koamPikKKkbeXtuIDW7CtoF42tOUfWsoNVQ48,59036
 google/generativeai/types/safety_types.py,sha256=PyXtF0-l2wjh56kaYtBt_J1WHNi9rUqINjmKGdnYC2s,10362
 google/generativeai/types/text_types.py,sha256=jdXBIEV-NNx-njOqLjMiL-YJFt9bkn2T8fVkwqxG-tU,2319
-google_generativeai-0.5.2.dist-info/LICENSE,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
-google_generativeai-0.5.2.dist-info/METADATA,sha256=domPMgB8gDxb5ESbC2x4eMTlkyeY1P7otczV3Yv9u1w,3921
-google_generativeai-0.5.2.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-google_generativeai-0.5.2.dist-info/namespace_packages.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
-google_generativeai-0.5.2.dist-info/top_level.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
-google_generativeai-0.5.2.dist-info/RECORD,,
+google_generativeai-0.5.3.dist-info/LICENSE,sha256=z8d0m5b2O9McPEK1xHG_dWgUBT6EfBDz6wA0F7xSPTA,11358
+google_generativeai-0.5.3.dist-info/METADATA,sha256=ufs1v4knpgk8NU0G3s4a6rDQkBp5PpQ_7XIkGh9RjyA,3921
+google_generativeai-0.5.3.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+google_generativeai-0.5.3.dist-info/namespace_packages.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
+google_generativeai-0.5.3.dist-info/top_level.txt,sha256=_1QvSJIhFAGfxb79D6DhB7SUw2X6T4rwnz_LLrbcD3c,7
+google_generativeai-0.5.3.dist-info/RECORD,,
```

