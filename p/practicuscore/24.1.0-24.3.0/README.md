# Comparing `tmp/practicuscore-24.1.0-cp311-none-win_amd64.whl.zip` & `tmp/practicuscore-24.3.0-cp312-none-macosx_10_9_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,54 +1,66 @@
-Zip file size: 3954241 bytes, number of entries: 52
--rw-rw-rw-  2.0 fat    15234 b- defN 24-Jan-20 15:46 practicuscore/__init__.py
--rw-rw-rw-  2.0 fat    81408 b- defN 24-Jan-20 16:40 practicuscore/airflow_client.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    69828 b- defN 24-Jan-17 21:38 practicuscore/api_base.py
--rw-rw-rw-  2.0 fat    15226 b- defN 24-Jan-17 21:38 practicuscore/api_def.py
--rw-rw-rw-  2.0 fat     8912 b- defN 24-Jan-17 21:38 practicuscore/api_k8s.py
--rw-rw-rw-  2.0 fat   310272 b- defN 24-Jan-20 16:40 practicuscore/api_manager.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   102912 b- defN 24-Jan-20 16:40 practicuscore/cloud_conf.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   659968 b- defN 24-Jan-20 16:40 practicuscore/cloud_def.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   425472 b- defN 24-Jan-20 16:41 practicuscore/cloud_node.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    47616 b- defN 24-Jan-20 16:41 practicuscore/conf_center.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat     3707 b- defN 23-Nov-13 23:43 practicuscore/conf_model.py
--rw-rw-rw-  2.0 fat    48128 b- defN 24-Jan-20 16:41 practicuscore/config_def.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   215552 b- defN 24-Jan-20 16:41 practicuscore/container_helper.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat     2695 b- defN 24-Jan-17 21:38 practicuscore/core.conf
--rw-rw-rw-  2.0 fat   401408 b- defN 24-Jan-20 16:41 practicuscore/core_conf.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    66560 b- defN 24-Jan-20 16:41 practicuscore/core_context.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   332800 b- defN 24-Jan-20 16:41 practicuscore/core_def.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   561152 b- defN 24-Jan-20 16:41 practicuscore/dataprep.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat  1789952 b- defN 24-Jan-20 16:41 practicuscore/dataprep2.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   148480 b- defN 24-Jan-20 16:41 practicuscore/defined_conn.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   450048 b- defN 24-Jan-20 16:41 practicuscore/dpi.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    82944 b- defN 24-Jan-20 16:41 practicuscore/experiment_service_config.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   326656 b- defN 24-Jan-20 16:41 practicuscore/file_transfer_handler.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   126976 b- defN 24-Jan-20 16:41 practicuscore/file_transfer_mgr.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   115200 b- defN 24-Jan-20 16:41 practicuscore/instance_files_mgr.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   397312 b- defN 24-Jan-20 16:41 practicuscore/k8s_console_client.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    97280 b- defN 24-Jan-20 16:41 practicuscore/key_pair_mgr.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    60928 b- defN 24-Jan-20 16:41 practicuscore/license_api.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   317440 b- defN 24-Jan-20 16:41 practicuscore/local_file.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    97280 b- defN 24-Jan-20 16:41 practicuscore/local_file_meta_tree.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    81920 b- defN 24-Jan-20 16:41 practicuscore/model_conf_parser.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   138752 b- defN 24-Jan-20 16:41 practicuscore/model_host_client.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    82944 b- defN 24-Jan-20 16:41 practicuscore/profiler.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    88064 b- defN 24-Jan-20 16:41 practicuscore/session_mgr.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   189440 b- defN 24-Jan-20 16:42 practicuscore/snippets_helper.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   282112 b- defN 24-Jan-20 16:42 practicuscore/steps.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   182784 b- defN 24-Jan-20 16:42 practicuscore/tcp_tunnel.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    79872 b- defN 24-Jan-20 16:42 practicuscore/token_mgr.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat      254 b- defN 23-Nov-15 01:58 practicuscore/uncompiled.py
--rw-rw-rw-  2.0 fat   237056 b- defN 24-Jan-20 16:42 practicuscore/user_manager.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat       44 b- defN 22-Nov-26 02:52 practicuscore/user_node.conf
--rw-rw-rw-  2.0 fat    53248 b- defN 24-Jan-20 16:42 practicuscore/user_node_conf.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   194048 b- defN 24-Jan-20 16:42 practicuscore/util.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   254976 b- defN 24-Jan-20 16:42 practicuscore/webs_client.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    59392 b- defN 24-Jan-20 16:42 practicuscore/webs_protocol.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat    58368 b- defN 24-Jan-20 16:42 practicuscore/workflow_dply_mgr.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   224768 b- defN 24-Jan-20 16:42 practicuscore/ws.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat   428032 b- defN 24-Jan-20 16:42 practicuscore/wsm.cp311-win_amd64.pyd
--rw-rw-rw-  2.0 fat     1842 b- defN 24-Jan-20 16:42 practicuscore-24.1.0.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-Jan-20 16:42 practicuscore-24.1.0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 24-Jan-20 16:42 practicuscore-24.1.0.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     5164 b- defN 24-Jan-20 16:42 practicuscore-24.1.0.dist-info/RECORD
-52 files, 10022540 bytes uncompressed, 3945885 bytes compressed:  60.6%
+Zip file size: 4999100 bytes, number of entries: 64
+-rw-r--r--  2.0 unx     1381 b- defN 24-May-13 02:22 practicuscore/__init__.py
+-rwxr-xr-x  2.0 unx    22488 b- defN 24-May-13 02:17 practicuscore/__main__.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    63592 b- defN 24-May-13 02:16 practicuscore/addon_helper.cpython-312-darwin.so
+-rw-r--r--  2.0 unx    70868 b- defN 24-May-13 02:22 practicuscore/api_base.py
+-rw-r--r--  2.0 unx    15350 b- defN 24-May-13 02:22 practicuscore/api_def.py
+-rw-r--r--  2.0 unx    11147 b- defN 24-May-13 02:22 practicuscore/api_k8s.py
+-rwxr-xr-x  2.0 unx   356680 b- defN 24-May-13 02:17 practicuscore/api_manager.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   241000 b- defN 24-May-13 02:16 practicuscore/cli.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   122096 b- defN 24-May-13 02:17 practicuscore/cloud_conf.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx  1620480 b- defN 24-May-13 02:22 practicuscore/cloud_def.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   553984 b- defN 24-May-13 02:15 practicuscore/cloud_node.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    53096 b- defN 24-May-13 02:16 practicuscore/conf_center.cpython-312-darwin.so
+-rw-r--r--  2.0 unx     3632 b- defN 24-May-13 02:22 practicuscore/conf_model.py
+-rwxr-xr-x  2.0 unx    53376 b- defN 24-May-13 02:15 practicuscore/config_def.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    67832 b- defN 24-May-13 02:16 practicuscore/conn_helper.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   273696 b- defN 24-May-13 02:16 practicuscore/container_helper.cpython-312-darwin.so
+-rw-r--r--  2.0 unx      523 b- defN 24-May-12 02:36 practicuscore/core.conf
+-rwxr-xr-x  2.0 unx   464440 b- defN 24-May-13 02:15 practicuscore/core_conf.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    97256 b- defN 24-May-13 02:17 practicuscore/core_context.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   507632 b- defN 24-May-13 02:15 practicuscore/core_def.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   572848 b- defN 24-May-13 02:14 practicuscore/dataprep.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx  1682096 b- defN 24-May-13 02:14 practicuscore/dataprep2.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   176648 b- defN 24-May-13 02:15 practicuscore/defined_conn.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   250288 b- defN 24-May-13 02:16 practicuscore/engine_helper.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    57680 b- defN 24-May-13 02:16 practicuscore/exceptions.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   112968 b- defN 24-May-13 02:16 practicuscore/experiment_helper.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   434560 b- defN 24-May-13 02:15 practicuscore/file_transfer_handler.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   146824 b- defN 24-May-13 02:16 practicuscore/file_transfer_mgr.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   143320 b- defN 24-May-13 02:16 practicuscore/instance_files_mgr.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   543504 b- defN 24-May-13 02:13 practicuscore/k8s_console_client.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   113224 b- defN 24-May-13 02:13 practicuscore/key_pair_mgr.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    71776 b- defN 24-May-13 02:16 practicuscore/license_api.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   402240 b- defN 24-May-13 02:22 practicuscore/local_file.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   113392 b- defN 24-May-13 02:17 practicuscore/local_file_meta_tree.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   166624 b- defN 24-May-13 02:15 practicuscore/log_manager.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    95352 b- defN 24-May-13 02:17 practicuscore/model_conf_parser.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    94904 b- defN 24-May-13 02:16 practicuscore/model_helper.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   172072 b- defN 24-May-13 02:17 practicuscore/model_host_client.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   538264 b- defN 24-May-13 02:16 practicuscore/proc_manager.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    97120 b- defN 24-May-13 02:17 practicuscore/profiler.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   447312 b- defN 24-May-13 02:17 practicuscore/region_manager.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   111696 b- defN 24-May-13 02:16 practicuscore/sampling.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   102464 b- defN 24-May-13 02:16 practicuscore/session_mgr.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   316096 b- defN 24-May-13 02:17 practicuscore/snippets_helper.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   340992 b- defN 24-May-13 02:13 practicuscore/steps.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   233000 b- defN 24-May-13 02:16 practicuscore/tcp_tunnel.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    95128 b- defN 24-May-13 02:17 practicuscore/token_mgr.cpython-312-darwin.so
+-rw-r--r--  2.0 unx      144 b- defN 24-May-13 02:22 practicuscore/uncompiled.py
+-rwxr-xr-x  2.0 unx   319328 b- defN 24-May-13 02:15 practicuscore/user_manager.cpython-312-darwin.so
+-rw-rw-r--  2.0 unx       43 b- defN 22-May-24 22:18 practicuscore/user_node.conf
+-rwxr-xr-x  2.0 unx    63304 b- defN 24-May-13 02:16 practicuscore/user_node_conf.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   464112 b- defN 24-May-13 02:15 practicuscore/util.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   342440 b- defN 24-May-13 02:13 practicuscore/webs_client.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    68960 b- defN 24-May-13 02:13 practicuscore/webs_protocol.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   250504 b- defN 24-May-13 02:16 practicuscore/worker_manager.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx    72432 b- defN 24-May-13 02:14 practicuscore/workflow_dply_mgr.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   132288 b- defN 24-May-13 02:13 practicuscore/workflow_helper.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   190136 b- defN 24-May-13 02:16 practicuscore/ws.cpython-312-darwin.so
+-rwxr-xr-x  2.0 unx   531960 b- defN 24-May-13 02:16 practicuscore/wsm.cpython-312-darwin.so
+-rw-r--r--  2.0 unx     1784 b- defN 24-May-13 02:22 practicuscore-24.3.0.dist-info/METADATA
+-rw-r--r--  2.0 unx      109 b- defN 24-May-13 02:22 practicuscore-24.3.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       46 b- defN 24-May-13 02:22 practicuscore-24.3.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       14 b- defN 24-May-13 02:22 practicuscore-24.3.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6509 b- defN 24-May-13 02:22 practicuscore-24.3.0.dist-info/RECORD
+64 files, 14677054 bytes uncompressed, 4988550 bytes compressed:  66.0%
```

## zipnote {}

```diff
@@ -1,157 +1,193 @@
 Filename: practicuscore/__init__.py
 Comment: 
 
-Filename: practicuscore/airflow_client.cp311-win_amd64.pyd
+Filename: practicuscore/__main__.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/addon_helper.cpython-312-darwin.so
 Comment: 
 
 Filename: practicuscore/api_base.py
 Comment: 
 
 Filename: practicuscore/api_def.py
 Comment: 
 
 Filename: practicuscore/api_k8s.py
 Comment: 
 
-Filename: practicuscore/api_manager.cp311-win_amd64.pyd
+Filename: practicuscore/api_manager.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/cli.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/cloud_conf.cp311-win_amd64.pyd
+Filename: practicuscore/cloud_conf.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/cloud_def.cp311-win_amd64.pyd
+Filename: practicuscore/cloud_def.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/cloud_node.cp311-win_amd64.pyd
+Filename: practicuscore/cloud_node.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/conf_center.cp311-win_amd64.pyd
+Filename: practicuscore/conf_center.cpython-312-darwin.so
 Comment: 
 
 Filename: practicuscore/conf_model.py
 Comment: 
 
-Filename: practicuscore/config_def.cp311-win_amd64.pyd
+Filename: practicuscore/config_def.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/conn_helper.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/container_helper.cp311-win_amd64.pyd
+Filename: practicuscore/container_helper.cpython-312-darwin.so
 Comment: 
 
 Filename: practicuscore/core.conf
 Comment: 
 
-Filename: practicuscore/core_conf.cp311-win_amd64.pyd
+Filename: practicuscore/core_conf.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/core_context.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/core_def.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/dataprep.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/core_context.cp311-win_amd64.pyd
+Filename: practicuscore/dataprep2.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/core_def.cp311-win_amd64.pyd
+Filename: practicuscore/defined_conn.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/dataprep.cp311-win_amd64.pyd
+Filename: practicuscore/engine_helper.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/dataprep2.cp311-win_amd64.pyd
+Filename: practicuscore/exceptions.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/defined_conn.cp311-win_amd64.pyd
+Filename: practicuscore/experiment_helper.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/dpi.cp311-win_amd64.pyd
+Filename: practicuscore/file_transfer_handler.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/experiment_service_config.cp311-win_amd64.pyd
+Filename: practicuscore/file_transfer_mgr.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/file_transfer_handler.cp311-win_amd64.pyd
+Filename: practicuscore/instance_files_mgr.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/file_transfer_mgr.cp311-win_amd64.pyd
+Filename: practicuscore/k8s_console_client.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/instance_files_mgr.cp311-win_amd64.pyd
+Filename: practicuscore/key_pair_mgr.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/k8s_console_client.cp311-win_amd64.pyd
+Filename: practicuscore/license_api.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/key_pair_mgr.cp311-win_amd64.pyd
+Filename: practicuscore/local_file.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/license_api.cp311-win_amd64.pyd
+Filename: practicuscore/local_file_meta_tree.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/local_file.cp311-win_amd64.pyd
+Filename: practicuscore/log_manager.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/local_file_meta_tree.cp311-win_amd64.pyd
+Filename: practicuscore/model_conf_parser.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/model_conf_parser.cp311-win_amd64.pyd
+Filename: practicuscore/model_helper.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/model_host_client.cp311-win_amd64.pyd
+Filename: practicuscore/model_host_client.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/profiler.cp311-win_amd64.pyd
+Filename: practicuscore/proc_manager.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/session_mgr.cp311-win_amd64.pyd
+Filename: practicuscore/profiler.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/snippets_helper.cp311-win_amd64.pyd
+Filename: practicuscore/region_manager.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/steps.cp311-win_amd64.pyd
+Filename: practicuscore/sampling.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/tcp_tunnel.cp311-win_amd64.pyd
+Filename: practicuscore/session_mgr.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/token_mgr.cp311-win_amd64.pyd
+Filename: practicuscore/snippets_helper.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/steps.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/tcp_tunnel.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/token_mgr.cpython-312-darwin.so
 Comment: 
 
 Filename: practicuscore/uncompiled.py
 Comment: 
 
-Filename: practicuscore/user_manager.cp311-win_amd64.pyd
+Filename: practicuscore/user_manager.cpython-312-darwin.so
 Comment: 
 
 Filename: practicuscore/user_node.conf
 Comment: 
 
-Filename: practicuscore/user_node_conf.cp311-win_amd64.pyd
+Filename: practicuscore/user_node_conf.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/util.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/webs_client.cpython-312-darwin.so
+Comment: 
+
+Filename: practicuscore/webs_protocol.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/util.cp311-win_amd64.pyd
+Filename: practicuscore/worker_manager.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/webs_client.cp311-win_amd64.pyd
+Filename: practicuscore/workflow_dply_mgr.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/webs_protocol.cp311-win_amd64.pyd
+Filename: practicuscore/workflow_helper.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/workflow_dply_mgr.cp311-win_amd64.pyd
+Filename: practicuscore/ws.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/ws.cp311-win_amd64.pyd
+Filename: practicuscore/wsm.cpython-312-darwin.so
 Comment: 
 
-Filename: practicuscore/wsm.cp311-win_amd64.pyd
+Filename: practicuscore-24.3.0.dist-info/METADATA
 Comment: 
 
-Filename: practicuscore-24.1.0.dist-info/METADATA
+Filename: practicuscore-24.3.0.dist-info/WHEEL
 Comment: 
 
-Filename: practicuscore-24.1.0.dist-info/WHEEL
+Filename: practicuscore-24.3.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: practicuscore-24.1.0.dist-info/top_level.txt
+Filename: practicuscore-24.3.0.dist-info/top_level.txt
 Comment: 
 
-Filename: practicuscore-24.1.0.dist-info/RECORD
+Filename: practicuscore-24.3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## practicuscore/__init__.py

```diff
@@ -1,361 +1,51 @@
-from typing import Optional, List, Union, TYPE_CHECKING
-
-from practicuscore.core_context import core_context_glbl
-from practicuscore.profiler import Profiler
-from practicuscore.dpi import DataPipelineInternal
-
-if TYPE_CHECKING:
-    from practicuscore.api_def import ModelSearchResults, ModelConfig
-    import pandas.core.frame
-    # noinspection PyUnresolvedReferences
-    import dask.dataframe.core
-    # noinspection PyUnresolvedReferences
-    import cudf.core.dataframe
-    # noinspection PyUnresolvedReferences
-    import dask_cudf.core
-    # noinspection PyUnresolvedReferences
-    import pyspark.pandas.frame
-
-__version__ = '24.1.0'
-
-
-class DataPipeline:
-    def __init__(self, df: Optional[Union['pandas.core.frame.DataFrame', 'dask.dataframe.core.DataFrame',
-                                          'cudf.core.dataframe.DataFrame', 'dask_cudf.core.DataFrame',
-                                          'pyspark.pandas.frame.DataFrame']] = None, logger=None):
-        """
-        Initializes DataPrep engine
-        :param df: (Optional) Pandas, DASK, RAPIDS (cudf), RAPIDS+DASK for multi GPU (dask_cudf) or Spark (pandas compatible) dataframe
-        """
-        self.dpi = DataPipelineInternal(df, logger)
-
-    def reset_steps(self, *args, **kwargs):
-        self.dpi.reset_recorded_steps()
-
-    def show_steps(self, *args, **kwargs):
-        self.dpi.show_recorded_steps()
-
-    def save_worksheet(
-            self,
-            file_path: str,
-            sampling_method: Optional[str] = "TOP",
-            sample_size: Optional[int] = 1000,
-            *args, **kwargs):
-        from practicuscore.core_def import CoreDef
-        if not file_path.endswith(CoreDef.APP_FILE_TYPE):
-            file_path += CoreDef.APP_FILE_TYPE
-        if sampling_method is not None:
-            sampling_message = f"A sample of {sampling_method} {sample_size} rows are included"
-        else:
-            sampling_message = "No sample included. \nPlease consider using dp.save_worksheet(file_path, " \
-                               "sample_df=df, sample_size=100) to include 100 samples of the dataframe in the saved " \
-                               "worksheet "
-        self.dpi.save_ws(file_path, sampling_method, sample_size)
-        print(f"Worksheet saved to {file_path}. {sampling_message}")
-
-    def run_steps(self, *args, **kwargs):
-        self.dpi.run_recorded_steps()
-
-    def get_activity_log(self, *args, **kwargs):
-        return self.dpi.get_ws_activity_log()
-
-    def show_activity_log(self, *args, **kwargs):
-        self.dpi.log_activities()
-
-    def get_cloud_worker_issues(self, *args, **kwargs):
-        return self.dpi.async_op_issue_list
-
-    def show_cloud_worker_issues(self, raise_error_on_issues=True, *args, **kwargs):
-        if self.dpi.using_node:
-            self.dpi.show_cloud_worker_issues(raise_error_on_issues)
-        else:
-            print("Not using Cloud Workers")
-
-    def show_history(self, raise_error_on_issues=False, *args, **kwargs):
-        self.show_activity_log()
-        if self.dpi.using_node:
-            self.show_cloud_worker_issues(raise_error_on_issues)
-
-    def delete_steps(self, *args, **kwargs):
-        self.dpi.delete_recorded_steps(*args)
-
-    def delete_columns(self, column_list: List[str], *args, **kwargs):
-        """
-        Deletes columns from the DataFrame
-        :param column_list: Column list to delete
-        """
-        self.dpi.delete_columns(column_list)
-
-    def rename_column(self, from_column_name: str, to_column_name: str, *args, **kwargs):
-        """
-        Renames a columns with inplace editing
-        :param from_column_name: Existing column name
-        :param to_column_name: New column name
-        :return: None
-        """
-        self.dpi.rename_column(from_column_name, to_column_name)
-
-    def rename_columns(self, columns_dict: dict, *args, **kwargs):
-        """
-        Renames a columns with inplace editing
-        :param columns_dict: A dictionary containing old and new column names(s)
-        :return: None
-        """
-        self.dpi.rename_columns(columns_dict)
-
-    def change_column_type(self, column_name: str, column_type: str, *args, **kwargs):
-        """
-        Changes column data type
-        :param column_name: Column name to change
-        :param column_type: Column Type- Text, Numeric, Date Time, Boolean
-        :return: None
-        """
-        self.dpi.change_column_type(column_name, column_type)
-
-    def filter(self, filter_expression: str, *args, **kwargs):
-        """
-        Filter on column
-        :param filter_expression: Filter expression on column
-        :return: None
-        """
-        self.dpi.filter(filter_expression)
-
-    def one_hot_encode(self, column_name: str, column_prefix: str, *args, **kwargs):
-        """
-        One hot encoding on data
-        :param column_name: Column name to one hot encoding
-        :param column_prefix: Column prefix for one hot encoding
-        :return: None
-        """
-        self.dpi.one_hot(column_name, column_prefix)
-
-    def categorical_map(self, column_name: str, column_suffix: str, *args, **kwargs):
-        """
-        Categorical mapping on data
-        :param column_name: Column name to categorical map
-        :param column_suffix: Column prefix for categorical mapping
-        :return: None
-        """
-        self.dpi.categorical_map(column_name, column_suffix)
-
-    def split_column(self, column_name: str, split_using: str, *args, **kwargs):
-        """
-        Split column data
-        :param column_name: Column name to split
-        :param split_using: Column prefix for split
-        :return: None
-        """
-        self.dpi.split_column(column_name, split_using)
-
-    def handle_missing(self, technique: str, column_list: List[str], custom_value: str, *args, **kwargs):
-        """
-        Handle missing values on column
-        :param technique: Handle missing technique
-        :param column_list: Column list to handle missing values
-        :param custom_value: Value for replace to missing values
-        :return: None
-        """
-        self.dpi.handle_missing(technique, column_list, custom_value)
-
-    def sort(self, column_list: List[str], ascending: Optional[List[bool]] = None, *args, **kwargs):
-        """
-        Sort on column
-        :param column_list: Column list to sort
-        :param ascending: True or False for ascending or descending order
-        :return: None
-        """
-        self.dpi.sort_column(column_list, ascending)
-
-    def group_by(self, columns: List[str], aggregation: dict, *args, **kwargs):
-        """
-        Group by column
-        :param columns: Column list to group by
-        :param aggregation: Column to group with technique
-        :return: None
-        """
-        self.dpi.group_by_column(columns, aggregation)
-
-    def time_sample(self, date_column: str, summary_column: str, summary_method: str, frequency: str, *args, **kwargs):
-        """
-        Time sample summarization
-        :param date_column: Date column to calculate time sample summary for
-        :param summary_column: Numeric column to calculate the summary
-        :param summary_method: Aggregation (mean, sum etc) method
-        :param frequency: D, M (Day, month etc.) frequency value. Uses Pandas standard
-        :return: None
-        """
-        self.dpi.time_sample(date_column, summary_column, summary_method, frequency)
-
-    def update_values(self, column_name: str, old_value: str, new_value: str, *args, **kwargs):
-        """
-        Updates column values to new values
-        :param column_name: Column name to update
-        :param old_value: Values in column to be update
-        :param new_value: New value for update
-        :return: None
-        """
-        self.dpi.update_values(column_name, old_value, new_value)
-
-    def run_formula(self, new_column_name: str, formula_expression: str, *args, **kwargs):
-        """
-        Run formula with expression
-        :param new_column_name: New Column name after formula applied
-        :param formula_expression: Formula expression with selected Column
-        :return: None
-        """
-        self.dpi.run_formula(new_column_name, formula_expression)
-
-    def run_code(self, custom_function, *args, **kwargs):
-        self.dpi.run_custom_code(custom_function)
-
-    def run_sql(self, sql_query: str, sql_table_name: str, *args, **kwargs):
-        self.dpi.run_custom_sql(sql_query, sql_table_name)
-
-    def register_udf(self, udf, *args, **kwargs):
-        self.dpi.register_udf_code(udf)
-
-    def build_model(
-            self,
-            model_conf_file: Optional[str] = None,
-            model_conf_json: Optional[str] = None,
-            timeout_min=300,
-            *args, **kwargs) -> Optional['ModelConfig']:
-        """
-        Builds AI model.
-        :param model_conf_file: (Optional) Model configuration file, explaining how the model should be built.
-        :param model_conf_json: (Optional) Model configuration as json string.
-        :param timeout_min: Time out in minutes. Default is 300 minutes
-        :return: ModelConfig. Details of the resulting model.
-        """
-        return self.dpi.build_model(model_conf_file, model_conf_json, timeout_min)
-
-    def register_model(self, *args, **kwargs):
-        """
-        Registers final AI model.
-        :return: None
-        """
-        self.dpi.register_model()
-
-    def find_model(self, model_text: str, *args, **kwargs) -> Optional['ModelSearchResults']:
-        return self.dpi.find_model(model_text)
-
-    def get_auth_token(self, host_url: str, email: str, password: str, *args, **kwargs) -> str:
-        return self.dpi.get_auth_token(host_url, email, password)
-
-    def deploy_model(
-            self,
-            host_url: str,
-            email: str,
-            auth_token: str,
-            deployment_key: str,
-            prefix: str,
-            model_name: str,
-            model_dir: Optional[str] = None,
-            *args, **kwargs):
-        self.dpi.deploy_model(
-            host_url=host_url, email=email, auth_token=auth_token, deployment_key=deployment_key,
-            prefix=prefix, model_name=model_name, model_dir=model_dir)
-
-    def predict(
-            self, api_url: str, model_id: int, api_token: Optional[str] = None,
-            column_names: Optional[List[str]] = None, new_column_name: Optional[str] = None,
-            batch_size: Optional[int] = None,
-            compression_algo: Optional[str] = None,
-            *args, **kwargs):
-        self.dpi.predict(
-            api_url=api_url, model_id=model_id, api_token=api_token,
-            column_names=column_names, new_column_name=new_column_name,
-            batch_size=batch_size,
-            compression_algo=compression_algo)
-
-    def predict_with_offline_model(
-            self, column_names: Optional[List[str]] = None, new_column_name: Optional[str] = None,
-            future_horizon: Optional[int] = None,
-            mlflow_model_uri: Optional[str] = None,
-            model_conf_path: Optional[str] = None, model_conf: Optional[str] = None,
-            problem_type: Optional[str] = None,
-            *args, **kwargs):
-        self.dpi.predict_with_offline_model(
-            column_names=column_names, new_column_name=new_column_name,
-            future_horizon=future_horizon,
-            mlflow_model_uri=mlflow_model_uri,
-            model_conf_path=model_conf_path, model_conf=model_conf,
-            problem_type=problem_type)
-
-    def join(self, left_key_col_name: str, right_key_col_name: str,
-             conn_conf_file: Optional[str] = None, conn_conf_json: Optional[str] = None, right_ws_name=None,
-             join_technique="Left", suffix_for_overlap="_right", summary_column=False, *args, **kwargs):
-        self.dpi.join(left_key_col_name=left_key_col_name, right_key_col_name=right_key_col_name,
-                      conn_conf_file=conn_conf_file, conn_conf_json=conn_conf_json, right_ws_name=right_ws_name,
-                      join_technique=join_technique, suffix_for_overlap=suffix_for_overlap, summary_column=summary_column)
-
-    def await_async(self, timeout_min=600, *args, **kwargs):
-        self.dpi.wait_until_ws_is_free(timeout_min)
-
-    def get_df_copy(
-            self,
-            timeout_min=600,
-            *args, **kwargs) -> Union['pandas.core.frame.DataFrame', 'dask.dataframe.core.DataFrame',
-                                      'cudf.core.dataframe.DataFrame', 'dask_cudf.core.DataFrame',
-                                      'pyspark.pandas.frame.DataFrame']:
-        return self.dpi.get_df_copy(timeout_min)
-
-    def load(
-            self,
-            conn_conf_file: Optional[str] = None,
-            conn_conf_json: Optional[str] = None,
-            engine="PANDAS",
-            *args, **kwargs):
-        if not self.dpi.using_node:
-            raise ConnectionError("Cannot run load() since you are not using a Cloud Worker.\n"
-                                  "To experiment without a Cloud Worker service, "
-                                  "please use a local dataframe and pass a df to DataPipeline()")
-        self.dpi.load(conn_conf_file, conn_conf_json, engine)
-
-    def save(
-            self,
-            conn_conf_file: Optional[str] = None,
-            conn_conf_json: Optional[str] = None,
-            timeout_min=600,
-            *args, **kwargs):
-        self.dpi.save(conn_conf_file, conn_conf_json, timeout_min)
-
-    def kill_worker(self, *args, **kwargs):
-        self.dpi.kill_worker()
-
-    def terminate_cloud_worker(self):
-        self.dpi.terminate_cloud_worker()
-
-    def run_all_steps(self, runner_module: str):
-        self.dpi.call_func(runner_module, "run_all_steps", self)
-
-
-def create_data_pipeline_cloud_worker(
-        config_file: Optional[str] = None, override: Optional[dict] = None, logger=None) -> DataPipeline:
-    if not logger:
-        import logging
-
-        if logging.getLogger().hasHandlers():
-            # E.g. from airflow
-            logger = logging.getLogger()
-            logger.debug("Using globally configured logger")
-        else:
-            from practicuscore.core_conf import log_manager_glbl
-            logger = log_manager_glbl.get_logger()
-    dp = DataPipeline(logger=logger)
-    dp.dpi.create_data_pipeline_worker(config_file=config_file, override=override)
-    return dp
-
-
-def configure_experiment(
-        experiment_name: str, service_key: Optional[str] = None, service_name: Optional[str] = None,
-        username: Optional[str] = None):
-    from practicuscore.experiment_service_config import TrackingServerMgr
-    tracking_server_mgr = TrackingServerMgr()
-    tracking_server_mgr.configure(
-        experiment_name=experiment_name, service_key=service_key, service_name=service_name, username=username)
-
-
-def test():
-    print(f"You are using Practicus AI Core version {__version__}.\n"
-          f"Please check https://practicus.ai for detailed usage instructions")
+"""
+Practicus AI Core Library
+=========================
+
+Overview
+--------
+Practicus AI Core library allows you to work on DataFrames, Practicus AI Workers and more.
+
+Sample Usage
+------------
+import practicuscore as prt
+prt.some_operation()
+"""
+from .log_manager import get_logger, Log, set_logging_level
+from .region_manager import regions
+from .engine_helper import engines
+from .experiment_helper import experiments
+from .workflow_helper import workflows
+from .model_helper import models
+from .cli import main
+
+
+__version__ = '24.3.0'
+logger = get_logger(Log.SDK)
+
+
+def _add_region_methods_to_globals():
+    # For convenience. Allows: prt.current_region() instead of prt.regions.current_region()
+    try:
+        import inspect
+
+        def is_cython_function(obj):
+            return callable(obj) and (inspect.isfunction(obj) or obj.__class__.__name__ == 'cython_function_or_method')
+
+        _region_methods = {}
+        for name, method in inspect.getmembers(regions, predicate=is_cython_function):
+            if not name.startswith("_"):
+                _region_methods[name] = method
+
+        globals().update(_region_methods)
+    except:
+        logger.error(
+            "Could not add regions methods to globals for convenience. Please use them as prt.regions.  ",
+            exc_info=True)
+
+
+_add_region_methods_to_globals()
+
+
+if __name__ == "__main__":
+    main()
```

## practicuscore/api_base.py

```diff
@@ -1,1685 +1,1782 @@
-import os.path
-import sys
-from abc import ABC
-import platform
-from dataclasses import dataclass, field, fields
-from datetime import datetime
-from typing import List, Optional, Union, cast, Type, Tuple
-
-from dataclasses_json import DataClassJsonMixin, config
-
-from practicuscore.core_def import PRTEng, PRTConn, CoreDef, OPResult
-
-
-def exclude_if_none(value):
-    return value is None
-
-
-class PRTValidator(ABC):
-    @staticmethod
-    def validate(dataclass_obj) -> Tuple[Optional[str], Optional[str]]:
-        """
-        Validates all fields on a dataclass, *if only* it has "validators" metadata.
-        "validators" can be a single tuple (lambda_func, "err message") OR a list of validation tuples
-           i.e. use a single validator:
-            some_field: int = field(
-                metadata={
-                    "validators": (lambda x: x > 0, "Must be > 0")
-                })
-           OR multiple validators:
-            some_field: int = field(
-                metadata={
-                    "validators": [(lambda x: x > 0, "Must be > 0"),
-                                   (lambda x: x < 10, "Must be < 10")]
-                })
-        :param dataclass_obj: The dataclass object to validate. Must have validators defined
-        :return: if a field has errors, (field name, error message) tuple. Or (None, None)
-        """
-        for fld in fields(dataclass_obj):
-            if "validators" in fld.metadata:
-                validator_or_validators = fld.metadata["validators"]
-
-                if isinstance(validator_or_validators, tuple):
-                    validators = [validator_or_validators]
-                else:
-                    validators = validator_or_validators
-                for validator_tuple in validators:
-                    assert isinstance(validator_tuple, tuple), \
-                        "Validator must be a tuple in the form of (validator_lambda, 'error message')"
-                    validator_func, validator_err_msg = validator_tuple
-                    field_val = getattr(dataclass_obj, fld.name)
-                    try:
-                        failed = False
-                        if not validator_func(field_val):
-                            failed = True
-                    except Exception as ex:
-                        failed = True
-                        validator_err_msg = f"Exception occurred while checking for '{validator_err_msg}', " \
-                                            f"\nException: {ex}"
-
-                    if failed:
-                        return fld.name, validator_err_msg  # return info about *first* encountered issue
-
-        return None, None  # no issues, nothing to return
-
-
-@dataclass
-class RequestMeta(DataClassJsonMixin):
-    meta_type: str = "Request"
-    meta_name: str = ""
-    req_time: Optional[datetime] = None
-    req_core_v: str = CoreDef.CORE_VERSION
-    req_os: str = platform.system()
-    req_os_v: str = platform.release()
-    req_py_minor_v: int = sys.version_info.minor
-
-
-@dataclass
-class PRTRequest(DataClassJsonMixin):
-    # Creating a meta class here caused a nasty bug. meta became a shared object between child classes
-    #   i.e. when __post_init() below updated meta_name for one type of Request class, al others got the new name
-    # meta: RequestMeta = RequestMeta()
-    meta: Optional[RequestMeta] = None
-
-    @property
-    def name(self) -> str:
-        return self.meta.meta_name
-
-    def __post_init__(self):
-        self.meta = RequestMeta()
-        self.meta.meta_name = self.__class__.__name__.rsplit("Request", 1)[0]
-        # do not assign defaults in class definition. Gets assigned static one time
-        self.meta.req_time = datetime.utcnow()
-
-
-@dataclass
-class ResponseMeta(DataClassJsonMixin):
-    meta_type: str = "Response"
-    meta_name: str = ""
-    resp_node_v: str = ""  # assigned later right before sending to client
-    resp_py_minor_v: int = sys.version_info.minor
-
-
-@dataclass
-class PRTResponse(DataClassJsonMixin):
-    meta: Optional[ResponseMeta] = None
-    op_result: Optional[OPResult] = None
-
-    # meta: ResponseMeta = ResponseMeta()  Don't instantiate here. Read notes for PRTRequest
-
-    @property
-    def name(self) -> str:
-        return self.meta.meta_name
-
-    def __post_init__(self):
-        self.meta = ResponseMeta()
-        self.meta.meta_name = self.__class__.__name__.rsplit("Response", 1)[0]
-        # do not assign defaults in class definition. Gets assigned static one time
-        self.meta.req_time = datetime.utcnow()
-
-
-@dataclass
-class EmptyResponse(PRTResponse):
-    # used when there's an error, no response can be created and we hae op_result send back
-    pass
-
-
-# Connection configuration classes
-
-@dataclass
-class UIMap(DataClassJsonMixin):
-    # Helps map an individual ConnConf field to a single GUI element.
-    # i.e. MYSQL related field "db_host" should be visible as "Database Host Address", it is required, ...
-    visible_name: Optional[str] = None
-    auto_display: bool = True  # some fields are displayed manually and not automated
-    tip: Optional[str] = None
-    default_value: Optional[str] = None
-    is_required: bool = True
-    is_password: bool = False
-
-
-@dataclass
-class ConnConf(DataClassJsonMixin):
-    _conn_type: Optional[PRTConn] = None
-    _enriched: Optional[bool] = field(
-        default=None,
-        metadata=config(exclude=exclude_if_none),
-    )
-    uuid: Optional[str] = field(
-        default=None,
-        metadata=config(exclude=exclude_if_none),
-    )
-    ws_uuid: Optional[str] = field(
-        default=None,
-        metadata=config(exclude=exclude_if_none),
-    )
-    sampling_method: Optional[str] = field(
-        default=None,
-        metadata=config(exclude=exclude_if_none),
-    )
-    sample_size: Optional[int] = field(
-        default=None,
-        metadata=config(exclude=exclude_if_none),
-    )
-    sample_size_app: Optional[int] = field(
-        default=None,
-        metadata=config(exclude=exclude_if_none),
-    )
-    column_list: Optional[List[str]] = field(
-        default=None,
-        metadata=config(exclude=exclude_if_none),
-    )
-    filter: Optional[str] = field(
-        default=None,
-        metadata=config(exclude=exclude_if_none),
-    )
-
-    @property
-    def enriched(self) -> bool:
-        return bool(self._enriched) if self._enriched is not None else False
-
-    @property
-    def conn_type(self) -> PRTConn:
-        return self._conn_type
-
-    @property
-    def friendly_desc(self) -> str:
-        # override with children class for a better user-friendly
-        return f"Cloud {self._conn_type}"
-
-    @property
-    def friendly_long_desc(self) -> str:
-        return self.friendly_desc
-
-    def copy_secure(self) -> 'ConnConf':
-        import copy
-        return copy.copy(self)
-
-    def copy_with_credentials(self, credentials: Optional[dict] = None) -> 'ConnConf':
-        return self.copy_secure()
-
-    def apply_credentials_to(self, other: 'ConnConf'):
-        self._apply_credentials_to(other)
-        self._enriched = True
-
-    def _apply_credentials_to(self, other: 'ConnConf'):
-        pass
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        pass
-
-    def __eq__(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, ConnConf):
-            return False
-        other = cast(ConnConf, other)
-        equals = self.conn_type == other.conn_type \
-                 and self.sample_size == other.sample_size \
-                 and self.sample_size_app == other.sample_size_app \
-                 and self.column_list == other.column_list \
-                 and self.filter == other.filter
-        if not equals:
-            return False
-        return self.internal_equals(other)
-
-
-@dataclass
-class InMemoryConnConf(ConnConf):
-    _conn_type: PRTConn = PRTConn.IN_MEMORY
-    df: Optional[object] = None
-
-    @property
-    def friendly_desc(self) -> str:
-        return "In memory dataframe"
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, InMemoryConnConf):
-            return False
-        other = cast(InMemoryConnConf, other)
-        return self.df.__eq__(other.df)
-
-
-@dataclass
-class LocalFileConnConf(ConnConf):
-    _conn_type: PRTConn = PRTConn.LOCAL_FILE
-    file_path: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="File Path",
-                        tip="Type path on local disk")
-    })
-
-    @property
-    def friendly_desc(self) -> str:
-        try:
-            if self.file_path:
-                return os.path.splitext(os.path.basename(self.file_path))[0]
-        except:
-            pass
-
-        return self._conn_type.lower()
-
-    @property
-    def friendly_long_desc(self) -> str:
-        return f"Local file: {self.file_path}"
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, LocalFileConnConf):
-            return False
-        other = cast(LocalFileConnConf, other)
-        return self.file_path == other.file_path
-
-    @property
-    def is_prt_file(self) -> bool:
-        return self.file_path.endswith(CoreDef.APP_FILE_TYPE)
-
-
-@dataclass
-class NodeFileConnConf(ConnConf):
-    _conn_type: PRTConn = PRTConn.NODE_FILE
-    file_path: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="File Path",
-                        tip="Type path on Cloud Worker local disk. E.g. /home/ubuntu/data/file.csv")
-    })
-
-    @property
-    def friendly_desc(self) -> str:
-        try:
-            if self.file_path:
-                return os.path.splitext(os.path.basename(self.file_path))[0]
-        except:
-            pass
-
-        return self._conn_type.lower()
-
-    @property
-    def friendly_long_desc(self) -> str:
-        return f"Cloud Worker file: {self.file_path}"
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, NodeFileConnConf):
-            return False
-        other = cast(NodeFileConnConf, other)
-        return self.file_path == other.file_path
-
-    @property
-    def is_prt_file(self) -> bool:
-        return self.file_path.endswith(CoreDef.APP_FILE_TYPE)
-
-
-@dataclass
-class S3ConnConf(ConnConf):
-    _conn_type: PRTConn = PRTConn.S3
-    aws_region: Optional[str] = None
-    aws_access_key_id: Optional[str] = None
-    aws_secret_access_key: Optional[str] = None
-    aws_session_token: Optional[str] = None
-    endpoint_url: Optional[str] = None
-    s3_bucket: Optional[str] = None
-    s3_keys: Optional[List[str]] = None
-    default_prefix: Optional[str] = None
-
-    @property
-    def friendly_desc(self) -> str:
-        try:
-            if self.s3_keys:
-                if len(self.s3_keys) >= 1:
-                    s3_key = self.s3_keys[0]
-                    if s3_key.find(".") > -1:
-                        return os.path.splitext(os.path.basename(s3_key))[0]
-                    else:
-                        return os.path.basename(os.path.normpath(s3_key))
-        except:
-            pass
-        return self._conn_type.lower()
-
-    @property
-    def friendly_long_desc(self) -> str:
-        if self.s3_bucket:
-            bucket_desc = f"s3://{self.s3_bucket}"
-        else:
-            # k8s currently send no bucket name..
-            bucket_desc = f"s3://_bucket_"
-        if self.s3_keys:
-            if len(self.s3_keys) == 1:
-                return f"{bucket_desc}/{self.s3_keys[0]}"
-            else:
-                return f"{bucket_desc}/{self.s3_keys[0]} .."
-        return bucket_desc
-
-    def copy_secure(self) -> ConnConf:
-        copy_conn_conf = super().copy_secure()
-        assert isinstance(copy_conn_conf, S3ConnConf)
-        copy_conn_conf = cast(S3ConnConf, copy_conn_conf)
-        copy_conn_conf.aws_access_key_id = None
-        copy_conn_conf.aws_secret_access_key = None
-        copy_conn_conf.aws_session_token = None
-        return copy_conn_conf
-
-    def copy_with_credentials(self, credentials: Optional[dict] = None) -> ConnConf:
-        copy_conn_conf = super().copy_secure()
-        copy_conn_conf = cast(S3ConnConf, copy_conn_conf)
-        if "aws_access_key_id" in credentials:
-            copy_conn_conf.aws_access_key_id = str(credentials["aws_access_key_id"])
-        if "aws_secret_access_key" in credentials:
-            copy_conn_conf.aws_secret_access_key = str(credentials["aws_secret_access_key"])
-        if "aws_session_token" in credentials:
-            copy_conn_conf.aws_session_token = str(credentials["aws_session_token"])
-        return copy_conn_conf
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, S3ConnConf):
-            return False
-        other = cast(S3ConnConf, other)
-        return self.aws_region == other.aws_region \
-            and self.aws_access_key_id == other.aws_access_key_id \
-            and self.aws_secret_access_key == other.aws_secret_access_key \
-            and self.aws_session_token == other.aws_session_token \
-            and self.s3_bucket == other.s3_bucket \
-            and self.s3_keys == other.s3_keys
-
-    def _apply_credentials_to(self, other: 'ConnConf'):
-        assert isinstance(other, S3ConnConf), "apply credentials failed. other must be S3ConnConf"
-        other = cast(S3ConnConf, other)
-        other.aws_region = self.aws_region
-        other.aws_access_key_id = self.aws_access_key_id
-        other.aws_secret_access_key = self.aws_secret_access_key
-        other.s3_bucket = self.s3_bucket
-        other.endpoint_url = self.endpoint_url
-
-
-@dataclass
-class RelationalConnConf(ConnConf):
-    sql_query: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="SQL Query",
-                        auto_display=False),
-        "validators": (lambda x: x and len(x) > 0, "No SQL query provided")
-    })
-
-    target_table_name: Optional[str] = None
-
-    def _find_table_name(self, keyword: str) -> Optional[str]:
-        if self.sql_query:
-            table_ind = self.sql_query.lower().find(f"{keyword} ") + len(f"{keyword} ")
-            if table_ind > -1:
-                desc = self.sql_query[table_ind:].strip().lower()
-                next_stop = desc.find(" ")
-                if next_stop > -1:
-                    desc = desc[:next_stop]
-                next_stop = desc.find(",")
-                if next_stop > -1:
-                    desc = desc[:next_stop]
-                next_stop = desc.find("\n")
-                if next_stop > -1:
-                    desc = desc[:next_stop]
-
-                desc = desc.strip()
-                if desc:
-                    return desc
-        return None
-
-    @property
-    def friendly_desc(self) -> str:
-        try:
-            table_name = self._find_table_name("from")
-            if not table_name:
-                table_name = self._find_table_name("into")
-
-            if table_name:
-                return table_name
-
-            if self.sql_query:
-                return self.sql_query[:10] + f"{'..' if len(self.sql_query) > 10 else ''}"
-        except:
-            pass
-        return self._conn_type.lower()
-
-    @property
-    def friendly_long_desc(self) -> str:
-        if self.target_table_name is not None:
-            return f"{self._conn_type.lower()} table: {self.target_table_name}"
-        else:
-            table_name = self._find_table_name("from")
-            if table_name:
-                desc = f"table: {table_name}"
-            else:
-                desc = ":"
-            desc += f" ({self.sql_query[:30]}{'...' if len(self.sql_query) > 30 else ''})"
-            desc = desc.replace("\n", " ")
-            return f"{self._conn_type.lower()} {desc}"
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, RelationalConnConf):
-            return False
-        other = cast(RelationalConnConf, other)
-        return self.sql_query == other.sql_query and self.target_table_name == other.target_table_name
-
-
-@dataclass
-class SqLiteConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.SQLITE
-    file_path: Optional[str] = field(default="None", metadata={
-        "ui_map": UIMap(visible_name="File Path",
-                        tip="Type path on Cloud Worker local disk. E.g. /home/ubuntu/data/database.db",
-                        default_value=CoreDef.NODE_HOME_PATH + "/samples/chinook.db"),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, SqLiteConnConf):
-            return False
-        other = cast(SqLiteConnConf, other)
-        return self.file_path == other.file_path
-
-    def _apply_credentials_to(self, other: 'ConnConf'):
-        assert isinstance(other, SqLiteConnConf), "apply credentials failed. other must be SqLiteConnConf"
-        other = cast(SqLiteConnConf, other)
-        other.file_path = self.file_path
-
-
-@dataclass
-class MYSQLConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.MYSQL
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="3306"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, MYSQLConnConf):
-            return False
-        other = cast(MYSQLConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_name == other.db_name \
-            and self.db_port == other.db_port \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'MYSQLConnConf'):
-        assert isinstance(other, MYSQLConnConf), "apply credentials failed. other must be MYSQLConnConf"
-        other = cast(MYSQLConnConf, other)
-        other.db_host = self.db_host
-        other.db_name = self.db_name
-        other.db_port = self.db_port
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class PostgreSQLConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.POSTGRESQL
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="5432"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, PostgreSQLConnConf):
-            return False
-        other = cast(PostgreSQLConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_name == other.db_name \
-            and self.db_port == other.db_port \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'PostgreSQLConnConf'):
-        assert isinstance(other, PostgreSQLConnConf), "apply credentials failed. other must be PostgreSQLConnConf"
-        other = cast(PostgreSQLConnConf, other)
-        other.db_host = self.db_host
-        other.db_name = self.db_name
-        other.db_port = self.db_port
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class RedshiftConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.REDSHIFT
-    # redshift_db_address: Optional[str] = None  # dummy
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="5439"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, RedshiftConnConf):
-            return False
-        other = cast(RedshiftConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_name == other.db_name \
-            and self.db_port == other.db_port \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'RedshiftConnConf'):
-        assert isinstance(other, RedshiftConnConf), "apply credentials failed. other must be RedshiftConnConf"
-        other = cast(RedshiftConnConf, other)
-        other.db_host = self.db_host
-        other.db_name = self.db_name
-        other.db_port = self.db_port
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class SnowflakeConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.SNOWFLAKE
-    # redshift_db_address: Optional[str] = None  # dummy
-
-    db_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    schema: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Schema",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    warehouse_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Warehouse Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    role: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Role",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    account: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Account Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        ),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, SnowflakeConnConf):
-            return False
-        other = cast(SnowflakeConnConf, other)
-        return self.db_name == other.db_name \
-            and self.schema == other.schema \
-            and self.warehouse_name == other.warehouse_name \
-            and self.user == other.user \
-            and self.role == other.role \
-            and self.account == other.account \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'SnowflakeConnConf'):
-        assert isinstance(other, SnowflakeConnConf), "apply credentials failed. other must be SnowflakeConnConf"
-        other = cast(SnowflakeConnConf, other)
-        other.db_name = self.db_name
-        other.schema = self.schema
-        other.warehouse_name = self.warehouse_name
-        other.user = self.user
-        other.role = self.role
-        other.account = self.account
-        other.password = self.password
-
-
-@dataclass
-class MSSQLConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.MSSQL
-    # redshift_db_address: Optional[str] = None  # dummy
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    # driver: Optional[str] = field(default=None, metadata={
-    #     "ui_map": UIMap(visible_name="Driver Name",
-    #                     default_value="SQL Server Native Client 10.0"),
-    #     "validators": (lambda x: x and len(x) > 0, "No value provided")
-    # })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="1433"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, MSSQLConnConf):
-            return False
-        other = cast(MSSQLConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_name == other.db_name \
-            and self.db_port == other.db_port \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'MSSQLConnConf'):
-        assert isinstance(other, MSSQLConnConf), "apply credentials failed. other must be MSSQLConnConf"
-        other = cast(MSSQLConnConf, other)
-        other.db_host = self.db_host
-        other.db_name = self.db_name
-        other.db_port = self.db_port
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class OracleConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.ORACLE
-    # redshift_db_address: Optional[str] = None  # dummy
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    # driver: str = field(default=None, metadata={
-    #     "ui_map": UIMap(visible_name="Driver Name",
-    #                     default_value="SQL Server Native Client 10.0"),
-    #     "validators": (lambda x: x and len(x) > 0, "No value provided")
-    # })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="1521"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, OracleConnConf):
-            return False
-        other = cast(OracleConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_name == other.db_name \
-            and self.db_port == other.db_port \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'OracleConnConf'):
-        assert isinstance(other, OracleConnConf), "apply credentials failed. other must be OracleConnConf"
-        other = cast(OracleConnConf, other)
-        other.db_host = self.db_host
-        other.db_name = self.db_name
-        other.db_port = self.db_port
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class HiveConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.HIVE
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="10000"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, HiveConnConf):
-            return False
-        other = cast(HiveConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_name == other.db_name \
-            and self.db_port == other.db_port \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'HiveConnConf'):
-        assert isinstance(other, HiveConnConf), "apply credentials failed. other must be HiveConnConf"
-        other = cast(HiveConnConf, other)
-        other.db_host = self.db_host
-        other.db_name = self.db_name
-        other.db_port = self.db_port
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class ClouderaConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.CLOUDERA
-    host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Cloudera Host",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="21050"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, ClouderaConnConf):
-            return False
-        other = cast(ClouderaConnConf, other)
-        return self.host == other.host \
-            and self.port == other.port \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'ClouderaConnConf'):
-        assert isinstance(other, ClouderaConnConf), "apply credentials failed. other must be ClouderaConnConf"
-        other = cast(ClouderaConnConf, other)
-        other.host = self.host
-        other.port = self.port
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class AthenaConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.ATHENA
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    s3_dir: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="S3 Location",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="443"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    access_key: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="AWS Access key ID",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    secret_key: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="AWS Secret access key",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, AthenaConnConf):
-            return False
-        other = cast(AthenaConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_name == other.db_name \
-            and self.s3_dir == other.s3_dir \
-            and self.db_port == other.db_port \
-            and self.access_key == other.access_key \
-            and self.secret_key == other.secret_key
-
-    def _apply_credentials_to(self, other: 'AthenaConnConf'):
-        assert isinstance(other, AthenaConnConf), "apply credentials failed. other must be AthenaConnConf"
-        other = cast(AthenaConnConf, other)
-        other.db_host = self.db_host
-        other.db_name = self.db_name
-        other.db_port = self.db_port
-        other.s3_dir = self.s3_dir
-        other.access_key = self.access_key
-        other.secret_key = self.secret_key
-
-
-@dataclass
-class ElasticSearchConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.ELASTICSEARCH
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        tip="E.g. test-2.latest-elasticsearch.abc-3.xyz.com or 192.168.0.1",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value=""),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, ElasticSearchConnConf):
-            return False
-        other = cast(ElasticSearchConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_port == other.db_port \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'ElasticSearchConnConf'):
-        assert isinstance(other, ElasticSearchConnConf), "apply credentials failed. other must be ElasticSearchConnConf"
-        other = cast(ElasticSearchConnConf, other)
-        other.db_host = self.db_host
-        other.db_port = self.db_port
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class OpenSearchConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.OPENSEARCH
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        tip="E.g. search-test-abcde.us-east-1.es.amazonaws.com",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="443"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, OpenSearchConnConf):
-            return False
-        other = cast(OpenSearchConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_port == other.db_port \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'OpenSearchConnConf'):
-        assert isinstance(other, OpenSearchConnConf), "apply credentials failed. other must be OpenSearchConnConf"
-        other = cast(OpenSearchConnConf, other)
-        other.db_host = self.db_host
-        other.db_port = self.db_port
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class TrinoConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.TRINO
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        tip="E.g. localhost",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="8080"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    catalog: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Catalog",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    schema: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Schema",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, TrinoConnConf):
-            return False
-        other = cast(TrinoConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_port == other.db_port \
-            and self.catalog == other.catalog \
-            and self.schema == other.schema \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'TrinoConnConf'):
-        assert isinstance(other, TrinoConnConf), "apply credentials failed. other must be TrinoConnConf"
-        other = cast(TrinoConnConf, other)
-        other.db_host = self.db_host
-        other.db_port = self.db_port
-        other.catalog = self.catalog
-        other.schema = self.schema
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class DremioConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.DREMIO
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        tip="E.g. localhost",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="31010"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, DremioConnConf):
-            return False
-        other = cast(TrinoConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_port == other.db_port \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'DremioConnConf'):
-        assert isinstance(other, DremioConnConf), "apply credentials failed. other must be DremioConnConf"
-        other = cast(DremioConnConf, other)
-        other.db_host = self.db_host
-        other.db_port = self.db_port
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class HanaConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.HANA
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="39015"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    db_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Name",
-                        is_required=False,
-                        default_value="")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, HanaConnConf):
-            return False
-        other = cast(HanaConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_port == other.db_port \
-            and self.db_name == other.db_name \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'HanaConnConf'):
-        assert isinstance(other, HanaConnConf), "apply credentials failed. other must be HanaConnConf"
-        other = cast(HanaConnConf, other)
-        other.db_host = self.db_host
-        other.db_port = self.db_port
-        other.db_name = self.db_name
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class TeradataConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.TERADATA
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, TeradataConnConf):
-            return False
-        other = cast(TeradataConnConf, other)
-        return self.db_host == other.db_host \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'TeradataConnConf'):
-        assert isinstance(other, TeradataConnConf), "apply credentials failed. other must be TeradataConnConf"
-        other = cast(TeradataConnConf, other)
-        other.db_host = self.db_host
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class Db2ConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.DB2
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="39015"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    db_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, Db2ConnConf):
-            return False
-        other = cast(Db2ConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_port == other.db_port \
-            and self.db_name == other.db_name \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'Db2ConnConf'):
-        assert isinstance(other, Db2ConnConf), "apply credentials failed. other must be Db2ConnConf"
-        other = cast(Db2ConnConf, other)
-        other.db_host = self.db_host
-        other.db_port = self.db_port
-        other.db_name = self.db_name
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class DynamoDBConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.DYNAMODB
-    access_key: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="AWS Access Key Id",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    secret_key: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="AWS Secret Access Key",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    region: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="AWS Region Name",
-                        tip="E.g. us-east-1",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, DynamoDBConnConf):
-            return False
-        other = cast(DynamoDBConnConf, other)
-        return self.access_key == other.access_key \
-            and self.secret_key == other.secret_key \
-            and self.region == other.region
-
-    def _apply_credentials_to(self, other: 'DynamoDBConnConf'):
-        assert isinstance(other, DynamoDBConnConf), "apply credentials failed. other must be DynamoDBConnConf"
-        other = cast(DynamoDBConnConf, other)
-        other.access_key = self.access_key
-        other.secret_key = self.secret_key
-        other.region = self.region
-
-
-@dataclass
-class CockroachDBConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.COCKROACHDB
-    db_host: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Server Address",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    db_port: Optional[int] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Port",
-                        default_value="26257"),
-        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
-    })
-    db_name: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Database Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    user: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="User Name",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-    password: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Password",
-                        is_password=True,
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, CockroachDBConnConf):
-            return False
-        other = cast(CockroachDBConnConf, other)
-        return self.db_host == other.db_host \
-            and self.db_port == other.db_port \
-            and self.db_name == other.db_name \
-            and self.user == other.user \
-            and self.password == other.password
-
-    def _apply_credentials_to(self, other: 'CockroachDBConnConf'):
-        assert isinstance(other, CockroachDBConnConf), "apply credentials failed. other must be CockroachDBConnConf"
-        other = cast(CockroachDBConnConf, other)
-        other.db_host = self.db_host
-        other.db_port = self.db_port
-        other.db_name = self.db_name
-        other.user = self.user
-        other.password = self.password
-
-
-@dataclass
-class CustomDBConnConf(RelationalConnConf):
-    _conn_type: PRTConn = PRTConn.CUSTOM_DB
-    # redshift_db_address: Optional[str] = None  # dummy
-    conn_string: Optional[str] = field(default=None, metadata={
-        "ui_map": UIMap(visible_name="Connection String",
-                        tip="Any SQLAlchemy compatible db conn str (might require driver installation)",
-                        default_value=""),
-        "validators": (lambda x: x and len(x) > 0, "No value provided")
-    })
-
-    def internal_equals(self, other: 'ConnConf') -> bool:
-        if not isinstance(other, CustomDBConnConf):
-            return False
-        other = cast(CustomDBConnConf, other)
-        return self.conn_string == other.conn_string
-
-    def _apply_credentials_to(self, other: 'CustomDBConnConf'):
-        assert isinstance(other, CustomDBConnConf), "apply credentials failed. other must be CustomDBConnConf"
-        other = cast(CustomDBConnConf, other)
-        other.conn_string = self.conn_string
-
-
-class ConnConfFactory:
-    @staticmethod
-    def create_or_get(conn_conf_json_dict_or_obj) -> ConnConf:
-        # due to json serialization this method can get json, dict or actual class
-        conn_conf: Optional[ConnConf] = None
-        if isinstance(conn_conf_json_dict_or_obj, str):
-            import json
-            conn_conf_json_dict_or_obj = json.loads(conn_conf_json_dict_or_obj)
-
-        if isinstance(conn_conf_json_dict_or_obj, dict):
-            conn_type_str = conn_conf_json_dict_or_obj['_conn_type']
-            if conn_type_str == PRTConn.LOCAL_FILE:
-                conn_conf = LocalFileConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.NODE_FILE:
-                conn_conf = NodeFileConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.S3:
-                conn_conf = S3ConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.SQLITE:
-                conn_conf = SqLiteConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.MYSQL:
-                conn_conf = MYSQLConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.POSTGRESQL:
-                conn_conf = PostgreSQLConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.REDSHIFT:
-                conn_conf = RedshiftConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.SNOWFLAKE:
-                conn_conf = SnowflakeConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.MSSQL:
-                conn_conf = MSSQLConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.ORACLE:
-                conn_conf = OracleConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.HIVE:
-                conn_conf = HiveConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.ATHENA:
-                conn_conf = AthenaConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.ELASTICSEARCH:
-                conn_conf = ElasticSearchConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.OPENSEARCH:
-                conn_conf = OpenSearchConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.TRINO:
-                conn_conf = TrinoConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.DREMIO:
-                conn_conf = DremioConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.HANA:
-                conn_conf = HanaConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.TERADATA:
-                conn_conf = TeradataConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.DB2:
-                conn_conf = Db2ConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.DYNAMODB:
-                conn_conf = DynamoDBConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.COCKROACHDB:
-                conn_conf = CockroachDBConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.CLOUDERA:
-                conn_conf = ClouderaConnConf.from_dict(conn_conf_json_dict_or_obj)
-            elif conn_type_str == PRTConn.CUSTOM_DB:
-                conn_conf = CustomDBConnConf.from_dict(conn_conf_json_dict_or_obj)
-            else:
-                raise AttributeError(f"Unknown connection type {conn_type_str}")
-        elif issubclass(type(conn_conf_json_dict_or_obj), ConnConf):
-            conn_conf = conn_conf_json_dict_or_obj
-        else:
-            raise SystemError(f"Unknown conn_conf type {type(conn_conf_json_dict_or_obj)}")
-
-        if conn_conf is not None:
-            return conn_conf
-        else:
-            raise SystemError(f"Unknown conn_conf {conn_conf_json_dict_or_obj}")
-
-
-# Engine Configuration Classes
-@dataclass
-class EngConf(DataClassJsonMixin):
-    _eng_type: Optional[PRTEng] = None
-
-    @property
-    def eng_type(self) -> PRTEng:
-        return self._eng_type  # type: ignore[return-value]
-
-
-@dataclass
-class AutoEngConf(EngConf):
-    _eng_type: PRTEng = PRTEng.AUTO
-
-
-@dataclass
-class PandasEngConf(EngConf):
-    _eng_type: PRTEng = PRTEng.PANDAS
-
-
-@dataclass
-class DaskEngConf(PandasEngConf):
-    _eng_type: PRTEng = PRTEng.DASK
-    worker_count: Optional[int] = None
-
-
-@dataclass
-class RapidsEngConf(PandasEngConf):
-    _eng_type: PRTEng = PRTEng.RAPIDS
-
-
-@dataclass
-class RapidsDaskEngConf(DaskEngConf):
-    _eng_type: PRTEng = PRTEng.RAPIDS_DASK
-    worker_count: Optional[int] = None
-
-
-@dataclass
-class SparkEngConf(PandasEngConf):
-    _eng_type: PRTEng = PRTEng.SPARK
-
-
-class EngConfFactory:
-    @staticmethod
-    def create(eng_conf_json_dict_or_obj) -> EngConf:
-        # due to json serialization this method can get json, dict or actual class
-        if not eng_conf_json_dict_or_obj:
-            return PandasEngConf()
-
-        if isinstance(eng_conf_json_dict_or_obj, str):
-            if eng_conf_json_dict_or_obj.strip().startswith("{"):
-                import json
-                eng_conf_json_dict_or_obj = json.loads(eng_conf_json_dict_or_obj)
-            else:
-                # simple engine name, might be coming from exported code library
-                eng_conf_json_dict_or_obj = {'_eng_type': eng_conf_json_dict_or_obj}
-
-        if isinstance(eng_conf_json_dict_or_obj, dict):
-            eng_type_str = str(eng_conf_json_dict_or_obj['_eng_type']).upper()
-            if eng_type_str == PRTEng.AUTO:
-                return AutoEngConf.from_dict(eng_conf_json_dict_or_obj)
-            elif eng_type_str == PRTEng.PANDAS:
-                return PandasEngConf.from_dict(eng_conf_json_dict_or_obj)
-            elif eng_type_str == PRTEng.DASK:
-                return DaskEngConf.from_dict(eng_conf_json_dict_or_obj)
-            elif eng_type_str == PRTEng.RAPIDS:
-                return RapidsEngConf.from_dict(eng_conf_json_dict_or_obj)
-            elif eng_type_str == PRTEng.RAPIDS_DASK:
-                return RapidsDaskEngConf.from_dict(eng_conf_json_dict_or_obj)
-            elif eng_type_str == PRTEng.SPARK:
-                return SparkEngConf.from_dict(eng_conf_json_dict_or_obj)
-            else:
-                raise AttributeError(f"Unknown engine type {eng_type_str}")
-        elif isinstance(eng_conf_json_dict_or_obj, EngConf):
-            return eng_conf_json_dict_or_obj
-        elif isinstance(eng_conf_json_dict_or_obj, PRTEng):
-            if eng_conf_json_dict_or_obj == PRTEng.AUTO:
-                return AutoEngConf()
-            elif eng_conf_json_dict_or_obj == PRTEng.PANDAS:
-                return PandasEngConf()
-            elif eng_conf_json_dict_or_obj == PRTEng.DASK:
-                return DaskEngConf()
-            elif eng_conf_json_dict_or_obj == PRTEng.RAPIDS:
-                return RapidsEngConf()
-            elif eng_conf_json_dict_or_obj == PRTEng.RAPIDS_DASK:
-                return RapidsDaskEngConf()
-            elif eng_conf_json_dict_or_obj == PRTEng.SPARK:
-                return SparkEngConf()
-            else:
-                raise AttributeError(f"Unknown PRTEng type {eng_conf_json_dict_or_obj}")
-        else:
-            raise SystemError(f"Unknown eng_conf type {type(eng_conf_json_dict_or_obj)}")
-
-
-@dataclass
-class PRTDataRequest(PRTRequest):
-    # ** We needed to add dict to this list since when dataclass_json cannot figure out type
-    #    it returns dict instead of actual class. need to override or use as dict
-    conn_conf: Optional[Union[
-        dict,
-        ConnConf,
-        NodeFileConnConf,
-        SqLiteConnConf,
-        S3ConnConf,
-        MYSQLConnConf,
-        PostgreSQLConnConf,
-        RedshiftConnConf,
-        SnowflakeConnConf,
-        MSSQLConnConf,
-        OracleConnConf,
-        HiveConnConf,
-        AthenaConnConf,
-        ElasticSearchConnConf,
-        OpenSearchConnConf,
-        TrinoConnConf,
-        DremioConnConf,
-        HanaConnConf,
-        TeradataConnConf,
-        Db2ConnConf,
-        DynamoDBConnConf,
-        CockroachDBConnConf,
-        ClouderaConnConf,
-        CustomDBConnConf,
-    ]] = None
-
-    eng_conf: Optional[Union[
-        dict,
-        PandasEngConf,
-        DaskEngConf,
-        RapidsEngConf,
-        RapidsDaskEngConf,
-        SparkEngConf,
-    ]] = None
-
-    # MySQLConnDef,
-    # AuroraMySQLConnDef,
-
-
-class ConnConfClassFactory:
-    @staticmethod
-    def get_conn_conf_class(conn_type: PRTConn) -> Union[
-        Type[NodeFileConnConf],
-        Type[S3ConnConf],
-        Type[SqLiteConnConf],
-        Type[PostgreSQLConnConf],
-        Type[MYSQLConnConf],
-        Type[RedshiftConnConf],
-        Type[SnowflakeConnConf],
-        Type[MSSQLConnConf],
-        Type[OracleConnConf],
-        Type[HiveConnConf],
-        Type[AthenaConnConf],
-        Type[ElasticSearchConnConf],
-        Type[OpenSearchConnConf],
-        Type[TrinoConnConf],
-        Type[DremioConnConf],
-        Type[HanaConnConf],
-        Type[TeradataConnConf],
-        Type[Db2ConnConf],
-        Type[DynamoDBConnConf],
-        Type[CockroachDBConnConf],
-        Type[ClouderaConnConf],
-        Type[CustomDBConnConf]
-    ]:
-        if conn_type == PRTConn.NODE_FILE:
-            return NodeFileConnConf
-        elif conn_type == PRTConn.S3:
-            return S3ConnConf
-        elif conn_type == PRTConn.SQLITE:
-            return SqLiteConnConf
-        elif conn_type == PRTConn.POSTGRESQL:
-            return PostgreSQLConnConf
-        elif conn_type == PRTConn.MYSQL:
-            return MYSQLConnConf
-        elif conn_type == PRTConn.REDSHIFT:
-            return RedshiftConnConf
-        elif conn_type == PRTConn.SNOWFLAKE:
-            return SnowflakeConnConf
-        elif conn_type == PRTConn.MSSQL:
-            return MSSQLConnConf
-        elif conn_type == PRTConn.ORACLE:
-            return OracleConnConf
-        elif conn_type == PRTConn.HIVE:
-            return HiveConnConf
-        elif conn_type == PRTConn.ATHENA:
-            return AthenaConnConf
-        elif conn_type == PRTConn.ELASTICSEARCH:
-            return ElasticSearchConnConf
-        elif conn_type == PRTConn.OPENSEARCH:
-            return OpenSearchConnConf
-        elif conn_type == PRTConn.TRINO:
-            return TrinoConnConf
-        elif conn_type == PRTConn.DREMIO:
-            return DremioConnConf
-        elif conn_type == PRTConn.HANA:
-            return HanaConnConf
-        elif conn_type == PRTConn.TERADATA:
-            return TeradataConnConf
-        elif conn_type == PRTConn.DB2:
-            return Db2ConnConf
-        elif conn_type == PRTConn.DYNAMODB:
-            return DynamoDBConnConf
-        elif conn_type == PRTConn.COCKROACHDB:
-            return CockroachDBConnConf
-        elif conn_type == PRTConn.CLOUDERA:
-            return ClouderaConnConf
-        elif conn_type == PRTConn.CUSTOM_DB:
-            return CustomDBConnConf
-        else:
-            raise SystemError(f"Unable to find conn_conf class for conn_type :{conn_type.name}")
-
-
-if __name__ == "__main__":
-    pass
-    # eng_conf = RapidsDaskEngConf()
-    # eng_conf = EngConfFactory.create(eng_conf)
-    # print(eng_conf)
+import os.path
+import sys
+from abc import ABC
+import platform
+from dataclasses import dataclass, field, fields
+from datetime import datetime
+from typing import List, Optional, Union, cast, Type, Tuple
+
+import dataclasses_json
+
+from practicuscore.core_def import PRTEng, PRTConn, CoreDef, OPResult
+
+
+class PrtDataClassJsonMixin(dataclasses_json.DataClassJsonMixin):
+    dataclass_json_config = dataclasses_json.config(  # type: ignore
+        undefined=dataclasses_json.Undefined.EXCLUDE,
+        exclude=lambda f: f is None  # type: ignore
+    )["dataclasses_json"]
+
+
+class PRTValidator(ABC):
+    @staticmethod
+    def validate(dataclass_obj) -> Tuple[Optional[str], Optional[str]]:
+        """
+        Validates all fields on a dataclass, *if only* it has "validators" metadata.
+        "validators" can be a single tuple (lambda_func, "err message") OR a list of validation tuples
+           i.e. use a single validator:
+            some_field: int = field(
+                metadata={
+                    "validators": (lambda x: x > 0, "Must be > 0")
+                })
+           OR multiple validators:
+            some_field: int = field(
+                metadata={
+                    "validators": [(lambda x: x > 0, "Must be > 0"),
+                                   (lambda x: x < 10, "Must be < 10")]
+                })
+        :param dataclass_obj: The dataclass object to validate. Must have validators defined
+        :return: if a field has errors, (field name, error message) tuple. Or (None, None)
+        """
+        for fld in fields(dataclass_obj):
+            if "validators" in fld.metadata:
+                validator_or_validators = fld.metadata["validators"]
+
+                if isinstance(validator_or_validators, tuple):
+                    validators = [validator_or_validators]
+                else:
+                    validators = validator_or_validators
+                for validator_tuple in validators:
+                    assert isinstance(validator_tuple, tuple), \
+                        "Validator must be a tuple in the form of (validator_lambda, 'error message')"
+                    validator_func, validator_err_msg = validator_tuple
+                    field_val = getattr(dataclass_obj, fld.name)
+                    try:
+                        failed = False
+                        if not validator_func(field_val):
+                            failed = True
+                    except Exception as ex:
+                        failed = True
+                        validator_err_msg = f"Exception occurred while checking for '{validator_err_msg}', " \
+                                            f"\nException: {ex}"
+
+                    if failed:
+                        return fld.name, validator_err_msg  # return info about *first* encountered issue
+
+        return None, None  # no issues, nothing to return
+
+
+@dataclass
+class RequestMeta(PrtDataClassJsonMixin):
+    meta_type: str = "Request"
+    meta_name: str = ""
+    req_time: Optional[datetime] = None
+    req_core_v: str = CoreDef.CORE_VERSION
+    req_os: str = platform.system()
+    req_os_v: str = platform.release()
+    req_py_minor_v: int = sys.version_info.minor
+
+
+@dataclass
+class PRTRequest(PrtDataClassJsonMixin):
+    # Creating a meta class here caused a nasty bug. meta became a shared object between child classes
+    #   i.e. when __post_init() below updated meta_name for one type of Request class, al others got the new name
+    # meta: RequestMeta = RequestMeta()
+    meta: Optional[RequestMeta] = None
+
+    @property
+    def name(self) -> str:
+        assert self.meta is not None
+        return self.meta.meta_name
+
+    def __post_init__(self):
+        self.meta = RequestMeta()
+        self.meta.meta_name = self.__class__.__name__.rsplit("Request", 1)[0]
+        # do not assign defaults in class definition. Gets assigned static one time
+        self.meta.req_time = datetime.utcnow()
+
+
+@dataclass
+class ResponseMeta(PrtDataClassJsonMixin):
+    meta_type: str = "Response"
+    meta_name: str = ""
+    resp_node_v: str = ""  # assigned later right before sending to client
+    resp_py_minor_v: int = sys.version_info.minor
+
+
+@dataclass
+class PRTResponse(PrtDataClassJsonMixin):
+    meta: Optional[ResponseMeta] = None
+    op_result: Optional[OPResult] = None
+
+    # meta: ResponseMeta = ResponseMeta()  Don't instantiate here. Read notes for PRTRequest
+
+    @property
+    def name(self) -> str:
+        assert self.meta is not None
+        return self.meta.meta_name
+
+    def __post_init__(self):
+        self.meta = ResponseMeta()
+        self.meta.meta_name = self.__class__.__name__.rsplit("Response", 1)[0]
+        # do not assign defaults in class definition. Gets assigned static one time
+        self.meta.req_time = datetime.utcnow()
+
+
+@dataclass
+class EmptyResponse(PRTResponse):
+    # used when there's an error, no response can be created and we hae op_result send back
+    pass
+
+
+# Connection configuration classes
+
+@dataclass
+class UIMap(PrtDataClassJsonMixin):
+    # Helps map an individual ConnConf field to a single GUI element.
+    # i.e. MYSQL related field "db_host" should be visible as "Database Host Address", it is required, ...
+    visible_name: Optional[str] = None
+    auto_display: bool = True  # some fields are displayed manually and not automated
+    tip: Optional[str] = None
+    default_value: Optional[str] = None
+    is_required: bool = True
+    is_password: bool = False
+
+
+@dataclass
+class ConnConf(PrtDataClassJsonMixin):
+    connection_type: Optional[PRTConn] = None
+    is_enriched: Optional[bool] = None
+    uuid: Optional[str] = None
+    ws_uuid: Optional[str] = None
+    ws_name: Optional[str] = None
+    sampling_method: Optional[str] = None
+    sample_size: Optional[int] = None
+    sample_size_app: Optional[int] = None
+    column_list: Optional[List[str]] = None
+    filter: Optional[str] = None
+
+    def __str__(self):
+        return str(self.to_json())
+
+    def __repr__(self):
+        return str(self)
+
+    @property
+    def enriched(self) -> bool:
+        return bool(self.is_enriched) if self.is_enriched is not None else False
+
+    @property
+    def conn_type(self) -> PRTConn:
+        assert self.connection_type is not None
+        return self.connection_type
+
+    @property
+    def friendly_desc(self) -> str:
+        # override with children class for a better user-friendly
+        return str(self.connection_type)
+
+    @property
+    def friendly_long_desc(self) -> str:
+        return self.friendly_desc
+
+    def copy_secure(self) -> 'ConnConf':
+        import copy
+        return copy.copy(self)
+
+    def copy_with_credentials(self, credentials: Optional[dict] = None) -> 'ConnConf':
+        return self.copy_secure()
+
+    def apply_conf_to(self, other: 'ConnConf'):
+        self._apply_conf_to(other)
+        self.is_enriched = True
+
+    def _apply_conf_to(self, other: 'ConnConf'):
+        pass
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        pass
+
+    def __eq__(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, ConnConf):
+            return False
+        other = cast(ConnConf, other)
+        equals = (self.conn_type == other.conn_type
+                  and self.sample_size == other.sample_size
+                  and self.sample_size_app == other.sample_size_app
+                  and self.column_list == other.column_list
+                  and self.filter == other.filter)
+        if not equals:
+            return False
+        return self.internal_equals(other)
+
+
+@dataclass
+class InMemoryConnConf(ConnConf):
+    connection_type: PRTConn = PRTConn.IN_MEMORY
+    df: Optional[object] = None
+
+    def __repr__(self):
+        return str(self)
+
+    @property
+    def friendly_desc(self) -> str:
+        return "In memory dataframe"
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, InMemoryConnConf):
+            return False
+        other = cast(InMemoryConnConf, other)
+        return self.df.__eq__(other.df)
+
+
+@dataclass
+class LocalFileConnConf(ConnConf):
+    connection_type: PRTConn = PRTConn.LOCAL_FILE
+    file_path: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="File Path",
+                        tip="Type path on local disk")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    @property
+    def friendly_desc(self) -> str:
+        try:
+            if self.file_path:
+                return os.path.splitext(os.path.basename(self.file_path))[0]
+        except:
+            pass
+
+        return self.conn_type.lower()
+
+    @property
+    def friendly_long_desc(self) -> str:
+        final_desc = self.file_path
+        if final_desc and len(final_desc) > 30:
+            final_desc = f"{final_desc[:15]} ... {final_desc[-10:]}"
+        return final_desc if final_desc else "?"
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, LocalFileConnConf):
+            return False
+        other = cast(LocalFileConnConf, other)
+        return self.file_path == other.file_path
+
+    @property
+    def is_prt_file(self) -> bool:
+        if self.file_path:
+            return self.file_path.endswith(CoreDef.APP_FILE_TYPE)
+        return False
+
+
+@dataclass
+class WorkerFileConnConf(ConnConf):
+    connection_type: PRTConn = PRTConn.WORKER_FILE
+    file_path: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="File Path",
+                        tip="Type path on Worker local disk. E.g. /home/ubuntu/data/file.csv")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    @property
+    def friendly_desc(self) -> str:
+        try:
+            if self.file_path:
+                return os.path.splitext(os.path.basename(self.file_path))[0]
+        except:
+            pass
+
+        return self.conn_type.lower()
+
+    @property
+    def friendly_long_desc(self) -> str:
+        final_desc = self.file_path
+        if final_desc and len(final_desc) > 30:
+            final_desc = f"{final_desc[:15]} ... {final_desc[-10:]}"
+        return final_desc if final_desc else "?"
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, WorkerFileConnConf):
+            return False
+        other = cast(WorkerFileConnConf, other)
+        return self.file_path == other.file_path
+
+    @property
+    def is_prt_file(self) -> bool:
+        if self.file_path:
+            return self.file_path.endswith(CoreDef.APP_FILE_TYPE)
+        return False
+
+
+@dataclass
+class S3ConnConf(ConnConf):
+    connection_type: PRTConn = PRTConn.S3
+    aws_region: Optional[str] = None
+    aws_access_key_id: Optional[str] = None
+    aws_secret_access_key: Optional[str] = None
+    aws_session_token: Optional[str] = None
+    endpoint_url: Optional[str] = None
+    s3_bucket: Optional[str] = None
+    s3_keys: Optional[List[str]] = None
+    default_prefix: Optional[str] = None
+
+    def __repr__(self):
+        return str(self)
+
+    @property
+    def friendly_desc(self) -> str:
+        try:
+            if self.s3_keys:
+                if len(self.s3_keys) >= 1:
+                    s3_key = self.s3_keys[0]
+                    if s3_key.find(".") > -1:
+                        return os.path.splitext(os.path.basename(s3_key))[0]
+                    else:
+                        return os.path.basename(os.path.normpath(s3_key))
+        except:
+            pass
+        return self.conn_type.lower()
+
+    @property
+    def friendly_long_desc(self) -> str:
+        if self.s3_bucket:
+            bucket_desc = f"s3://{self.s3_bucket}"
+        else:
+            # k8s currently send no bucket name..
+            bucket_desc = f"s3://_bucket_"
+        keys_desc = "?"
+        if self.s3_keys:
+            if len(self.s3_keys) == 1:
+                keys_desc = self.s3_keys[0]
+            else:
+                keys_desc = f"{self.s3_keys[0]} .."
+
+        final_desc = f"{bucket_desc}/{keys_desc}"
+        if len(final_desc) > 30:
+            final_desc = f"{final_desc[:15]} ... {final_desc[-10:]}"
+        return final_desc
+
+    def copy_secure(self) -> ConnConf:
+        copy_conn_conf = super().copy_secure()
+        assert isinstance(copy_conn_conf, S3ConnConf)
+        copy_conn_conf = cast(S3ConnConf, copy_conn_conf)
+        copy_conn_conf.aws_access_key_id = None
+        copy_conn_conf.aws_secret_access_key = None
+        copy_conn_conf.aws_session_token = None
+        return copy_conn_conf
+
+    def copy_with_credentials(self, credentials: Optional[dict] = None) -> ConnConf:
+        copy_conn_conf = super().copy_secure()
+        copy_conn_conf = cast(S3ConnConf, copy_conn_conf)
+        if credentials is not None:
+            if "aws_access_key_id" in credentials:
+                copy_conn_conf.aws_access_key_id = str(credentials["aws_access_key_id"])
+            if "aws_secret_access_key" in credentials:
+                copy_conn_conf.aws_secret_access_key = str(credentials["aws_secret_access_key"])
+            if "aws_session_token" in credentials:
+                copy_conn_conf.aws_session_token = str(credentials["aws_session_token"])
+        return copy_conn_conf
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, S3ConnConf):
+            return False
+        other = cast(S3ConnConf, other)
+        return self.aws_region == other.aws_region \
+            and self.aws_access_key_id == other.aws_access_key_id \
+            and self.aws_secret_access_key == other.aws_secret_access_key \
+            and self.aws_session_token == other.aws_session_token \
+            and self.s3_bucket == other.s3_bucket \
+            and self.s3_keys == other.s3_keys
+
+    def _apply_conf_to(self, other: 'ConnConf'):
+        assert isinstance(other, S3ConnConf), "apply credentials failed. other must be S3ConnConf"
+        other = cast(S3ConnConf, other)
+        other.aws_region = self.aws_region
+        other.aws_access_key_id = self.aws_access_key_id
+        other.aws_secret_access_key = self.aws_secret_access_key
+        other.s3_bucket = self.s3_bucket
+        other.endpoint_url = self.endpoint_url
+
+
+@dataclass
+class RelationalConnConf(ConnConf):
+    sql_query: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="SQL Query",
+                        auto_display=False),
+        "validators": (lambda x: x and len(x) > 0, "No SQL query provided")
+    })
+
+    target_table_name: Optional[str] = None
+    target_schema: Optional[str] = None
+    target_table_if_exists: Optional[str] = None
+    chunk_size: Optional[int] = None
+    prefer_db_api: Optional[bool] = None
+
+    def __repr__(self):
+        return str(self)
+
+    def _find_table_name(self, keyword: str) -> Optional[str]:
+        if self.sql_query:
+            table_ind = self.sql_query.lower().find(f"{keyword} ") + len(f"{keyword} ")
+            if table_ind > -1:
+                desc = self.sql_query[table_ind:].strip().lower()
+                next_stop = desc.find(" ")
+                if next_stop > -1:
+                    desc = desc[:next_stop]
+                next_stop = desc.find(",")
+                if next_stop > -1:
+                    desc = desc[:next_stop]
+                next_stop = desc.find("\n")
+                if next_stop > -1:
+                    desc = desc[:next_stop]
+
+                desc = desc.strip()
+                if desc:
+                    return desc
+        return None
+
+    @property
+    def friendly_desc(self) -> str:
+        try:
+            table_name = self._find_table_name("from")
+            if not table_name:
+                table_name = self._find_table_name("into")
+
+            if table_name:
+                return table_name
+
+            if self.sql_query:
+                return self.sql_query[:10] + f"{'..' if len(self.sql_query) > 10 else ''}"
+        except:
+            pass
+        assert self.connection_type is not None
+        return self.connection_type.lower()
+
+    @property
+    def friendly_long_desc(self) -> str:
+        assert self.connection_type is not None
+        table_name: Optional[str]
+        if self.target_table_name:
+            table_name = self.target_table_name
+            if self.target_schema:
+                table_name = f"{self.target_schema}.{table_name}"
+            return f"{self.connection_type.capitalize()} table: {table_name}"
+        else:
+            table_name = self._find_table_name("from")
+            if not table_name:
+                table_name = self._find_table_name("into")
+
+            if self.target_schema and table_name:
+                table_name = f"{self.target_schema}.{table_name}"
+
+            desc = ""
+            if table_name:
+                desc = f": {table_name}"
+            elif self.sql_query:
+                desc += f": {self.sql_query[:20]}{'...' if len(self.sql_query) > 20 else ''}"
+            desc = desc.replace("\n", " ")
+            final_desc = f"{self.connection_type.capitalize()}{desc}"
+            if len(final_desc) > 30:
+                final_desc = f"{final_desc[:15]} ... {final_desc[-10:]}"
+            return final_desc
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, RelationalConnConf):
+            return False
+        other = cast(RelationalConnConf, other)
+        return self.sql_query == other.sql_query and self.target_table_name == other.target_table_name
+
+
+@dataclass
+class SqLiteConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.SQLITE
+    file_path: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="File Path",
+                        tip="Type path on Worker local disk. E.g. /home/ubuntu/data/database.db",
+                        default_value=CoreDef.NODE_HOME_PATH + "/samples/chinook.db"),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, SqLiteConnConf):
+            return False
+        other = cast(SqLiteConnConf, other)
+        return self.file_path == other.file_path
+
+    def _apply_conf_to(self, other: 'ConnConf'):
+        assert isinstance(other, SqLiteConnConf), "apply credentials failed. other must be SqLiteConnConf"
+        other = cast(SqLiteConnConf, other)
+        other.file_path = self.file_path
+
+
+@dataclass
+class MYSQLConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.MYSQL
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="3306"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, MYSQLConnConf):
+            return False
+        other = cast(MYSQLConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_name == other.db_name \
+            and self.db_port == other.db_port \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'MYSQLConnConf'):
+        assert isinstance(other, MYSQLConnConf), "apply credentials failed. other must be MYSQLConnConf"
+        other = cast(MYSQLConnConf, other)
+        other.db_host = self.db_host
+        other.db_name = self.db_name
+        other.db_port = self.db_port
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class PostgreSQLConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.POSTGRESQL
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="5432"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, PostgreSQLConnConf):
+            return False
+        other = cast(PostgreSQLConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_name == other.db_name \
+            and self.db_port == other.db_port \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'PostgreSQLConnConf'):
+        assert isinstance(other, PostgreSQLConnConf), "apply credentials failed. other must be PostgreSQLConnConf"
+        other = cast(PostgreSQLConnConf, other)
+        other.db_host = self.db_host
+        other.db_name = self.db_name
+        other.db_port = self.db_port
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class RedshiftConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.REDSHIFT
+    # redshift_db_address: Optional[str] = None  # dummy
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="5439"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, RedshiftConnConf):
+            return False
+        other = cast(RedshiftConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_name == other.db_name \
+            and self.db_port == other.db_port \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'RedshiftConnConf'):
+        assert isinstance(other, RedshiftConnConf), "apply credentials failed. other must be RedshiftConnConf"
+        other = cast(RedshiftConnConf, other)
+        other.db_host = self.db_host
+        other.db_name = self.db_name
+        other.db_port = self.db_port
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class SnowflakeConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.SNOWFLAKE
+    db_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_schema: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Schema",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    warehouse_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Warehouse Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    role: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Role",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    account: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Account Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        ),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, SnowflakeConnConf):
+            return False
+        other = cast(SnowflakeConnConf, other)
+        return self.db_name == other.db_name \
+            and self.db_schema == other.db_schema \
+            and self.warehouse_name == other.warehouse_name \
+            and self.user == other.user \
+            and self.role == other.role \
+            and self.account == other.account \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'SnowflakeConnConf'):
+        assert isinstance(other, SnowflakeConnConf), "apply credentials failed. other must be SnowflakeConnConf"
+        other = cast(SnowflakeConnConf, other)
+        other.db_name = self.db_name
+        other.db_schema = self.db_schema
+        other.warehouse_name = self.warehouse_name
+        other.user = self.user
+        other.role = self.role
+        other.account = self.account
+        other.password = self.password
+
+
+@dataclass
+class MSSQLConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.MSSQL
+    # redshift_db_address: Optional[str] = None  # dummy
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    # driver: Optional[str] = field(default=None, metadata={
+    #     "ui_map": UIMap(visible_name="Driver Name",
+    #                     default_value="SQL Server Native Client 10.0"),
+    #     "validators": (lambda x: x and len(x) > 0, "No value provided")
+    # })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="1433"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, MSSQLConnConf):
+            return False
+        other = cast(MSSQLConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_name == other.db_name \
+            and self.db_port == other.db_port \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'MSSQLConnConf'):
+        assert isinstance(other, MSSQLConnConf), "apply credentials failed. other must be MSSQLConnConf"
+        other = cast(MSSQLConnConf, other)
+        other.db_host = self.db_host
+        other.db_name = self.db_name
+        other.db_port = self.db_port
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class OracleConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.ORACLE
+    # redshift_db_address: Optional[str] = None  # dummy
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    service_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Service Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    sid: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Sid",
+                        is_required=False,
+                        default_value="")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="1521"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, OracleConnConf):
+            return False
+        other = cast(OracleConnConf, other)
+        return self.db_host == other.db_host \
+            and self.service_name == other.service_name \
+            and self.sid == other.sid \
+            and self.db_port == other.db_port \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'OracleConnConf'):
+        assert isinstance(other, OracleConnConf), "apply credentials failed. other must be OracleConnConf"
+        other = cast(OracleConnConf, other)
+        other.db_host = self.db_host
+        other.service_name = self.service_name
+        other.sid = self.sid
+        other.db_port = self.db_port
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class HiveConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.HIVE
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="10000"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, HiveConnConf):
+            return False
+        other = cast(HiveConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_name == other.db_name \
+            and self.db_port == other.db_port \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'HiveConnConf'):
+        assert isinstance(other, HiveConnConf), "apply credentials failed. other must be HiveConnConf"
+        other = cast(HiveConnConf, other)
+        other.db_host = self.db_host
+        other.db_name = self.db_name
+        other.db_port = self.db_port
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class ClouderaConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.CLOUDERA
+    host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Cloudera Host",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="21050"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, ClouderaConnConf):
+            return False
+        other = cast(ClouderaConnConf, other)
+        return self.host == other.host \
+            and self.port == other.port \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'ClouderaConnConf'):
+        assert isinstance(other, ClouderaConnConf), "apply credentials failed. other must be ClouderaConnConf"
+        other = cast(ClouderaConnConf, other)
+        other.host = self.host
+        other.port = self.port
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class AthenaConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.ATHENA
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        tip="E.g. test.abcde.us-east-1.rds.amazonaws.com or 192.168.0.1",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    s3_dir: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="S3 Location",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="443"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    access_key: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="AWS Access key ID",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    secret_key: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="AWS Secret access key",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, AthenaConnConf):
+            return False
+        other = cast(AthenaConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_name == other.db_name \
+            and self.s3_dir == other.s3_dir \
+            and self.db_port == other.db_port \
+            and self.access_key == other.access_key \
+            and self.secret_key == other.secret_key
+
+    def _apply_conf_to(self, other: 'AthenaConnConf'):
+        assert isinstance(other, AthenaConnConf), "apply credentials failed. other must be AthenaConnConf"
+        other = cast(AthenaConnConf, other)
+        other.db_host = self.db_host
+        other.db_name = self.db_name
+        other.db_port = self.db_port
+        other.s3_dir = self.s3_dir
+        other.access_key = self.access_key
+        other.secret_key = self.secret_key
+
+
+@dataclass
+class ElasticSearchConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.ELASTICSEARCH
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        tip="E.g. test-2.latest-elasticsearch.abc-3.xyz.com or 192.168.0.1",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value=""),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, ElasticSearchConnConf):
+            return False
+        other = cast(ElasticSearchConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_port == other.db_port \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'ElasticSearchConnConf'):
+        assert isinstance(other, ElasticSearchConnConf), "apply credentials failed. other must be ElasticSearchConnConf"
+        other = cast(ElasticSearchConnConf, other)
+        other.db_host = self.db_host
+        other.db_port = self.db_port
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class OpenSearchConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.OPENSEARCH
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        tip="E.g. search-test-abcde.us-east-1.es.amazonaws.com",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="443"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, OpenSearchConnConf):
+            return False
+        other = cast(OpenSearchConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_port == other.db_port \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'OpenSearchConnConf'):
+        assert isinstance(other, OpenSearchConnConf), "apply credentials failed. other must be OpenSearchConnConf"
+        other = cast(OpenSearchConnConf, other)
+        other.db_host = self.db_host
+        other.db_port = self.db_port
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class TrinoConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.TRINO
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        tip="E.g. localhost",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="8080"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    catalog: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Catalog",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_schema: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Schema",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, TrinoConnConf):
+            return False
+        other = cast(TrinoConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_port == other.db_port \
+            and self.catalog == other.catalog \
+            and self.db_schema == other.db_schema \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'TrinoConnConf'):
+        assert isinstance(other, TrinoConnConf), "apply credentials failed. other must be TrinoConnConf"
+        other = cast(TrinoConnConf, other)
+        other.db_host = self.db_host
+        other.db_port = self.db_port
+        other.catalog = self.catalog
+        other.db_schema = self.db_schema
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class DremioConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.DREMIO
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        tip="E.g. localhost",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="31010"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, DremioConnConf):
+            return False
+        other = cast(TrinoConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_port == other.db_port \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'DremioConnConf'):
+        assert isinstance(other, DremioConnConf), "apply credentials failed. other must be DremioConnConf"
+        other = cast(DremioConnConf, other)
+        other.db_host = self.db_host
+        other.db_port = self.db_port
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class HanaConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.HANA
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="39015"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    db_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Name",
+                        is_required=False,
+                        default_value="")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, HanaConnConf):
+            return False
+        other = cast(HanaConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_port == other.db_port \
+            and self.db_name == other.db_name \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'HanaConnConf'):
+        assert isinstance(other, HanaConnConf), "apply credentials failed. other must be HanaConnConf"
+        other = cast(HanaConnConf, other)
+        other.db_host = self.db_host
+        other.db_port = self.db_port
+        other.db_name = self.db_name
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class TeradataConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.TERADATA
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, TeradataConnConf):
+            return False
+        other = cast(TeradataConnConf, other)
+        return self.db_host == other.db_host \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'TeradataConnConf'):
+        assert isinstance(other, TeradataConnConf), "apply credentials failed. other must be TeradataConnConf"
+        other = cast(TeradataConnConf, other)
+        other.db_host = self.db_host
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class Db2ConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.DB2
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="39015"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    db_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, Db2ConnConf):
+            return False
+        other = cast(Db2ConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_port == other.db_port \
+            and self.db_name == other.db_name \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'Db2ConnConf'):
+        assert isinstance(other, Db2ConnConf), "apply credentials failed. other must be Db2ConnConf"
+        other = cast(Db2ConnConf, other)
+        other.db_host = self.db_host
+        other.db_port = self.db_port
+        other.db_name = self.db_name
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class DynamoDBConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.DYNAMODB
+    access_key: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="AWS Access Key Id",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    secret_key: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="AWS Secret Access Key",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    region: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="AWS Region Name",
+                        tip="E.g. us-east-1",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, DynamoDBConnConf):
+            return False
+        other = cast(DynamoDBConnConf, other)
+        return self.access_key == other.access_key \
+            and self.secret_key == other.secret_key \
+            and self.region == other.region
+
+    def _apply_conf_to(self, other: 'DynamoDBConnConf'):
+        assert isinstance(other, DynamoDBConnConf), "apply credentials failed. other must be DynamoDBConnConf"
+        other = cast(DynamoDBConnConf, other)
+        other.access_key = self.access_key
+        other.secret_key = self.secret_key
+        other.region = self.region
+
+
+@dataclass
+class CockroachDBConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.COCKROACHDB
+    db_host: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Server Address",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    db_port: Optional[int] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Port",
+                        default_value="26257"),
+        "validators": (lambda x: 1 <= int(x) <= 65_535, "Port must be between 1 and 65,535")
+    })
+    db_name: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Database Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    user: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="User Name",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+    password: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Password",
+                        is_password=True,
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, CockroachDBConnConf):
+            return False
+        other = cast(CockroachDBConnConf, other)
+        return self.db_host == other.db_host \
+            and self.db_port == other.db_port \
+            and self.db_name == other.db_name \
+            and self.user == other.user \
+            and self.password == other.password
+
+    def _apply_conf_to(self, other: 'CockroachDBConnConf'):
+        assert isinstance(other, CockroachDBConnConf), "apply credentials failed. other must be CockroachDBConnConf"
+        other = cast(CockroachDBConnConf, other)
+        other.db_host = self.db_host
+        other.db_port = self.db_port
+        other.db_name = self.db_name
+        other.user = self.user
+        other.password = self.password
+
+
+@dataclass
+class CustomDBConnConf(RelationalConnConf):
+    connection_type: PRTConn = PRTConn.CUSTOM_DB
+    # redshift_db_address: Optional[str] = None  # dummy
+    conn_string: Optional[str] = field(default=None, metadata={
+        "ui_map": UIMap(visible_name="Connection String",
+                        tip="Any SQLAlchemy compatible db conn str (might require driver installation)",
+                        default_value=""),
+        "validators": (lambda x: x and len(x) > 0, "No value provided")
+    })
+
+    def __repr__(self):
+        return str(self)
+
+    def internal_equals(self, other: 'ConnConf') -> bool:
+        if not isinstance(other, CustomDBConnConf):
+            return False
+        other = cast(CustomDBConnConf, other)
+        return self.conn_string == other.conn_string
+
+    def _apply_conf_to(self, other: 'CustomDBConnConf'):
+        assert isinstance(other, CustomDBConnConf), "apply credentials failed. other must be CustomDBConnConf"
+        other = cast(CustomDBConnConf, other)
+        other.conn_string = self.conn_string
+
+
+class ConnConfFactory:
+    @staticmethod
+    def create_or_get(conn_conf_json_dict_or_obj) -> ConnConf:
+        # due to json serialization this method can get json, dict or actual class
+        conn_conf: Optional[ConnConf]
+        if isinstance(conn_conf_json_dict_or_obj, str):
+            import json
+            conn_conf_json_dict_or_obj = json.loads(conn_conf_json_dict_or_obj)
+
+        if isinstance(conn_conf_json_dict_or_obj, dict):
+            if 'connection_type' in conn_conf_json_dict_or_obj:
+                conn_type_str = conn_conf_json_dict_or_obj['connection_type']
+            else:
+                print("WARNING: Using _conn_type in connection json will be retired. Please replace this connection with a newer version")
+                # future: retire in 2024-12. _conn_type is legacy definition for connection_type
+                conn_type_str = conn_conf_json_dict_or_obj['_conn_type']
+            if conn_type_str == PRTConn.LOCAL_FILE:
+                conn_conf = LocalFileConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.WORKER_FILE:
+                conn_conf = WorkerFileConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.S3:
+                conn_conf = S3ConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.SQLITE:
+                conn_conf = SqLiteConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.MYSQL:
+                conn_conf = MYSQLConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.POSTGRESQL:
+                conn_conf = PostgreSQLConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.REDSHIFT:
+                conn_conf = RedshiftConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.SNOWFLAKE:
+                conn_conf = SnowflakeConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.MSSQL:
+                conn_conf = MSSQLConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.ORACLE:
+                conn_conf = OracleConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.HIVE:
+                conn_conf = HiveConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.ATHENA:
+                conn_conf = AthenaConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.ELASTICSEARCH:
+                conn_conf = ElasticSearchConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.OPENSEARCH:
+                conn_conf = OpenSearchConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.TRINO:
+                conn_conf = TrinoConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.DREMIO:
+                conn_conf = DremioConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.HANA:
+                conn_conf = HanaConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.TERADATA:
+                conn_conf = TeradataConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.DB2:
+                conn_conf = Db2ConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.DYNAMODB:
+                conn_conf = DynamoDBConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.COCKROACHDB:
+                conn_conf = CockroachDBConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.CLOUDERA:
+                conn_conf = ClouderaConnConf.from_dict(conn_conf_json_dict_or_obj)
+            elif conn_type_str == PRTConn.CUSTOM_DB:
+                conn_conf = CustomDBConnConf.from_dict(conn_conf_json_dict_or_obj)
+            else:
+                raise AttributeError(f"Unknown connection type {conn_type_str}")
+        elif issubclass(type(conn_conf_json_dict_or_obj), ConnConf):
+            conn_conf = conn_conf_json_dict_or_obj
+        else:
+            raise SystemError(f"Unknown conn_conf type {type(conn_conf_json_dict_or_obj)}")
+
+        if conn_conf is not None:
+            return conn_conf
+        else:
+            raise SystemError(f"Unknown conn_conf {conn_conf_json_dict_or_obj}")
+
+
+# Engine Configuration Classes
+@dataclass
+class EngConf(PrtDataClassJsonMixin):
+    _eng_type: Optional[PRTEng] = None
+
+    @property
+    def eng_type(self) -> PRTEng:
+        return self._eng_type  # type: ignore[return-value]
+
+
+@dataclass
+class AutoEngConf(EngConf):
+    _eng_type: PRTEng = PRTEng.AUTO
+
+
+@dataclass
+class PandasEngConf(EngConf):
+    _eng_type: PRTEng = PRTEng.PANDAS
+
+
+@dataclass
+class DaskEngConf(PandasEngConf):
+    _eng_type: PRTEng = PRTEng.DASK
+    worker_count: Optional[int] = None
+
+
+@dataclass
+class RapidsEngConf(PandasEngConf):
+    _eng_type: PRTEng = PRTEng.RAPIDS
+
+
+@dataclass
+class RapidsDaskEngConf(DaskEngConf):
+    _eng_type: PRTEng = PRTEng.RAPIDS_DASK
+    worker_count: Optional[int] = None
+
+
+@dataclass
+class SparkEngConf(PandasEngConf):
+    _eng_type: PRTEng = PRTEng.SPARK
+
+
+class EngConfFactory:
+    @staticmethod
+    def create_or_get(eng_conf_json_dict_or_obj) -> EngConf:
+        # due to json serialization this method can get json, dict or actual class
+        if not eng_conf_json_dict_or_obj:
+            return PandasEngConf()
+
+        if isinstance(eng_conf_json_dict_or_obj, str):
+            if eng_conf_json_dict_or_obj.strip().startswith("{"):
+                import json
+                eng_conf_json_dict_or_obj = json.loads(eng_conf_json_dict_or_obj)
+            else:
+                # simple engine name, might be coming from exported code library
+                eng_conf_json_dict_or_obj = {'_eng_type': eng_conf_json_dict_or_obj}
+
+        if isinstance(eng_conf_json_dict_or_obj, dict):
+            eng_type_str = str(eng_conf_json_dict_or_obj['_eng_type']).upper()
+            if eng_type_str == PRTEng.AUTO:
+                return AutoEngConf.from_dict(eng_conf_json_dict_or_obj)
+            elif eng_type_str == PRTEng.PANDAS:
+                return PandasEngConf.from_dict(eng_conf_json_dict_or_obj)
+            elif eng_type_str == PRTEng.DASK:
+                return DaskEngConf.from_dict(eng_conf_json_dict_or_obj)
+            elif eng_type_str == PRTEng.RAPIDS:
+                return RapidsEngConf.from_dict(eng_conf_json_dict_or_obj)
+            elif eng_type_str == PRTEng.RAPIDS_DASK:
+                return RapidsDaskEngConf.from_dict(eng_conf_json_dict_or_obj)
+            elif eng_type_str == PRTEng.SPARK:
+                return SparkEngConf.from_dict(eng_conf_json_dict_or_obj)
+            else:
+                raise AttributeError(f"Unknown engine type {eng_type_str}")
+        elif isinstance(eng_conf_json_dict_or_obj, EngConf):
+            return eng_conf_json_dict_or_obj
+        elif isinstance(eng_conf_json_dict_or_obj, PRTEng):
+            if eng_conf_json_dict_or_obj == PRTEng.AUTO:
+                return AutoEngConf()
+            elif eng_conf_json_dict_or_obj == PRTEng.PANDAS:
+                return PandasEngConf()
+            elif eng_conf_json_dict_or_obj == PRTEng.DASK:
+                return DaskEngConf()
+            elif eng_conf_json_dict_or_obj == PRTEng.RAPIDS:
+                return RapidsEngConf()
+            elif eng_conf_json_dict_or_obj == PRTEng.RAPIDS_DASK:
+                return RapidsDaskEngConf()
+            elif eng_conf_json_dict_or_obj == PRTEng.SPARK:
+                return SparkEngConf()
+            else:
+                raise AttributeError(f"Unknown PRTEng type {eng_conf_json_dict_or_obj}")
+        else:
+            raise SystemError(f"Unknown eng_conf type {type(eng_conf_json_dict_or_obj)}")
+
+
+@dataclass
+class PRTDataRequest(PRTRequest):
+    # ** We needed to add dict to this list since when dataclass_json cannot figure out type
+    #    it returns dict instead of actual class. need to override or use as dict
+    conn_conf: Optional[Union[
+        dict,
+        ConnConf,
+        WorkerFileConnConf,
+        SqLiteConnConf,
+        S3ConnConf,
+        MYSQLConnConf,
+        PostgreSQLConnConf,
+        RedshiftConnConf,
+        SnowflakeConnConf,
+        MSSQLConnConf,
+        OracleConnConf,
+        HiveConnConf,
+        AthenaConnConf,
+        ElasticSearchConnConf,
+        OpenSearchConnConf,
+        TrinoConnConf,
+        DremioConnConf,
+        HanaConnConf,
+        TeradataConnConf,
+        Db2ConnConf,
+        DynamoDBConnConf,
+        CockroachDBConnConf,
+        ClouderaConnConf,
+        CustomDBConnConf,
+    ]] = None
+
+    eng_conf: Optional[Union[
+        dict,
+        PandasEngConf,
+        DaskEngConf,
+        RapidsEngConf,
+        RapidsDaskEngConf,
+        SparkEngConf,
+    ]] = None
+
+    # MySQLConnDef,
+    # AuroraMySQLConnDef,
+
+
+class ConnConfClassFactory:
+    @staticmethod
+    def get_conn_conf_class(conn_type: PRTConn) -> Union[
+        Type[WorkerFileConnConf],
+        Type[S3ConnConf],
+        Type[SqLiteConnConf],
+        Type[PostgreSQLConnConf],
+        Type[MYSQLConnConf],
+        Type[RedshiftConnConf],
+        Type[SnowflakeConnConf],
+        Type[MSSQLConnConf],
+        Type[OracleConnConf],
+        Type[HiveConnConf],
+        Type[AthenaConnConf],
+        Type[ElasticSearchConnConf],
+        Type[OpenSearchConnConf],
+        Type[TrinoConnConf],
+        Type[DremioConnConf],
+        Type[HanaConnConf],
+        Type[TeradataConnConf],
+        Type[Db2ConnConf],
+        Type[DynamoDBConnConf],
+        Type[CockroachDBConnConf],
+        Type[ClouderaConnConf],
+        Type[CustomDBConnConf]
+    ]:
+        if conn_type == PRTConn.WORKER_FILE:
+            return WorkerFileConnConf
+        elif conn_type == PRTConn.S3:
+            return S3ConnConf
+        elif conn_type == PRTConn.SQLITE:
+            return SqLiteConnConf
+        elif conn_type == PRTConn.POSTGRESQL:
+            return PostgreSQLConnConf
+        elif conn_type == PRTConn.MYSQL:
+            return MYSQLConnConf
+        elif conn_type == PRTConn.REDSHIFT:
+            return RedshiftConnConf
+        elif conn_type == PRTConn.SNOWFLAKE:
+            return SnowflakeConnConf
+        elif conn_type == PRTConn.MSSQL:
+            return MSSQLConnConf
+        elif conn_type == PRTConn.ORACLE:
+            return OracleConnConf
+        elif conn_type == PRTConn.HIVE:
+            return HiveConnConf
+        elif conn_type == PRTConn.ATHENA:
+            return AthenaConnConf
+        elif conn_type == PRTConn.ELASTICSEARCH:
+            return ElasticSearchConnConf
+        elif conn_type == PRTConn.OPENSEARCH:
+            return OpenSearchConnConf
+        elif conn_type == PRTConn.TRINO:
+            return TrinoConnConf
+        elif conn_type == PRTConn.DREMIO:
+            return DremioConnConf
+        elif conn_type == PRTConn.HANA:
+            return HanaConnConf
+        elif conn_type == PRTConn.TERADATA:
+            return TeradataConnConf
+        elif conn_type == PRTConn.DB2:
+            return Db2ConnConf
+        elif conn_type == PRTConn.DYNAMODB:
+            return DynamoDBConnConf
+        elif conn_type == PRTConn.COCKROACHDB:
+            return CockroachDBConnConf
+        elif conn_type == PRTConn.CLOUDERA:
+            return ClouderaConnConf
+        elif conn_type == PRTConn.CUSTOM_DB:
+            return CustomDBConnConf
+        else:
+            raise SystemError(f"Unable to find conn_conf class for conn_type :{conn_type.name}")
```

## practicuscore/api_def.py

```diff
@@ -1,579 +1,622 @@
-from enum import Enum
-
-# DO NOT REMOVE the below import
-from practicuscore.api_base import *  # pylint:disable=wildcard-import,unused-wildcard-import
-
-
-@dataclass
-class CreateWorkerRequest(PRTRequest):
-    pass
-
-
-@dataclass
-class CreateWorkerResponse(PRTResponse):
-    worker_id: int = -1
-
-
-@dataclass
-class StartExtSvcRequest(PRTRequest):
-    svc_name: str = ""
-    port: Optional[int] = None
-    dark_mode: bool = True
-    auto_start_after_failure: bool = False
-    singleton_service_per_node: bool = True
-    options: Optional[dict] = None
-
-
-@dataclass
-class StartExtSvcResponse(PRTResponse):
-    port: int = -1
-    options: Optional[dict] = None
-
-
-@dataclass
-class RestartNodeSvcRequest(PRTRequest):
-    restart_reason_to_log: Optional[str] = None
-
-
-@dataclass
-class KillWorkerRequest(PRTRequest):
-    worker_id: int = -1
-    worker_uuid: Optional[str] = None
-
-
-@dataclass
-class KillWorkersRequest(PRTRequest):
-    worker_id_list: Optional[List[int]] = None
-
-
-@dataclass
-class PingRequest(PRTRequest):
-    pass
-
-
-@dataclass
-class HeartBeatRequest(PRTRequest):
-    payload: Optional[dict] = None
-
-
-@dataclass
-class HeartBeatResponse(PRTResponse):
-    payload: Optional[dict] = None
-
-
-@dataclass
-class CloneLogsRequest(PRTRequest):
-    pass
-
-
-@dataclass
-class LoadRequest(PRTDataRequest):
-    ws_uuid: Optional[str] = None  # ws_uuid of the app is synced with ws in Node, when possible.
-    # response is csv, no class needed
-
-
-@dataclass
-class ExportDataRequest(PRTDataRequest):
-    # conn_conf in base class is a mandatory field and is the destination of save
-    source_conn_conf: Optional[Union[
-        dict,
-        ConnConf,
-        NodeFileConnConf,
-        SqLiteConnConf,
-        S3ConnConf,
-        MYSQLConnConf,
-        PostgreSQLConnConf,
-        RedshiftConnConf,
-        SnowflakeConnConf,
-        MSSQLConnConf,
-        OracleConnConf,
-        HiveConnConf,
-        AthenaConnConf,
-        TrinoConnConf,
-        DremioConnConf,
-        HanaConnConf,
-        TeradataConnConf,
-        Db2ConnConf,
-        DynamoDBConnConf,
-        CockroachDBConnConf,
-        ClouderaConnConf,
-        CustomDBConnConf,
-    ]] = None
-    step_dict_list: Optional[List[dict]] = None
-    # response is op_result
-
-
-@dataclass
-class GetDFRequest(PRTRequest):
-    sampling_method: Optional[str] = None
-    sample_size_app: Optional[int] = None
-
-
-class WSStateKeys:
-    DF_FULL_TYPE_NAME = "DF_FULL_TYPE_NAME"
-    DF_LOADED_ROWS_COUNT = "DF_LOADED_ROWS_COUNT"
-
-
-@dataclass
-class GetWSStateRequest(PRTRequest):
-    wait_for_free_sec: float = 600.0
-    generic_attributes_keys: Optional[List[str]] = None
-
-
-@dataclass
-class GetWSStateResponse(PRTResponse):
-    busy: bool = False
-    step_dict_list: Optional[List[dict]] = None
-    async_op_issues_json_list: Optional[List[str]] = None
-    generic_attributes_dict: Optional[dict] = None
-
-
-@dataclass
-class RunStepsRequest(PRTRequest):
-    # used to run for "Node only" steps. Using dict, since Step is not dataclass
-    step_dict_list: Optional[List[dict]] = None
-    reset_steps: bool = False
-
-
-@dataclass
-class GetObjectStorageMetaRequest(PRTDataRequest):
-    prefix: Optional[str] = None
-    max_size: Optional[int] = None
-    starting_token: Optional[str] = None
-    element_uuid: Optional[str] = None
-
-
-class StorageMetaChildrenLoadStatus(str, Enum):
-    NOT_LOADED = "NOT_LOADED"
-    LOADED = "LOADED"
-    WONT_LOAD = "WONT_LOAD"
-
-
-@dataclass
-class ObjectStorageMeta(DataClassJsonMixin):
-    key: Optional[str] = None
-    name: Optional[str] = None
-    prefix: Optional[str] = None
-    is_folder: Optional[bool] = None
-    size: Optional[int] = None
-    last_modified: Optional[datetime] = None
-    level: int = 0
-    children: Optional[List['ObjectStorageMeta']] = None
-    children_loaded: StorageMetaChildrenLoadStatus = StorageMetaChildrenLoadStatus.NOT_LOADED
-
-    @property
-    def is_file(self) -> bool:
-        return not self.is_folder
-
-
-@dataclass
-class GetObjectStorageMetaResponse(PRTResponse):
-    meta_list: Optional[List[ObjectStorageMeta]] = None
-
-
-@dataclass
-class ConnSelectionStats(DataClassJsonMixin):
-    # statistics about a selected key or keys
-    size_per_row: Optional[int] = None
-    sample_size_in_bytes: Optional[int] = None
-    sample_rows: Optional[int] = None
-    total_size_in_bytes: Optional[int] = None
-    total_rows: Optional[int] = None
-
-
-@dataclass
-class PreviewRequest(PRTDataRequest):
-    pass
-
-
-@dataclass
-class PreviewResponse(PRTResponse):
-    selection_stats: Optional[ConnSelectionStats] = None
-    csv_str: Optional[str] = None
-    preview_text: Optional[str] = None
-
-
-@dataclass
-class TestRelationalConnRequest(PRTDataRequest):
-    pass
-
-
-@dataclass
-class GetFileStatusRequest(PRTRequest):
-    node_path_list: Optional[List[str]] = None
-    recursive: bool = False
-
-
-@dataclass
-class FileStatus(DataClassJsonMixin):
-    file_path: str
-    file_size: int
-    file_epoch: float
-
-
-@dataclass
-class GetFileStatusResponse(PRTResponse):
-    file_status_list: Optional[List[FileStatus]] = None
-
-
-@dataclass
-class UploadFilesRequest(PRTRequest):
-    # opens a multipart app to Cloud Worker communication channel. files/file parts are communicated chunk by chunk
-    pass
-
-
-@dataclass
-class UploadFilesToCloudRequest(PRTRequest):
-    conn_conf: Optional[Union[
-        S3ConnConf
-    ]] = None
-
-
-@dataclass
-class UploadNodeFilesToCloudRequest(PRTRequest):
-    conn_conf: Optional[Union[
-        S3ConnConf
-    ]] = None
-    source_path_list: Optional[List[str]] = None
-    target_dir_path: Optional[str] = None
-    source_path_to_cut: Optional[str] = None
-
-
-@dataclass
-class DownloadFilesRequest(PRTRequest):
-    node_path_list: Optional[List[str]] = None
-    recursive: bool = False
-
-
-@dataclass
-class CopyFilesRequest(PRTRequest):
-    source_path_list: Optional[List[str]] = None
-    target_dir_path: Optional[str] = None
-    source_path_to_cut: Optional[str] = None
-
-
-@dataclass
-class ProfileWSRequest(PRTRequest):
-    profile_uuid: Optional[str] = None
-    title: Optional[str] = None
-    compare_to_original: Optional[bool] = None
-
-
-@dataclass
-class ProfileWSResponse(PRTResponse):
-    started_profiling: Optional[bool] = None
-
-
-@dataclass
-class ViewLogsRequest(PRTRequest):
-    view_practicus_log: bool = True
-    # todo: retire in 6 months, after 2024-06-01
-    view_practicus_audit_log: bool = False
-    log_size_mb: int = 10
-
-
-@dataclass
-class ViewLogsResponse(PRTResponse):
-    practicus_log: Optional[str] = None
-    # todo: retire in 6 months, after 2024-06-01
-    practicus_audit_log: Optional[str] = None
-
-
-@dataclass
-class TestGenericRequest(PRTRequest):
-    some_str: Optional[str] = None
-
-
-@dataclass
-class TestGenericResponse(PRTResponse):
-    some_result: Optional[str] = None
-
-
-@dataclass
-class RunScriptRequest(PRTRequest):
-    script_path: Optional[str] = None
-    run_as_sudo: bool = False
-    timeout_secs: int = 120
-    wait_for_end: bool = True
-
-
-@dataclass
-class RunScriptResponse(PRTResponse):
-    std_out: str = ""
-    std_err: str = ""
-
-
-@dataclass
-class FlushLogsRequest(PRTRequest):
-    pass
-
-
-@dataclass
-class XLImportRequest(PRTRequest):
-    file_name: str = ""
-
-
-@dataclass
-class XLImportResponse(PRTResponse):
-    dp_content: str = ""
-    dp_err_warning: str = ""
-
-
-@dataclass
-class TestCodeRequest(PRTRequest):
-    sampling_method: str = "ALL"
-    sample_size: int = 1000
-    code_block_encoded: Optional[str] = None
-    is_sql: Optional[bool] = None
-    sql_table_name: Optional[str] = None
-
-
-@dataclass
-class TestCodeResponse(PRTResponse):
-    test_result_csv_b: Optional[str] = None
-
-
-@dataclass
-class GenerateCodeRequest(PRTRequest):
-    engine: Optional[str] = None
-    template_list: Optional[List[str]] = None
-    worksheet_name: Optional[str] = None
-    app_user_name: Optional[str] = None
-    export_name: Optional[str] = None
-    export_data_step_dict: Optional[dict] = None
-    build_model_step_dict: Optional[dict] = None
-    dag_flow: Optional[str] = None
-    schedule_start_date_ts: Optional[float] = None
-    schedule_interval: Optional[str] = None
-    # todo: retire in 2024-06-01
-    save_conn_in_files: bool = True
-    save_cloud_credentials: bool = False
-    params: Optional[dict] = None  # Cloud Worker + auth details (if requested by user)
-
-
-@dataclass
-class GenerateCodeResponse(PRTResponse):
-    generated_file_paths: Optional[List[str]] = None
-
-
-@dataclass
-class CreateFolderRequest(PRTDataRequest):
-    full_path: Optional[str] = None
-
-
-@dataclass
-class ModelConfig(DataClassJsonMixin):
-    state: Optional[str] = None
-    percent_complete: int = 0
-    model_name: Optional[str] = None
-    model_desc: Optional[str] = None
-    target: Optional[str] = None
-    re_sample_size: Optional[int] = None
-    model_dir: Optional[str] = None
-    short_model_name: Optional[str] = None
-    version_name: Optional[str] = None
-    problem_type: Optional[str] = None
-    limit_to_models: Optional[str] = None
-    use_gpu: Optional[bool] = False
-    explain: Optional[bool] = None
-    sensitive_features: Optional[str] = None
-    user_name: Optional[str] = None
-    node_name: Optional[str] = None
-    node_instance_id: Optional[str] = None
-    setup_params: Optional[dict] = None
-    tune_params: Optional[dict] = None
-    # Binary or Excel model
-    build_full_fledged: Optional[bool] = None
-    build_for_excel: Optional[bool] = None
-    excel_rows_to_export: Optional[int] = None
-    model_signature_json: Optional[str] = None
-    # Feature selection
-    feature_selection_percent: Optional[int] = None
-    features_ignored: Optional[str] = None
-    # Time Series
-    time_feature: Optional[str] = None
-    time_frequency: Optional[str] = None
-    # Clustering
-    num_clusters: Optional[int] = None
-    # Engines etc. versions
-    py_version: Optional[str] = None
-    auto_ml_engine: Optional[str] = None
-    auto_ml_version: Optional[str] = None
-    # Experiment logging
-    log_experiments: bool = True  # todo: retire on 2024-06-01
-    log_exp_name: Optional[str] = None
-    log_experiment_service_key: Optional[str] = None
-    log_experiment_service_name: Optional[str] = None
-    log_exp_id: Optional[str] = None
-    log_exp_full_parent_run_id: Optional[str] = None
-    log_exp_full_final_run_id: Optional[str] = None
-    log_exp_excel_parent_run_id: Optional[str] = None
-    log_exp_excel_final_run_id: Optional[str] = None
-    final_model: Optional[str] = None
-    score: Optional[float] = None
-    errors: Optional[str] = None
-    excel_final_model: Optional[str] = None
-    excel_score: Optional[float] = None
-    excel_errors: Optional[str] = None
-    summary: str = ""
-
-    @property
-    def input_columns(self) -> List[str]:
-        input_cols = []
-        try:
-            import json
-            signature_json = json.loads(self.model_signature_json)
-            if "inputs" in signature_json:
-                inputs_dict_list = json.loads(signature_json["inputs"])
-                for input_dict in inputs_dict_list:
-                    input_cols.append(input_dict["name"])
-        except:
-            from practicuscore.core_conf import log_manager_glbl
-            logger = log_manager_glbl.get_logger()
-            logger.error(
-                f"Unable to extract input columns from model_signature_json: {self.model_signature_json}.",
-                exc_info=True)
-        finally:
-            return input_cols
-
-
-class ModelConfFactory:
-    @staticmethod
-    def create_or_get(model_conf_json_dict_or_obj) -> Optional[ModelConfig]:
-        if isinstance(model_conf_json_dict_or_obj, str):
-            import json
-            model_conf_json_dict_or_obj = json.loads(model_conf_json_dict_or_obj)
-
-        if isinstance(model_conf_json_dict_or_obj, dict):
-            return ModelConfig.from_dict(model_conf_json_dict_or_obj)
-
-        return None
-
-
-@dataclass
-class CreateModelRequest(PRTRequest):
-    model_config: Optional[ModelConfig] = None
-    status_check: bool = False
-    last_reported_log_byte: int = 0
-
-
-@dataclass
-class CreateModelResponse(PRTResponse):
-    model_config: Optional[ModelConfig] = None
-    current_log: Optional[str] = None
-    last_reported_log_byte: int = 0
-
-
-@dataclass
-class RegisterModelRequest(PRTRequest):
-    model_dir: Optional[str] = None
-
-
-@dataclass
-class ModelSearchResult(DataClassJsonMixin):
-    model_name: Optional[str] = None
-    latest_v: Optional[int] = None
-    latest_v_timestamp: Optional[int] = None
-    latest_staging_v: Optional[int] = None
-    latest_staging_timestamp: Optional[int] = None
-    latest_prod_v: Optional[int] = None
-    latest_prod_timestamp: Optional[int] = None
-
-
-@dataclass
-class ModelSearchResults(DataClassJsonMixin):
-    results: Optional[List[ModelSearchResult]] = None
-
-
-@dataclass
-class SearchModelsRequest(PRTRequest):
-    filter_string_b64: Optional[str] = None
-    max_results: int = 100
-
-
-@dataclass
-class SearchModelsResponse(PRTResponse):
-    model_search_results: Optional[ModelSearchResults] = None
-
-
-@dataclass
-class GetModelMetaRequest(PRTRequest):
-    model_uri: Optional[str] = None
-    model_json_path: Optional[str] = None
-
-
-@dataclass
-class GetModelMetaResponse(PRTResponse):
-    model_config_json: Optional[str] = None
-    prepare_ws_b64: Optional[str] = None
-
-
-@dataclass
-class XLModelRequest(PRTRequest):
-    model_conf_path: str = ""
-    xl_name: str = ""
-    num_rows: int = 1_000
-
-
-@dataclass
-class XLModelResponse(PRTResponse):
-    xl_path: str = ""
-
-
-@dataclass
-class GetSystemStatRequest(PRTRequest):
-    pass
-
-
-@dataclass
-class GetSystemStatResponse(PRTResponse):
-    system_stat: Optional[dict] = None
-    node_version: Optional[str] = None
-
-
-@dataclass
-class DeleteKeysRequest(PRTDataRequest):
-    keys: Optional[List[str]] = None
-    delete_sub_keys: bool = False
-
-
-@dataclass
-class ListBucketsRequest(PRTDataRequest):
-    pass
-
-
-@dataclass
-class ListBucketsResponse(PRTResponse):
-    buckets: Optional[List[str]] = None
-
-
-@dataclass
-class ReplicateNodeRequest(PRTRequest):
-    source_node_name: Optional[str] = None
-    source_node_dns: Optional[str] = None
-    source_node_pem_data: Optional[str] = None
-    timeout_secs: int = 30 * 60  # 30 minutes
-
-
-@dataclass
-class UploadModelFilesRequest(PRTRequest):
-    model_dir: Optional[str] = None
-    region_url: Optional[str] = None
-    deployment_key: Optional[str] = None
-    token: Optional[str] = None
-    prefix: Optional[str] = None
-    model_id: Optional[int] = None
-    model_name: Optional[str] = None
-    version: Optional[str] = None
-
-
-@dataclass
-class UploadModelFilesResponse(PRTResponse):
-    model_url: Optional[str] = None
-
-
-@dataclass
-class DeployWorkflowRequest(PRTRequest):
-    workflow_service_key: Optional[str] = None
-    destination_dir_path: Optional[str] = None
-    files_dir_path: Optional[str] = None
+from enum import Enum
+
+# DO NOT REMOVE the below import
+from practicuscore.api_base import *  # pylint:disable=wildcard-import,unused-wildcard-import
+
+
+@dataclass
+class CreateProcessRequest(PRTRequest):
+    pass
+
+
+@dataclass
+class CreateProcessResponse(PRTResponse):
+    process_id: int = -1
+    os_pid: int = -1
+
+
+@dataclass
+class StartExtSvcRequest(PRTRequest):
+    svc_name: str = ""
+    port: Optional[int] = None
+    dark_mode: bool = True
+    auto_start_after_failure: bool = False
+    singleton_service_per_node: bool = True
+    options: Optional[dict] = None
+
+
+@dataclass
+class StartExtSvcResponse(PRTResponse):
+    port: int = -1
+    options: Optional[dict] = None
+
+
+@dataclass
+class RestartNodeSvcRequest(PRTRequest):
+    restart_reason_to_log: Optional[str] = None
+
+
+@dataclass
+class KillProcessRequest(PRTRequest):
+    process_id: int = -1
+    process_uuid: Optional[str] = None
+
+
+@dataclass
+class KillProcessesRequest(PRTRequest):
+    process_id_list: Optional[List[int]] = None
+
+
+@dataclass
+class PingRequest(PRTRequest):
+    pass
+
+
+@dataclass
+class HeartBeatRequest(PRTRequest):
+    payload: Optional[dict] = None
+
+
+@dataclass
+class HeartBeatResponse(PRTResponse):
+    payload: Optional[dict] = None
+
+
+@dataclass
+class CloneLogsRequest(PRTRequest):
+    pass
+
+
+@dataclass
+class LoadRequest(PRTDataRequest):
+    pass
+    # response is csv, no class needed
+
+
+@dataclass
+class ExportDataRequest(PRTDataRequest):
+    # conn_conf in base class is a mandatory field and is the destination of save
+    source_conn_conf: Optional[Union[
+        dict,
+        ConnConf,
+        WorkerFileConnConf,
+        SqLiteConnConf,
+        S3ConnConf,
+        MYSQLConnConf,
+        PostgreSQLConnConf,
+        RedshiftConnConf,
+        SnowflakeConnConf,
+        MSSQLConnConf,
+        OracleConnConf,
+        HiveConnConf,
+        AthenaConnConf,
+        TrinoConnConf,
+        DremioConnConf,
+        HanaConnConf,
+        TeradataConnConf,
+        Db2ConnConf,
+        DynamoDBConnConf,
+        CockroachDBConnConf,
+        ClouderaConnConf,
+        CustomDBConnConf,
+    ]] = None
+    step_dict_list: Optional[List[dict]] = None
+    # response is op_result
+
+
+@dataclass
+class GetDFRequest(PRTRequest):
+    sampling_method: Optional[str] = None
+    sample_size_app: Optional[int] = None
+
+
+class WSStateKeys:
+    DF_FULL_TYPE_NAME = "DF_FULL_TYPE_NAME"
+    DF_LOADED_ROWS_COUNT = "DF_LOADED_ROWS_COUNT"
+
+
+@dataclass
+class GetWSStateRequest(PRTRequest):
+    wait_for_free_sec: float = 600.0
+    generic_attributes_keys: Optional[List[str]] = None
+
+
+@dataclass
+class GetWSStateResponse(PRTResponse):
+    busy: bool = False
+    step_dict_list: Optional[List[dict]] = None
+    async_op_issues_json_list: Optional[List[str]] = None
+    generic_attributes_dict: Optional[dict] = None
+
+
+@dataclass
+class RunStepsRequest(PRTRequest):
+    # used to run for "Node only" steps. Using dict, since Step is not dataclass
+    step_dict_list: Optional[List[dict]] = None
+    reset_steps: bool = False
+
+
+@dataclass
+class GetObjectStorageMetaRequest(PRTDataRequest):
+    prefix: Optional[str] = None
+    max_size: Optional[int] = None
+    starting_token: Optional[str] = None
+    element_uuid: Optional[str] = None
+
+
+class StorageMetaChildrenLoadStatus(str, Enum):
+    NOT_LOADED = "NOT_LOADED"
+    LOADED = "LOADED"
+    WONT_LOAD = "WONT_LOAD"
+
+
+@dataclass
+class ObjectStorageMeta(PrtDataClassJsonMixin):
+    key: Optional[str] = None
+    name: Optional[str] = None
+    prefix: Optional[str] = None
+    is_folder: Optional[bool] = None
+    size: Optional[int] = None
+    last_modified: Optional[datetime] = None
+    level: int = 0
+    children: Optional[List['ObjectStorageMeta']] = None
+    children_loaded: StorageMetaChildrenLoadStatus = StorageMetaChildrenLoadStatus.NOT_LOADED
+
+    @property
+    def is_file(self) -> bool:
+        return not self.is_folder
+
+
+@dataclass
+class GetObjectStorageMetaResponse(PRTResponse):
+    meta_list: Optional[List[ObjectStorageMeta]] = None
+
+
+@dataclass
+class ConnSelectionStats(PrtDataClassJsonMixin):
+    # statistics about a selected key or keys
+    size_per_row: Optional[int] = None
+    sample_size_in_bytes: Optional[int] = None
+    sample_rows: Optional[int] = None
+    total_size_in_bytes: Optional[int] = None
+    total_rows: Optional[int] = None
+
+
+@dataclass
+class PreviewRequest(PRTDataRequest):
+    pass
+
+
+@dataclass
+class PreviewResponse(PRTResponse):
+    selection_stats: Optional[ConnSelectionStats] = None
+    csv_str: Optional[str] = None
+    preview_text: Optional[str] = None
+
+
+@dataclass
+class TestConnectionRequest(PRTDataRequest):
+    pass
+
+
+@dataclass
+class GetFileStatusRequest(PRTRequest):
+    node_path_list: Optional[List[str]] = None
+    recursive: bool = False
+
+
+@dataclass
+class FileStatus(PrtDataClassJsonMixin):
+    file_path: str
+    file_size: int
+    file_epoch: float
+
+
+@dataclass
+class GetFileStatusResponse(PRTResponse):
+    file_status_list: Optional[List[FileStatus]] = None
+
+
+@dataclass
+class UploadFilesRequest(PRTRequest):
+    # opens a multipart app to Worker communication channel. files/file parts are communicated chunk by chunk
+    pass
+
+
+@dataclass
+class UploadFilesToCloudRequest(PRTRequest):
+    conn_conf: Optional[Union[
+        S3ConnConf
+    ]] = None
+
+
+@dataclass
+class UploadWorkerFilesRequest(PRTRequest):
+    conn_conf: Optional[Union[
+        S3ConnConf
+    ]] = None
+    source_path_list: Optional[List[str]] = None
+    target_dir_path: Optional[str] = None
+    source_path_to_cut: Optional[str] = None
+
+
+@dataclass
+class DownloadFilesRequest(PRTRequest):
+    node_path_list: Optional[List[str]] = None
+    recursive: bool = False
+
+
+@dataclass
+class CopyFilesRequest(PRTRequest):
+    source_path_list: Optional[List[str]] = None
+    target_dir_path: Optional[str] = None
+    source_path_to_cut: Optional[str] = None
+
+
+@dataclass
+class ProfileWSRequest(PRTRequest):
+    profile_uuid: Optional[str] = None
+    title: Optional[str] = None
+    compare_to_original: Optional[bool] = None
+
+
+@dataclass
+class ProfileWSResponse(PRTResponse):
+    started_profiling: Optional[bool] = None
+
+
+@dataclass
+class ViewLogsRequest(PRTRequest):
+    view_practicus_log: bool = True
+    log_size_mb: int = 10
+
+
+@dataclass
+class ViewLogsResponse(PRTResponse):
+    practicus_log: Optional[str] = None
+
+
+@dataclass
+class TestGenericRequest(PRTRequest):
+    some_str: Optional[str] = None
+
+
+@dataclass
+class TestGenericResponse(PRTResponse):
+    some_result: Optional[str] = None
+
+
+@dataclass
+class RunScriptRequest(PRTRequest):
+    script_path: Optional[str] = None
+    run_as_sudo: bool = False
+    timeout_secs: int = 120
+    wait_for_end: bool = True
+
+
+@dataclass
+class RunScriptResponse(PRTResponse):
+    std_out: str = ""
+    std_err: str = ""
+
+
+@dataclass
+class FlushLogsRequest(PRTRequest):
+    pass
+
+
+@dataclass
+class XLImportRequest(PRTRequest):
+    file_name: str = ""
+
+
+@dataclass
+class XLImportResponse(PRTResponse):
+    dp_content: str = ""
+    dp_err_warning: str = ""
+
+
+@dataclass
+class TestCodeRequest(PRTRequest):
+    sampling_method: Optional[str] = "ALL"
+    sample_size: Optional[int] = 1000
+    code_block_encoded: Optional[str] = None
+    is_sql: Optional[bool] = None
+    sql_table_name: Optional[str] = None
+
+
+@dataclass
+class TestCodeResponse(PRTResponse):
+    test_result_csv_b: Optional[str] = None
+
+
+@dataclass
+class GenerateCodeRequest(PRTRequest):
+    worksheets_dict: Optional[dict] = None
+    template: Optional[str] = None
+    app_user_name: Optional[str] = None
+    export_name: Optional[str] = None
+    dag_flow: Optional[str] = None
+    schedule_start_date_ts: Optional[float] = None
+    schedule_interval: Optional[str] = None
+    save_cloud_credentials: bool = False
+    params: Optional[dict] = None  # Worker + auth details (if requested by user)
+
+
+@dataclass
+class GenerateCodeResponse(PRTResponse):
+    generated_file_paths: Optional[List[str]] = None
+
+
+@dataclass
+class CreateFolderRequest(PRTDataRequest):
+    full_path: Optional[str] = None
+
+
+@dataclass
+class ModelAPIHeaderMeta(PrtDataClassJsonMixin):
+    # x-prt-... Http headers of a model api
+    model_id: int | None = None
+    model_version: str | None = None
+    model_deployment_key: str | None = None
+    pod_name: str | None = None
+    model_prefix: str | None = None
+    model_name: str | None = None
+    traffic_weight: int | None = None
+    auth_detail: str | None = None
+    extra_config: str | None = None
+
+
+@dataclass
+class ModelConfig(PrtDataClassJsonMixin):
+    state: Optional[str] = None
+    percent_complete: Optional[int] = None
+    model_name: Optional[str] = None
+    model_desc: Optional[str] = None
+    target: Optional[str] = None
+    re_sample_size: Optional[int] = None
+    model_dir: Optional[str] = None
+    short_model_name: Optional[str] = None
+    version_name: Optional[str] = None
+    problem_type: Optional[str] = None
+    limit_to_models: Optional[str] = None
+    use_gpu: Optional[bool] = None
+    explain: Optional[bool] = None
+    sensitive_features: Optional[str] = None
+    user_name: Optional[str] = None
+    node_name: Optional[str] = None
+    node_instance_id: Optional[str] = None
+    setup_params: Optional[dict] = None
+    tune_params: Optional[dict] = None
+    model_signature_json: Optional[str] = None
+    # Feature selection
+    feature_selection_percent: Optional[int] = None
+    features_ignored: Optional[str] = None
+    # Time Series
+    time_feature: Optional[str] = None
+    time_frequency: Optional[str] = None
+    # Clustering
+    num_clusters: Optional[int] = None
+    # Engines etc. versions
+    py_version: Optional[str] = None
+    auto_ml_engine: Optional[str] = None
+    auto_ml_version: Optional[str] = None
+    # Experiment logging
+    log_exp_name: Optional[str] = None
+    log_experiment_service_key: Optional[str] = None
+    log_experiment_service_name: Optional[str] = None
+    log_exp_id: Optional[str] = None
+    log_exp_full_parent_run_id: Optional[str] = None
+    log_exp_full_final_run_id: Optional[str] = None
+    final_model: Optional[str] = None
+    score: Optional[float] = None
+    errors: Optional[str] = None
+    summary: Optional[str] = None
+
+    @property
+    def input_columns(self) -> List[str]:
+        input_cols = []
+        try:
+            if self.model_signature_json is not None:
+                import json
+                signature_json = json.loads(self.model_signature_json)
+                if "inputs" in signature_json:
+                    inputs_dict_list = json.loads(signature_json["inputs"])
+                    for input_dict in inputs_dict_list:
+                        input_cols.append(input_dict["name"])
+        except:
+            from practicuscore.log_manager import get_logger, Log
+            logger = get_logger(Log.CORE)
+            logger.error(
+                f"Unable to extract input columns from model_signature_json: {self.model_signature_json}.",
+                exc_info=True)
+        finally:
+            return input_cols
+
+    def save(self, json_path: str):
+        with open(json_path, "wt") as f:
+            f.write(self.to_json())
+
+    def __str__(self):
+        return self.to_json(indent=4)
+
+    @staticmethod
+    def load(model_conf: str | dict) -> Optional['ModelConfig']:
+        """
+        Model configuration Json or dictionary
+        :param model_conf:
+        :return:
+        """
+        if isinstance(model_conf, str):
+            import json
+            model_conf = json.loads(model_conf)
+
+        if isinstance(model_conf, dict):
+            return ModelConfig.from_dict(model_conf)
+
+        return None
+
+
+@dataclass
+class CreateModelRequest(PRTRequest):
+    model_config: Optional[ModelConfig] = None
+    status_check: bool = False
+    last_reported_log_byte: int = 0
+
+
+@dataclass
+class CreateModelResponse(PRTResponse):
+    model_config: Optional[ModelConfig] = None
+    current_log: Optional[str] = None
+    last_reported_log_byte: int = 0
+
+
+@dataclass
+class RegisterModelRequest(PRTRequest):
+    model_dir: Optional[str] = None
+
+
+@dataclass
+class ModelSearchResult(PrtDataClassJsonMixin):
+    model_name: Optional[str] = None
+    latest_v: Optional[int] = None
+    latest_v_timestamp: Optional[int] = None
+    latest_staging_v: Optional[int] = None
+    latest_staging_timestamp: Optional[int] = None
+    latest_prod_v: Optional[int] = None
+    latest_prod_timestamp: Optional[int] = None
+
+
+@dataclass
+class ModelSearchResults(PrtDataClassJsonMixin):
+    results: Optional[List[ModelSearchResult]] = None
+
+
+@dataclass
+class SearchModelsRequest(PRTRequest):
+    filter_string_b64: Optional[str] = None
+    max_results: int = 100
+
+
+@dataclass
+class SearchModelsResponse(PRTResponse):
+    model_search_results: Optional[ModelSearchResults] = None
+
+
+@dataclass
+class GetModelMetaRequest(PRTRequest):
+    model_uri: Optional[str] = None
+    model_json_path: Optional[str] = None
+
+
+@dataclass
+class GetModelMetaResponse(PRTResponse):
+    model_config_json: Optional[str] = None
+    prepare_ws_b64: Optional[str] = None
+
+
+@dataclass
+class GetSystemStatRequest(PRTRequest):
+    pass
+
+
+@dataclass
+class GetSystemStatResponse(PRTResponse):
+    system_stat: Optional[dict] = None
+    node_version: Optional[str] = None
+
+
+@dataclass
+class DeleteKeysRequest(PRTDataRequest):
+    keys: Optional[List[str]] = None
+    delete_sub_keys: bool = False
+
+
+@dataclass
+class ListBucketsRequest(PRTDataRequest):
+    pass
+
+
+@dataclass
+class ListBucketsResponse(PRTResponse):
+    buckets: Optional[List[str]] = None
+
+
+@dataclass
+class ReplicateNodeRequest(PRTRequest):
+    source_node_name: Optional[str] = None
+    source_node_dns: Optional[str] = None
+    source_node_pem_data: Optional[str] = None
+    timeout_secs: int = 30 * 60  # 30 minutes
+
+
+@dataclass
+class UploadModelFilesRequest(PRTRequest):
+    model_dir: Optional[str] = None
+    region_url: Optional[str] = None
+    deployment_key: Optional[str] = None
+    token: Optional[str] = None
+    prefix: Optional[str] = None
+    model_id: Optional[int] = None
+    model_name: Optional[str] = None
+    version: Optional[str] = None
+
+
+@dataclass
+class UploadModelFilesResponse(PRTResponse):
+    model_url: Optional[str] = None
+
+
+@dataclass
+class DeployWorkflowRequest(PRTRequest):
+    workflow_service_key: Optional[str] = None
+    destination_dir_path: Optional[str] = None
+    files_dir_path: Optional[str] = None
+
+
+@dataclass
+class CreatePlotRequest(PRTRequest):
+    dark_mode: bool = False
+
+
+@dataclass
+class CreatePlotResponse(PRTResponse):
+    plot_token: Optional[str] = None
+
+
+@dataclass
+class UpdateWsNameRequest(PRTRequest):
+    ws_name: Optional[str] = None
+
+
+@dataclass
+class RunTaskRequest(PRTRequest):
+    task_uuid: Optional[str] = None
+    task_file_path: Optional[str] = None
+
+
+@dataclass
+class CheckTaskStateRequest(PRTRequest):
+    task_uuid: Optional[str] = None
+
+
+class TaskState(str, Enum):
+    UNKNOWN = "UNKNOWN"
+    RUNNING = "RUNNING"
+    SUCCESS = "SUCCESS"
+    ERROR = "ERROR"
+
+    @classmethod
+    def from_value(cls, value: Union[str, Enum]) -> 'TaskState':
+        str_val = str(value.value if hasattr(value, "value") else value).upper()
+        for i, enum_val in enumerate(cls):
+            # noinspection PyUnresolvedReferences
+            if str(enum_val.value).upper() == str_val:
+                return cls(enum_val)
+
+        raise ValueError(f'{value} is not a valid {cls}')
+
+
+@dataclass
+class CheckTaskStateResponse(PRTResponse):
+    task_state: TaskState = TaskState.UNKNOWN
```

## practicuscore/api_k8s.py

```diff
@@ -1,249 +1,317 @@
-import hashlib
-from abc import ABC
-from dataclasses import dataclass
-from enum import Enum
-from typing import Optional, List, Union
-
-from practicuscore.util import CryptoUtil
-
-
-class K8sAuthToken:
-    def __init__(self, refresh_token: str, access_token: str, username: Optional[str] = None) -> None:
-        self.refresh_token = refresh_token
-        self.access_token = access_token
-        self.username = username
-
-
-@dataclass
-class K8sClusterDefinition:
-    name: str = ""
-    region_name: str = ""
-
-
-class K8sConfig:
-    def __init__(self, host_url: str, email: str, refresh_token: Optional[str] = None, username: Optional[str] = None):
-        super().__init__()
-        self.host_url = host_url
-        self.email = email
-        self.refresh_token = refresh_token
-        self.password: Optional[str] = None
-        self.cluster_name: Optional[str] = None
-        self.region_name: Optional[str] = None
-        self._username: Optional[str] = username
-
-    def to_dict(self) -> dict:
-        conf_dict = {
-            'host_url': self.host_url,
-            'email': self.email,
-            'username': self.username,
-        }
-
-        if self.password is not None:
-            conf_dict['password'] = self.password
-
-        if self.refresh_token is not None:
-            conf_dict['refresh_token'] = self.refresh_token
-
-        if self.cluster_name is not None:
-            conf_dict['cluster_name'] = self.cluster_name
-
-        if self.region_name is not None:
-            conf_dict['region_name'] = self.region_name
-
-        return conf_dict
-
-    @staticmethod
-    def from_dict(dict_item: dict) -> 'K8sConfig':
-        username = dict_item['username'] if 'username' in dict_item else None
-        k8s_config = K8sConfig(
-            host_url=dict_item['host_url'], email=dict_item['email'], refresh_token=dict_item['refresh_token'],
-            username=username)
-        if 'password' in dict_item:
-            k8s_config.password = dict_item['password']
-        if 'cluster_name' in dict_item:
-            k8s_config.cluster_name = dict_item['cluster_name']
-        if 'region_name' in dict_item:
-            k8s_config.region_name = dict_item['region_name']
-        return k8s_config
-
-    def set_password(self, password_plain_text: str):
-        self.password = CryptoUtil.encrypt(password_plain_text)
-
-    @property
-    def password_in_plain_text(self) -> Optional[str]:
-        if self.password:
-            return CryptoUtil.decrypt(self.password)
-        else:
-            return None
-
-    @property
-    def ssl(self) -> bool:
-        return self.host_url.startswith("https")
-
-    @property
-    def host_dns(self) -> str:
-        return self.host_url.replace("https://", "").replace("http://", "")
-
-    @property
-    def hash_text(self) -> str:
-        return f"{self.host_url}-{self.email}-{self.refresh_token}-{self.password}"
-
-    @property
-    def hash_key(self) -> str:
-        m = hashlib.md5()
-        m.update(bytes(self.hash_text, "utf-8"))
-        return str(m.hexdigest())
-
-    @property
-    def username(self) -> str:
-        try:
-            if self._username:
-                return self._username
-            else:
-                return self.email.split("@")[0]
-        except Exception as e:
-            print(f"ERROR: Could not get user name from email of k8s region. Err: {e}")
-            return "practicus_ai"
-
-
-class ModelPrefix:
-    def __init__(self, key: str, prefix: str) -> None:
-        super().__init__()
-        self.key = key
-        self.prefix = prefix
-
-
-class ModelDeployment:
-    def __init__(self, key: str, name: str) -> None:
-        super().__init__()
-        self.key = key
-        self.name = name
-
-
-class ModelVersionInfo:
-    def __init__(self, version_tag: str, version: Optional[str] = None):
-        self.version_tag = version_tag
-        self.version = version
-
-    @staticmethod
-    def create_from_version(version: str) -> 'ModelVersionInfo':
-        return ModelVersionInfo(version_tag=f"v{version}", version=version)
-
-    @staticmethod
-    def create_latest() -> 'ModelVersionInfo':
-        return ModelVersionInfo(version_tag="latest")
-
-    @staticmethod
-    def create_production() -> 'ModelVersionInfo':
-        return ModelVersionInfo(version_tag="production")
-
-    @staticmethod
-    def create_staging() -> 'ModelVersionInfo':
-        return ModelVersionInfo(version_tag="staging")
-
-
-class ModelMetaVersion:
-    def __init__(self, version_id: int, version: str, model_deployment: ModelDeployment, stage: Optional[str] = None) -> None:
-        super().__init__()
-        self.id = version_id
-        self.version = version
-        self.stage = stage
-        self.model_deployment = model_deployment
-
-    def to_model_version_info(self) -> ModelVersionInfo:
-        return ModelVersionInfo.create_from_version(self.version)
-
-
-class ModelMeta:
-
-    def __init__(self, model_id: int, name: str, model_prefix: ModelPrefix, model_versions: List[ModelMetaVersion]) \
-            -> None:
-        super().__init__()
-        self.model_id = model_id
-        self.name = name
-        self.model_prefix = model_prefix
-        self.model_versions = model_versions
-
-    @property
-    def production_version(self) -> Optional[ModelMetaVersion]:
-        for model_meta_version in self.model_versions:
-            if model_meta_version.stage == "Production":
-                return model_meta_version
-        return None
-
-    @property
-    def staging_version(self) -> Optional[ModelMetaVersion]:
-        for model_meta_version in self.model_versions:
-            if model_meta_version.stage == "Staging":
-                return model_meta_version
-        return None
-
-
-class ExternalServiceType(str, Enum):
-    AIRFLOW = "AIRFLOW"
-    MLFLOW = "MLFLOW"
-    JUPYTER_LAB = "JUPYTER_LAB"
-
-    @classmethod
-    def from_value(cls, value: Union[str, Enum]) -> 'ExternalServiceType':
-        str_val = str(value.value if hasattr(value, "value") else value).upper()
-        for i, enum_val in enumerate(cls):
-            # noinspection PyUnresolvedReferences
-            if str(enum_val.value).upper() == str_val:
-                return cls(enum_val)
-
-        raise ValueError(f'{value} is not a valid {cls}')
-
-
-# Pages that use hardware acceleration can fail on Chrome + K8s workspace
-# Sample test site: https://webglsamples.org/blob/blob.html
-# https://bugs.chromium.org/p/chromium/issues/detail?id=1506249#c2
-KNOWN_COMPLEX_WEB_SERVICES = [ExternalServiceType.JUPYTER_LAB]
-
-
-class ExternalService(ABC):
-    def __init__(
-            self, key: str, name: str, service_type: ExternalServiceType, url: Optional[str] = None,
-            oauth_key: Optional[str] = None, db_key: Optional[str] = None, obj_key: Optional[str] = None,
-            git_key: Optional[str] = None, configuration: Optional[dict] = None,
-    ) -> None:
-        self.key: str = key
-        self.name: str = name
-        self.service_type: ExternalServiceType = service_type
-        self.url: Optional[str] = url
-        self.oauth_key: Optional[str] = oauth_key
-        self.db_key: Optional[str] = db_key
-        self.obj_key: Optional[str] = obj_key
-        self.git_key: Optional[str] = git_key
-        self.configuration: dict = configuration if configuration is not None else {}
-
-
-class WorkflowService(ExternalService):
-    MY_SVC_TYPE = ExternalServiceType.AIRFLOW
-
-    def __init__(
-            self, key: str, name: str, url: Optional[str] = None,
-            oauth_key: Optional[str] = None, db_key: Optional[str] = None, obj_key: Optional[str] = None,
-            git_key: Optional[str] = None, configuration: Optional[dict] = None,
-    ) -> None:
-        super().__init__(
-            key=key, name=name, service_type=self.MY_SVC_TYPE, url=url, oauth_key=oauth_key,
-            db_key=db_key, obj_key=obj_key, git_key=git_key, configuration=configuration)
-
-        assert self.url, f"url must be defined for {type(self)}. key: {key}, name: {name}"
-        assert self.git_key, f"git_key must be defined for {type(self)}. key: {key}, name: {name}"
-
-
-class ExperimentService(ExternalService):
-    MY_SVC_TYPE = ExternalServiceType.MLFLOW
-
-    def __init__(
-            self, key: str, name: str, url: Optional[str] = None,
-            oauth_key: Optional[str] = None, db_key: Optional[str] = None, obj_key: Optional[str] = None,
-            git_key: Optional[str] = None, configuration: Optional[dict] = None,
-    ) -> None:
-        super().__init__(
-            key=key, name=name, service_type=self.MY_SVC_TYPE, url=url, oauth_key=oauth_key,
-            db_key=db_key, obj_key=obj_key, git_key=git_key, configuration=configuration)
-
-        assert self.url, f"url must be defined for {type(self)}. key: {key}, name: {name}"
+import hashlib
+from abc import ABC
+from dataclasses import dataclass
+from enum import Enum
+from typing import Optional, List, Union
+
+from practicuscore import get_logger, Log
+from practicuscore.util import CryptoUtil
+
+
+class K8sAuthToken:
+    def __init__(self, refresh_token: str, access_token: str, username: Optional[str] = None) -> None:
+        self.refresh_token = refresh_token
+        self.access_token = access_token
+        self.username = username
+
+
+@dataclass
+class K8sClusterDefinition:
+    name: str = ""
+    region_name: str = ""
+
+
+class K8sConfig:
+    def __init__(self, host_url: str, email: str, refresh_token: Optional[str] = None, username: Optional[str] = None):
+        super().__init__()
+        self.host_url = host_url
+        self.email = email
+        self.refresh_token: str | None = refresh_token
+        self.password: Optional[str] = None
+        self.cluster_name: Optional[str] = None
+        self.region_name: Optional[str] = None
+        self._username: Optional[str] = username
+
+    def to_dict(self) -> dict:
+        conf_dict = {
+            'host_url': self.host_url,
+            'email': self.email,
+            'username': self.username,
+        }
+
+        if self.password is not None:
+            conf_dict['password'] = self.password
+
+        if self.refresh_token is not None:
+            conf_dict['refresh_token'] = self.refresh_token
+
+        if self.cluster_name is not None:
+            conf_dict['cluster_name'] = self.cluster_name
+
+        if self.region_name is not None:
+            conf_dict['region_name'] = self.region_name
+
+        return conf_dict
+
+    @staticmethod
+    def from_dict(dict_item: dict) -> 'K8sConfig':
+        username = dict_item['username'] if 'username' in dict_item else None
+        k8s_config = K8sConfig(
+            host_url=dict_item['host_url'], email=dict_item['email'], refresh_token=dict_item['refresh_token'],
+            username=username)
+        if 'password' in dict_item:
+            k8s_config.password = dict_item['password']
+        if 'cluster_name' in dict_item:
+            k8s_config.cluster_name = dict_item['cluster_name']
+        if 'region_name' in dict_item:
+            k8s_config.region_name = dict_item['region_name']
+        return k8s_config
+
+    def set_password(self, password_plain_text: str):
+        self.password = CryptoUtil.encrypt(password_plain_text)
+
+    @property
+    def password_in_plain_text(self) -> Optional[str]:
+        if self.password:
+            return CryptoUtil.decrypt(self.password)
+        else:
+            return None
+
+    @property
+    def ssl(self) -> bool:
+        return self.host_url.startswith("https")
+
+    @property
+    def host_dns(self) -> str:
+        return self.host_url.replace("https://", "").replace("http://", "")
+
+    @property
+    def hash_text(self) -> str:
+        return f"{self.host_url}-{self.email}-{self.refresh_token}-{self.password}"
+
+    @property
+    def hash_key(self) -> str:
+        m = hashlib.md5()
+        m.update(bytes(self.hash_text, "utf-8"))
+        return str(m.hexdigest())
+
+    @property
+    def username(self) -> str:
+        try:
+            if self._username:
+                return self._username
+            else:
+                return self.email.split("@")[0]
+        except Exception as e:
+            print(f"ERROR: Could not get user name from email of k8s region. Err: {e}")
+            return "practicus_ai"
+
+    @property
+    def key(self) -> str:
+        return f"{self.username}@{self.host_dns}"
+
+
+class ModelPrefix:
+    logger = get_logger(Log.SDK)
+
+    def __init__(self, key: str, prefix: str) -> None:
+        super().__init__()
+        self.key = key
+        self.prefix = prefix
+
+    def get_csv_header(self) -> str:
+        # Updating? change __str__()
+        return "key,prefix"
+
+    def __str__(self):
+        # Updating? change get_csv_header()
+        return f"{self.key},{self.prefix}"
+
+    def __repr__(self):
+        header_items = self.get_csv_header().split(",")
+        row_items = str(self).split(",")
+        final_items: list[str] = []
+        if len(header_items) == len(row_items):
+            for i in range(len(header_items)):
+                final_items.append(f"{header_items[i]}: {row_items[i]}")
+            return "\n".join(final_items)
+        else:
+            self.logger.warning(
+                "Header items do not match string items for ModelPrefix. Will return a simpler representation.")
+            return str(self)
+
+
+class ModelDeployment:
+    logger = get_logger(Log.SDK)
+
+    def __init__(self, key: str, name: str) -> None:
+        super().__init__()
+        self.key = key
+        self.name = name
+
+    def get_csv_header(self) -> str:
+        # Updating? change __str__()
+        return "key,name"
+
+    def __str__(self):
+        # Updating? change get_csv_header()
+        return f"{self.key},{self.name}"
+
+    def __repr__(self):
+        header_items = self.get_csv_header().split(",")
+        row_items = str(self).split(",")
+        final_items: list[str] = []
+        if len(header_items) == len(row_items):
+            for i in range(len(header_items)):
+                final_items.append(f"{header_items[i]}: {row_items[i]}")
+            return "\n".join(final_items)
+        else:
+            self.logger.warning(
+                "Header items do not match string items for ModelDeployment. Will return a simpler representation.")
+            return str(self)
+
+
+class ModelVersionInfo:
+    def __init__(self, version_tag: str, version: Optional[str] = None):
+        self.version_tag = version_tag
+        self.version = version
+
+    @staticmethod
+    def create_from_version(version: str) -> 'ModelVersionInfo':
+        return ModelVersionInfo(version_tag=f"v{version}", version=version)
+
+    @staticmethod
+    def create_latest() -> 'ModelVersionInfo':
+        return ModelVersionInfo(version_tag="latest")
+
+    @staticmethod
+    def create_production() -> 'ModelVersionInfo':
+        return ModelVersionInfo(version_tag="production")
+
+    @staticmethod
+    def create_staging() -> 'ModelVersionInfo':
+        return ModelVersionInfo(version_tag="staging")
+
+
+class ModelMetaVersion:
+    def __init__(self, version_id: int, version: str, model_deployment: ModelDeployment, stage: Optional[str] = None) -> None:
+        super().__init__()
+        self.id = version_id
+        self.version = version
+        self.stage = stage
+        self.model_deployment = model_deployment
+
+    def to_model_version_info(self) -> ModelVersionInfo:
+        return ModelVersionInfo.create_from_version(self.version)
+
+
+class ModelMeta:
+
+    def __init__(self, model_id: int, name: str, model_prefix: ModelPrefix, model_versions: List[ModelMetaVersion]) \
+            -> None:
+        super().__init__()
+        self.model_id = model_id
+        self.name = name
+        self.model_prefix = model_prefix
+        self.model_versions = model_versions
+
+    @property
+    def production_version(self) -> Optional[ModelMetaVersion]:
+        for model_meta_version in self.model_versions:
+            if model_meta_version.stage == "Production":
+                return model_meta_version
+        return None
+
+    @property
+    def staging_version(self) -> Optional[ModelMetaVersion]:
+        for model_meta_version in self.model_versions:
+            if model_meta_version.stage == "Staging":
+                return model_meta_version
+        return None
+
+
+class ExternalServiceType(str, Enum):
+    AIRFLOW = "AIRFLOW"
+    MLFLOW = "MLFLOW"
+    JUPYTER_LAB = "JUPYTER_LAB"
+    SPARKUI = "SPARKUI"
+    GRAFANA = "GRAFANA"
+
+    @classmethod
+    def from_value(cls, value: Union[str, Enum]) -> 'ExternalServiceType':
+        str_val = str(value.value if hasattr(value, "value") else value).upper()
+        for i, enum_val in enumerate(cls):
+            # noinspection PyUnresolvedReferences
+            if str(enum_val.value).upper() == str_val:
+                return cls(enum_val)
+
+        raise ValueError(f'{value} is not a valid {cls}')
+
+
+# Pages that use hardware acceleration can fail on Chrome + K8s workspace
+# Sample test site: https://webglsamples.org/blob/blob.html
+# https://bugs.chromium.org/p/chromium/issues/detail?id=1506249#c2
+KNOWN_COMPLEX_WEB_SERVICES = [ExternalServiceType.JUPYTER_LAB]
+
+
+class ExternalService(ABC):
+    def __init__(
+            self, key: str, name: str, service_type: ExternalServiceType, url: Optional[str] = None,
+            oauth_key: Optional[str] = None, db_key: Optional[str] = None, obj_key: Optional[str] = None,
+            git_key: Optional[str] = None, configuration: Optional[dict] = None,
+    ) -> None:
+        self.key: str = key
+        self.name: str = name
+        self.service_type: ExternalServiceType = service_type
+        self.url: Optional[str] = url
+        self.oauth_key: Optional[str] = oauth_key
+        self.db_key: Optional[str] = db_key
+        self.obj_key: Optional[str] = obj_key
+        self.git_key: Optional[str] = git_key
+        self.configuration: dict = configuration if configuration is not None else {}
+
+
+class WorkflowService(ExternalService):
+    MY_SVC_TYPE = ExternalServiceType.AIRFLOW
+
+    def __init__(
+            self, key: str, name: str, url: Optional[str] = None,
+            oauth_key: Optional[str] = None, db_key: Optional[str] = None, obj_key: Optional[str] = None,
+            git_key: Optional[str] = None, configuration: Optional[dict] = None,
+    ) -> None:
+        super().__init__(
+            key=key, name=name, service_type=self.MY_SVC_TYPE, url=url, oauth_key=oauth_key,
+            db_key=db_key, obj_key=obj_key, git_key=git_key, configuration=configuration)
+
+        assert self.url, f"url must be defined for {type(self)}. key: {key}, name: {name}"
+        assert self.git_key, f"git_key must be defined for {type(self)}. key: {key}, name: {name}"
+
+
+class ExperimentService(ExternalService):
+    MY_SVC_TYPE = ExternalServiceType.MLFLOW
+
+    def __init__(
+            self, key: str, name: str, url: Optional[str] = None,
+            oauth_key: Optional[str] = None, db_key: Optional[str] = None, obj_key: Optional[str] = None,
+            git_key: Optional[str] = None, configuration: Optional[dict] = None,
+    ) -> None:
+        super().__init__(
+            key=key, name=name, service_type=self.MY_SVC_TYPE, url=url, oauth_key=oauth_key,
+            db_key=db_key, obj_key=obj_key, git_key=git_key, configuration=configuration)
+
+        assert self.url, f"url must be defined for {type(self)}. key: {key}, name: {name}"
+
+
+class ObservabilityService(ExternalService):
+    MY_SVC_TYPE = ExternalServiceType.GRAFANA
+
+    def __init__(
+            self, key: str, name: str, url: Optional[str] = None,
+            oauth_key: Optional[str] = None, db_key: Optional[str] = None, obj_key: Optional[str] = None,
+            git_key: Optional[str] = None, configuration: Optional[dict] = None,
+    ) -> None:
+        super().__init__(
+            key=key, name=name, service_type=self.MY_SVC_TYPE, url=url, oauth_key=oauth_key,
+            db_key=db_key, obj_key=obj_key, git_key=git_key, configuration=configuration)
+
+        assert self.url, f"url must be defined for {type(self)}. key: {key}, name: {name}"
```

## practicuscore/conf_model.py

```diff
@@ -1,94 +1,93 @@
-import json
-from dataclasses import dataclass
-from typing import Union, List
-import base64
-
-from dataclasses_json import DataClassJsonMixin
-
-from practicuscore.api_base import ConnConf, NodeFileConnConf, SqLiteConnConf, S3ConnConf, MYSQLConnConf, \
-    PostgreSQLConnConf, RedshiftConnConf, SnowflakeConnConf, MSSQLConnConf, OracleConnConf, HiveConnConf, \
-    AthenaConnConf, ElasticSearchConnConf, OpenSearchConnConf, CustomDBConnConf, TrinoConnConf, DremioConnConf, \
-    HanaConnConf, TeradataConnConf, Db2ConnConf, DynamoDBConnConf, CockroachDBConnConf, ClouderaConnConf, \
-    ConnConfFactory
-
-
-@dataclass
-class DefinedConnectionConfiguration(DataClassJsonMixin):
-    uuid: str
-    key: str
-    cloud: str
-    region_name: str
-    conn_conf: Union[
-        ConnConf,
-        NodeFileConnConf,
-        SqLiteConnConf,
-        S3ConnConf,
-        MYSQLConnConf,
-        PostgreSQLConnConf,
-        RedshiftConnConf,
-        SnowflakeConnConf,
-        MSSQLConnConf,
-        OracleConnConf,
-        HiveConnConf,
-        AthenaConnConf,
-        ElasticSearchConnConf,
-        OpenSearchConnConf,
-        TrinoConnConf,
-        DremioConnConf,
-        HanaConnConf,
-        TeradataConnConf,
-        Db2ConnConf,
-        DynamoDBConnConf,
-        CockroachDBConnConf,
-        ClouderaConnConf,
-        CustomDBConnConf,
-    ]
-    can_write: bool = True
-
-
-@dataclass
-class DefinedConnectionConfigurations(DataClassJsonMixin):
-    defined_conn_conf_list: List[DefinedConnectionConfiguration]
-
-    def to_base64_str(self) -> str:
-        return str(base64.b64encode(bytes(self.to_json(), encoding="utf-8")), "utf-8")
-
-    @staticmethod
-    def from_base64_str(data: str) -> 'DefinedConnectionConfigurations':
-        json_str = str(base64.b64decode(data), "utf-8")
-        defined_conn_confs = DefinedConnectionConfigurations.from_json(json_str)
-
-        # Why the below?
-        # When we load DefinedConnectionConfigurations using json, Dataclasses json cannot properly detect the type of
-        # child conn_conf classes. e.g. S3ConnConf becomes ConnConf and all it's info, aws_access_key etc gets lost
-        # So we create a proper S3ConnConf and override
-        # Future: Dataclasses jason parent marshmallow have a schema() features;
-        # E.g. you dump a class, it saves __type="actual class name" with it. and uses that to deserialize (I think)
-        def_conn_conf_dict = json.loads(json_str)
-        raw_dict_conn_conf_list = def_conn_conf_dict["defined_conn_conf_list"]
-        assert len(defined_conn_confs.defined_conn_conf_list) == len(raw_dict_conn_conf_list)
-        for i in range(len(defined_conn_confs.defined_conn_conf_list)):
-            conn_conf_from_raw = ConnConfFactory.create_or_get(raw_dict_conn_conf_list[i]["conn_conf"])
-            defined_conn_confs.defined_conn_conf_list[i].conn_conf = conn_conf_from_raw
-
-        return defined_conn_confs
-
-
-# if __name__ == '__main__':
-#     cn_cnf = NodeFileConnConf(file_path="/users/blah")
-#     a = DefinedConnectionConfiguration(uuid="a", key="a", cloud="a", region_name="a", conn_conf=cn_cnf)
-#     cn_cnf2 = NodeFileConnConf(file_path="/users/blah2")
-#     b = DefinedConnectionConfiguration(uuid="b", key="b", cloud="b", region_name="b", conn_conf=cn_cnf2)
-#
-#     l = DefinedConnectionConfigurations(defined_conn_conf_list=[a, b])
-#
-#     _json = l.to_json()
-#     print(_json)
-#     l2 = DefinedConnectionConfigurations.from_json(_json)
-#
-#     print(l2.defined_conn_conf_list[1].uuid)
-#
-#     b64 = l2.to_base64_str()
-#     print(b64)
-#     l3 = DefinedConnectionConfigurations.from_base64_str(b64)
-#     print(l3.to_json())
+import json
+from dataclasses import dataclass
+from typing import Union, List
+import base64
+
+from practicuscore.api_base import PrtDataClassJsonMixin, ConnConf, WorkerFileConnConf, SqLiteConnConf, S3ConnConf, MYSQLConnConf, \
+    PostgreSQLConnConf, RedshiftConnConf, SnowflakeConnConf, MSSQLConnConf, OracleConnConf, HiveConnConf, \
+    AthenaConnConf, ElasticSearchConnConf, OpenSearchConnConf, CustomDBConnConf, TrinoConnConf, DremioConnConf, \
+    HanaConnConf, TeradataConnConf, Db2ConnConf, DynamoDBConnConf, CockroachDBConnConf, ClouderaConnConf, \
+    ConnConfFactory
+
+
+@dataclass
+class DefinedConnectionConfiguration(PrtDataClassJsonMixin):
+    uuid: str
+    key: str
+    cloud: str
+    region_name: str
+    conn_conf: Union[
+        ConnConf,
+        WorkerFileConnConf,
+        SqLiteConnConf,
+        S3ConnConf,
+        MYSQLConnConf,
+        PostgreSQLConnConf,
+        RedshiftConnConf,
+        SnowflakeConnConf,
+        MSSQLConnConf,
+        OracleConnConf,
+        HiveConnConf,
+        AthenaConnConf,
+        ElasticSearchConnConf,
+        OpenSearchConnConf,
+        TrinoConnConf,
+        DremioConnConf,
+        HanaConnConf,
+        TeradataConnConf,
+        Db2ConnConf,
+        DynamoDBConnConf,
+        CockroachDBConnConf,
+        ClouderaConnConf,
+        CustomDBConnConf,
+    ]
+    can_write: bool = True
+    owner_email: str = "?@?.?"
+
+
+@dataclass
+class DefinedConnectionConfigurations(PrtDataClassJsonMixin):
+    defined_conn_conf_list: List[DefinedConnectionConfiguration]
+
+    def to_base64_str(self) -> str:
+        return str(base64.b64encode(bytes(self.to_json(), encoding="utf-8")), "utf-8")
+
+    @staticmethod
+    def from_base64_str(data: str) -> 'DefinedConnectionConfigurations':
+        json_str = str(base64.b64decode(data), "utf-8")
+        defined_conn_confs = DefinedConnectionConfigurations.from_json(json_str)
+
+        # Why the below?
+        # When we load DefinedConnectionConfigurations using json, Dataclasses json cannot properly detect the type of
+        # child conn_conf classes. e.g. S3ConnConf becomes ConnConf and all it's info, aws_access_key etc gets lost
+        # So we create a proper S3ConnConf and override
+        # Future: Dataclasses jason parent marshmallow have a schema() features;
+        # E.g. you dump a class, it saves __type="actual class name" with it. and uses that to deserialize (I think)
+        def_conn_conf_dict = json.loads(json_str)
+        raw_dict_conn_conf_list = def_conn_conf_dict["defined_conn_conf_list"]
+        assert len(defined_conn_confs.defined_conn_conf_list) == len(raw_dict_conn_conf_list)
+        for i in range(len(defined_conn_confs.defined_conn_conf_list)):
+            conn_conf_from_raw = ConnConfFactory.create_or_get(raw_dict_conn_conf_list[i]["conn_conf"])
+            defined_conn_confs.defined_conn_conf_list[i].conn_conf = conn_conf_from_raw
+
+        return defined_conn_confs
+
+
+# if __name__ == '__main__':
+#     cn_cnf = WorkerFileConnConf(file_path="/users/blah")
+#     a = DefinedConnectionConfiguration(uuid="a", key="a", cloud="a", region_name="a", conn_conf=cn_cnf)
+#     cn_cnf2 = WorkerFileConnConf(file_path="/users/blah2")
+#     b = DefinedConnectionConfiguration(uuid="b", key="b", cloud="b", region_name="b", conn_conf=cn_cnf2)
+#
+#     l = DefinedConnectionConfigurations(defined_conn_conf_list=[a, b])
+#
+#     _json = l.to_json()
+#     print(_json)
+#     l2 = DefinedConnectionConfigurations.from_json(_json)
+#
+#     print(l2.defined_conn_conf_list[1].uuid)
+#
+#     b64 = l2.to_base64_str()
+#     print(b64)
+#     l3 = DefinedConnectionConfigurations.from_base64_str(b64)
+#     print(l3.to_json())
```

## practicuscore/core.conf

```diff
@@ -1,169 +1,33 @@
-00000000: 5b63 6c6f 7564 5d0d 0a64 6566 6175 6c74  [cloud]..default
-00000010: 5f63 6f6e 6669 6720 3d20 2d31 0d0a 6c61  _config = -1..la
-00000020: 7465 7374 5f6e 6f64 655f 6e75 6d62 6572  test_node_number
-00000030: 203d 2030 0d0a 7570 6c6f 6164 5f63 6875   = 0..upload_chu
-00000040: 6e6b 5f6d 6220 3d20 350d 0a64 6f77 6e6c  nk_mb = 5..downl
-00000050: 6f61 645f 6368 756e 6b5f 6d62 203d 2035  oad_chunk_mb = 5
-00000060: 0d0a 6175 746f 5f76 6572 6966 795f 6d70  ..auto_verify_mp
-00000070: 5f73 7562 7363 7269 7074 696f 6e73 203d  _subscriptions =
-00000080: 2054 7275 650d 0a64 6566 6175 6c74 5f63   True..default_c
-00000090: 7075 5f6e 6f64 655f 6469 736b 5f73 697a  pu_node_disk_siz
-000000a0: 6520 3d20 3230 0d0a 6465 6661 756c 745f  e = 20..default_
-000000b0: 6770 755f 6e6f 6465 5f64 6973 6b5f 7369  gpu_node_disk_si
-000000c0: 7a65 203d 2036 300d 0a6c 6f63 616c 5f63  ze = 60..local_c
-000000d0: 6f6e 7461 696e 6572 5f65 6e61 626c 6564  ontainer_enabled
-000000e0: 203d 2046 616c 7365 0d0a 0d0a 5b6f 7267   = False....[org
-000000f0: 5f63 6f6e 6669 675d 0d0a 6c61 7374 5f63  _config]..last_c
-00000100: 6865 636b 5f64 6174 6520 3d0d 0a68 6173  heck_date =..has
-00000110: 685f 7661 6c75 6520 3d0d 0a72 6566 7265  h_value =..refre
-00000120: 7368 5f69 6e74 6572 7661 6c20 3d20 3134  sh_interval = 14
-00000130: 3430 0d0a 0d0a 5b6e 6574 776f 726b 5d0d  40....[network].
-00000140: 0a62 7970 6173 735f 7373 6c5f 7665 7269  .bypass_ssl_veri
-00000150: 6669 6361 7469 6f6e 203d 2046 616c 7365  fication = False
-00000160: 0d0a 6177 735f 6375 7374 6f6d 5f76 7063  ..aws_custom_vpc
-00000170: 203d 0d0a 6177 735f 6375 7374 6f6d 5f73   =..aws_custom_s
-00000180: 7562 6e65 7420 3d0d 0a70 726f 7879 5f68  ubnet =..proxy_h
-00000190: 6f73 7420 3d0d 0a70 726f 7879 5f70 6f72  ost =..proxy_por
-000001a0: 7420 3d0d 0a70 726f 7879 5f75 7365 7220  t =..proxy_user 
-000001b0: 3d0d 0a70 726f 7879 5f70 7764 203d 0d0a  =..proxy_pwd =..
-000001c0: 7072 6f78 795f 6375 7374 6f6d 5f73 7368  proxy_custom_ssh
-000001d0: 203d 0d0a 0d0a 5b63 6f6e 7461 696e 6572   =....[container
-000001e0: 5d0d 0a65 6e67 696e 6520 3d20 646f 636b  ]..engine = dock
-000001f0: 6572 0d0a 0d0a 2320 2a2a 2a20 4c6f 6767  er....# *** Logg
-00000200: 696e 6720 5365 7474 696e 6773 202a 2a2a  ing Settings ***
-00000210: 0d0a 0d0a 5b76 6572 626f 7365 5f6c 6f67  ....[verbose_log
-00000220: 6769 6e67 5d0d 0a74 6872 6561 6469 6e67  ging]..threading
-00000230: 203d 2046 616c 7365 0d0a 6e65 7477 6f72   = False..networ
-00000240: 6b69 6e67 203d 2046 616c 7365 0d0a 6c6f  king = False..lo
-00000250: 675f 7765 6273 6f63 6b65 745f 7472 6166  g_websocket_traf
-00000260: 6669 635f 6b62 203d 2030 0d0a 0d0a 5b6c  fic_kb = 0....[l
-00000270: 6f67 6765 7273 5d0d 0a6b 6579 7320 3d20  oggers]..keys = 
-00000280: 7072 6163 7469 6375 732c 2072 6f6f 742c  practicus, root,
-00000290: 2062 6f74 6f63 6f72 652c 2072 6571 7565   botocore, reque
-000002a0: 7374 732c 2075 726c 6c69 6233 2c20 746f  sts, urllib3, to
-000002b0: 726e 6164 6f5f 6163 6365 7373 2c20 746f  rnado_access, to
-000002c0: 726e 6164 6f5f 6170 706c 6963 6174 696f  rnado_applicatio
-000002d0: 6e2c 2074 6f72 6e61 646f 5f67 656e 6572  n, tornado_gener
-000002e0: 616c 0d0a 0d0a 5b68 616e 646c 6572 735d  al....[handlers]
-000002f0: 0d0a 6b65 7973 203d 2063 6f6e 736f 6c65  ..keys = console
-00000300: 4861 6e64 6c65 722c 2072 6f74 6174 696e  Handler, rotatin
-00000310: 6746 696c 6548 616e 646c 6572 0d0a 0d0a  gFileHandler....
-00000320: 5b66 6f72 6d61 7474 6572 735d 0d0a 6b65  [formatters]..ke
-00000330: 7973 203d 2073 696d 706c 6546 6f72 6d61  ys = simpleForma
-00000340: 7474 6572 0d0a 0d0a 5b68 616e 646c 6572  tter....[handler
-00000350: 5f63 6f6e 736f 6c65 4861 6e64 6c65 725d  _consoleHandler]
-00000360: 0d0a 636c 6173 7320 3d20 5374 7265 616d  ..class = Stream
-00000370: 4861 6e64 6c65 720d 0a6c 6576 656c 203d  Handler..level =
-00000380: 2044 4542 5547 0d0a 666f 726d 6174 7465   DEBUG..formatte
-00000390: 7220 3d20 7369 6d70 6c65 466f 726d 6174  r = simpleFormat
-000003a0: 7465 720d 0a61 7267 7320 3d20 2873 7973  ter..args = (sys
-000003b0: 2e73 7464 6f75 742c 290d 0a0d 0a5b 6861  .stdout,)....[ha
-000003c0: 6e64 6c65 725f 726f 7461 7469 6e67 4669  ndler_rotatingFi
-000003d0: 6c65 4861 6e64 6c65 725d 0d0a 636c 6173  leHandler]..clas
-000003e0: 7320 3d20 6861 6e64 6c65 7273 2e52 6f74  s = handlers.Rot
-000003f0: 6174 696e 6746 696c 6548 616e 646c 6572  atingFileHandler
-00000400: 0d0a 6c65 7665 6c20 3d20 4445 4255 470d  ..level = DEBUG.
-00000410: 0a66 6f72 6d61 7474 6572 203d 2073 696d  .formatter = sim
-00000420: 706c 6546 6f72 6d61 7474 6572 0d0a 6172  pleFormatter..ar
-00000430: 6773 203d 2028 6f73 2e70 6174 682e 6a6f  gs = (os.path.jo
-00000440: 696e 286f 732e 7061 7468 2e65 7870 616e  in(os.path.expan
-00000450: 6475 7365 7228 227e 2229 2c20 222e 7072  duser("~"), ".pr
-00000460: 6163 7469 6375 7322 2c20 226c 6f67 7322  acticus", "logs"
-00000470: 2c20 2270 7261 6374 6963 7573 2e6c 6f67  , "practicus.log
-00000480: 2229 2c20 2761 272c 2032 3020 2a20 3130  "), 'a', 20 * 10
-00000490: 3234 202a 2031 3032 342c 2033 290d 0a0d  24 * 1024, 3)...
-000004a0: 0a5b 666f 726d 6174 7465 725f 7369 6d70  .[formatter_simp
-000004b0: 6c65 466f 726d 6174 7465 725d 0d0a 666f  leFormatter]..fo
-000004c0: 726d 6174 203d 2025 286c 6576 656c 6e61  rmat = %(levelna
-000004d0: 6d65 2973 207c 2025 2861 7363 7469 6d65  me)s | %(asctime
-000004e0: 2973 207c 2025 286d 6f64 756c 6529 732e  )s | %(module)s.
-000004f0: 2528 6675 6e63 4e61 6d65 2973 207c 2025  %(funcName)s | %
-00000500: 286d 6573 7361 6765 2973 0d0a 6461 7465  (message)s..date
-00000510: 666d 7420 3d20 2559 2d25 6d2d 2564 2025  fmt = %Y-%m-%d %
-00000520: 493a 254d 3a25 5320 2570 0d0a 0d0a 5b6c  I:%M:%S %p....[l
-00000530: 6f67 6765 725f 726f 6f74 5d0d 0a6c 6576  ogger_root]..lev
-00000540: 656c 203d 2044 4542 5547 0d0a 6861 6e64  el = DEBUG..hand
-00000550: 6c65 7273 203d 2063 6f6e 736f 6c65 4861  lers = consoleHa
-00000560: 6e64 6c65 722c 2072 6f74 6174 696e 6746  ndler, rotatingF
-00000570: 696c 6548 616e 646c 6572 0d0a 0d0a 5b6c  ileHandler....[l
-00000580: 6f67 6765 725f 7072 6163 7469 6375 735d  ogger_practicus]
-00000590: 0d0a 6c65 7665 6c20 3d20 4445 4255 470d  ..level = DEBUG.
-000005a0: 0a68 616e 646c 6572 7320 3d20 636f 6e73  .handlers = cons
-000005b0: 6f6c 6548 616e 646c 6572 2c20 726f 7461  oleHandler, rota
-000005c0: 7469 6e67 4669 6c65 4861 6e64 6c65 720d  tingFileHandler.
-000005d0: 0a71 7561 6c6e 616d 6520 3d20 7072 6163  .qualname = prac
-000005e0: 7469 6375 730d 0a70 726f 7061 6761 7465  ticus..propagate
-000005f0: 203d 2030 0d0a 0d0a 5b6c 6f67 6765 725f   = 0....[logger_
-00000600: 626f 746f 636f 7265 5d0d 0a6c 6576 656c  botocore]..level
-00000610: 203d 2049 4e46 4f0d 0a68 616e 646c 6572   = INFO..handler
-00000620: 7320 3d20 636f 6e73 6f6c 6548 616e 646c  s = consoleHandl
-00000630: 6572 2c20 726f 7461 7469 6e67 4669 6c65  er, rotatingFile
-00000640: 4861 6e64 6c65 720d 0a71 7561 6c6e 616d  Handler..qualnam
-00000650: 6520 3d20 626f 746f 636f 7265 0d0a 7072  e = botocore..pr
-00000660: 6f70 6167 6174 6520 3d20 300d 0a0d 0a5b  opagate = 0....[
-00000670: 6c6f 6767 6572 5f72 6571 7565 7374 735d  logger_requests]
-00000680: 0d0a 6c65 7665 6c20 3d20 494e 464f 0d0a  ..level = INFO..
-00000690: 6861 6e64 6c65 7273 203d 2063 6f6e 736f  handlers = conso
-000006a0: 6c65 4861 6e64 6c65 722c 2072 6f74 6174  leHandler, rotat
-000006b0: 696e 6746 696c 6548 616e 646c 6572 0d0a  ingFileHandler..
-000006c0: 7175 616c 6e61 6d65 203d 2072 6571 7565  qualname = reque
-000006d0: 7374 730d 0a70 726f 7061 6761 7465 203d  sts..propagate =
-000006e0: 2030 0d0a 0d0a 5b6c 6f67 6765 725f 7572   0....[logger_ur
-000006f0: 6c6c 6962 335d 0d0a 6c65 7665 6c20 3d20  llib3]..level = 
-00000700: 494e 464f 0d0a 6861 6e64 6c65 7273 203d  INFO..handlers =
-00000710: 2063 6f6e 736f 6c65 4861 6e64 6c65 722c   consoleHandler,
-00000720: 2072 6f74 6174 696e 6746 696c 6548 616e   rotatingFileHan
-00000730: 646c 6572 0d0a 7175 616c 6e61 6d65 203d  dler..qualname =
-00000740: 2075 726c 6c69 6233 0d0a 7072 6f70 6167   urllib3..propag
-00000750: 6174 6520 3d20 300d 0a0d 0a5b 6c6f 6767  ate = 0....[logg
-00000760: 6572 5f74 6f72 6e61 646f 5f61 6363 6573  er_tornado_acces
-00000770: 735d 0d0a 6c65 7665 6c20 3d20 5741 524e  s]..level = WARN
-00000780: 494e 470d 0a68 616e 646c 6572 7320 3d20  ING..handlers = 
-00000790: 636f 6e73 6f6c 6548 616e 646c 6572 2c20  consoleHandler, 
-000007a0: 726f 7461 7469 6e67 4669 6c65 4861 6e64  rotatingFileHand
-000007b0: 6c65 720d 0a71 7561 6c6e 616d 6520 3d20  ler..qualname = 
-000007c0: 746f 726e 6164 6f2e 6163 6365 7373 0d0a  tornado.access..
-000007d0: 7072 6f70 6167 6174 6520 3d20 300d 0a0d  propagate = 0...
-000007e0: 0a5b 6c6f 6767 6572 5f74 6f72 6e61 646f  .[logger_tornado
-000007f0: 5f61 7070 6c69 6361 7469 6f6e 5d0d 0a6c  _application]..l
-00000800: 6576 656c 203d 2057 4152 4e49 4e47 0d0a  evel = WARNING..
-00000810: 6861 6e64 6c65 7273 203d 2063 6f6e 736f  handlers = conso
-00000820: 6c65 4861 6e64 6c65 722c 2072 6f74 6174  leHandler, rotat
-00000830: 696e 6746 696c 6548 616e 646c 6572 0d0a  ingFileHandler..
-00000840: 7175 616c 6e61 6d65 203d 2074 6f72 6e61  qualname = torna
-00000850: 646f 2e61 7070 6c69 6361 7469 6f6e 0d0a  do.application..
-00000860: 7072 6f70 6167 6174 6520 3d20 300d 0a0d  propagate = 0...
-00000870: 0a5b 6c6f 6767 6572 5f74 6f72 6e61 646f  .[logger_tornado
-00000880: 5f67 656e 6572 616c 5d0d 0a6c 6576 656c  _general]..level
-00000890: 203d 2057 4152 4e49 4e47 0d0a 6861 6e64   = WARNING..hand
-000008a0: 6c65 7273 203d 2063 6f6e 736f 6c65 4861  lers = consoleHa
-000008b0: 6e64 6c65 722c 2072 6f74 6174 696e 6746  ndler, rotatingF
-000008c0: 696c 6548 616e 646c 6572 0d0a 7175 616c  ileHandler..qual
-000008d0: 6e61 6d65 203d 2074 6f72 6e61 646f 2e67  name = tornado.g
-000008e0: 656e 6572 616c 0d0a 7072 6f70 6167 6174  eneral..propagat
-000008f0: 6520 3d20 300d 0a0d 0a5b 6c6f 6767 6572  e = 0....[logger
-00000900: 5f6d 6174 706c 6f74 6c69 625d 0d0a 6c65  _matplotlib]..le
-00000910: 7665 6c20 3d20 5741 524e 494e 470d 0a68  vel = WARNING..h
-00000920: 616e 646c 6572 7320 3d20 636f 6e73 6f6c  andlers = consol
-00000930: 6548 616e 646c 6572 2c20 726f 7461 7469  eHandler, rotati
-00000940: 6e67 4669 6c65 4861 6e64 6c65 720d 0a71  ngFileHandler..q
-00000950: 7561 6c6e 616d 6520 3d20 6d61 7470 6c6f  ualname = matplo
-00000960: 746c 6962 0d0a 7072 6f70 6167 6174 6520  tlib..propagate 
-00000970: 3d20 300d 0a0d 0a5b 6c6f 6767 6572 5f6d  = 0....[logger_m
-00000980: 6174 706c 6f74 6c69 625f 666f 6e74 5f6d  atplotlib_font_m
-00000990: 616e 6167 6572 5d0d 0a6c 6576 656c 203d  anager]..level =
-000009a0: 2057 4152 4e49 4e47 0d0a 6861 6e64 6c65   WARNING..handle
-000009b0: 7273 203d 2063 6f6e 736f 6c65 4861 6e64  rs = consoleHand
-000009c0: 6c65 722c 2072 6f74 6174 696e 6746 696c  ler, rotatingFil
-000009d0: 6548 616e 646c 6572 0d0a 7175 616c 6e61  eHandler..qualna
-000009e0: 6d65 203d 206d 6174 706c 6f74 6c69 622e  me = matplotlib.
-000009f0: 666f 6e74 5f6d 616e 6167 6572 0d0a 7072  font_manager..pr
-00000a00: 6f70 6167 6174 6520 3d20 300d 0a0d 0a5b  opagate = 0....[
-00000a10: 6c6f 6767 6572 5f63 6f6c 6f72 6261 725d  logger_colorbar]
-00000a20: 0d0a 6c65 7665 6c20 3d20 5741 524e 494e  ..level = WARNIN
-00000a30: 470d 0a68 616e 646c 6572 7320 3d20 636f  G..handlers = co
-00000a40: 6e73 6f6c 6548 616e 646c 6572 2c20 726f  nsoleHandler, ro
-00000a50: 7461 7469 6e67 4669 6c65 4861 6e64 6c65  tatingFileHandle
-00000a60: 720d 0a71 7561 6c6e 616d 6520 3d20 636f  r..qualname = co
-00000a70: 6c6f 7262 6172 0d0a 7072 6f70 6167 6174  lorbar..propagat
-00000a80: 6520 3d20 300d 0a                        e = 0..
+00000000: 5b63 6c6f 7564 5d0a 6465 6661 756c 745f  [cloud].default_
+00000010: 636f 6e66 6967 203d 202d 310a 6c61 7465  config = -1.late
+00000020: 7374 5f6e 6f64 655f 6e75 6d62 6572 203d  st_node_number =
+00000030: 2030 0a75 706c 6f61 645f 6368 756e 6b5f   0.upload_chunk_
+00000040: 6d62 203d 2035 0a64 6f77 6e6c 6f61 645f  mb = 5.download_
+00000050: 6368 756e 6b5f 6d62 203d 2035 0a61 7574  chunk_mb = 5.aut
+00000060: 6f5f 7665 7269 6679 5f6d 705f 7375 6273  o_verify_mp_subs
+00000070: 6372 6970 7469 6f6e 7320 3d20 5472 7565  criptions = True
+00000080: 0a64 6566 6175 6c74 5f63 7075 5f6e 6f64  .default_cpu_nod
+00000090: 655f 6469 736b 5f73 697a 6520 3d20 3230  e_disk_size = 20
+000000a0: 0a64 6566 6175 6c74 5f67 7075 5f6e 6f64  .default_gpu_nod
+000000b0: 655f 6469 736b 5f73 697a 6520 3d20 3630  e_disk_size = 60
+000000c0: 0a6c 6f63 616c 5f63 6f6e 7461 696e 6572  .local_container
+000000d0: 5f65 6e61 626c 6564 203d 2046 616c 7365  _enabled = False
+000000e0: 0a67 6f6f 676c 655f 6d61 7073 5f61 7069  .google_maps_api
+000000f0: 5f6b 6579 203d 0a0a 5b6f 7267 5f63 6f6e  _key =..[org_con
+00000100: 6669 675d 0a6c 6173 745f 6368 6563 6b5f  fig].last_check_
+00000110: 6461 7465 203d 0a68 6173 685f 7661 6c75  date =.hash_valu
+00000120: 6520 3d0a 7265 6672 6573 685f 696e 7465  e =.refresh_inte
+00000130: 7276 616c 203d 2031 3434 300a 0a5b 6e65  rval = 1440..[ne
+00000140: 7477 6f72 6b5d 0a62 7970 6173 735f 7373  twork].bypass_ss
+00000150: 6c5f 7665 7269 6669 6361 7469 6f6e 203d  l_verification =
+00000160: 2046 616c 7365 0a61 7773 5f63 7573 746f   False.aws_custo
+00000170: 6d5f 7670 6320 3d0a 6177 735f 6375 7374  m_vpc =.aws_cust
+00000180: 6f6d 5f73 7562 6e65 7420 3d0a 7072 6f78  om_subnet =.prox
+00000190: 795f 686f 7374 203d 0a70 726f 7879 5f70  y_host =.proxy_p
+000001a0: 6f72 7420 3d0a 7072 6f78 795f 7573 6572  ort =.proxy_user
+000001b0: 203d 0a70 726f 7879 5f70 7764 203d 0a70   =.proxy_pwd =.p
+000001c0: 726f 7879 5f63 7573 746f 6d5f 7373 6820  roxy_custom_ssh 
+000001d0: 3d0a 0a5b 636f 6e74 6169 6e65 725d 0a65  =..[container].e
+000001e0: 6e67 696e 6520 3d20 646f 636b 6572 0a0a  ngine = docker..
+000001f0: 5b6b 3873 5d0a 6465 6661 756c 745f 7265  [k8s].default_re
+00000200: 6769 6f6e 5f6b 6579 203d 0a              gion_key =.
```

## practicuscore/uncompiled.py

```diff
@@ -1,7 +1,5 @@
-class FugueSQLHelper:
-    @staticmethod
-    def run_sql(table__4__sql, sql: str):
-        # noinspection PyUnresolvedReferences
-        from fugue.api import fugue_sql
-        _ = table__4__sql
-        return fugue_sql(sql, fsql_ignore_case=True)
+class SQLHelper:
+    @staticmethod
+    def run_sql(query, table__4__sql, sql: str):
+        _ = table__4__sql
+        return query(sql).to_df()
```

## practicuscore/user_node.conf

 * *Ordering differences only*

```diff
@@ -1,2 +1,2 @@
-[user_settings]
+[user_settings]
 auto_shut_down_minutes = 90
```

## Comparing `practicuscore-24.1.0.dist-info/METADATA` & `practicuscore-24.3.0.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,40 +1,39 @@
-Metadata-Version: 2.1
-Name: practicuscore
-Version: 24.1.0
-Summary: Practicus AI core library is the SDK that offers programmatic access for Practicus AI platform
-Home-page: https://practicus.ai/
-Author: Practicus AI
-Author-email: info@practicus.ai
-Requires-Python: >=3.10
-Description-Content-Type: text/markdown
-Requires-Dist: pandas
-Requires-Dist: requests
-Requires-Dist: websocket-client
-Requires-Dist: lz4
-Requires-Dist: boto3
-Requires-Dist: pyOpenSSL
-Requires-Dist: dataclasses-json
-Requires-Dist: chardet
-Requires-Dist: openpyxl
-Requires-Dist: PyJWT[crypto]
-Requires-Dist: packaging
-Requires-Dist: cryptography
-Requires-Dist: certifi
-
-[Practicus AI](https://practicus.ai/) is an end-to-end Data Science platform, which works like a spreadsheet. It works for both technical and non-technical users. 
-
-You can **explore and analyze** large amounts of raw data and find unique insights and patterns. 
-
-Practicus AI allows you to easily **transform and prepare** data, and extend with coding only if necessary. 
-
-You can create AI models using **AutoML** and **predict** the future with just a few clicks. When working with complex problems, 
-you can share your experiments exported code with the data science team, so they can take it further without starting from scratch.
-
-Several Practicus AI features work directly inside Spreadsheets, allowing you to bring more than 1 billion Spreadsheet 
-users into your Data Science projects. 
-
-If needed, you can click to generate the **data processing pipeline code**, and go to production with ease. 
-
-Practicus AI **app is forever free**, and the cloud platform offers a **forever free tier** as well. 
-If you need more cloud capacity, you can **pay as you go**, which shows as a line item in your cloud invoice. 
-No commitments or credit card is needed.
+Metadata-Version: 2.1
+Name: practicuscore
+Version: 24.3.0
+Summary: Practicus AI core library is the SDK that offers programmatic access for Practicus AI platform
+Home-page: https://practicus.ai/
+Author: Practicus AI
+Author-email: info@practicus.ai
+Requires-Python: >=3.10
+Description-Content-Type: text/markdown
+Requires-Dist: pandas
+Requires-Dist: requests
+Requires-Dist: websocket-client
+Requires-Dist: lz4
+Requires-Dist: pyOpenSSL
+Requires-Dist: dataclasses-json
+Requires-Dist: chardet
+Requires-Dist: PyJWT[crypto]
+Requires-Dist: packaging
+Requires-Dist: cryptography
+Requires-Dist: certifi
+Requires-Dist: argcomplete
+
+[Practicus AI](https://practicus.ai/) is an end-to-end Data Science platform, which works like a spreadsheet. It works for both technical and non-technical users. 
+
+You can **explore and analyze** large amounts of raw data and find unique insights and patterns. 
+
+Practicus AI allows you to easily **transform and prepare** data, and extend with coding only if necessary. 
+
+You can create AI models using **AutoML** and **predict** the future with just a few clicks. When working with complex problems, 
+you can share your experiments exported code with the data science team, so they can take it further without starting from scratch.
+
+Several Practicus AI features work directly inside Spreadsheets, allowing you to bring more than 1 billion Spreadsheet 
+users into your Data Science projects. 
+
+If needed, you can click to generate the **data processing pipeline code**, and go to production with ease. 
+
+Practicus AI **app is forever free**, and the cloud platform offers a **forever free tier** as well. 
+If you need more cloud capacity, you can **pay as you go**, which shows as a line item in your cloud invoice. 
+No commitments or credit card is needed.
```

