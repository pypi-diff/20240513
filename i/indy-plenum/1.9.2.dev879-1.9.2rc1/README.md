# Comparing `tmp/indy-plenum-1.9.2.dev879.tar.gz` & `tmp/indy-plenum-1.9.2rc1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/indy-plenum-1.9.2.dev879.tar", last modified: Thu Aug 29 11:08:45 2019, max compression
+gzip compressed data, was "dist/indy-plenum-1.9.2rc1.tar", last modified: Wed Aug 28 15:08:00 2019, max compression
```

## Comparing `indy-plenum-1.9.2.dev879.tar` & `indy-plenum-1.9.2rc1.tar`

### file list

```diff
@@ -1,1383 +1,1370 @@
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/storage/
--rw-rw-r--   0 sovrin    (1003)     1004     3890 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/optimistic_kv_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     2887 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/kv_store_leveldb.py
--rw-rw-r--   0 sovrin    (1003)     1004     1504 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/kv_store_rocksdb_int_keys.py
--rw-rw-r--   0 sovrin    (1003)     1004     5642 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/kv_store_file.py
--rw-rw-r--   0 sovrin    (1003)     1004     1471 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/state_ts_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     2893 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/binary_file_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     2874 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/kv_store_single_file.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     6304 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/kv_store_rocksdb.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/storage/test/
--rw-rw-r--   0 sovrin    (1003)     1004     3741 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/test/test_ts_store.py
--rw-rw-r--   0 sovrin    (1003)     1004      725 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/test/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     4035 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/test/test_optimistic_kv_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     4830 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/test/test_kv_store_comparator.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/test/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1400 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/test/test_kv_storages_read_only.py
--rw-rw-r--   0 sovrin    (1003)     1004     3470 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/test/test_chunked_file_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     2418 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/test/test_stores_equailty.py
--rw-rw-r--   0 sovrin    (1003)     1004     1416 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/test/test_kv_storage_get_equal_or_prev.py
--rw-rw-r--   0 sovrin    (1003)     1004     2907 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/test/test_kv_storages.py
--rw-rw-r--   0 sovrin    (1003)     1004     1404 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/kv_store_leveldb_int_keys.py
--rw-rw-r--   0 sovrin    (1003)     1004    10441 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/chunked_file_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     1777 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/directory_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     1760 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/kv_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     2762 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/kv_in_memory.py
--rw-rw-r--   0 sovrin    (1003)     1004     4488 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004      374 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/store_utils.py
--rw-rw-r--   0 sovrin    (1003)     1004     1862 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/text_file_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     1029 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/storage/binary_serializer_based_file_store.py
--rw-rw-r--   0 sovrin    (1003)     1004       90 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/MANIFEST.in
--rw-rw-r--   0 sovrin    (1003)     1004     4153 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/README.md
--rw-rw-r--   0 sovrin    (1003)     1004      104 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/setup.cfg
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/
--rw-rw-r--   0 sovrin    (1003)     1004      111 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/types.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/loop/
--rw-rw-r--   0 sovrin    (1003)     1004      142 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/loop/exceptions.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/loop/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     6566 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/loop/eventually.py
--rw-rw-r--   0 sovrin    (1003)     1004     1084 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/loop/startable.py
--rw-rw-r--   0 sovrin    (1003)     1004     2554 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/loop/motor.py
--rw-rw-r--   0 sovrin    (1003)     1004    10804 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/loop/looper.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/common/
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/common/config/
--rw-rw-r--   0 sovrin    (1003)     1004      429 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/common/config/util.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/common/config/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      423 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/common/error.py
--rw-rw-r--   0 sovrin    (1003)     1004      498 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/common/temp_file_util.py
--rw-rw-r--   0 sovrin    (1003)     1004     1593 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/common/util.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/common/logging/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/common/logging/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     3509 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/common/logging/CompressingFileHandler.py
--rw-rw-r--   0 sovrin    (1003)     1004     2199 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/common/logging/handlers.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/common/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004       65 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/common/constants.py
--rw-rw-r--   0 sovrin    (1003)     1004     4229 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/common/log.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/network/
--rw-rw-r--   0 sovrin    (1003)     1004     4661 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/network/keep_in_touch.py
--rw-rw-r--   0 sovrin    (1003)     1004     1661 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/network/exceptions.py
--rw-rw-r--   0 sovrin    (1003)     1004     2528 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/network/util.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/network/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      214 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/network/auth_mode.py
--rw-rw-r--   0 sovrin    (1003)     1004     2774 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/network/port_dispenser.py
--rw-rw-r--   0 sovrin    (1003)     1004     5799 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/network/network_interface.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/validators/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/validators/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      489 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/validators/message_length_validator.py
--rw-rw-r--   0 sovrin    (1003)     1004      125 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/error_codes.py
--rw-rw-r--   0 sovrin    (1003)     1004     1464 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/config.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2791 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/ratchet.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/test/
--rw-rw-r--   0 sovrin    (1003)     1004      346 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/test/conftest.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/test/loop/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/test/loop/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      258 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/test/loop/test_looper.py
--rw-rw-r--   0 sovrin    (1003)     1004      424 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/test/loop/test_eventually.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/test/common/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/test/common/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      173 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/test/common/test_logger.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/test/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/test/crypto/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/test/crypto/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      242 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/test/crypto/test_util.py
--rw-rw-r--   0 sovrin    (1003)     1004     4275 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/test/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004      441 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/test/test_msg_len_validator.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_core/crypto/
--rw-rw-r--   0 sovrin    (1003)     1004      467 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/crypto/encoding.py
--rw-rw-r--   0 sovrin    (1003)     1004    16836 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/crypto/nacl_wrappers.py
--rw-rw-r--   0 sovrin    (1003)     1004     2922 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/crypto/util.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/crypto/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      499 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_core/crypto/signer.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/common/
--rw-rw-r--   0 sovrin    (1003)     1004      339 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/error.py
--rw-rw-r--   0 sovrin    (1003)     1004     2049 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/exceptions.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/common/test/
--rw-rw-r--   0 sovrin    (1003)     1004     4371 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/test/test_msgpack_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004     3740 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/test/test_signing_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/test/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/common/test/version/
--rw-rw-r--   0 sovrin    (1003)     1004    11240 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/test/version/test_version.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/test/version/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     8752 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/test/test_compact_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004     1282 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/test/test_exceptions.py
--rw-rw-r--   0 sovrin    (1003)     1004     1742 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/test/test_json_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004      972 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/test/test_fields.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/common/serializers/
--rw-rw-r--   0 sovrin    (1003)     1004     2771 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/serializers/compact_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004     1859 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/serializers/msgpack_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004     1553 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/serializers/serialization.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/serializers/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      600 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/serializers/field.py
--rw-rw-r--   0 sovrin    (1003)     1004     2130 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/serializers/json_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004      325 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/serializers/base58_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004      152 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/serializers/stream_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004     2937 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/serializers/signing_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004      245 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/serializers/mapping_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004      309 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/serializers/base64_serializer.py
--rw-rw-r--   0 sovrin    (1003)     1004     9669 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/common/version.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/data/
--rw-rw-r--   0 sovrin    (1003)     1004     1673 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/data/domain_transactions_local_genesis
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/data/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1369 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/data/pool_transactions_sandbox_genesis
--rw-rw-r--   0 sovrin    (1003)     1004      930 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/data/domain_transactions_sandbox_genesis
--rw-rw-r--   0 sovrin    (1003)     1004     1408 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/data/pool_transactions_local_genesis
--rw-rw-r--   0 sovrin    (1003)     1004     3169 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/setup.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/indy_plenum.egg-info/
--rw-r--r--   0 sovrin    (1003)     1004       64 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/indy_plenum.egg-info/top_level.txt
--rw-r--r--   0 sovrin    (1003)     1004    60918 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/indy_plenum.egg-info/SOURCES.txt
--rw-r--r--   0 sovrin    (1003)     1004      555 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/indy_plenum.egg-info/requires.txt
--rw-r--r--   0 sovrin    (1003)     1004        1 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/indy_plenum.egg-info/dependency_links.txt
--rw-r--r--   0 sovrin    (1003)     1004      602 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/indy_plenum.egg-info/PKG-INFO
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/recorder/
--rw-rw-r--   0 sovrin    (1003)     1004      927 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/recorder/simple_zstack_with_silencer.py
--rw-rw-r--   0 sovrin    (1003)     1004     1822 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/recorder/combined_recorder.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/recorder/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     8857 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/recorder/replayer.py
--rw-rw-r--   0 sovrin    (1003)     1004     5825 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/recorder/recorder.py
--rw-rw-r--   0 sovrin    (1003)     1004     5686 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/recorder/replayable_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     1762 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/recorder/simple_zstack_with_recorder.py
--rw-rw-r--   0 sovrin    (1003)     1004      446 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/recorder/silencer.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/
--rw-rw-r--   0 sovrin    (1003)     1004     4177 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/database_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     2434 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/replica_stasher.py
--rw-rw-r--   0 sovrin    (1003)     1004     3950 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/replica_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004    30423 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/validator_info_tool.py
--rw-rw-r--   0 sovrin    (1003)     1004      669 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/blacklister.py
--rw-rw-r--   0 sovrin    (1003)     1004     5775 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/replica_validator.py
--rw-rw-r--   0 sovrin    (1003)     1004     1386 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/quorums.py
--rw-rw-r--   0 sovrin    (1003)     1004      694 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/replica_validator_enums.py
--rw-rw-r--   0 sovrin    (1003)     1004     2947 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/replica_freshness_checker.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/catchup/
--rw-rw-r--   0 sovrin    (1003)     1004    19564 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/catchup/cons_proof_service.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/catchup/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004    20433 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/catchup/catchup_rep_service.py
--rw-rw-r--   0 sovrin    (1003)     1004     4645 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/catchup/ledger_leecher_service.py
--rw-rw-r--   0 sovrin    (1003)     1004     2841 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/catchup/node_catchup_data.py
--rw-rw-r--   0 sovrin    (1003)     1004     8368 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/catchup/seeder_service.py
--rw-rw-r--   0 sovrin    (1003)     1004    10381 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/catchup/node_leecher_service.py
--rw-rw-r--   0 sovrin    (1003)     1004     2948 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/catchup/utils.py
--rw-rw-r--   0 sovrin    (1003)     1004    10418 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/client_authn.py
--rw-rw-r--   0 sovrin    (1003)     1004    34959 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/replica.py
--rw-rw-r--   0 sovrin    (1003)     1004     5081 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/plugin_loader.py
--rw-rw-r--   0 sovrin    (1003)     1004      164 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/stats_consumer.py
--rw-rw-r--   0 sovrin    (1003)     1004     1376 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/msg_filter.py
--rw-rw-r--   0 sovrin    (1003)     1004    17034 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/pool_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     2169 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/quota_control.py
--rw-rw-r--   0 sovrin    (1003)     1004     5036 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/has_action_queue.py
--rw-rw-r--   0 sovrin    (1003)     1004     4856 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/last_sent_pp_store_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     2691 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/req_authenticator.py
--rw-rw-r--   0 sovrin    (1003)     1004    12669 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/propagator.py
--rw-rw-r--   0 sovrin    (1003)     1004   155906 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/node.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/view_change/
--rw-rw-r--   0 sovrin    (1003)     1004     1032 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/view_change/view_change_msg_filter.py
--rw-rw-r--   0 sovrin    (1003)     1004     3675 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/view_change/node_view_changer.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/view_change/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004    30409 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/view_change/view_changer.py
--rw-rw-r--   0 sovrin    (1003)     1004     6392 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/view_change/pre_view_change_strategies.py
--rw-rw-r--   0 sovrin    (1003)     1004     5250 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/view_change/instance_change_provider.py
--rw-rw-r--   0 sovrin    (1003)     1004     3859 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/router.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/observer/
--rw-rw-r--   0 sovrin    (1003)     1004     1187 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/observer/observable_sync_policy_each_batch.py
--rw-rw-r--   0 sovrin    (1003)     1004     6227 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/observer/observer_sync_policy_each_batch.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/observer/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1416 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/observer/observable_sync_policy.py
--rw-rw-r--   0 sovrin    (1003)     1004     3489 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/observer/observable.py
--rw-rw-r--   0 sovrin    (1003)     1004      875 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/observer/observer.py
--rw-rw-r--   0 sovrin    (1003)     1004      579 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/observer/observer_sync_policy.py
--rw-rw-r--   0 sovrin    (1003)     1004     1554 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/observer/observer_node.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/consensus/
--rw-rw-r--   0 sovrin    (1003)     1004     4247 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/consensus/ordering_service_msg_validator.py
--rw-rw-r--   0 sovrin    (1003)     1004     2622 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/consensus/replica_service.py
--rw-rw-r--   0 sovrin    (1003)     1004    13849 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/consensus/checkpoint_service.py
--rw-rw-r--   0 sovrin    (1003)     1004      596 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/consensus/metrics_decorator.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/consensus/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     3093 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/consensus/primary_selector.py
--rw-rw-r--   0 sovrin    (1003)     1004    18472 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/consensus/view_change_service.py
--rw-rw-r--   0 sovrin    (1003)     1004     3963 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/consensus/consensus_shared_data.py
--rw-rw-r--   0 sovrin    (1003)     1004     4332 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/consensus/msg_validator.py
--rw-rw-r--   0 sovrin    (1003)     1004   102252 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/consensus/ordering_service.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/request_managers/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_managers/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1117 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_managers/action_request_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     1447 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_managers/request_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004    15516 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_managers/write_request_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     1224 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_managers/read_request_manager.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/
--rw-rw-r--   0 sovrin    (1003)     1004     4672 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/nym_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     3645 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/get_txn_author_agreement_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     3133 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/get_txn_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     3637 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/txn_author_agreement_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     2965 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/get_txn_author_agreement_aml_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004    10011 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/node_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004       38 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/state_constants.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/handler_interfaces/
--rw-rw-r--   0 sovrin    (1003)     1004      577 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/handler_interfaces/action_request_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     3258 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/handler_interfaces/write_request_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/handler_interfaces/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1432 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/handler_interfaces/request_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     4157 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/handler_interfaces/read_request_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     1038 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/audit_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     2825 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/static_taa_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     3008 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/txn_author_agreement_aml_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     1818 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/request_handlers/utils.py
--rw-rw-r--   0 sovrin    (1003)     1004     5429 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/notifier_plugin_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     1177 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/instances.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/
--rw-rw-r--   0 sovrin    (1003)     1004      535 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/pool_batch_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     3016 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/batch_request_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     3353 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/three_pc_batch.py
--rw-rw-r--   0 sovrin    (1003)     1004      609 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/config_batch_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1358 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/ts_store_batch_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004      541 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/domain_batch_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     9429 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/audit_batch_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     8987 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/message_handlers.py
--rw-rw-r--   0 sovrin    (1003)     1004     5336 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/node_bootstrap.py
--rw-rw-r--   0 sovrin    (1003)     1004    32342 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/monitor.py
--rw-rw-r--   0 sovrin    (1003)     1004     3616 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/models.py
--rw-rw-r--   0 sovrin    (1003)     1004     5580 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/backup_instance_faulty_processor.py
--rw-rw-r--   0 sovrin    (1003)     1004     4789 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/suspicion_codes.py
--rw-rw-r--   0 sovrin    (1003)     1004     2468 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/message_req_processor.py
--rw-rw-r--   0 sovrin    (1003)     1004     4775 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/future_primaries_batch_handler.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/plugin/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/plugin/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2436 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/plugin/has_plugin_loader_helper.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/plugin/stats_consumer/
--rw-rw-r--   0 sovrin    (1003)     1004     3749 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/plugin/stats_consumer/stats_publisher.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/plugin/stats_consumer/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     4013 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/plugin/stats_consumer/plugin_firebase_stats_consumer.py
--rw-rw-r--   0 sovrin    (1003)     1004     1031 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/inconsistency_watchers.py
--rw-rw-r--   0 sovrin    (1003)     1004    11671 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/ledgers_bootstrap.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/server/general_config/
--rw-rw-r--   0 sovrin    (1003)     1004      458 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/general_config/ubuntu_platform_config.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/general_config/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004       17 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/general_config/windows_platform_config.py
--rw-rw-r--   0 sovrin    (1003)     1004    10390 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/server/replicas.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/client/
--rw-rw-r--   0 sovrin    (1003)     1004    14987 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/client/wallet.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/client/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/bls/
--rw-rw-r--   0 sovrin    (1003)     1004     1445 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/bls/bls_key_register_pool_ledger.py
--rw-rw-r--   0 sovrin    (1003)     1004     1396 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/bls/bls_crypto_factory.py
--rw-rw-r--   0 sovrin    (1003)     1004     1739 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/bls/bls_bft_factory.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/bls/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1534 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/bls/bls_key_manager_file.py
--rw-rw-r--   0 sovrin    (1003)     1004     1469 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/bls/bls_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     1847 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/bls/bls_key_register_pool_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004    11482 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/bls/bls_bft_replica_plenum.py
--rw-r--r--   0 sovrin    (1003)     1004      138 2019-08-29 11:08:44.000000 indy-plenum-1.9.2.dev879/plenum/__manifest__.json
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/common/
--rw-rw-r--   0 sovrin    (1003)     1004     1712 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/config_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     4076 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/transaction_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     8116 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/batched.py
--rw-rw-r--   0 sovrin    (1003)     1004      768 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/jsonpickle_util.py
--rw-rw-r--   0 sovrin    (1003)     1004     5206 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/types.py
--rw-rw-r--   0 sovrin    (1003)     1004     1138 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/prepare_batch.py
--rw-rw-r--   0 sovrin    (1003)     1004      670 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/ledger_info.py
--rw-rw-r--   0 sovrin    (1003)     1004     1358 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/did_method.py
--rw-rw-r--   0 sovrin    (1003)     1004     6795 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/script_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     2557 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/latency_measurements.py
--rw-rw-r--   0 sovrin    (1003)     1004      144 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/error.py
--rw-rw-r--   0 sovrin    (1003)     1004     7068 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/exceptions.py
--rw-rw-r--   0 sovrin    (1003)     1004      461 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/temp_file_util.py
--rw-rw-r--   0 sovrin    (1003)     1004     3694 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/gc_trackers.py
--rw-rw-r--   0 sovrin    (1003)     1004     2876 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/throttler.py
--rw-rw-r--   0 sovrin    (1003)     1004    11206 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/txn_util.py
--rw-rw-r--   0 sovrin    (1003)     1004     3012 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/keygen_utils.py
--rw-rw-r--   0 sovrin    (1003)     1004     2185 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/signer_simple.py
--rw-rw-r--   0 sovrin    (1003)     1004      818 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/sys_util.py
--rw-rw-r--   0 sovrin    (1003)     1004     3998 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/signer_did.py
--rw-rw-r--   0 sovrin    (1003)     1004     2538 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/stashing_deque.py
--rw-rw-r--   0 sovrin    (1003)     1004      407 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/roles.py
--rw-rw-r--   0 sovrin    (1003)     1004     3736 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/metrics_stats.py
--rw-rw-r--   0 sovrin    (1003)     1004     1652 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/verifier.py
--rw-rw-r--   0 sovrin    (1003)     1004    18163 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/util.py
--rw-rw-r--   0 sovrin    (1003)     1004     2337 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/router.py
--rw-rw-r--   0 sovrin    (1003)     1004     1460 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/bitmask_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2684 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/timer.py
--rw-rw-r--   0 sovrin    (1003)     1004      520 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/tools.py
--rw-rw-r--   0 sovrin    (1003)     1004     1641 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/pkg_util.py
--rw-rw-r--   0 sovrin    (1003)     1004     2078 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/moving_average.py
--rw-rw-r--   0 sovrin    (1003)     1004     4320 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/value_accumulator.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/common/messages/
--rw-rw-r--   0 sovrin    (1003)     1004     4450 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/messages/node_message_factory.py
--rw-rw-r--   0 sovrin    (1003)     1004    22174 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/messages/fields.py
--rw-rw-r--   0 sovrin    (1003)     1004     9855 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/messages/client_request.py
--rw-rw-r--   0 sovrin    (1003)     1004    15886 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/messages/node_messages.py
--rw-rw-r--   0 sovrin    (1003)     1004     2321 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/messages/internal_messages.py
--rw-rw-r--   0 sovrin    (1003)     1004     5968 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/messages/message_base.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/messages/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     6971 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/stashing_router.py
--rw-rw-r--   0 sovrin    (1003)     1004      588 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/transactions.py
--rw-rw-r--   0 sovrin    (1003)     1004     6674 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/constants.py
--rw-rw-r--   0 sovrin    (1003)     1004     2587 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/perf_util.py
--rw-rw-r--   0 sovrin    (1003)     1004    16198 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/metrics_collector.py
--rw-rw-r--   0 sovrin    (1003)     1004      985 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/event_bus.py
--rw-rw-r--   0 sovrin    (1003)     1004     1806 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/startable.py
--rw-rw-r--   0 sovrin    (1003)     1004     2582 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/plugin_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     3728 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/monitor_strategies.py
--rw-rw-r--   0 sovrin    (1003)     1004     6950 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/ledger_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     7439 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/throughput_measurements.py
--rw-rw-r--   0 sovrin    (1003)     1004    13935 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/test_network_setup.py
--rw-rw-r--   0 sovrin    (1003)     1004     2603 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/ledger_uncommitted_tracker.py
--rw-rw-r--   0 sovrin    (1003)     1004     5832 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/request.py
--rw-rw-r--   0 sovrin    (1003)     1004     1932 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/message_processor.py
--rw-rw-r--   0 sovrin    (1003)     1004      619 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/average_strategies.py
--rw-rw-r--   0 sovrin    (1003)     1004     2516 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/motor.py
--rw-rw-r--   0 sovrin    (1003)     1004      881 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/has_file_storage.py
--rw-rw-r--   0 sovrin    (1003)     1004      432 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/plenum_protocol_version.py
--rw-rw-r--   0 sovrin    (1003)     1004     5527 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/channel.py
--rw-rw-r--   0 sovrin    (1003)     1004     6644 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/ledger.py
--rw-rw-r--   0 sovrin    (1003)     1004     9355 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/stacks.py
--rw-rw-r--   0 sovrin    (1003)     1004    10484 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/stack_manager.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/common/member/
--rw-rw-r--   0 sovrin    (1003)     1004     3176 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/member/steward.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/member/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      293 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/member/trustee.py
--rw-rw-r--   0 sovrin    (1003)     1004     1109 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/member/member.py
--rw-rw-r--   0 sovrin    (1003)     1004      818 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/init_util.py
--rw-rw-r--   0 sovrin    (1003)     1004     3981 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/common/config_util.py
--rw-rw-r--   0 sovrin    (1003)     1004    14108 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/config.py
--rw-rw-r--   0 sovrin    (1003)     1004     3380 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2478 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/__metadata__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/
--rw-rw-r--   0 sovrin    (1003)     1004      930 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     3230 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_safe_request.py
--rw-rw-r--   0 sovrin    (1003)     1004     2066 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_message_serialization.py
--rw-rw-r--   0 sovrin    (1003)     1004     4545 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_strict_schema.py
--rw-rw-r--   0 sovrin    (1003)     1004     3391 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_message_factory.py
--rw-rw-r--   0 sovrin    (1003)     1004     1877 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_node_op.py
--rw-rw-r--   0 sovrin    (1003)     1004      748 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_taa.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/
--rw-rw-r--   0 sovrin    (1003)     1004     1591 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_sha256_hex_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      647 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_merkle_tree_root_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      436 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_ledger_id_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      467 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_time_among_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      614 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_fixed_length_string_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      352 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_bool_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      427 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_message_field.py
--rw-rw-r--   0 sovrin    (1003)     1004     2067 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_bls_multisig_value_field.py
--rw-rw-r--   0 sovrin    (1003)     1004     1422 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_request_identifier_field.py
--rw-rw-r--   0 sovrin    (1003)     1004     1643 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_base58_field.py
--rw-rw-r--   0 sovrin    (1003)     1004     1700 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_iterable_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      689 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_protocol_version_field.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      379 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_non_negative_number_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      379 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_serializedvalue_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      242 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_non_empty_string_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      472 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_txn_seq_no_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      782 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_verkey_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      697 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_limited_length_string_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      275 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_ledger_info_field.py
--rw-rw-r--   0 sovrin    (1003)     1004     1986 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_bls_multisig_field.py
--rw-rw-r--   0 sovrin    (1003)     1004     1972 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_version_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      351 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_timestamp_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      632 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_batch_id_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      510 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_hex_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      608 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_identifier_field.py
--rw-rw-r--   0 sovrin    (1003)     1004      548 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_handle_one_node_message.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      742 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_message_base.py
--rw-rw-r--   0 sovrin    (1003)     1004    12106 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_request.py
--rw-rw-r--   0 sovrin    (1003)     1004      526 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/constants.py
--rw-rw-r--   0 sovrin    (1003)     1004     1434 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_taa_aml_op.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/
--rw-rw-r--   0 sovrin    (1003)     1004      780 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_catchuprep_message.py
--rw-rw-r--   0 sovrin    (1003)     1004     2034 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_observed_data.py
--rw-rw-r--   0 sovrin    (1003)     1004     1107 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_client_message.py
--rw-rw-r--   0 sovrin    (1003)     1004      761 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_instanceChange_message.py
--rw-rw-r--   0 sovrin    (1003)     1004      868 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_checkpoint_message.py
--rw-rw-r--   0 sovrin    (1003)     1004     1059 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_prepare_message.py
--rw-rw-r--   0 sovrin    (1003)     1004     1233 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_ordered_message.py
--rw-rw-r--   0 sovrin    (1003)     1004      869 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_commit_message.py
--rw-rw-r--   0 sovrin    (1003)     1004     3082 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_batch_committed.py
--rw-rw-r--   0 sovrin    (1003)     1004      846 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_catchupreq_message.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      829 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_viewchangedone_messsage.py
--rw-rw-r--   0 sovrin    (1003)     1004      794 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_new_view_message.py
--rw-rw-r--   0 sovrin    (1003)     1004      856 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_view_change_message.py
--rw-rw-r--   0 sovrin    (1003)     1004     1438 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_preprepare_message.py
--rw-rw-r--   0 sovrin    (1003)     1004     1055 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_consistencyproof_message.py
--rw-rw-r--   0 sovrin    (1003)     1004      759 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_currentstate_message.py
--rw-rw-r--   0 sovrin    (1003)     1004      717 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_batch_message.py
--rw-rw-r--   0 sovrin    (1003)     1004      971 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_ledgerstatus_message.py
--rw-rw-r--   0 sovrin    (1003)     1004      811 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_view_change_message_ack.py
--rw-rw-r--   0 sovrin    (1003)     1004      805 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_propagate_message.py
--rw-rw-r--   0 sovrin    (1003)     1004     1376 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_taa_op.py
--rw-rw-r--   0 sovrin    (1003)     1004     1369 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_get_txn_op.py
--rw-rw-r--   0 sovrin    (1003)     1004     8888 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     1023 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/stub_messages.py
--rw-rw-r--   0 sovrin    (1003)     1004      385 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/utils.py
--rw-rw-r--   0 sovrin    (1003)     1004     1500 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_nym_op.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/storage/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/storage/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2091 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/storage/test_hash_stores.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/signing/
--rw-rw-r--   0 sovrin    (1003)     1004     1309 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/signing/test_signing_without_identifier.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/signing/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2850 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/signing/test_signing.py
--rw-rw-r--   0 sovrin    (1003)     1004     4022 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/signing/test_create_did_without_endorser.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/
--rw-rw-r--   0 sovrin    (1003)     1004     7972 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_unit_setup_for_non_master.py
--rw-rw-r--   0 sovrin    (1003)     1004     5398 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_node_got_no_preprepare.py
--rw-rw-r--   0 sovrin    (1003)     1004     2319 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_setup_for_non_master.py
--rw-rw-r--   0 sovrin    (1003)     1004     4498 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_2_nodes_got_only_preprepare.py
--rw-rw-r--   0 sovrin    (1003)     1004     3500 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_send_node_with_invalid_verkey.py
--rw-rw-r--   0 sovrin    (1003)     1004     3232 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_request_schema_validation.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/
--rw-rw-r--   0 sovrin    (1003)     1004     4403 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/test_3pc_timestamp.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2724 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/test_timestamp_post_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     2243 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/test_timestamp_new_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     2666 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/test_clock_disruption.py
--rw-rw-r--   0 sovrin    (1003)     1004     1288 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     4351 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_quorum_f_plus_2_nodes_including_primary_off_and_on.py
--rw-rw-r--   0 sovrin    (1003)     1004     3409 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_discard_3pc_for_ordered.py
--rw-rw-r--   0 sovrin    (1003)     1004     8385 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_belated_request_not_processed.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/
--rw-rw-r--   0 sovrin    (1003)     1004     5343 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence.py
--rw-rw-r--   0 sovrin    (1003)     1004     1984 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_f_plus_one_faults.py
--rw-rw-r--   0 sovrin    (1003)     1004      769 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_num_of_sufficient_preprepare.py
--rw-rw-r--   0 sovrin    (1003)     1004      682 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_one_fault.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004       82 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_num_of_preprepare_with_zero_faulty_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     2160 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_primary_sends_preprepare_of_high_num.py
--rw-rw-r--   0 sovrin    (1003)     1004     1085 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_ignore_pre_prepare_pp_seq_no_less_than_expected.py
--rw-rw-r--   0 sovrin    (1003)     1004     2636 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_non_primary_sends_a_pre_prepare.py
--rw-rw-r--   0 sovrin    (1003)     1004     3148 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_pass_for_stashed.py
--rw-rw-r--   0 sovrin    (1003)     1004     2304 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_fail_for_delayed.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     3330 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_reply_from_ledger_for_request.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/
--rw-rw-r--   0 sovrin    (1003)     1004      724 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_num_of_propagate_with_zero_faulty_node.py
--rw-rw-r--   0 sovrin    (1003)     1004      661 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_num_of_propagate_with_one_fault.py
--rw-rw-r--   0 sovrin    (1003)     1004      875 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_no_reauth.py
--rw-rw-r--   0 sovrin    (1003)     1004     3579 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_node_lacks_finalised_requests.py
--rw-rw-r--   0 sovrin    (1003)     1004     1002 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_num_of_propagate_with_f_plus_one_faulty_nodes.py
--rw-rw-r--   0 sovrin    (1003)     1004     2226 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_node_request_only_needed_propagates.py
--rw-rw-r--   0 sovrin    (1003)     1004     1382 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_clean_verified_reqs.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1774 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_num_of_sufficient_propagate.py
--rw-rw-r--   0 sovrin    (1003)     1004      377 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     1401 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_node_request_propagates_with_delay.py
--rw-rw-r--   0 sovrin    (1003)     1004     3128 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_request_forwarding.py
--rw-rw-r--   0 sovrin    (1003)     1004     4290 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_quorum_f_plus_2_nodes_but_not_primary_off_and_on.py
--rw-rw-r--   0 sovrin    (1003)     1004     4934 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_different_ledger_request_interleave.py
--rw-rw-r--   0 sovrin    (1003)     1004     1713 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_quorum_faulty.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/
--rw-rw-r--   0 sovrin    (1003)     1004     1069 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     3791 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_node_requests_missing_preprepares_and_prepares.py
--rw-rw-r--   0 sovrin    (1003)     1004     5003 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_preprepare_request.py
--rw-rw-r--   0 sovrin    (1003)     1004     5173 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_node_requests_missing_preprepares_prepares_and_commits.py
--rw-rw-r--   0 sovrin    (1003)     1004    10010 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_valid_message_request.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2700 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_node_requests_missing_preprepare.py
--rw-rw-r--   0 sovrin    (1003)     1004     4773 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_node_requests_missing_preprepares_and_prepares_after_long_disconnection.py
--rw-rw-r--   0 sovrin    (1003)     1004      408 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_request_ls_for_incorrect_ledger.py
--rw-rw-r--   0 sovrin    (1003)     1004     1334 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     2944 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_requested_preprepare_handling.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_order/
--rw-rw-r--   0 sovrin    (1003)     1004     2520 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_order/test_request_ordering_2.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_order/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1592 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_order/test_request_ordering_1.py
--rw-rw-r--   0 sovrin    (1003)     1004     2139 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_order/test_ordering_when_pre_prepare_not_received.py
--rw-rw-r--   0 sovrin    (1003)     1004     1648 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_split_non_3pc_messages_on_batches.py
--rw-rw-r--   0 sovrin    (1003)     1004     1778 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_quorum_disconnected.py
--rw-rw-r--   0 sovrin    (1003)     1004    16062 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/node_request_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     2407 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_req_idr_to_txn.py
--rw-rw-r--   0 sovrin    (1003)     1004     1255 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/helper.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_prepare/
--rw-rw-r--   0 sovrin    (1003)     1004      692 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_prepare/test_num_of_prepare_with_one_fault.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_prepare/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004       76 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_prepare/test_num_of_prepare_with_zero_faulty_node.py
--rw-rw-r--   0 sovrin    (1003)     1004      983 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_prepare/test_num_of_sufficient_prepare.py
--rw-rw-r--   0 sovrin    (1003)     1004     1253 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_prepare/test_num_prepare_with_2_of_6_faulty.py
--rw-rw-r--   0 sovrin    (1003)     1004     1705 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_prepare/test_num_of_prepare_with_f_plus_one_faults.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/
--rw-rw-r--   0 sovrin    (1003)     1004     1269 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_num_commit_with_2_of_6_faulty.py
--rw-rw-r--   0 sovrin    (1003)     1004      992 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_num_of_sufficient_commit.py
--rw-rw-r--   0 sovrin    (1003)     1004      595 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_num_commit_with_one_fault.py
--rw-rw-r--   0 sovrin    (1003)     1004     1520 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_num_of_commit_with_f_plus_one_faults.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1252 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_commits_without_prepares.py
--rw-rw-r--   0 sovrin    (1003)     1004     2279 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_commits_dequeue_commits.py
--rw-rw-r--   0 sovrin    (1003)     1004     1838 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_commits_recvd_first.py
--rw-rw-r--   0 sovrin    (1003)     1004       76 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_num_of_commit_with_zero_faulty_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     2269 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_1_node_got_only_preprepare.py
--rw-rw-r--   0 sovrin    (1003)     1004     1191 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_msg_len_limit_large_enough.py
--rw-rw-r--   0 sovrin    (1003)     1004     4082 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_request/test_already_processed_request.py
--rw-rw-r--   0 sovrin    (1003)     1004    40048 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     9458 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/malicious_behaviors_node.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/
--rw-rw-r--   0 sovrin    (1003)     1004     7343 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     1307 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_taa_aml_module.py
--rw-rw-r--   0 sovrin    (1003)     1004     2220 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_taa_aml_integration.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/
--rw-rw-r--   0 sovrin    (1003)     1004     7254 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     1233 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/test_taa_not_set.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1333 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/test_taa_acceptance_integration_validation.py
--rw-rw-r--   0 sovrin    (1003)     1004     2183 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004    10421 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/test_taa_acceptance_validation.py
--rw-rw-r--   0 sovrin    (1003)     1004     6578 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_config_req_handler_taa_utils.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2847 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_txn_author_agreement.py
--rw-rw-r--   0 sovrin    (1003)     1004     8063 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_get_txn_author_agreement.py
--rw-rw-r--   0 sovrin    (1003)     1004     7394 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     2786 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_get_empty_txn_author_agreement.py
--rw-rw-r--   0 sovrin    (1003)     1004     8134 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_get_taa_aml.py
--rw-rw-r--   0 sovrin    (1003)     1004      858 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/profiler.py
--rw-rw-r--   0 sovrin    (1003)     1004     1302 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_round_trip_with_one_faulty_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     1498 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_memory_consumpion.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/blacklist/
--rw-rw-r--   0 sovrin    (1003)     1004     1433 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/blacklist/test_blacklist_client.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/blacklist/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     4218 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_req_authenticator.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/tracker/
--rw-rw-r--   0 sovrin    (1003)     1004     9416 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/tracker/test_ledger_uncommitted_tracker.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/tracker/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1648 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_delay.py
--rw-rw-r--   0 sovrin    (1003)     1004     3689 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_stashing_queue.py
--rw-rw-r--   0 sovrin    (1003)     1004     5547 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/deep_eq.py
--rw-rw-r--   0 sovrin    (1003)     1004     1948 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_request_executed_once_and_without_failing_behind.py
--rw-rw-r--   0 sovrin    (1003)     1004     4328 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_get_txn_state_proof.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/
--rw-rw-r--   0 sovrin    (1003)     1004      617 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/test_node_msgs_recording.py
--rw-rw-r--   0 sovrin    (1003)     1004     1101 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004      863 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/test_replayer.py
--rw-rw-r--   0 sovrin    (1003)     1004     6601 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/test_recorder.py
--rw-rw-r--   0 sovrin    (1003)     1004     2243 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/test_combined_recorder.py
--rw-rw-r--   0 sovrin    (1003)     1004     1210 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/test_replay_on_new_node.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     3303 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/test_replay_node_bouncing.py
--rw-rw-r--   0 sovrin    (1003)     1004     2162 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/test_replay_with_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     3934 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     1605 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/recorder/test_replay.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/instances/
--rw-rw-r--   0 sovrin    (1003)     1004     2359 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/instances/test_multiple_pre_prepare.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/instances/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1743 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/instances/test_msgs_from_slow_instances.py
--rw-rw-r--   0 sovrin    (1003)     1004     1904 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/instances/test_multiple_instance_change_msgs.py
--rw-rw-r--   0 sovrin    (1003)     1004     2777 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/instances/test_multiple_commit.py
--rw-rw-r--   0 sovrin    (1003)     1004     2636 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/instances/test_multiple_prepare.py
--rw-rw-r--   0 sovrin    (1003)     1004     2142 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/instances/test_prepare_digest.py
--rw-rw-r--   0 sovrin    (1003)     1004     1089 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/instances/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     2335 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/instances/test_pre_prepare_digest.py
--rw-rw-r--   0 sovrin    (1003)     1004     3961 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/instances/test_instance_cannot_become_active_with_less_than_four_servers.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/server/
--rw-rw-r--   0 sovrin    (1003)     1004    10124 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/server/test_instance_change_provider.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/server/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      560 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_config_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     1974 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/grouped_load_scheduling.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/client/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/client/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     5907 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/client/test_protocol_version.py
--rw-rw-r--   0 sovrin    (1003)     1004     2605 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/client/test_client.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/
--rw-rw-r--   0 sovrin    (1003)     1004     3618 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_not_depend_on_node_reg.py
--rw-rw-r--   0 sovrin    (1003)     1004     1020 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     3214 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_key_registry_pool_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     1606 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_no_state_proof.py
--rw-rw-r--   0 sovrin    (1003)     1004     1118 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_send_txns_bls_consensus.py
--rw-rw-r--   0 sovrin    (1003)     1004     4508 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_commit_signature_validation_integration.py
--rw-rw-r--   0 sovrin    (1003)     1004     2435 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_sign_validation_for_key_proof_exist_ordering.py
--rw-rw-r--   0 sovrin    (1003)     1004      588 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_send_txns_full_bls.py
--rw-rw-r--   0 sovrin    (1003)     1004      838 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_send_txns_bls_less_than_consensus.py
--rw-rw-r--   0 sovrin    (1003)     1004     1780 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_get_state_proof.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     3648 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_add_incorrect_bls_key.py
--rw-rw-r--   0 sovrin    (1003)     1004     2794 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_update_bls_key.py
--rw-rw-r--   0 sovrin    (1003)     1004     4274 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_crypto_factory.py
--rw-rw-r--   0 sovrin    (1003)     1004     2101 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_sign_validation_for_key_proof_exist.py
--rw-rw-r--   0 sovrin    (1003)     1004     2281 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_store.py
--rw-rw-r--   0 sovrin    (1003)     1004    22917 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_bft_replica.py
--rw-rw-r--   0 sovrin    (1003)     1004      576 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_send_txns_no_bls.py
--rw-rw-r--   0 sovrin    (1003)     1004     1875 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_bft_factory.py
--rw-rw-r--   0 sovrin    (1003)     1004     1953 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_key_manager_file.py
--rw-rw-r--   0 sovrin    (1003)     1004    11919 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     3831 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_add_bls_key.py
--rw-rw-r--   0 sovrin    (1003)     1004     7251 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_state_proof.py
--rw-rw-r--   0 sovrin    (1003)     1004     4084 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_update_incorrect_bls_key.py
--rw-rw-r--   0 sovrin    (1003)     1004     1120 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/bls/test_multi_signature_verifier.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/
--rw-rw-r--   0 sovrin    (1003)     1004     3675 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/test_basic_batching.py
--rw-rw-r--   0 sovrin    (1003)     1004      351 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004      268 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/test_client_requests.py
--rw-rw-r--   0 sovrin    (1003)     1004     2962 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/test_batch_rejection.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2158 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_freeing_forwarded_not_preprepared_request.py
--rw-rw-r--   0 sovrin    (1003)     1004     2245 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_freeing_forwarded_preprepared_request.py
--rw-rw-r--   0 sovrin    (1003)     1004     2049 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_catchup_during_3pc.py
--rw-rw-r--   0 sovrin    (1003)     1004     3352 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_state_reverted_before_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004      923 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_3pc_paused_during_catch_up.py
--rw-rw-r--   0 sovrin    (1003)     1004     3574 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_clearing_requests_after_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     2776 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     2157 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/test_batching_scenarios.py
--rw-rw-r--   0 sovrin    (1003)     1004       88 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/exceptions.py
--rw-rw-r--   0 sovrin    (1003)     1004      572 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/msgs.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/
--rw-rw-r--   0 sovrin    (1003)     1004     1310 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_unaligned_prepare_certificates_on_half_nodes.py
--rw-rw-r--   0 sovrin    (1003)     1004     1355 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_delay_on_one_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     3407 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_advancing_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     1287 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_unaligned_prepare_certificates_on_one_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     3493 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_delayed_instance_changes.py
--rw-rw-r--   0 sovrin    (1003)     1004     3890 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_during_unstash.py
--rw-rw-r--   0 sovrin    (1003)     1004     1576 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_two_view_changes_with_delay_on_one_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     1894 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_different_prepare_certificate.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1301 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_delayed_commits.py
--rw-rw-r--   0 sovrin    (1003)     1004     1768 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_two_view_changes_with_delayed_commits.py
--rw-rw-r--   0 sovrin    (1003)     1004    11344 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     1410 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_propagate_primary_on_one_delayed_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     1635 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_two_view_changes_with_propagate_primary_on_one_delayed_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     6813 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/stasher.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/sdk/
--rw-rw-r--   0 sovrin    (1003)     1004     3096 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/sdk/test_sdk_bindings.py
--rw-rw-r--   0 sovrin    (1003)     1004      555 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/sdk/test_sdk_many_stewards.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/sdk/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/
--rw-rw-r--   0 sovrin    (1003)     1004     4258 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     5234 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_stashing_checkpoints_after_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     3856 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits.py
--rw-rw-r--   0 sovrin    (1003)     1004     1335 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits_received_before_catchup_pool.py
--rw-rw-r--   0 sovrin    (1003)     1004     6430 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup_only_checkpoints.py
--rw-rw-r--   0 sovrin    (1003)     1004     3639 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_slow_catchup_while_ordering.py
--rw-rw-r--   0 sovrin    (1003)     1004     1337 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits_received_before_catchup_audit.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     5079 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_limited_stashing_3pc_while_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     6306 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup_checkpoints.py
--rw-rw-r--   0 sovrin    (1003)     1004     3702 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     4028 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_event_bus.py
--rw-rw-r--   0 sovrin    (1003)     1004     1651 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/buy_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004      715 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_connections_with_converted_key.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/watermarks/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/watermarks/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2325 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/watermarks/test_watermarks_after_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     9766 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/delayers.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/common/
--rw-rw-r--   0 sovrin    (1003)     1004      450 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_transactions.py
--rw-rw-r--   0 sovrin    (1003)     1004     6443 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_config_util.py
--rw-rw-r--   0 sovrin    (1003)     1004     9145 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_digest_validation.py
--rw-rw-r--   0 sovrin    (1003)     1004     1693 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_random_string.py
--rw-rw-r--   0 sovrin    (1003)     1004      871 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_pool_file_raises_descriptive_error.py
--rw-rw-r--   0 sovrin    (1003)     1004     2679 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_parse_ledger.py
--rw-rw-r--   0 sovrin    (1003)     1004     1448 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_splitting_large_messages.py
--rw-rw-r--   0 sovrin    (1003)     1004     1556 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_throttler.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1067 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_verifier.py
--rw-rw-r--   0 sovrin    (1003)     1004     1698 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_replicas_suspicious.py
--rw-rw-r--   0 sovrin    (1003)     1004     2935 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_database_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     1426 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_signers.py
--rw-rw-r--   0 sovrin    (1003)     1004      394 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_roles.py
--rw-rw-r--   0 sovrin    (1003)     1004     2604 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/common/test_prepare_batch.py
--rw-rw-r--   0 sovrin    (1003)     1004      800 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_node_reg.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/
--rw-rw-r--   0 sovrin    (1003)     1004     5342 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_to_inconsistent_state.py
--rw-rw-r--   0 sovrin    (1003)     1004      771 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_4_all_wp.py
--rw-rw-r--   0 sovrin    (1003)     1004      764 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_6_wp.py
--rw-rw-r--   0 sovrin    (1003)     1004      764 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_4_np.py
--rw-rw-r--   0 sovrin    (1003)     1004      771 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_6_all_wp.py
--rw-rw-r--   0 sovrin    (1003)     1004      765 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_6_np.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      772 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_node_4_all.py
--rw-rw-r--   0 sovrin    (1003)     1004      742 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_7.py
--rw-rw-r--   0 sovrin    (1003)     1004      749 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_7_all.py
--rw-rw-r--   0 sovrin    (1003)     1004     3483 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_network_inconsistency_watcher.py
--rw-rw-r--   0 sovrin    (1003)     1004     5844 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_node_with_view_changes.py
--rw-rw-r--   0 sovrin    (1003)     1004      765 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes.py
--rw-rw-r--   0 sovrin    (1003)     1004     2134 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004      772 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_6_all_np.py
--rw-rw-r--   0 sovrin    (1003)     1004     3098 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_to_same_view_with_killed_primary.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/
--rw-rw-r--   0 sovrin    (1003)     1004     4985 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_nym_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     4468 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_txn_author_agreement_aml_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     2373 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_get_txn_author_agreement_aml_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     3416 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_get_value_from_state.py
--rw-rw-r--   0 sovrin    (1003)     1004     1965 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_get_txn_author_agreement_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     5209 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_txn_author_agreement_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     7441 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_node_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004      489 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_database_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     7310 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_request_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004      765 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     3923 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_node_req_handler_static_validation.py
--rw-rw-r--   0 sovrin    (1003)     1004     3488 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_stasher.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/
--rw-rw-r--   0 sovrin    (1003)     1004     7290 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     1428 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_consensus_dp_batches.py
--rw-rw-r--   0 sovrin    (1003)     1004     1776 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_catchup_after_replica_removing.py
--rw-rw-r--   0 sovrin    (1003)     1004     2859 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_buffers_cleaning.py
--rw-rw-r--   0 sovrin    (1003)     1004     1876 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_primary_marked_suspicious_for_sending_prepare.py
--rw-rw-r--   0 sovrin    (1003)     1004     2647 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_api.py
--rw-rw-r--   0 sovrin    (1003)     1004     4606 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_ordered_tracker.py
--rw-rw-r--   0 sovrin    (1003)     1004     2420 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_create_3pc_batch.py
--rw-rw-r--   0 sovrin    (1003)     1004     4302 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_3pc_messages_validation.py
--rw-rw-r--   0 sovrin    (1003)     1004     3539 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_get_last_timestamp_from_state.py
--rw-rw-r--   0 sovrin    (1003)     1004      697 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_replicas_primary_names.py
--rw-rw-r--   0 sovrin    (1003)     1004     6675 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_update_watermarks_api.py
--rw-rw-r--   0 sovrin    (1003)     1004     3920 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_catchup_after_replica_addition.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/stashing/
--rw-rw-r--   0 sovrin    (1003)     1004     3900 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/stashing/test_stash_future_view.py
--rw-rw-r--   0 sovrin    (1003)     1004     4293 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/stashing/test_stash_out_of_watermarks.py
--rw-rw-r--   0 sovrin    (1003)     1004     1483 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/stashing/test_replica_unstashing.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/stashing/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     5643 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/stashing/test_unstash_after_catchup_in_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     4380 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/stashing/test_replica_stasher.py
--rw-rw-r--   0 sovrin    (1003)     1004     2638 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_max_3pc_batches_in_flight.py
--rw-rw-r--   0 sovrin    (1003)     1004     5534 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_replica_reject_same_pre_prepare.py
--rw-rw-r--   0 sovrin    (1003)     1004     2632 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_replica_clear_collections_after_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     2372 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_bitmask_apply.py
--rw-rw-r--   0 sovrin    (1003)     1004    16220 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_replica_3pc_validation.py
--rw-rw-r--   0 sovrin    (1003)     1004     2057 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_monitor_reset_after_replica_addition.py
--rw-rw-r--   0 sovrin    (1003)     1004     1086 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_revert_from_malicious.py
--rw-rw-r--   0 sovrin    (1003)     1004      770 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     3202 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_replica_received_preprepare_with_unknown_request.py
--rw-rw-r--   0 sovrin    (1003)     1004    18367 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica/test_instance_faulty_processor.py
--rw-rw-r--   0 sovrin    (1003)     1004     3913 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_log_rotation.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/
--rw-rw-r--   0 sovrin    (1003)     1004     2630 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_restarted_node_not_complete_vc_before_others.py
--rw-rw-r--   0 sovrin    (1003)     1004     2629 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_changes_if_master_primary_disconnected.py
--rw-rw-r--   0 sovrin    (1003)     1004     1561 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_by_current_state.py
--rw-rw-r--   0 sovrin    (1003)     1004     2796 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     1475 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_pp_seq_no_starts_from_1.py
--rw-rw-r--   0 sovrin    (1003)     1004     1253 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_if_primary_disconnected.py
--rw-rw-r--   0 sovrin    (1003)     1004     2399 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     2080 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_vc_started_in_different_time.py
--rw-rw-r--   0 sovrin    (1003)     1004     1612 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_n_minus_f_quorum.py
--rw-rw-r--   0 sovrin    (1003)     1004      318 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_not_gamable.py
--rw-rw-r--   0 sovrin    (1003)     1004     2627 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_pre_vc_strategy_3PC_msgs.py
--rw-rw-r--   0 sovrin    (1003)     1004     2212 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_resend_inst_ch_in_progress_v_ch.py
--rw-rw-r--   0 sovrin    (1003)     1004     2231 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_discard_inst_chng_msg_from_past_view.py
--rw-rw-r--   0 sovrin    (1003)     1004     2511 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_no_instance_change_before_node_is_ready.py
--rw-rw-r--   0 sovrin    (1003)     1004     4671 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_that_domain_ledger_the_same_after_restart_for_all_nodes.py
--rw-rw-r--   0 sovrin    (1003)     1004     1666 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_add_vc_start_msg_during_start_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     1634 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_timeout_reset.py
--rw-rw-r--   0 sovrin    (1003)     1004      940 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_last_completed_view_no_set_after_vc_complete.py
--rw-rw-r--   0 sovrin    (1003)     1004      536 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_api.py
--rw-rw-r--   0 sovrin    (1003)     1004     6882 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_timeout.py
--rw-rw-r--   0 sovrin    (1003)     1004     6009 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_by_catchup_and_order.py
--rw-rw-r--   0 sovrin    (1003)     1004     2094 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_client_req_during_view_change_integration.py
--rw-rw-r--   0 sovrin    (1003)     1004     1070 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_can_finish_despite_perpetual_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     8536 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_no_propagate_request_on_different_last_ordered_before_vc.py
--rw-rw-r--   0 sovrin    (1003)     1004     4587 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_after_back_to_quorum_with_disconnected_primary_and_slow_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     4744 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_on_master_degraded.py
--rw-rw-r--   0 sovrin    (1003)     1004     3338 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_not_changed_when_primary_disconnected_from_less_than_quorum.py
--rw-rw-r--   0 sovrin    (1003)     1004     2171 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_no_view_change_while_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     2096 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_missing_pp_before_starting_vc.py
--rw-rw-r--   0 sovrin    (1003)     1004     2709 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_done_delayed.py
--rw-rw-r--   0 sovrin    (1003)     1004     1602 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_start_without_primary.py
--rw-rw-r--   0 sovrin    (1003)     1004     1108 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_instance_change_from_unknown.py
--rw-rw-r--   0 sovrin    (1003)     1004     2937 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_max_catchup_rounds.py
--rw-rw-r--   0 sovrin    (1003)     1004     3540 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_catchup_to_next_view_during_view_change_by_primary.py
--rw-rw-r--   0 sovrin    (1003)     1004     3044 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_re_order_pre_prepares.py
--rw-rw-r--   0 sovrin    (1003)     1004     2681 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_complete_with_delayed_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     4792 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_prepare_in_queue_before_vc.py
--rw-rw-r--   0 sovrin    (1003)     1004     2019 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_with_instance_change_lost_due_to_restarts.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2754 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_not_changed_when_short_disconnection.py
--rw-rw-r--   0 sovrin    (1003)     1004     4065 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_instance_change_if_needed.py
--rw-rw-r--   0 sovrin    (1003)     1004     1298 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_not_changed_if_backup_primary_disconnected.py
--rw-rw-r--   0 sovrin    (1003)     1004     2469 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_last_ordered_reset_for_new_view.py
--rw-rw-r--   0 sovrin    (1003)     1004      860 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_msgHasAcceptableViewNo.py
--rw-rw-r--   0 sovrin    (1003)     1004     3923 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_after_back_to_quorum_with_disconnected_primary.py
--rw-rw-r--   0 sovrin    (1003)     1004     1279 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_start_view_change_ts_set.py
--rw-rw-r--   0 sovrin    (1003)     1004      910 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_reset_monitor_after_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     1165 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_instance_change_msg_checking.py
--rw-rw-r--   0 sovrin    (1003)     1004     2973 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_checkPerformance.py
--rw-rw-r--   0 sovrin    (1003)     1004     6505 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_catchup_to_next_view_during_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     4125 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_new_node_joins_after_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     2965 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_queueing_req_from_future_view.py
--rw-rw-r--   0 sovrin    (1003)     1004      987 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_select_primary_after_removed_backup.py
--rw-rw-r--   0 sovrin    (1003)     1004     1716 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_no_future_view_change_while_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     3992 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_diconnected_node_reconnects_after_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     1178 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_not_changed.py
--rw-rw-r--   0 sovrin    (1003)     1004      885 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_disable_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004    16185 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     4625 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_reverted_unordered.py
--rw-rw-r--   0 sovrin    (1003)     1004     2801 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_no_future_view_change_while_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     2807 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_3pc_msgs_during_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     1482 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_old_instance_change_discarding.py
--rw-rw-r--   0 sovrin    (1003)     1004     2497 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_client_req_during_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     4697 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_6th_node_join_after_view_change_by_primary_restart.py
--rw-rw-r--   0 sovrin    (1003)     1004     2439 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_future_vc_done.py
--rw-rw-r--   0 sovrin    (1003)     1004     2298 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_resend_instance_change_messages.py
--rw-rw-r--   0 sovrin    (1003)     1004     4933 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_node_detecting_lag_from_view_change_messages.py
--rw-rw-r--   0 sovrin    (1003)     1004     2363 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_wont_happen_if_ic_is_discarded.py
--rw-rw-r--   0 sovrin    (1003)     1004     1633 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_happens_post_timeout.py
--rw-rw-r--   0 sovrin    (1003)     1004     1387 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_master_primary_different_from_previous.py
--rw-rw-r--   0 sovrin    (1003)     1004     3412 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_min_cathup_timeout.py
--rw-rw-r--   0 sovrin    (1003)     1004     9453 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_vc_start_msg_strategy.py
--rw-rw-r--   0 sovrin    (1003)     1004     3060 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_without_any_reqs.py
--rw-rw-r--   0 sovrin    (1003)     1004     2094 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change/test_vc_finished_when_less_than_quorum_started.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/
--rw-rw-r--   0 sovrin    (1003)     1004    12317 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/test_last_sent_pp_store_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004      549 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     3869 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/test_backup_primary_restores_pp_seq_no_if_view_is_same.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     4093 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/test_node_erases_last_sent_pp_key_on_pool_restart.py
--rw-rw-r--   0 sovrin    (1003)     1004     2141 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/test_node_erases_last_sent_pp_key_on_view_change.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_helpers/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_helpers/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      154 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_helpers/node_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     2694 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_batch_handler.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/
--rw-rw-r--   0 sovrin    (1003)     1004     3317 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_large_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     2937 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_build_ledger_status.py
--rw-rw-r--   0 sovrin    (1003)     1004     6024 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_revert_during_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     2845 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004      847 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_ledger_manager.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/catchup_req/
--rw-rw-r--   0 sovrin    (1003)     1004      211 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/catchup_req/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/catchup_req/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     3573 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/catchup_req/test_catchup_with_one_slow_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     3326 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/catchup_req/test_catchup_with_disconnected_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     3195 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_after_checkpoints.py
--rw-rw-r--   0 sovrin    (1003)     1004     4302 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_ledger_info.py
--rw-rw-r--   0 sovrin    (1003)     1004     2453 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_new_node_catchup2.py
--rw-rw-r--   0 sovrin    (1003)     1004     2434 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_uses_only_nodes_with_cons_proofs.py
--rw-rw-r--   0 sovrin    (1003)     1004     2567 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_after_restart_no_txns.py
--rw-rw-r--   0 sovrin    (1003)     1004     2992 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_req_distribution.py
--rw-rw-r--   0 sovrin    (1003)     1004     3117 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_from_unequal_nodes_without_reasking.py
--rw-rw-r--   0 sovrin    (1003)     1004     2621 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_discard_view_no.py
--rw-rw-r--   0 sovrin    (1003)     1004     2982 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_request_missing_transactions.py
--rw-rw-r--   0 sovrin    (1003)     1004     3607 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_incorrect_catchup_request.py
--rw-rw-r--   0 sovrin    (1003)     1004     3902 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_reject_invalid_txn_during_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     5120 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_ts_store_after_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2870 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_with_all_nodes_sending_cons_proofs_dead.py
--rw-rw-r--   0 sovrin    (1003)     1004     3494 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_set_H_to_maxsize_and_not_stash_on_backup.py
--rw-rw-r--   0 sovrin    (1003)     1004     3518 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_request_consistency_proof.py
--rw-rw-r--   0 sovrin    (1003)     1004     9461 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_config_ledger.py
--rw-rw-r--   0 sovrin    (1003)     1004     2531 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_not_set_H_as_maxsize_for_backup_if_is_primary.py
--rw-rw-r--   0 sovrin    (1003)     1004     9531 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_process_catchup_replies.py
--rw-rw-r--   0 sovrin    (1003)     1004     2968 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_inlcuding_3PC.py
--rw-rw-r--   0 sovrin    (1003)     1004     4592 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_get_last_txn_3PC_key.py
--rw-rw-r--   0 sovrin    (1003)     1004     2922 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_f_plus_one.py
--rw-rw-r--   0 sovrin    (1003)     1004     2225 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_after_disconnect.py
--rw-rw-r--   0 sovrin    (1003)     1004     2916 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_remove_request_keys_post_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     5290 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_not_triggered_if_another_in_progress.py
--rw-rw-r--   0 sovrin    (1003)     1004     2766 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_with_only_one_available_node.py
--rw-rw-r--   0 sovrin    (1003)     1004       61 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_and_view_change_after_start.py
--rw-rw-r--   0 sovrin    (1003)     1004     2487 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_demoted.py
--rw-rw-r--   0 sovrin    (1003)     1004     1271 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_start.py
--rw-rw-r--   0 sovrin    (1003)     1004     3894 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_with_ledger_statuses_in_old_format.py
--rw-rw-r--   0 sovrin    (1003)     1004     9395 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     3398 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_causes_no_desync.py
--rw-rw-r--   0 sovrin    (1003)     1004     3891 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_delayed_nodes.py
--rw-rw-r--   0 sovrin    (1003)     1004     4396 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_when_3_not_primary_node_restarted.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_req_id_key_error.py
--rw-rw-r--   0 sovrin    (1003)     1004     2562 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_reply.py
--rw-rw-r--   0 sovrin    (1003)     1004     2778 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_with_old_txn_metadata_digest_format.py
--rw-rw-r--   0 sovrin    (1003)     1004     1037 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_post_genesis_txn_from_catchup_added_to_ledger.py
--rw-rw-r--   0 sovrin    (1003)     1004     1966 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_new_node_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     3783 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_from_unequal_nodes_without_waiting.py
--rw-rw-r--   0 sovrin    (1003)     1004     4035 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_reasking.py
--rw-rw-r--   0 sovrin    (1003)     1004     7899 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_with_connection_problem.py
--rw-rw-r--   0 sovrin    (1003)     1004     5752 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_after_restart_after_txns.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/logging/
--rw-rw-r--   0 sovrin    (1003)     1004     1722 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/logging/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/logging/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     5518 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/logging/test_logging_txn_state.py
--rw-rw-r--   0 sovrin    (1003)     1004      273 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004    11804 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_performance.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/observer/
--rw-rw-r--   0 sovrin    (1003)     1004     2208 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/observer/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     1154 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/observer/test_observable_each_batch.py
--rw-rw-r--   0 sovrin    (1003)     1004     1547 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/observer/test_observable_each_batch_node_integration.py
--rw-rw-r--   0 sovrin    (1003)     1004     1087 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/observer/test_observable_node_integration.py
--rw-rw-r--   0 sovrin    (1003)     1004     3488 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/observer/test_observer_node_each_batch.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/observer/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1846 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/observer/test_observable.py
--rw-rw-r--   0 sovrin    (1003)     1004     9841 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/observer/test_observer_policy_each_batch.py
--rw-rw-r--   0 sovrin    (1003)     1004     1736 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/observer/test_observer_node_integration.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/
--rw-rw-r--   0 sovrin    (1003)     1004     1245 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     1624 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_demote_backup_primary.py
--rw-rw-r--   0 sovrin    (1003)     1004     2705 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_first_audit_catchup_during_ordering.py
--rw-rw-r--   0 sovrin    (1003)     1004     4976 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     2537 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_future_primaries_addition.py
--rw-rw-r--   0 sovrin    (1003)     1004     4278 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_future_primaries_unit.py
--rw-rw-r--   0 sovrin    (1003)     1004    18331 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     4376 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_freshness.py
--rw-rw-r--   0 sovrin    (1003)     1004     2303 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_demote_backup_primary_without_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1315 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_calc_catchup_till.py
--rw-rw-r--   0 sovrin    (1003)     1004     7203 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_handler_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     3483 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_primaries_in_ordered.py
--rw-rw-r--   0 sovrin    (1003)     1004     8672 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_handler_multiple_commits.py
--rw-rw-r--   0 sovrin    (1003)     1004     5880 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_multiple_ledgers_in_one_batch.py
--rw-rw-r--   0 sovrin    (1003)     1004     3990 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     7361 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_ordering.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/metrics/
--rw-rw-r--   0 sovrin    (1003)     1004      545 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/metrics/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/metrics/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     3169 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/metrics/test_value_accumulator.py
--rw-rw-r--   0 sovrin    (1003)     1004     4266 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/metrics/test_metrics_config.py
--rw-rw-r--   0 sovrin    (1003)     1004     2011 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/metrics/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     8270 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/metrics/test_metrics_collector.py
--rw-rw-r--   0 sovrin    (1003)     1004     6829 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/metrics/test_metrics_stats.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/
--rw-rw-r--   0 sovrin    (1003)     1004     3478 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_clientstack_restart_trigger.py
--rw-rw-r--   0 sovrin    (1003)     1004     2508 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_send_too_many_reqs.py
--rw-rw-r--   0 sovrin    (1003)     1004     2794 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_send_client_msgs_with_delay_reqs.py
--rw-rw-r--   0 sovrin    (1003)     1004     2516 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_zstack_reconnection.py
--rw-rw-r--   0 sovrin    (1003)     1004     1261 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_4_of_4_nodes.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1264 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_2_of_4_nodes.py
--rw-rw-r--   0 sovrin    (1003)     1004     1265 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_3_of_4_nodes.py
--rw-rw-r--   0 sovrin    (1003)     1004      204 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_restart_client_stack.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/simulation/
--rw-rw-r--   0 sovrin    (1003)     1004     5648 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/simulation/test_sim_network.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/simulation/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2085 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/simulation/test_sim_random.py
--rw-rw-r--   0 sovrin    (1003)     1004     1845 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/simulation/sim_network.py
--rw-rw-r--   0 sovrin    (1003)     1004     1551 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/simulation/sim_random.py
--rw-rw-r--   0 sovrin    (1003)     1004     1669 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/get_buy_handler.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/
--rw-rw-r--   0 sovrin    (1003)     1004     4824 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/test_consensus_dp_batches.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/view_change/
--rw-rw-r--   0 sovrin    (1003)     1004    17474 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/view_change/test_view_change_service.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/view_change/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     3619 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/view_change/test_sim_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     3591 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/view_change/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004    22924 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/view_change/test_new_view_builder.py
--rw-rw-r--   0 sovrin    (1003)     1004     9451 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/view_change/test_view_change_msg_creation.py
--rw-rw-r--   0 sovrin    (1003)     1004     2794 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/view_change/test_primary_selector.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/
--rw-rw-r--   0 sovrin    (1003)     1004     4550 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     4697 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_pp_obsolescence.py
--rw-rw-r--   0 sovrin    (1003)     1004      857 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_buffers_cleaning.py
--rw-rw-r--   0 sovrin    (1003)     1004     2669 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_ordering_process_commit.py
--rw-rw-r--   0 sovrin    (1003)     1004     3337 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_can_send_3pc.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     5988 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_ordering_process_prepare.py
--rw-rw-r--   0 sovrin    (1003)     1004     2014 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_orderer_api.py
--rw-rw-r--   0 sovrin    (1003)     1004      463 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_can_order.py
--rw-rw-r--   0 sovrin    (1003)     1004     7517 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_ordering_process_preprepare.py
--rw-rw-r--   0 sovrin    (1003)     1004      822 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     6697 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_ordering_service_on_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     9083 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_ordering_msg_validator.py
--rw-rw-r--   0 sovrin    (1003)     1004    11853 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/test_three_pc_validator.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/checkpoint_service/
--rw-rw-r--   0 sovrin    (1003)     1004      705 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/checkpoint_service/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     1213 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/checkpoint_service/test_update_watermarks_api.py
--rw-rw-r--   0 sovrin    (1003)     1004     6188 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/checkpoint_service/test_checkpoint_service_on_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/checkpoint_service/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     4510 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/checkpoint_service/test_checkpoint_validation.py
--rw-rw-r--   0 sovrin    (1003)     1004     9579 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/checkpoint_service/test_checkpoint_service_unit.py
--rw-rw-r--   0 sovrin    (1003)     1004      794 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/test_consensus_data_provider.py
--rw-rw-r--   0 sovrin    (1003)     1004     6879 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/consensus/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     8107 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_stashing_router.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/wallet/
--rw-rw-r--   0 sovrin    (1003)     1004     7818 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/wallet/test_wallet_storage_helper.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/wallet/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2751 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/wallet/test_wallet.py
--rw-rw-r--   0 sovrin    (1003)     1004     1159 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_bootstrapping.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/package/
--rw-rw-r--   0 sovrin    (1003)     1004     2222 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/package/test_metadata.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/package/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004       36 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/__main__.py
--rw-rw-r--   0 sovrin    (1003)     1004      817 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_lazy_field.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/validator_info/
--rw-rw-r--   0 sovrin    (1003)     1004     1451 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/validator_info/test_start_vc_ts_in_node_info.py
--rw-rw-r--   0 sovrin    (1003)     1004     1582 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/validator_info/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/validator_info/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     4761 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/validator_info/test_validator_info_vc.py
--rw-rw-r--   0 sovrin    (1003)     1004     1256 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/validator_info/test_validator_info_dump.py
--rw-rw-r--   0 sovrin    (1003)     1004    11172 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/validator_info/test_validator_info.py
--rw-rw-r--   0 sovrin    (1003)     1004     2024 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/validator_info/test_upgrade_log.py
--rw-rw-r--   0 sovrin    (1003)     1004     6694 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_node_connection.py
--rw-rw-r--   0 sovrin    (1003)     1004     2911 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_repeating_timer.py
--rw-rw-r--   0 sovrin    (1003)     1004       68 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/constants.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/
--rw-rw-r--   0 sovrin    (1003)     1004      528 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_not_removing_by_latency.py
--rw-rw-r--   0 sovrin    (1003)     1004     4613 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_after_node_started.py
--rw-rw-r--   0 sovrin    (1003)     1004     1099 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_acc_quorum.py
--rw-rw-r--   0 sovrin    (1003)     1004     1098 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_acc_local.py
--rw-rw-r--   0 sovrin    (1003)     1004     1098 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_quorum.py
--rw-rw-r--   0 sovrin    (1003)     1004     3813 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_after_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1097 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_local.py
--rw-rw-r--   0 sovrin    (1003)     1004     2765 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_with_primary_disconnected.py
--rw-rw-r--   0 sovrin    (1003)     1004     6958 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing.py
--rw-rw-r--   0 sovrin    (1003)     1004     4289 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/replica_removing/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     4581 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_queue_timer.py
--rw-rw-r--   0 sovrin    (1003)     1004     7866 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_testable.py
--rw-rw-r--   0 sovrin    (1003)     1004     2060 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/spy_helpers.py
--rw-rw-r--   0 sovrin    (1003)     1004     1941 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_dirty_read.py
--rw-rw-r--   0 sovrin    (1003)     1004      766 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/random_buy_handler.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/
--rw-rw-r--   0 sovrin    (1003)     1004     1524 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_new_primary.py
--rw-rw-r--   0 sovrin    (1003)     1004      910 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_all_nodes.py
--rw-rw-r--   0 sovrin    (1003)     1004     1434 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_old_primary.py
--rw-rw-r--   0 sovrin    (1003)     1004     4835 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_gc_all_nodes_random_delay.py
--rw-rw-r--   0 sovrin    (1003)     1004     1272 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_non_primary.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2030 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_complex.py
--rw-rw-r--   0 sovrin    (1003)     1004     1280 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_old_and_new_primary.py
--rw-rw-r--   0 sovrin    (1003)     1004     1133 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_all_nodes_random_delay.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/
--rw-rw-r--   0 sovrin    (1003)     1004     1041 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     3657 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_watermarks_on_delayed_backup.py
--rw-rw-r--   0 sovrin    (1003)     1004     2145 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_message_outside_watermark1.py
--rw-rw-r--   0 sovrin    (1003)     1004     4794 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_checkpoints_removal_after_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     1944 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_discard_old_checkpoint_messages.py
--rw-rw-r--   0 sovrin    (1003)     1004     4065 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_checkpoints_removal_after_catchup_during_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     5087 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_complete_short_checkpoint_not_included_in_lag_for_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     2542 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_basic_checkpointing.py
--rw-rw-r--   0 sovrin    (1003)     1004     2812 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_lagged_checkpoint_completion.py
--rw-rw-r--   0 sovrin    (1003)     1004     5038 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_checkpoints_removal_in_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     2824 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_checkpoint_stabilization_after_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     4536 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_message_outside_watermark.py
--rw-rw-r--   0 sovrin    (1003)     1004     2874 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_stashed_checkpoint_processing.py
--rw-rw-r--   0 sovrin    (1003)     1004    10402 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_backup_replica_resumes_ordering_on_lag_in_checkpoints.py
--rw-rw-r--   0 sovrin    (1003)     1004     2191 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_ordering_after_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     3280 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_checkpoint_stable_while_unstashing.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     4028 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_view_change_after_checkpoint.py
--rw-rw-r--   0 sovrin    (1003)     1004     2093 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_checkpoint_bounds_after_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     3257 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     4710 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_incomplete_short_checkpoint_included_in_lag_for_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     6162 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_stashed_messages_processed_on_backup_replica_ordering_resumption.py
--rw-rw-r--   0 sovrin    (1003)     1004     1231 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_stable_checkpoint1.py
--rw-rw-r--   0 sovrin    (1003)     1004     2771 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_stable_checkpoint.py
--rw-rw-r--   0 sovrin    (1003)     1004     3084 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_lag_size_for_catchup.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/
--rw-rw-r--   0 sovrin    (1003)     1004     6405 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_moving_average.py
--rw-rw-r--   0 sovrin    (1003)     1004     2555 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_warn_unordered_log_msg.py
--rw-rw-r--   0 sovrin    (1003)     1004     2147 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     2518 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_post_monitoring_stats.py
--rw-rw-r--   0 sovrin    (1003)     1004     3975 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_safe_start_ema_throughput_measurement.py
--rw-rw-r--   0 sovrin    (1003)     1004     4977 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_stats_publisher.py
--rw-rw-r--   0 sovrin    (1003)     1004     2674 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_backup_throughput_measurement.py
--rw-rw-r--   0 sovrin    (1003)     1004      201 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_monitor_attributes.py
--rw-rw-r--   0 sovrin    (1003)     1004      251 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_monitor_reconnection.py
--rw-rw-r--   0 sovrin    (1003)     1004     1582 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_latency_measurement_class.py
--rw-rw-r--   0 sovrin    (1003)     1004    12045 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_throughput_based_master_degradation_detection.py
--rw-rw-r--   0 sovrin    (1003)     1004     7793 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_request_time_tracker.py
--rw-rw-r--   0 sovrin    (1003)     1004     1692 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_system_stats.py
--rw-rw-r--   0 sovrin    (1003)     1004     1478 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_latency_median_avg.py
--rw-rw-r--   0 sovrin    (1003)     1004     1587 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_monitoring_params_with_zfn.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     4488 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_acc_monitor_strategy.py
--rw-rw-r--   0 sovrin    (1003)     1004     4166 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_instance_change_with_Delta.py
--rw-rw-r--   0 sovrin    (1003)     1004    17130 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_revival_spike_resistant_ema_throughput_measurement.py
--rw-rw-r--   0 sovrin    (1003)     1004     1435 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_no_check_if_no_new_requests.py
--rw-rw-r--   0 sovrin    (1003)     1004     1670 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_invalid_reqs_in_monitor.py
--rw-rw-r--   0 sovrin    (1003)     1004     1909 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_throughput_median_avg.py
--rw-rw-r--   0 sovrin    (1003)     1004     3262 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_instance_change_with_req_Lambda.py
--rw-rw-r--   0 sovrin    (1003)     1004     2902 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_EMALatencyMeasurementForAllClient.py
--rw-rw-r--   0 sovrin    (1003)     1004      907 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_throughput.py
--rw-rw-r--   0 sovrin    (1003)     1004     1202 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_avg_latency.py
--rw-rw-r--   0 sovrin    (1003)     1004    40407 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_node.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/nodestack/
--rw-rw-r--   0 sovrin    (1003)     1004     4460 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/nodestack/test_resend_stashed_msgs.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/nodestack/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      707 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/run_continuously.py
--rw-rw-r--   0 sovrin    (1003)     1004     4496 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_request.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/
--rw-rw-r--   0 sovrin    (1003)     1004     2705 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_adding_stewards.py
--rw-rw-r--   0 sovrin    (1003)     1004     1984 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004      912 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_start_many_nodes.py
--rw-rw-r--   0 sovrin    (1003)     1004     5600 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_get_txn_request.py
--rw-rw-r--   0 sovrin    (1003)     1004     1858 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_on_pool_membership_changes.py
--rw-rw-r--   0 sovrin    (1003)     1004     1931 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_add_inactive_node_then_activate.py
--rw-rw-r--   0 sovrin    (1003)     1004     4790 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_add_node_with_invalid_data.py
--rw-rw-r--   0 sovrin    (1003)     1004     3256 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_change_ha_persists_post_nodes_restart.py
--rw-rw-r--   0 sovrin    (1003)     1004     4612 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_nodes_data_changed.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     6676 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_nodes_with_pool_txns.py
--rw-rw-r--   0 sovrin    (1003)     1004     1353 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_add_node_with_not_unique_alias.py
--rw-rw-r--   0 sovrin    (1003)     1004     5422 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_add_stewards_and_client.py
--rw-rw-r--   0 sovrin    (1003)     1004     2938 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_txn_pool_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     1846 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_demote_nonexisted.py
--rw-rw-r--   0 sovrin    (1003)     1004    22272 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     3047 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_suspend_node.py
--rw-rw-r--   0 sovrin    (1003)     1004     2053 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_add_node_with_invalid_key_proof.py
--rw-rw-r--   0 sovrin    (1003)     1004     4047 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_z_node_key_changed.py
--rw-rw-r--   0 sovrin    (1003)     1004     1784 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_client_with_pool_txns.py
--rw-rw-r--   0 sovrin    (1003)     1004     2595 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_nodes_ha_change_back.py
--rw-rw-r--   0 sovrin    (1003)     1004     2073 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_action_queue.py
--rw-rw-r--   0 sovrin    (1003)     1004     2326 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_config_req_handler.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/node/
--rw-rw-r--   0 sovrin    (1003)     1004     2030 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node/test_api.py
--rw-rw-r--   0 sovrin    (1003)     1004     2390 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node/test_quota_control.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/node/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     5929 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_node_genesis.py
--rw-rw-r--   0 sovrin    (1003)     1004      920 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/greek.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/forced_request/
--rw-rw-r--   0 sovrin    (1003)     1004     1269 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/forced_request/test_forced_request_validation.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/forced_request/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/persistence/
--rw-rw-r--   0 sovrin    (1003)     1004      279 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/persistence/test_client_req_rep_store_file.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/persistence/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     4277 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_stack.py
--rw-rw-r--   0 sovrin    (1003)     1004     2533 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_node_basic.py
--rw-rw-r--   0 sovrin    (1003)     1004    54182 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     2123 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_node_request.py
--rw-rw-r--   0 sovrin    (1003)     1004     5601 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/testable.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/
--rw-rw-r--   0 sovrin    (1003)     1004     1338 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_view_change_happens_if_primary_is_slow_to_update_freshness.py
--rw-rw-r--   0 sovrin    (1003)     1004     1191 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_during_pool_ordering.py
--rw-rw-r--   0 sovrin    (1003)     1004     1591 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_in_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     7465 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_replica_freshness.py
--rw-rw-r--   0 sovrin    (1003)     1004     2713 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     3925 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_batch_updates_last_ordered.py
--rw-rw-r--   0 sovrin    (1003)     1004     1074 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness.py
--rw-rw-r--   0 sovrin    (1003)     1004    10052 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_replica_freshness_checker.py
--rw-rw-r--   0 sovrin    (1003)     1004     2887 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_after_catchup.py
--rw-rw-r--   0 sovrin    (1003)     1004     2032 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_instance_changes_are_sent_continuosly.py
--rw-rw-r--   0 sovrin    (1003)     1004     6429 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     1132 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_during_domain_ordering.py
--rw-rw-r--   0 sovrin    (1003)     1004     1266 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/freshness/test_view_change_happens_if_ordering_is_halted.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/
--rw-rw-r--   0 sovrin    (1003)     1004     1170 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     2154 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_commit_txns.py
--rw-rw-r--   0 sovrin    (1003)     1004     2275 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_discard_txns.py
--rw-rw-r--   0 sovrin    (1003)     1004     1789 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_append_txns.py
--rw-rw-r--   0 sovrin    (1003)     1004     1661 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_get_by_seqno.py
--rw-rw-r--   0 sovrin    (1003)     1004      992 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_add_txns.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      928 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_append_txns_result.py
--rw-rw-r--   0 sovrin    (1003)     1004      699 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledgers.py
--rw-rw-r--   0 sovrin    (1003)     1004     1341 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_get_last_txn.py
--rw-rw-r--   0 sovrin    (1003)     1004      358 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_reset_uncommitted.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/propagate/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/propagate/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2511 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/propagate/test_propagate_recvd_after_request.py
--rw-rw-r--   0 sovrin    (1003)     1004     1380 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/propagate/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     1947 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/propagate/test_propagate_recvd_before_request.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/script/
--rw-rw-r--   0 sovrin    (1003)     1004     7179 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/script/test_bootstrap_test_node.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/script/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     1083 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/script/test_change_primary_node_ha.py
--rw-rw-r--   0 sovrin    (1003)     1004     1051 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/script/test_change_non_primary_node_ha.py
--rw-rw-r--   0 sovrin    (1003)     1004     2850 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/script/helper.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/
--rw-rw-r--   0 sovrin    (1003)     1004     1516 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     2725 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_propagate_primary_after_primary_restart_view_0.py
--rw-rw-r--   0 sovrin    (1003)     1004     3894 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_new_node_accepts_chosen_primary.py
--rw-rw-r--   0 sovrin    (1003)     1004     3896 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_promotion_leads_to_primary_inconsistency.py
--rw-rw-r--   0 sovrin    (1003)     1004     2010 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection_pool_txn.py
--rw-rw-r--   0 sovrin    (1003)     1004     4736 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_reconnect_primary_and_not_primary.py
--rw-rw-r--   0 sovrin    (1003)     1004     2851 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_propagate_primary_after_primary_restart_view_1.py
--rw-rw-r--   0 sovrin    (1003)     1004     4103 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection.py
--rw-rw-r--   0 sovrin    (1003)     1004     2878 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_catchup_multiple_rounds.py
--rw-rw-r--   0 sovrin    (1003)     1004     3255 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_add_node_with_f_changed.py
--rw-rw-r--   0 sovrin    (1003)     1004     4200 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_recover_more_than_f_failure.py
--rw-rw-r--   0 sovrin    (1003)     1004     2904 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_pool_restart.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004    11020 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_view_changer_primary_selection.py
--rw-rw-r--   0 sovrin    (1003)     1004     4591 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection_after_demoted_node_promotion.py
--rw-rw-r--   0 sovrin    (1003)     1004     3188 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_view_changes.py
--rw-rw-r--   0 sovrin    (1003)     1004     3339 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_add_node_to_pool_with_large_ppseqno.py
--rw-rw-r--   0 sovrin    (1003)     1004     4251 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_catchup_needed_check.py
--rw-rw-r--   0 sovrin    (1003)     1004     2669 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_selection_f_plus_one_quorum.py
--rw-rw-r--   0 sovrin    (1003)     1004     3457 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_recover_primary_no_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     2246 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     4243 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_catchup_after_view_change.py
--rw-rw-r--   0 sovrin    (1003)     1004     3284 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_recover_after_demoted.py
--rw-rw-r--   0 sovrin    (1003)     1004     2551 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_promotion.py
--rw-rw-r--   0 sovrin    (1003)     1004     1177 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_ledger_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     5253 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_util.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/
--rw-rw-r--   0 sovrin    (1003)     1004      374 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004    10961 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/test_notifier_plugin_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      225 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/helper.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/stats_consumer/
--rw-rw-r--   0 sovrin    (1003)     1004     3280 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/stats_consumer/plugin_stats_consumer.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/stats_consumer/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/
--rw-rw-r--   0 sovrin    (1003)     1004     2858 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     2982 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_request_digest.py
--rw-rw-r--   0 sovrin    (1003)     1004     5050 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_plugin_request_handling.py
--rw-rw-r--   0 sovrin    (1003)     1004      269 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/config.py
--rw-rw-r--   0 sovrin    (1003)     1004     1309 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_freshness_during_ordering.py
--rw-rw-r--   0 sovrin    (1003)     1004      667 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2734 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_catchup.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/
--rw-rw-r--   0 sovrin    (1003)     1004      962 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/auction_start_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     2087 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/abstract_auction_req_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     1163 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/get_bal_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004     1962 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/place_bid_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      761 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/auction_end_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004      869 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/storage.py
--rw-rw-r--   0 sovrin    (1003)     1004     1513 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_freshness.py
--rw-rw-r--   0 sovrin    (1003)     1004      616 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/transactions.py
--rw-rw-r--   0 sovrin    (1003)     1004      303 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/constants.py
--rw-rw-r--   0 sovrin    (1003)     1004     3063 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/main.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/batch_handlers/
--rw-rw-r--   0 sovrin    (1003)     1004      604 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/batch_handlers/auction_batch_handler.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/batch_handlers/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      760 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     2077 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_plugin_basic.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/transactions/
--rw-rw-r--   0 sovrin    (1003)     1004     5109 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/transactions/test_new_txn_format.py
--rw-rw-r--   0 sovrin    (1003)     1004     3510 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/transactions/test_txn_general_access_utils.py
--rw-rw-r--   0 sovrin    (1003)     1004     3342 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/transactions/test_txn_init_utils.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/transactions/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     4766 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/transactions/test_req_to_txn.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/test/util/
--rw-rw-r--   0 sovrin    (1003)     1004      267 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/util/test_common_util.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/util/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2338 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_state_regenerated_from_ledger.py
--rw-rw-r--   0 sovrin    (1003)     1004     8833 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/waits.py
--rw-rw-r--   0 sovrin    (1003)     1004     1888 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/testing_utils.py
--rw-rw-r--   0 sovrin    (1003)     1004     4161 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/test/test_crypto.py
--rw-rw-r--   0 sovrin    (1003)     1004       22 2019-08-29 11:08:44.000000 indy-plenum-1.9.2.dev879/plenum/__version__.json
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/plenum/persistence/
--rw-rw-r--   0 sovrin    (1003)     1004     2547 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/persistence/req_id_to_txn.py
--rw-rw-r--   0 sovrin    (1003)     1004      192 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/persistence/util.py
--rw-rw-r--   0 sovrin    (1003)     1004     3631 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/persistence/db_hash_store.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/persistence/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      776 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/persistence/storage.py
--rw-rw-r--   0 sovrin    (1003)     1004     1457 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/persistence/client_txn_log.py
--rw-rw-r--   0 sovrin    (1003)     1004     1919 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/persistence/client_req_rep_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     6831 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/plenum/persistence/client_req_rep_store_file.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/crypto/
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/crypto/bls/
--rw-rw-r--   0 sovrin    (1003)     1004     1585 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/bls/bls_crypto.py
--rw-rw-r--   0 sovrin    (1003)     1004     4308 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/bls/bls_bft_replica.py
--rw-rw-r--   0 sovrin    (1003)     1004     1007 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/bls/bls_key_manager.py
--rw-rw-r--   0 sovrin    (1003)     1004     3141 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/bls/bls_factory.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/bls/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      667 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/bls/bls_bft.py
--rw-rw-r--   0 sovrin    (1003)     1004      706 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/bls/bls_key_register.py
--rw-rw-r--   0 sovrin    (1003)     1004     4574 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/bls/bls_multi_signature.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/crypto/bls/indy_crypto/
--rw-rw-r--   0 sovrin    (1003)     1004     5541 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/bls/indy_crypto/bls_crypto_indy_crypto.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/bls/indy_crypto/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/crypto/test/
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/crypto/test/bls/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/test/bls/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/crypto/test/bls/indy_crypto/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/test/bls/indy_crypto/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004    14224 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/test/bls/indy_crypto/test_bls_crypto_indy_crypto.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/test/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     6591 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/crypto/test/test_multi_signature.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/state/
--rw-rw-r--   0 sovrin    (1003)     1004     5789 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/pruning_state.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/state/db/
--rw-rw-r--   0 sovrin    (1003)     1004     6513 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/db/refcount_db.py
--rw-rw-r--   0 sovrin    (1003)     1004      808 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/db/persistent_db.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/db/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      309 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/db/db.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2056 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/state.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/state/test/
--rw-rw-r--   0 sovrin    (1003)     1004      124 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     1806 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/test_remove_from_trie.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/state/test/db/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/db/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      443 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/db/test_RefcountDB.py
--rw-rw-r--   0 sovrin    (1003)     1004       38 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/bench.py
--rw-rw-r--   0 sovrin    (1003)     1004     4250 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/test_state_proof_verification.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/state/test/trie/
--rw-rw-r--   0 sovrin    (1003)     1004      501 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/trie/test_api.py
--rw-rw-r--   0 sovrin    (1003)     1004     4518 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/trie/test_prefix_nodes.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/trie/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2468 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/trie/test_trie_values_at_different_roots.py
--rw-rw-r--   0 sovrin    (1003)     1004    15601 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/trie/test_proof.py
--rw-rw-r--   0 sovrin    (1003)     1004     9360 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/test/test_pruning_state.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/state/trie/
--rw-rw-r--   0 sovrin    (1003)     1004    39199 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/trie/pruning_trie.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/trie/__init__.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/state/util/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/util/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     3414 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/util/fast_rlp.py
--rw-rw-r--   0 sovrin    (1003)     1004     9693 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/state/util/utils.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_zmq/
--rw-rw-r--   0 sovrin    (1003)     1004     5909 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/remote.py
--rw-rw-r--   0 sovrin    (1003)     1004     2046 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/simple_zstack.py
--rw-rw-r--   0 sovrin    (1003)     1004     4908 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/kit_zstack.py
--rw-rw-r--   0 sovrin    (1003)     1004     5566 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/util.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004    38631 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/zstack.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/
--rw-rw-r--   0 sovrin    (1003)     1004     6197 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_send_to_disconnected.py
--rw-rw-r--   0 sovrin    (1003)     1004     1118 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_kitzstack.py
--rw-rw-r--   0 sovrin    (1003)     1004     3654 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004     2446 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_quotas.py
--rw-rw-r--   0 sovrin    (1003)     1004      405 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_utils.py
--rw-rw-r--   0 sovrin    (1003)     1004     1773 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_large_messages.py
--rw-rw-r--   0 sovrin    (1003)     1004     7209 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_stashed_client_messages.py
--rw-rw-r--   0 sovrin    (1003)     1004     3525 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_node_to_node_quota.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     2209 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_zstack_communication.py
--rw-rw-r--   0 sovrin    (1003)     1004    12184 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_zstack.py
--rw-rw-r--   0 sovrin    (1003)     1004     1812 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_stashed_ping_pong.py
--rw-rw-r--   0 sovrin    (1003)     1004     4002 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     4101 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_heartbeats.py
--rw-rw-r--   0 sovrin    (1003)     1004     4970 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/test/test_reconnect.py
--rw-rw-r--   0 sovrin    (1003)     1004     3648 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/authenticator.py
--rw-rw-r--   0 sovrin    (1003)     1004     4809 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/stp_zmq/client_message_provider.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/ledger/
--rw-rw-r--   0 sovrin    (1003)     1004     4462 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/error.py
--rw-rw-r--   0 sovrin    (1003)     1004     2628 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/tree_hasher.py
--rw-rw-r--   0 sovrin    (1003)     1004     1295 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/util.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004    11169 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/compact_merkle_tree.py
--rw-rw-r--   0 sovrin    (1003)     1004      577 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/immutable_store.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/ledger/hash_stores/
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/hash_stores/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004     4941 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/hash_stores/hash_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     4261 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/hash_stores/file_hash_store.py
--rw-rw-r--   0 sovrin    (1003)     1004     1129 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/hash_stores/memory_hash_store.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/ledger/genesis_txn/
--rw-rw-r--   0 sovrin    (1003)     1004      547 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/genesis_txn/genesis_txn_file_util.py
--rw-rw-r--   0 sovrin    (1003)     1004      394 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/genesis_txn/genesis_txn_initiator_from_mem.py
--rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/genesis_txn/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004      304 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/genesis_txn/genesis_txn_initiator.py
--rw-rw-r--   0 sovrin    (1003)     1004     2039 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/genesis_txn/genesis_txn_initiator_from_file.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/ledger/test/
--rw-rw-r--   0 sovrin    (1003)     1004     4400 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/conftest.py
--rw-rw-r--   0 sovrin    (1003)     1004      425 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/test_utils.py
--rw-rw-r--   0 sovrin    (1003)     1004     3308 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/test_init_genesis_txns.py
--rw-rw-r--   0 sovrin    (1003)     1004     9504 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/test_merkle_proof.py
--rw-rw-r--   0 sovrin    (1003)     1004      917 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/test_txn_persistence.py
--rw-rw-r--   0 sovrin    (1003)     1004     2984 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/test_file_hash_store.py
--rw-rw-r--   0 sovrin    (1003)     1004       87 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/__init__.py
--rw-rw-r--   0 sovrin    (1003)     1004    26636 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/merkle_test.py
--rw-rw-r--   0 sovrin    (1003)     1004       36 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/__main__.py
--rw-rw-r--   0 sovrin    (1003)     1004     6847 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/helper.py
--rw-rw-r--   0 sovrin    (1003)     1004     1481 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/test_file_store_perf.py
--rw-rw-r--   0 sovrin    (1003)     1004    12717 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/test/test_ledger.py
--rw-rw-r--   0 sovrin    (1003)     1004     1585 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/merkle_tree.py
--rw-rw-r--   0 sovrin    (1003)     1004    10417 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/ledger.py
--rw-rw-r--   0 sovrin    (1003)     1004    11162 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/ledger/merkle_verifier.py
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/scripts/
-drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/scripts/process_logs/
--rwxrwxr-x   0 sovrin    (1003)     1004    34001 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/process_logs/process_logs
--rw-rw-r--   0 sovrin    (1003)     1004    11097 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/process_logs/process_logs.yml
--rw-rw-r--   0 sovrin    (1003)     1004     2574 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/init_plenum_keys
--rw-rw-r--   0 sovrin    (1003)     1004     1816 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/log_stats
--rw-rw-r--   0 sovrin    (1003)     1004     3498 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/filter_log
--rwxrwxr-x   0 sovrin    (1003)     1004     1620 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/start_plenum_node
--rwxrwxr-x   0 sovrin    (1003)     1004     2346 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/gen_node
--rwxrwxr-x   0 sovrin    (1003)     1004      820 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/export-gen-txns
--rwxrwxr-x   0 sovrin    (1003)     1004     1532 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/get_keys
--rwxrwxr-x   0 sovrin    (1003)     1004     1001 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/init_bls_keys
--rwxrwxr-x   0 sovrin    (1003)     1004      606 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/generate_plenum_pool_transactions
--rwxrwxr-x   0 sovrin    (1003)     1004     1489 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/gen_steward_key
--rwxrwxr-x   0 sovrin    (1003)     1004      595 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/udp_receiver
--rwxrwxr-x   0 sovrin    (1003)     1004     1061 2019-08-29 11:08:38.000000 indy-plenum-1.9.2.dev879/scripts/udp_sender
--rw-r--r--   0 sovrin    (1003)     1004      602 2019-08-29 11:08:45.000000 indy-plenum-1.9.2.dev879/PKG-INFO
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/storage/
+-rw-rw-r--   0 sovrin    (1003)     1004     3890 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/optimistic_kv_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2887 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/kv_store_leveldb.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1504 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/kv_store_rocksdb_int_keys.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5642 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/kv_store_file.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1471 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/state_ts_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2893 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/binary_file_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2874 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/kv_store_single_file.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6304 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/kv_store_rocksdb.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/storage/test/
+-rw-rw-r--   0 sovrin    (1003)     1004     3741 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/test/test_ts_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004      725 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/test/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4035 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/test/test_optimistic_kv_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4830 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/test/test_kv_store_comparator.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/test/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1400 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/test/test_kv_storages_read_only.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3470 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/test/test_chunked_file_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2418 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/test/test_stores_equailty.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1416 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/test/test_kv_storage_get_equal_or_prev.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2907 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/test/test_kv_storages.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1404 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/kv_store_leveldb_int_keys.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10441 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/chunked_file_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1777 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/directory_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1760 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/kv_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2688 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/kv_in_memory.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4438 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004      374 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/store_utils.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1862 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/text_file_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1029 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/storage/binary_serializer_based_file_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004       90 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/MANIFEST.in
+-rw-rw-r--   0 sovrin    (1003)     1004     4153 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/README.md
+-rw-rw-r--   0 sovrin    (1003)     1004      104 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/setup.cfg
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/
+-rw-rw-r--   0 sovrin    (1003)     1004      111 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/types.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/loop/
+-rw-rw-r--   0 sovrin    (1003)     1004      142 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/loop/exceptions.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/loop/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6566 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/loop/eventually.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1084 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/loop/startable.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2554 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/loop/motor.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10804 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/loop/looper.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/common/
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/common/config/
+-rw-rw-r--   0 sovrin    (1003)     1004      429 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/common/config/util.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/common/config/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      423 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/common/error.py
+-rw-rw-r--   0 sovrin    (1003)     1004      498 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/common/temp_file_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1593 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/common/util.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/common/logging/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/common/logging/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3509 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/common/logging/CompressingFileHandler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2199 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/common/logging/handlers.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/common/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004       65 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/common/constants.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4229 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/common/log.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/network/
+-rw-rw-r--   0 sovrin    (1003)     1004     4661 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/network/keep_in_touch.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1661 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/network/exceptions.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2528 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/network/util.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/network/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      214 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/network/auth_mode.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2774 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/network/port_dispenser.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5799 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/network/network_interface.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/validators/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/validators/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      489 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/validators/message_length_validator.py
+-rw-rw-r--   0 sovrin    (1003)     1004      125 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/error_codes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1430 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/config.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2791 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/ratchet.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/test/
+-rw-rw-r--   0 sovrin    (1003)     1004      346 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/test/conftest.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/test/loop/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/test/loop/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      258 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/test/loop/test_looper.py
+-rw-rw-r--   0 sovrin    (1003)     1004      424 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/test/loop/test_eventually.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/test/common/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/test/common/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      173 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/test/common/test_logger.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/test/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/test/crypto/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/test/crypto/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      242 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/test/crypto/test_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4275 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/test/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004      441 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/test/test_msg_len_validator.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_core/crypto/
+-rw-rw-r--   0 sovrin    (1003)     1004      467 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/crypto/encoding.py
+-rw-rw-r--   0 sovrin    (1003)     1004    16836 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/crypto/nacl_wrappers.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2922 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/crypto/util.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/crypto/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      499 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_core/crypto/signer.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/common/
+-rw-rw-r--   0 sovrin    (1003)     1004      339 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/error.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2049 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/exceptions.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/common/test/
+-rw-rw-r--   0 sovrin    (1003)     1004     4371 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/test/test_msgpack_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3740 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/test/test_signing_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/test/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/common/test/version/
+-rw-rw-r--   0 sovrin    (1003)     1004    11240 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/test/version/test_version.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/test/version/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8752 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/test/test_compact_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1282 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/test/test_exceptions.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1742 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/test/test_json_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004      972 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/test/test_fields.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/common/serializers/
+-rw-rw-r--   0 sovrin    (1003)     1004     2771 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/serializers/compact_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1859 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/serializers/msgpack_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1553 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/serializers/serialization.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/serializers/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      600 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/serializers/field.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2130 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/serializers/json_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004      325 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/serializers/base58_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004      152 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/serializers/stream_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2937 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/serializers/signing_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004      245 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/serializers/mapping_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004      309 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/serializers/base64_serializer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9669 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/common/version.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/data/
+-rw-rw-r--   0 sovrin    (1003)     1004     1673 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/data/domain_transactions_local_genesis
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/data/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1369 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/data/pool_transactions_sandbox_genesis
+-rw-rw-r--   0 sovrin    (1003)     1004      930 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/data/domain_transactions_sandbox_genesis
+-rw-rw-r--   0 sovrin    (1003)     1004     1408 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/data/pool_transactions_local_genesis
+-rw-rw-r--   0 sovrin    (1003)     1004     3169 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/setup.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/indy_plenum.egg-info/
+-rw-r--r--   0 sovrin    (1003)     1004       64 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/indy_plenum.egg-info/top_level.txt
+-rw-r--r--   0 sovrin    (1003)     1004    60206 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/indy_plenum.egg-info/SOURCES.txt
+-rw-r--r--   0 sovrin    (1003)     1004      555 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/indy_plenum.egg-info/requires.txt
+-rw-r--r--   0 sovrin    (1003)     1004        1 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/indy_plenum.egg-info/dependency_links.txt
+-rw-r--r--   0 sovrin    (1003)     1004      594 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/indy_plenum.egg-info/PKG-INFO
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/recorder/
+-rw-rw-r--   0 sovrin    (1003)     1004      927 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/recorder/simple_zstack_with_silencer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1822 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/recorder/combined_recorder.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/recorder/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8857 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/recorder/replayer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5825 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/recorder/recorder.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5650 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/recorder/replayable_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1762 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/recorder/simple_zstack_with_recorder.py
+-rw-rw-r--   0 sovrin    (1003)     1004      446 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/recorder/silencer.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/
+-rw-rw-r--   0 sovrin    (1003)     1004     3484 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/database_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2434 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/replica_stasher.py
+-rw-rw-r--   0 sovrin    (1003)     1004    30357 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/validator_info_tool.py
+-rw-rw-r--   0 sovrin    (1003)     1004      669 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/blacklister.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5734 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/replica_validator.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1386 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/quorums.py
+-rw-rw-r--   0 sovrin    (1003)     1004      490 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/replica_validator_enums.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2947 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/replica_freshness_checker.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/catchup/
+-rw-rw-r--   0 sovrin    (1003)     1004    19564 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/catchup/cons_proof_service.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/catchup/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004    20433 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/catchup/catchup_rep_service.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4645 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/catchup/ledger_leecher_service.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2841 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/catchup/node_catchup_data.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8368 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/catchup/seeder_service.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10381 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/catchup/node_leecher_service.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2948 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/catchup/utils.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10418 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/client_authn.py
+-rw-rw-r--   0 sovrin    (1003)     1004   128687 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/replica.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5081 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/plugin_loader.py
+-rw-rw-r--   0 sovrin    (1003)     1004      164 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/stats_consumer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1376 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/msg_filter.py
+-rw-rw-r--   0 sovrin    (1003)     1004    16806 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/pool_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2169 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/quota_control.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5036 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/has_action_queue.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4966 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/primary_decider.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4735 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/last_sent_pp_store_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2691 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/req_authenticator.py
+-rw-rw-r--   0 sovrin    (1003)     1004    12522 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/propagator.py
+-rw-rw-r--   0 sovrin    (1003)     1004   153029 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/node.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/view_change/
+-rw-rw-r--   0 sovrin    (1003)     1004     1032 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/view_change/view_change_msg_filter.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3634 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/view_change/node_view_changer.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/view_change/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004    30224 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/view_change/view_changer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6445 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/view_change/pre_view_change_strategies.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5250 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/view_change/instance_change_provider.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3859 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/router.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/observer/
+-rw-rw-r--   0 sovrin    (1003)     1004     1187 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/observer/observable_sync_policy_each_batch.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6227 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/observer/observer_sync_policy_each_batch.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/observer/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1416 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/observer/observable_sync_policy.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3489 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/observer/observable.py
+-rw-rw-r--   0 sovrin    (1003)     1004      875 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/observer/observer.py
+-rw-rw-r--   0 sovrin    (1003)     1004      579 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/observer/observer_sync_policy.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1554 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/observer/observer_node.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/consensus/
+-rw-rw-r--   0 sovrin    (1003)     1004     1876 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/consensus/replica_service.py
+-rw-rw-r--   0 sovrin    (1003)     1004      339 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/consensus/checkpoint_service.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/consensus/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004    15941 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/consensus/view_change_service.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2626 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/consensus/consensus_shared_data.py
+-rw-rw-r--   0 sovrin    (1003)     1004    93512 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/consensus/ordering_service.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/request_managers/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_managers/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1117 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_managers/action_request_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1447 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_managers/request_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004    14964 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_managers/write_request_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1224 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_managers/read_request_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8127 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/primary_selector.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/
+-rw-rw-r--   0 sovrin    (1003)     1004     4672 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/nym_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3645 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/get_txn_author_agreement_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3133 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/get_txn_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3637 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/txn_author_agreement_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2965 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/get_txn_author_agreement_aml_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10011 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/node_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004       38 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/state_constants.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/handler_interfaces/
+-rw-rw-r--   0 sovrin    (1003)     1004      577 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/handler_interfaces/action_request_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3258 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/handler_interfaces/write_request_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/handler_interfaces/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1432 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/handler_interfaces/request_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4157 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/handler_interfaces/read_request_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1038 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/audit_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2825 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/static_taa_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3008 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/txn_author_agreement_aml_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1818 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/request_handlers/utils.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5429 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/notifier_plugin_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1177 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/instances.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/batch_handlers/
+-rw-rw-r--   0 sovrin    (1003)     1004      535 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/batch_handlers/pool_batch_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3016 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/batch_handlers/batch_request_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3353 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/batch_handlers/three_pc_batch.py
+-rw-rw-r--   0 sovrin    (1003)     1004      609 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/batch_handlers/config_batch_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/batch_handlers/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1358 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/batch_handlers/ts_store_batch_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004      541 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/batch_handlers/domain_batch_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9429 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/batch_handlers/audit_batch_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9002 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/message_handlers.py
+-rw-rw-r--   0 sovrin    (1003)     1004    15529 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/node_bootstrap.py
+-rw-rw-r--   0 sovrin    (1003)     1004    32342 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/monitor.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3616 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/models.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5580 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/backup_instance_faulty_processor.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4789 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/suspicion_codes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2468 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/message_req_processor.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4346 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/future_primaries_batch_handler.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/plugin/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/plugin/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2436 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/plugin/has_plugin_loader_helper.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/plugin/stats_consumer/
+-rw-rw-r--   0 sovrin    (1003)     1004     3749 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/plugin/stats_consumer/stats_publisher.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/plugin/stats_consumer/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4013 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/plugin/stats_consumer/plugin_firebase_stats_consumer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1031 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/inconsistency_watchers.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/server/general_config/
+-rw-rw-r--   0 sovrin    (1003)     1004      458 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/general_config/ubuntu_platform_config.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/general_config/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004       17 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/general_config/windows_platform_config.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9927 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/server/replicas.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/client/
+-rw-rw-r--   0 sovrin    (1003)     1004    14987 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/client/wallet.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/client/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/bls/
+-rw-rw-r--   0 sovrin    (1003)     1004     1445 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/bls/bls_key_register_pool_ledger.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1396 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/bls/bls_crypto_factory.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1739 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/bls/bls_bft_factory.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/bls/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1534 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/bls/bls_key_manager_file.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1469 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/bls/bls_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1847 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/bls/bls_key_register_pool_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004    11482 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/bls/bls_bft_replica_plenum.py
+-rw-r--r--   0 sovrin    (1003)     1004      134 2019-08-28 15:07:59.000000 indy-plenum-1.9.2rc1/plenum/__manifest__.json
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/common/
+-rw-rw-r--   0 sovrin    (1003)     1004     1712 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/config_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4076 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/transaction_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8116 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/batched.py
+-rw-rw-r--   0 sovrin    (1003)     1004      768 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/jsonpickle_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5206 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/types.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1138 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/prepare_batch.py
+-rw-rw-r--   0 sovrin    (1003)     1004      670 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/ledger_info.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1358 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/did_method.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6795 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/script_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2557 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/latency_measurements.py
+-rw-rw-r--   0 sovrin    (1003)     1004      144 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/error.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7068 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/exceptions.py
+-rw-rw-r--   0 sovrin    (1003)     1004      461 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/temp_file_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3694 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/gc_trackers.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2876 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/throttler.py
+-rw-rw-r--   0 sovrin    (1003)     1004    11206 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/txn_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3012 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/keygen_utils.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2185 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/signer_simple.py
+-rw-rw-r--   0 sovrin    (1003)     1004      818 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/sys_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3998 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/signer_did.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2538 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/stashing_deque.py
+-rw-rw-r--   0 sovrin    (1003)     1004      407 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/roles.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3736 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/metrics_stats.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1652 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/verifier.py
+-rw-rw-r--   0 sovrin    (1003)     1004    18163 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/util.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1460 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/bitmask_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2684 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/timer.py
+-rw-rw-r--   0 sovrin    (1003)     1004      520 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/tools.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1641 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/pkg_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2078 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/moving_average.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4320 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/value_accumulator.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/common/messages/
+-rw-rw-r--   0 sovrin    (1003)     1004     4450 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/messages/node_message_factory.py
+-rw-rw-r--   0 sovrin    (1003)     1004    21142 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/messages/fields.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9855 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/messages/client_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004    17280 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/messages/node_messages.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1417 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/messages/internal_messages.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5968 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/messages/message_base.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/messages/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5916 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/stashing_router.py
+-rw-rw-r--   0 sovrin    (1003)     1004      588 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/transactions.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7410 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/constants.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2587 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/perf_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004    16214 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/metrics_collector.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1291 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/event_bus.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1806 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/startable.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2582 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/plugin_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3728 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/monitor_strategies.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6950 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/ledger_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7439 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/throughput_measurements.py
+-rw-rw-r--   0 sovrin    (1003)     1004    13935 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/test_network_setup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2603 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/ledger_uncommitted_tracker.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5832 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/request.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1932 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/message_processor.py
+-rw-rw-r--   0 sovrin    (1003)     1004      619 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/average_strategies.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2516 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/motor.py
+-rw-rw-r--   0 sovrin    (1003)     1004      881 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/has_file_storage.py
+-rw-rw-r--   0 sovrin    (1003)     1004      432 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/plenum_protocol_version.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5527 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/channel.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6644 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/ledger.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9355 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/stacks.py
+-rw-rw-r--   0 sovrin    (1003)     1004      763 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/hook_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10484 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/stack_manager.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/common/member/
+-rw-rw-r--   0 sovrin    (1003)     1004     3176 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/member/steward.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/member/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      293 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/member/trustee.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1109 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/member/member.py
+-rw-rw-r--   0 sovrin    (1003)     1004      818 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/init_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3981 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/common/config_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004    14108 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/config.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3380 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2478 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/__metadata__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/
+-rw-rw-r--   0 sovrin    (1003)     1004      930 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3230 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_safe_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2066 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_message_serialization.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4545 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_strict_schema.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3391 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_message_factory.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1877 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_node_op.py
+-rw-rw-r--   0 sovrin    (1003)     1004      748 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_taa.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/
+-rw-rw-r--   0 sovrin    (1003)     1004     1591 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_sha256_hex_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      647 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_merkle_tree_root_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      436 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_ledger_id_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      467 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_time_among_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      614 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_fixed_length_string_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      352 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_bool_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      427 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_message_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2067 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_bls_multisig_value_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1422 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_request_identifier_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1643 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_base58_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1700 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_iterable_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      689 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_protocol_version_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      379 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_non_negative_number_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      379 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_serializedvalue_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      242 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_non_empty_string_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      472 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_txn_seq_no_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      782 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_verkey_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      697 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_limited_length_string_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      275 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_ledger_info_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1986 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_bls_multisig_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1972 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_version_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      351 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_timestamp_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      510 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_hex_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      608 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_identifier_field.py
+-rw-rw-r--   0 sovrin    (1003)     1004      548 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_handle_one_node_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      742 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_message_base.py
+-rw-rw-r--   0 sovrin    (1003)     1004    12106 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004      526 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/constants.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1434 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_taa_aml_op.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/
+-rw-rw-r--   0 sovrin    (1003)     1004      780 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_catchuprep_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2034 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_observed_data.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1107 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_client_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004      761 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_instanceChange_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004      900 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_checkpoint_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1059 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_prepare_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1233 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_ordered_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004      900 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_commit_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3082 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_batch_committed.py
+-rw-rw-r--   0 sovrin    (1003)     1004      846 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_catchupreq_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      829 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_viewchangedone_messsage.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1438 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_preprepare_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1055 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_consistencyproof_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004      759 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_currentstate_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004      717 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_batch_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004      971 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_ledgerstatus_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004      853 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_nomination_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004      805 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_propagate_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004      825 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_primary_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004      819 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_reelection_message.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1376 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_taa_op.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1369 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_get_txn_op.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8888 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1023 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/stub_messages.py
+-rw-rw-r--   0 sovrin    (1003)     1004      385 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/utils.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1500 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_nym_op.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/storage/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/storage/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1968 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/storage/test_hash_stores.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/signing/
+-rw-rw-r--   0 sovrin    (1003)     1004     1309 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/signing/test_signing_without_identifier.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/signing/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2944 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/signing/test_signing.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4022 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/signing/test_create_did_without_endorser.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/
+-rw-rw-r--   0 sovrin    (1003)     1004     7355 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_unit_setup_for_non_master.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5362 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_node_got_no_preprepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2250 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_setup_for_non_master.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4246 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_2_nodes_got_only_preprepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3500 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_send_node_with_invalid_verkey.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3232 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_request_schema_validation.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/
+-rw-rw-r--   0 sovrin    (1003)     1004     4275 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/test_3pc_timestamp.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2724 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/test_timestamp_post_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2243 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/test_timestamp_new_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2666 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/test_clock_disruption.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1003 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4351 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_quorum_f_plus_2_nodes_including_primary_off_and_on.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3365 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_discard_3pc_for_ordered.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8385 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_belated_request_not_processed.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/
+-rw-rw-r--   0 sovrin    (1003)     1004     4652 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2079 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_f_plus_one_faults.py
+-rw-rw-r--   0 sovrin    (1003)     1004      808 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_num_of_sufficient_preprepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004      714 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_one_fault.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004       82 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_num_of_preprepare_with_zero_faulty_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2142 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_primary_sends_preprepare_of_high_num.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1067 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_ignore_pre_prepare_pp_seq_no_less_than_expected.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2560 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_non_primary_sends_a_pre_prepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3148 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_pass_for_stashed.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2250 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_fail_for_delayed.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3330 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_reply_from_ledger_for_request.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/
+-rw-rw-r--   0 sovrin    (1003)     1004      724 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_num_of_propagate_with_zero_faulty_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004      661 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_num_of_propagate_with_one_fault.py
+-rw-rw-r--   0 sovrin    (1003)     1004      875 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_no_reauth.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3579 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_node_lacks_finalised_requests.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1097 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_num_of_propagate_with_f_plus_one_faulty_nodes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2226 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_node_request_only_needed_propagates.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1351 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_clean_verified_reqs.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1813 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_num_of_sufficient_propagate.py
+-rw-rw-r--   0 sovrin    (1003)     1004      261 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1401 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_node_request_propagates_with_delay.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3169 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_request_forwarding.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4290 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_quorum_f_plus_2_nodes_but_not_primary_off_and_on.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4934 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_different_ledger_request_interleave.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1752 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_quorum_faulty.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/
+-rw-rw-r--   0 sovrin    (1003)     1004     1051 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3618 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_node_requests_missing_preprepares_and_prepares.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4881 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_preprepare_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5090 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_node_requests_missing_preprepares_prepares_and_commits.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10010 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_valid_message_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2639 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_node_requests_missing_preprepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4600 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_node_requests_missing_preprepares_and_prepares_after_long_disconnection.py
+-rw-rw-r--   0 sovrin    (1003)     1004      408 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_request_ls_for_incorrect_ledger.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1315 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2900 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_requested_preprepare_handling.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_order/
+-rw-rw-r--   0 sovrin    (1003)     1004     2520 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_order/test_request_ordering_2.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_order/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1554 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_order/test_request_ordering_1.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1989 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_order/test_ordering_when_pre_prepare_not_received.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1648 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_split_non_3pc_messages_on_batches.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1778 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_quorum_disconnected.py
+-rw-rw-r--   0 sovrin    (1003)     1004    15303 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/node_request_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2407 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_req_idr_to_txn.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1255 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/helper.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_prepare/
+-rw-rw-r--   0 sovrin    (1003)     1004      776 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_prepare/test_num_of_prepare_with_one_fault.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_prepare/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004       76 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_prepare/test_num_of_prepare_with_zero_faulty_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1078 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_prepare/test_num_of_sufficient_prepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1253 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_prepare/test_num_prepare_with_2_of_6_faulty.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1800 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_prepare/test_num_of_prepare_with_f_plus_one_faults.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/
+-rw-rw-r--   0 sovrin    (1003)     1004     1269 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_num_commit_with_2_of_6_faulty.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1108 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_num_of_sufficient_commit.py
+-rw-rw-r--   0 sovrin    (1003)     1004      595 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_num_commit_with_one_fault.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1615 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_num_of_commit_with_f_plus_one_faults.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1144 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_commits_without_prepares.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2117 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_commits_dequeue_commits.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1676 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_commits_recvd_first.py
+-rw-rw-r--   0 sovrin    (1003)     1004       76 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_num_of_commit_with_zero_faulty_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2269 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_1_node_got_only_preprepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1191 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_msg_len_limit_large_enough.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4082 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_request/test_already_processed_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004    42680 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9420 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/malicious_behaviors_node.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/
+-rw-rw-r--   0 sovrin    (1003)     1004     7343 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1307 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_taa_aml_module.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2220 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_taa_aml_integration.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/
+-rw-rw-r--   0 sovrin    (1003)     1004     7218 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1233 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/test_taa_not_set.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1333 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/test_taa_acceptance_integration_validation.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2183 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10421 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/test_taa_acceptance_validation.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6578 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_config_req_handler_taa_utils.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2847 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_txn_author_agreement.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8063 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_get_txn_author_agreement.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7394 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2768 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_get_empty_txn_author_agreement.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8116 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_get_taa_aml.py
+-rw-rw-r--   0 sovrin    (1003)     1004      858 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/profiler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1405 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_round_trip_with_one_faulty_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1426 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_memory_consumpion.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/blacklist/
+-rw-rw-r--   0 sovrin    (1003)     1004     1433 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/blacklist/test_blacklist_client.py
+-rw-rw-r--   0 sovrin    (1003)     1004      942 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/blacklist/test_blacklist_node_on_multiple_nominations.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/blacklist/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1018 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/blacklist/test_blacklist_node_on_multiple_primary_declarations.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4218 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_req_authenticator.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/tracker/
+-rw-rw-r--   0 sovrin    (1003)     1004     9416 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/tracker/test_ledger_uncommitted_tracker.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/tracker/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3423 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_delay.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3689 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_stashing_queue.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5547 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/deep_eq.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1948 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_request_executed_once_and_without_failing_behind.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4328 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_get_txn_state_proof.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/
+-rw-rw-r--   0 sovrin    (1003)     1004      617 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/test_node_msgs_recording.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1101 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004      863 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/test_replayer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6601 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/test_recorder.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2243 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/test_combined_recorder.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1210 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/test_replay_on_new_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3303 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/test_replay_node_bouncing.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2162 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/test_replay_with_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3934 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1605 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/recorder/test_replay.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/instances/
+-rw-rw-r--   0 sovrin    (1003)     1004     2359 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/instances/test_multiple_pre_prepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/instances/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1725 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/instances/test_msgs_from_slow_instances.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1904 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/instances/test_multiple_instance_change_msgs.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2757 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/instances/test_multiple_commit.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2636 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/instances/test_multiple_prepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2142 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/instances/test_prepare_digest.py
+-rw-rw-r--   0 sovrin    (1003)     1004      836 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/instances/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2335 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/instances/test_pre_prepare_digest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3961 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/instances/test_instance_cannot_become_active_with_less_than_four_servers.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/server/
+-rw-rw-r--   0 sovrin    (1003)     1004    10124 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/server/test_instance_change_provider.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/server/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      560 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_config_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1974 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/grouped_load_scheduling.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/client/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/client/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5907 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/client/test_protocol_version.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2605 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/client/test_client.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/bls/
+-rw-rw-r--   0 sovrin    (1003)     1004     3582 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_not_depend_on_node_reg.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1020 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3214 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_key_registry_pool_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1606 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_no_state_proof.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1118 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_send_txns_bls_consensus.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4471 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_commit_signature_validation_integration.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2435 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_sign_validation_for_key_proof_exist_ordering.py
+-rw-rw-r--   0 sovrin    (1003)     1004      588 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_send_txns_full_bls.py
+-rw-rw-r--   0 sovrin    (1003)     1004      838 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_send_txns_bls_less_than_consensus.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1780 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_get_state_proof.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3648 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_add_incorrect_bls_key.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2794 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_update_bls_key.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4274 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_crypto_factory.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2101 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_sign_validation_for_key_proof_exist.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2281 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004    22917 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_bft_replica.py
+-rw-rw-r--   0 sovrin    (1003)     1004      576 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_send_txns_no_bls.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1875 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_bft_factory.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1953 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_key_manager_file.py
+-rw-rw-r--   0 sovrin    (1003)     1004    11919 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3360 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_add_bls_key.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7251 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_state_proof.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3614 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_update_incorrect_bls_key.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1120 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/bls/test_multi_signature_verifier.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/
+-rw-rw-r--   0 sovrin    (1003)     1004     3725 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/test_basic_batching.py
+-rw-rw-r--   0 sovrin    (1003)     1004      351 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004      268 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/test_client_requests.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2860 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/test_batch_rejection.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2158 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_freeing_forwarded_not_preprepared_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2245 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_freeing_forwarded_preprepared_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2049 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_catchup_during_3pc.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3352 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_state_reverted_before_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004      923 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_3pc_paused_during_catch_up.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3538 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_clearing_requests_after_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2776 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2135 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/batching_3pc/test_batching_scenarios.py
+-rw-rw-r--   0 sovrin    (1003)     1004       88 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/exceptions.py
+-rw-rw-r--   0 sovrin    (1003)     1004      572 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/msgs.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/
+-rw-rw-r--   0 sovrin    (1003)     1004     1310 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_unaligned_prepare_certificates_on_half_nodes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1355 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_delay_on_one_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3407 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_advancing_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1287 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_unaligned_prepare_certificates_on_one_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3493 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_delayed_instance_changes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3890 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_during_unstash.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1576 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_two_view_changes_with_delay_on_one_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1873 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_different_prepare_certificate.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1301 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_delayed_commits.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1768 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_two_view_changes_with_delayed_commits.py
+-rw-rw-r--   0 sovrin    (1003)     1004    11271 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1410 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_propagate_primary_on_one_delayed_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1635 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_two_view_changes_with_propagate_primary_on_one_delayed_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6813 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/stasher.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/sdk/
+-rw-rw-r--   0 sovrin    (1003)     1004     3096 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/sdk/test_sdk_bindings.py
+-rw-rw-r--   0 sovrin    (1003)     1004      555 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/sdk/test_sdk_many_stewards.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/sdk/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/
+-rw-rw-r--   0 sovrin    (1003)     1004     4172 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5113 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_stashing_checkpoints_after_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3856 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1335 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits_received_before_catchup_pool.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6358 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup_only_checkpoints.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3639 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_slow_catchup_while_ordering.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1337 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits_received_before_catchup_audit.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5079 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_limited_stashing_3pc_while_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6174 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup_checkpoints.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3683 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3738 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_event_bus.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1597 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/buy_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004      715 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_connections_with_converted_key.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/watermarks/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/watermarks/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2325 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/watermarks/test_watermarks_after_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10567 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/delayers.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/common/
+-rw-rw-r--   0 sovrin    (1003)     1004      450 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_transactions.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6443 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_config_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8771 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_digest_validation.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1693 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_random_string.py
+-rw-rw-r--   0 sovrin    (1003)     1004      871 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_pool_file_raises_descriptive_error.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2679 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_parse_ledger.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1448 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_splitting_large_messages.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1556 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_throttler.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1067 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_verifier.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1529 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_replicas_suspicious.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2935 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_database_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1426 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_signers.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1526 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_hook_mananger.py
+-rw-rw-r--   0 sovrin    (1003)     1004      394 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_roles.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2604 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/common/test_prepare_batch.py
+-rw-rw-r--   0 sovrin    (1003)     1004      800 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_node_reg.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/restart/
+-rw-rw-r--   0 sovrin    (1003)     1004     5342 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_to_inconsistent_state.py
+-rw-rw-r--   0 sovrin    (1003)     1004      771 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_4_all_wp.py
+-rw-rw-r--   0 sovrin    (1003)     1004      764 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_6_wp.py
+-rw-rw-r--   0 sovrin    (1003)     1004      764 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_4_np.py
+-rw-rw-r--   0 sovrin    (1003)     1004      771 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_6_all_wp.py
+-rw-rw-r--   0 sovrin    (1003)     1004      765 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_6_np.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      772 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_node_4_all.py
+-rw-rw-r--   0 sovrin    (1003)     1004      742 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_7.py
+-rw-rw-r--   0 sovrin    (1003)     1004      749 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_7_all.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3483 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_network_inconsistency_watcher.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5844 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_node_with_view_changes.py
+-rw-rw-r--   0 sovrin    (1003)     1004      765 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2134 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004      772 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_6_all_np.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3098 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_to_same_view_with_killed_primary.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/
+-rw-rw-r--   0 sovrin    (1003)     1004     4985 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/test_nym_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4468 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/test_txn_author_agreement_aml_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2373 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/test_get_txn_author_agreement_aml_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3416 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/test_get_value_from_state.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1965 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/test_get_txn_author_agreement_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5209 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/test_txn_author_agreement_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7441 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/test_node_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004      489 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/test_database_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7310 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/test_request_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004      765 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3923 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/req_handler/test_node_req_handler_static_validation.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3488 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_stasher.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/replica/
+-rw-rw-r--   0 sovrin    (1003)     1004     6724 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1776 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_catchup_after_replica_removing.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2220 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_buffers_cleaning.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1783 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_primary_marked_suspicious_for_sending_prepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4118 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_api.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4599 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_ordered_tracker.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2353 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_create_3pc_batch.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4617 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_replica_checkpoint_validation.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4146 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_3pc_messages_validation.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4018 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_consensus_data_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3521 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_get_last_timestamp_from_state.py
+-rw-rw-r--   0 sovrin    (1003)     1004      697 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_replicas_primary_names.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6173 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_update_watermarks_api.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3920 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_catchup_after_replica_addition.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3797 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_process_prepare.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/replica/stashing/
+-rw-rw-r--   0 sovrin    (1003)     1004     3843 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/stashing/test_stash_future_view.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4253 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/stashing/test_stash_out_of_watermarks.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1415 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/stashing/test_replica_unstashing.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/stashing/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5524 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/stashing/test_unstash_after_catchup_in_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4380 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/stashing/test_replica_stasher.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2638 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_max_3pc_batches_in_flight.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7698 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_process_preprepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5432 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_replica_reject_same_pre_prepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2614 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_replica_clear_collections_after_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2372 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_bitmask_apply.py
+-rw-rw-r--   0 sovrin    (1003)     1004    16139 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_replica_3pc_validation.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2057 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_monitor_reset_after_replica_addition.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1050 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_revert_from_malicious.py
+-rw-rw-r--   0 sovrin    (1003)     1004      752 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3024 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_replica_received_preprepare_with_unknown_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004    18405 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica/test_instance_faulty_processor.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3913 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_log_rotation.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/
+-rw-rw-r--   0 sovrin    (1003)     1004     2630 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_restarted_node_not_complete_vc_before_others.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2629 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_changes_if_master_primary_disconnected.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1561 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_by_current_state.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2734 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1475 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_pp_seq_no_starts_from_1.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1253 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_if_primary_disconnected.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2399 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2080 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_vc_started_in_different_time.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1612 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_n_minus_f_quorum.py
+-rw-rw-r--   0 sovrin    (1003)     1004      318 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_not_gamable.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2627 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_pre_vc_strategy_3PC_msgs.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2212 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_resend_inst_ch_in_progress_v_ch.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2231 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_discard_inst_chng_msg_from_past_view.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2511 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_no_instance_change_before_node_is_ready.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4671 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_that_domain_ledger_the_same_after_restart_for_all_nodes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1666 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_add_vc_start_msg_during_start_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1634 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_timeout_reset.py
+-rw-rw-r--   0 sovrin    (1003)     1004      940 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_last_completed_view_no_set_after_vc_complete.py
+-rw-rw-r--   0 sovrin    (1003)     1004      536 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_api.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6661 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_timeout.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6009 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_by_catchup_and_order.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2094 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_client_req_during_view_change_integration.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1070 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_can_finish_despite_perpetual_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8500 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_no_propagate_request_on_different_last_ordered_before_vc.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4587 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_after_back_to_quorum_with_disconnected_primary_and_slow_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4744 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_on_master_degraded.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3338 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_not_changed_when_primary_disconnected_from_less_than_quorum.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2171 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_no_view_change_while_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2096 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_missing_pp_before_starting_vc.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2709 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_done_delayed.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1602 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_start_without_primary.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1108 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_instance_change_from_unknown.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2917 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_max_catchup_rounds.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3522 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_catchup_to_next_view_during_view_change_by_primary.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2681 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_complete_with_delayed_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4756 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_prepare_in_queue_before_vc.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2019 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_with_instance_change_lost_due_to_restarts.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2754 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_not_changed_when_short_disconnection.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4065 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_instance_change_if_needed.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1298 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_not_changed_if_backup_primary_disconnected.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2469 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_last_ordered_reset_for_new_view.py
+-rw-rw-r--   0 sovrin    (1003)     1004      860 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_msgHasAcceptableViewNo.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3923 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_after_back_to_quorum_with_disconnected_primary.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1279 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_start_view_change_ts_set.py
+-rw-rw-r--   0 sovrin    (1003)     1004      912 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_reset_monitor_after_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1165 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_instance_change_msg_checking.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2973 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_checkPerformance.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6505 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_catchup_to_next_view_during_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4125 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_new_node_joins_after_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2905 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_queueing_req_from_future_view.py
+-rw-rw-r--   0 sovrin    (1003)     1004      987 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_select_primary_after_removed_backup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1716 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_no_future_view_change_while_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3992 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_diconnected_node_reconnects_after_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1178 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_not_changed.py
+-rw-rw-r--   0 sovrin    (1003)     1004      885 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_disable_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004    16023 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4546 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_reverted_unordered.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2801 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_no_future_view_change_while_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2807 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_3pc_msgs_during_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1482 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_old_instance_change_discarding.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2485 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_client_req_during_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4697 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_6th_node_join_after_view_change_by_primary_restart.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2439 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_future_vc_done.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2298 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_resend_instance_change_messages.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4933 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_node_detecting_lag_from_view_change_messages.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2363 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_wont_happen_if_ic_is_discarded.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1633 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_happens_post_timeout.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3420 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_master_primary_different_from_previous.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3412 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_min_cathup_timeout.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9195 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_vc_start_msg_strategy.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3060 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_without_any_reqs.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2094 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change/test_vc_finished_when_less_than_quorum_started.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/
+-rw-rw-r--   0 sovrin    (1003)     1004    12262 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/test_last_sent_pp_store_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004      549 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3869 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/test_backup_primary_restores_pp_seq_no_if_view_is_same.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4093 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/test_node_erases_last_sent_pp_key_on_pool_restart.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2141 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/test_node_erases_last_sent_pp_key_on_view_change.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_helpers/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_helpers/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      154 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_helpers/node_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2694 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_batch_handler.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/
+-rw-rw-r--   0 sovrin    (1003)     1004     3656 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_with_new_ls_form.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3317 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_large_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2937 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_build_ledger_status.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5894 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_revert_during_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2823 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004      847 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_ledger_manager.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/catchup_req/
+-rw-rw-r--   0 sovrin    (1003)     1004      211 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/catchup_req/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/catchup_req/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3573 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/catchup_req/test_catchup_with_one_slow_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3326 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/catchup_req/test_catchup_with_disconnected_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3195 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_after_checkpoints.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4302 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_ledger_info.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2453 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_new_node_catchup2.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2434 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_uses_only_nodes_with_cons_proofs.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2567 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_after_restart_no_txns.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2992 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_req_distribution.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3117 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_from_unequal_nodes_without_reasking.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2591 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_discard_view_no.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2982 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_request_missing_transactions.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3607 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_incorrect_catchup_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3902 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_reject_invalid_txn_during_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5120 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_ts_store_after_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2870 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_with_all_nodes_sending_cons_proofs_dead.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3261 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_set_H_to_maxsize_and_not_stash_on_backup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3518 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_request_consistency_proof.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9461 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_config_ledger.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2531 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_not_set_H_as_maxsize_for_backup_if_is_primary.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9531 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_process_catchup_replies.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2968 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_inlcuding_3PC.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4592 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_get_last_txn_3PC_key.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2922 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_f_plus_one.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2225 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_after_disconnect.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2898 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_remove_request_keys_post_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5214 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_not_triggered_if_another_in_progress.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2766 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_with_only_one_available_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004       61 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_and_view_change_after_start.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2487 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_demoted.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1271 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_start.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3894 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_with_ledger_statuses_in_old_format.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9395 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3380 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_causes_no_desync.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3891 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_delayed_nodes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4396 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_when_3_not_primary_node_restarted.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_req_id_key_error.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2562 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_reply.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2778 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_with_old_txn_metadata_digest_format.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1037 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_post_genesis_txn_from_catchup_added_to_ledger.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1966 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_new_node_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3783 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_from_unequal_nodes_without_waiting.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4035 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_reasking.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7899 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_with_connection_problem.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5752 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_after_restart_after_txns.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/logging/
+-rw-rw-r--   0 sovrin    (1003)     1004     1722 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/logging/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/logging/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5488 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/logging/test_logging_txn_state.py
+-rw-rw-r--   0 sovrin    (1003)     1004      273 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004    11804 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_performance.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/observer/
+-rw-rw-r--   0 sovrin    (1003)     1004     2208 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/observer/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1154 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/observer/test_observable_each_batch.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1547 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/observer/test_observable_each_batch_node_integration.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1087 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/observer/test_observable_node_integration.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3488 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/observer/test_observer_node_each_batch.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/observer/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1846 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/observer/test_observable.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9841 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/observer/test_observer_policy_each_batch.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1736 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/observer/test_observer_node_integration.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/
+-rw-rw-r--   0 sovrin    (1003)     1004     1245 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1624 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_demote_backup_primary.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2705 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_first_audit_catchup_during_ordering.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4966 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2537 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_future_primaries_addition.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4022 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_future_primaries_unit.py
+-rw-rw-r--   0 sovrin    (1003)     1004    18331 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4298 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_freshness.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2303 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_demote_backup_primary_without_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1315 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_calc_catchup_till.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7203 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_handler_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3044 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_primaries_in_ordered.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8672 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_handler_multiple_commits.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5880 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_multiple_ledgers_in_one_batch.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3990 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7151 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_ordering.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/metrics/
+-rw-rw-r--   0 sovrin    (1003)     1004      545 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/metrics/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/metrics/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3169 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/metrics/test_value_accumulator.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4744 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/metrics/test_metrics_config.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2011 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/metrics/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8270 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/metrics/test_metrics_collector.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6829 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/metrics/test_metrics_stats.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/zstack_tests/
+-rw-rw-r--   0 sovrin    (1003)     1004     3478 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_clientstack_restart_trigger.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2508 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_send_too_many_reqs.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2794 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_send_client_msgs_with_delay_reqs.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2516 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_zstack_reconnection.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1261 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_4_of_4_nodes.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/zstack_tests/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1264 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_2_of_4_nodes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1265 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_3_of_4_nodes.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/simulation/
+-rw-rw-r--   0 sovrin    (1003)     1004     5648 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/simulation/test_sim_network.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/simulation/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2085 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/simulation/test_sim_random.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1845 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/simulation/sim_network.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1551 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/simulation/sim_random.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1669 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/get_buy_handler.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/
+-rw-rw-r--   0 sovrin    (1003)     1004     2978 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1825 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/test_consensus_dp_batches.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/view_change/
+-rw-rw-r--   0 sovrin    (1003)     1004     4534 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/view_change/test_view_change_service.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/view_change/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3479 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/view_change/test_sim_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5003 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/view_change/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004    22673 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/view_change/test_new_view_builder.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9269 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/view_change/test_view_change_msg_creation.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/
+-rw-rw-r--   0 sovrin    (1003)     1004     5670 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4703 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/test_pp_obsolescence.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1578 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/test_buffers_cleaning.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3331 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/test_can_send_3pc.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3609 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/test_ordering_process_prepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1606 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/test_orderer_api.py
+-rw-rw-r--   0 sovrin    (1003)     1004      439 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/test_can_order.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6613 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/test_ordering_process_preprepare.py
+-rw-rw-r--   0 sovrin    (1003)     1004      419 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004    11776 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/test_three_pc_validator.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      794 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/consensus/test_consensus_data_provider.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8029 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_stashing_router.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/wallet/
+-rw-rw-r--   0 sovrin    (1003)     1004     7818 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/wallet/test_wallet_storage_helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/wallet/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2751 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/wallet/test_wallet.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1159 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_bootstrapping.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/package/
+-rw-rw-r--   0 sovrin    (1003)     1004     2222 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/package/test_metadata.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/package/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004       36 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/__main__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      817 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_lazy_field.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/validator_info/
+-rw-rw-r--   0 sovrin    (1003)     1004     1451 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/validator_info/test_start_vc_ts_in_node_info.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1582 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/validator_info/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/validator_info/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4761 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/validator_info/test_validator_info_vc.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1256 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/validator_info/test_validator_info_dump.py
+-rw-rw-r--   0 sovrin    (1003)     1004    11172 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/validator_info/test_validator_info.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2024 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/validator_info/test_upgrade_log.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6694 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_node_connection.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2911 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_repeating_timer.py
+-rw-rw-r--   0 sovrin    (1003)     1004       68 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/constants.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/
+-rw-rw-r--   0 sovrin    (1003)     1004      528 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_not_removing_by_latency.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4613 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_after_node_started.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1099 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_acc_quorum.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1098 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_acc_local.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1098 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_quorum.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3813 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_after_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1097 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_local.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2765 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_with_primary_disconnected.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6796 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4289 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/replica_removing/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4581 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_queue_timer.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7866 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_testable.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2060 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/spy_helpers.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1941 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_dirty_read.py
+-rw-rw-r--   0 sovrin    (1003)     1004      766 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/random_buy_handler.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/
+-rw-rw-r--   0 sovrin    (1003)     1004     1524 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_new_primary.py
+-rw-rw-r--   0 sovrin    (1003)     1004      910 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_all_nodes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1434 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_old_primary.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4817 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_gc_all_nodes_random_delay.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1272 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_non_primary.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2030 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_complex.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1280 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_old_and_new_primary.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1133 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_all_nodes_random_delay.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/
+-rw-rw-r--   0 sovrin    (1003)     1004     1041 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3575 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_watermarks_on_delayed_backup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2067 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_message_outside_watermark1.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8710 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_checkpoints_removal_after_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1325 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_discard_old_checkpoint_messages.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6944 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_checkpoints_removal_after_catchup_during_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4972 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_complete_short_checkpoint_not_included_in_lag_for_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1876 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_basic_checkpointing.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3114 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_lagged_checkpoint_completion.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4958 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_checkpoints_removal_in_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3372 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_checkpoint_stabilization_after_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4362 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_message_outside_watermark.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3250 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_stashed_checkpoint_processing.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10115 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_backup_replica_resumes_ordering_on_lag_in_checkpoints.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2191 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_ordering_after_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3072 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_checkpoint_stable_while_unstashing.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4013 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_view_change_after_checkpoint.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1513 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_checkpoint_bounds_after_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1319 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4645 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_incomplete_short_checkpoint_included_in_lag_for_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5992 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_stashed_messages_processed_on_backup_replica_ordering_resumption.py
+-rw-rw-r--   0 sovrin    (1003)     1004      991 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_stable_checkpoint1.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2608 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_stable_checkpoint.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3017 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_lag_size_for_catchup.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/
+-rw-rw-r--   0 sovrin    (1003)     1004     6405 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_moving_average.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2555 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_warn_unordered_log_msg.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2147 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2518 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_post_monitoring_stats.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3975 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_safe_start_ema_throughput_measurement.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4977 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_stats_publisher.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2674 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_backup_throughput_measurement.py
+-rw-rw-r--   0 sovrin    (1003)     1004      201 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_monitor_attributes.py
+-rw-rw-r--   0 sovrin    (1003)     1004      251 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_monitor_reconnection.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1582 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_latency_measurement_class.py
+-rw-rw-r--   0 sovrin    (1003)     1004    12045 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_throughput_based_master_degradation_detection.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7793 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_request_time_tracker.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1692 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_system_stats.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1478 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_latency_median_avg.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1569 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_monitoring_params_with_zfn.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4488 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_acc_monitor_strategy.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4166 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_instance_change_with_Delta.py
+-rw-rw-r--   0 sovrin    (1003)     1004    17130 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_revival_spike_resistant_ema_throughput_measurement.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1435 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_no_check_if_no_new_requests.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1620 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_invalid_reqs_in_monitor.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1909 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_throughput_median_avg.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3262 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_instance_change_with_req_Lambda.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2902 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_EMALatencyMeasurementForAllClient.py
+-rw-rw-r--   0 sovrin    (1003)     1004      907 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_throughput.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1202 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/monitoring/test_avg_latency.py
+-rw-rw-r--   0 sovrin    (1003)     1004    38929 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_node.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/nodestack/
+-rw-rw-r--   0 sovrin    (1003)     1004     4460 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/nodestack/test_resend_stashed_msgs.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/nodestack/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      707 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/run_continuously.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4496 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_request.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/
+-rw-rw-r--   0 sovrin    (1003)     1004     2705 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_adding_stewards.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1984 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004      912 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_start_many_nodes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5600 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_get_txn_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1858 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_on_pool_membership_changes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1931 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_add_inactive_node_then_activate.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4790 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_add_node_with_invalid_data.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3256 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_change_ha_persists_post_nodes_restart.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4612 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_nodes_data_changed.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6676 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_nodes_with_pool_txns.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1353 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_add_node_with_not_unique_alias.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5422 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_add_stewards_and_client.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2938 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_txn_pool_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1846 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_demote_nonexisted.py
+-rw-rw-r--   0 sovrin    (1003)     1004    22272 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3047 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_suspend_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2053 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_add_node_with_invalid_key_proof.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4047 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_z_node_key_changed.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1784 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_client_with_pool_txns.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2595 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_nodes_ha_change_back.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2073 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_action_queue.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2324 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_config_req_handler.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/node/
+-rw-rw-r--   0 sovrin    (1003)     1004     2030 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node/test_api.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2390 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node/test_quota_control.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/node/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5929 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_node_genesis.py
+-rw-rw-r--   0 sovrin    (1003)     1004      920 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/greek.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/forced_request/
+-rw-rw-r--   0 sovrin    (1003)     1004     1269 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/forced_request/test_forced_request_validation.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/forced_request/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/persistence/
+-rw-rw-r--   0 sovrin    (1003)     1004      279 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/persistence/test_client_req_rep_store_file.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/persistence/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4277 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_stack.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2533 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_node_basic.py
+-rw-rw-r--   0 sovrin    (1003)     1004    50264 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2123 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_node_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5601 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/testable.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/
+-rw-rw-r--   0 sovrin    (1003)     1004     1338 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_view_change_happens_if_primary_is_slow_to_update_freshness.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1191 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_during_pool_ordering.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1591 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_in_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7355 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_replica_freshness.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2713 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3909 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_batch_updates_last_ordered.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1074 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10052 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_replica_freshness_checker.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2887 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_after_catchup.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2032 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_instance_changes_are_sent_continuosly.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6429 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1132 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_during_domain_ordering.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1266 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/freshness/test_view_change_happens_if_ordering_is_halted.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/
+-rw-rw-r--   0 sovrin    (1003)     1004     1170 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2154 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_commit_txns.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2275 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_discard_txns.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1789 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_append_txns.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1661 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_get_by_seqno.py
+-rw-rw-r--   0 sovrin    (1003)     1004      992 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_add_txns.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      928 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_append_txns_result.py
+-rw-rw-r--   0 sovrin    (1003)     1004      699 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledgers.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1341 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_get_last_txn.py
+-rw-rw-r--   0 sovrin    (1003)     1004      358 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_reset_uncommitted.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/propagate/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/propagate/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2511 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/propagate/test_propagate_recvd_after_request.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1311 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/propagate/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1947 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/propagate/test_propagate_recvd_before_request.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/script/
+-rw-rw-r--   0 sovrin    (1003)     1004     7179 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/script/test_bootstrap_test_node.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/script/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1083 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/script/test_change_primary_node_ha.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1051 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/script/test_change_non_primary_node_ha.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2850 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/script/helper.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/
+-rw-rw-r--   0 sovrin    (1003)     1004     1516 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2725 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_propagate_primary_after_primary_restart_view_0.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3894 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_new_node_accepts_chosen_primary.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3896 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_promotion_leads_to_primary_inconsistency.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2010 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection_pool_txn.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4736 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_reconnect_primary_and_not_primary.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2851 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_propagate_primary_after_primary_restart_view_1.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4103 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2878 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_catchup_multiple_rounds.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3255 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_add_node_with_f_changed.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3801 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_recover_more_than_f_failure.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2904 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_pool_restart.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4591 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection_after_demoted_node_promotion.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3188 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_view_changes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3299 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_add_node_to_pool_with_large_ppseqno.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4317 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_catchup_needed_check.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2669 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_selection_f_plus_one_quorum.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2909 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_recover_primary_no_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2246 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4225 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_catchup_after_view_change.py
+-rw-rw-r--   0 sovrin    (1003)     1004      125 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection_routes.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3284 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_recover_after_demoted.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2551 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_promotion.py
+-rw-rw-r--   0 sovrin    (1003)     1004    17778 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selector.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1177 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_ledger_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5253 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_util.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/
+-rw-rw-r--   0 sovrin    (1003)     1004      374 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10961 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/test_notifier_plugin_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      225 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/helper.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/stats_consumer/
+-rw-rw-r--   0 sovrin    (1003)     1004     3280 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/stats_consumer/plugin_stats_consumer.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/stats_consumer/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/
+-rw-rw-r--   0 sovrin    (1003)     1004     2795 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2982 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_request_digest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5050 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_plugin_request_handling.py
+-rw-rw-r--   0 sovrin    (1003)     1004      269 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/config.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1309 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_freshness_during_ordering.py
+-rw-rw-r--   0 sovrin    (1003)     1004      667 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2734 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_catchup.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/
+-rw-rw-r--   0 sovrin    (1003)     1004      962 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/auction_start_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2087 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/abstract_auction_req_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1163 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/get_bal_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1962 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/place_bid_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      761 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/auction_end_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004      869 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/storage.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1513 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_freshness.py
+-rw-rw-r--   0 sovrin    (1003)     1004      616 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/transactions.py
+-rw-rw-r--   0 sovrin    (1003)     1004      303 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/constants.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3063 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/main.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/batch_handlers/
+-rw-rw-r--   0 sovrin    (1003)     1004      604 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/batch_handlers/auction_batch_handler.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/batch_handlers/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      760 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2077 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_plugin_basic.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/transactions/
+-rw-rw-r--   0 sovrin    (1003)     1004     5109 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/transactions/test_new_txn_format.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3510 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/transactions/test_txn_general_access_utils.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3342 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/transactions/test_txn_init_utils.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/transactions/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4766 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/transactions/test_req_to_txn.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/test/util/
+-rw-rw-r--   0 sovrin    (1003)     1004      267 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/util/test_common_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/util/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2338 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_state_regenerated_from_ledger.py
+-rw-rw-r--   0 sovrin    (1003)     1004     8833 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/waits.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1888 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/testing_utils.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4161 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/test/test_crypto.py
+-rw-rw-r--   0 sovrin    (1003)     1004       19 2019-08-28 15:07:59.000000 indy-plenum-1.9.2rc1/plenum/__version__.json
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/plenum/persistence/
+-rw-rw-r--   0 sovrin    (1003)     1004     2547 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/persistence/req_id_to_txn.py
+-rw-rw-r--   0 sovrin    (1003)     1004      192 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/persistence/util.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3631 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/persistence/db_hash_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/persistence/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      776 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/persistence/storage.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1457 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/persistence/client_txn_log.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1919 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/persistence/client_req_rep_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6831 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/plenum/persistence/client_req_rep_store_file.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/crypto/
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/crypto/bls/
+-rw-rw-r--   0 sovrin    (1003)     1004     1585 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/bls/bls_crypto.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4308 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/bls/bls_bft_replica.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1007 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/bls/bls_key_manager.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3141 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/bls/bls_factory.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/bls/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      667 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/bls/bls_bft.py
+-rw-rw-r--   0 sovrin    (1003)     1004      706 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/bls/bls_key_register.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4574 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/bls/bls_multi_signature.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/crypto/bls/indy_crypto/
+-rw-rw-r--   0 sovrin    (1003)     1004     5541 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/bls/indy_crypto/bls_crypto_indy_crypto.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/bls/indy_crypto/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/crypto/test/
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/crypto/test/bls/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/test/bls/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/crypto/test/bls/indy_crypto/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/test/bls/indy_crypto/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004    14224 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/test/bls/indy_crypto/test_bls_crypto_indy_crypto.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/test/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6591 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/crypto/test/test_multi_signature.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/state/
+-rw-rw-r--   0 sovrin    (1003)     1004     5789 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/pruning_state.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/state/db/
+-rw-rw-r--   0 sovrin    (1003)     1004     6513 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/db/refcount_db.py
+-rw-rw-r--   0 sovrin    (1003)     1004      808 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/db/persistent_db.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/db/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      309 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/db/db.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2056 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/state.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/state/test/
+-rw-rw-r--   0 sovrin    (1003)     1004      124 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/conftest.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/state/test/db/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/db/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      443 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/db/test_RefcountDB.py
+-rw-rw-r--   0 sovrin    (1003)     1004       38 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/bench.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4250 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/test_state_proof_verification.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/state/test/trie/
+-rw-rw-r--   0 sovrin    (1003)     1004      501 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/trie/test_api.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4518 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/trie/test_prefix_nodes.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/trie/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2468 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/trie/test_trie_values_at_different_roots.py
+-rw-rw-r--   0 sovrin    (1003)     1004    15601 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/trie/test_proof.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9360 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/test/test_pruning_state.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/state/trie/
+-rw-rw-r--   0 sovrin    (1003)     1004    39199 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/trie/pruning_trie.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/trie/__init__.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/state/util/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/util/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3414 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/util/fast_rlp.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9563 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/state/util/utils.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_zmq/
+-rw-rw-r--   0 sovrin    (1003)     1004     5909 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/remote.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2046 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/simple_zstack.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4908 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/kit_zstack.py
+-rw-rw-r--   0 sovrin    (1003)     1004     5566 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/util.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004    38159 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/zstack.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/stp_zmq/test/
+-rw-rw-r--   0 sovrin    (1003)     1004     6197 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_send_to_disconnected.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1118 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_kitzstack.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3654 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2446 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_quotas.py
+-rw-rw-r--   0 sovrin    (1003)     1004      405 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_utils.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1773 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_large_messages.py
+-rw-rw-r--   0 sovrin    (1003)     1004     7209 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_stashed_client_messages.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3525 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_node_to_node_quota.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2209 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_zstack_communication.py
+-rw-rw-r--   0 sovrin    (1003)     1004    12184 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_zstack.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1812 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_stashed_ping_pong.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4002 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4101 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_heartbeats.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4970 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/test/test_reconnect.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3648 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/authenticator.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4813 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/stp_zmq/client_message_provider.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/ledger/
+-rw-rw-r--   0 sovrin    (1003)     1004     4462 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/error.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2628 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/tree_hasher.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1295 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/util.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004    11169 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/compact_merkle_tree.py
+-rw-rw-r--   0 sovrin    (1003)     1004      577 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/immutable_store.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/ledger/hash_stores/
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/hash_stores/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4941 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/hash_stores/hash_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     4261 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/hash_stores/file_hash_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1137 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/hash_stores/memory_hash_store.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/ledger/genesis_txn/
+-rw-rw-r--   0 sovrin    (1003)     1004      547 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/genesis_txn/genesis_txn_file_util.py
+-rw-rw-r--   0 sovrin    (1003)     1004        0 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/genesis_txn/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004      304 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/genesis_txn/genesis_txn_initiator.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2039 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/genesis_txn/genesis_txn_initiator_from_file.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/ledger/test/
+-rw-rw-r--   0 sovrin    (1003)     1004     4400 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/conftest.py
+-rw-rw-r--   0 sovrin    (1003)     1004      425 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/test_utils.py
+-rw-rw-r--   0 sovrin    (1003)     1004     3308 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/test_init_genesis_txns.py
+-rw-rw-r--   0 sovrin    (1003)     1004     9504 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/test_merkle_proof.py
+-rw-rw-r--   0 sovrin    (1003)     1004      917 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/test_txn_persistence.py
+-rw-rw-r--   0 sovrin    (1003)     1004     2984 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/test_file_hash_store.py
+-rw-rw-r--   0 sovrin    (1003)     1004       87 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/__init__.py
+-rw-rw-r--   0 sovrin    (1003)     1004    26636 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/merkle_test.py
+-rw-rw-r--   0 sovrin    (1003)     1004       36 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/__main__.py
+-rw-rw-r--   0 sovrin    (1003)     1004     6847 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/helper.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1481 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/test_file_store_perf.py
+-rw-rw-r--   0 sovrin    (1003)     1004    12717 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/test/test_ledger.py
+-rw-rw-r--   0 sovrin    (1003)     1004     1585 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/merkle_tree.py
+-rw-rw-r--   0 sovrin    (1003)     1004    10417 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/ledger.py
+-rw-rw-r--   0 sovrin    (1003)     1004    11162 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/ledger/merkle_verifier.py
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/scripts/
+drwxr-xr-x   0 sovrin    (1003)     1004        0 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/scripts/process_logs/
+-rwxrwxr-x   0 sovrin    (1003)     1004    34001 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/process_logs/process_logs
+-rw-rw-r--   0 sovrin    (1003)     1004    11097 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/process_logs/process_logs.yml
+-rw-rw-r--   0 sovrin    (1003)     1004     2574 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/init_plenum_keys
+-rw-rw-r--   0 sovrin    (1003)     1004     1816 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/log_stats
+-rw-rw-r--   0 sovrin    (1003)     1004     3498 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/filter_log
+-rwxrwxr-x   0 sovrin    (1003)     1004     1620 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/start_plenum_node
+-rwxrwxr-x   0 sovrin    (1003)     1004     2346 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/gen_node
+-rwxrwxr-x   0 sovrin    (1003)     1004      820 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/export-gen-txns
+-rwxrwxr-x   0 sovrin    (1003)     1004     1532 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/get_keys
+-rwxrwxr-x   0 sovrin    (1003)     1004     1001 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/init_bls_keys
+-rwxrwxr-x   0 sovrin    (1003)     1004      606 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/generate_plenum_pool_transactions
+-rwxrwxr-x   0 sovrin    (1003)     1004     1489 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/gen_steward_key
+-rwxrwxr-x   0 sovrin    (1003)     1004      595 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/udp_receiver
+-rwxrwxr-x   0 sovrin    (1003)     1004     1061 2019-08-28 15:07:53.000000 indy-plenum-1.9.2rc1/scripts/udp_sender
+-rw-r--r--   0 sovrin    (1003)     1004      594 2019-08-28 15:08:00.000000 indy-plenum-1.9.2rc1/PKG-INFO
```

### Comparing `indy-plenum-1.9.2.dev879/storage/optimistic_kv_store.py` & `indy-plenum-1.9.2rc1/storage/optimistic_kv_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/kv_store_leveldb.py` & `indy-plenum-1.9.2rc1/storage/kv_store_leveldb.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/kv_store_rocksdb_int_keys.py` & `indy-plenum-1.9.2rc1/storage/kv_store_rocksdb_int_keys.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/kv_store_file.py` & `indy-plenum-1.9.2rc1/storage/kv_store_file.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/state_ts_store.py` & `indy-plenum-1.9.2rc1/storage/state_ts_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/binary_file_store.py` & `indy-plenum-1.9.2rc1/storage/binary_file_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/kv_store_single_file.py` & `indy-plenum-1.9.2rc1/storage/kv_store_single_file.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/kv_store_rocksdb.py` & `indy-plenum-1.9.2rc1/storage/kv_store_rocksdb.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/test/test_ts_store.py` & `indy-plenum-1.9.2rc1/storage/test/test_ts_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/test/conftest.py` & `indy-plenum-1.9.2rc1/storage/test/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/test/test_optimistic_kv_store.py` & `indy-plenum-1.9.2rc1/storage/test/test_optimistic_kv_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/test/test_kv_store_comparator.py` & `indy-plenum-1.9.2rc1/storage/test/test_kv_store_comparator.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/test/test_kv_storages_read_only.py` & `indy-plenum-1.9.2rc1/storage/test/test_kv_storages_read_only.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/test/test_chunked_file_store.py` & `indy-plenum-1.9.2rc1/storage/test/test_chunked_file_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/test/test_stores_equailty.py` & `indy-plenum-1.9.2rc1/storage/test/test_stores_equailty.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/test/test_kv_storage_get_equal_or_prev.py` & `indy-plenum-1.9.2rc1/storage/test/test_kv_storage_get_equal_or_prev.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/test/test_kv_storages.py` & `indy-plenum-1.9.2rc1/storage/test/test_kv_storages.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/kv_store_leveldb_int_keys.py` & `indy-plenum-1.9.2rc1/storage/kv_store_leveldb_int_keys.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/chunked_file_store.py` & `indy-plenum-1.9.2rc1/storage/chunked_file_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/directory_store.py` & `indy-plenum-1.9.2rc1/storage/directory_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/kv_store.py` & `indy-plenum-1.9.2rc1/storage/kv_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/kv_in_memory.py` & `indy-plenum-1.9.2rc1/storage/kv_in_memory.py`

 * *Files 5% similar despite different names*

```diff
@@ -8,15 +8,14 @@
 
 
 databases = {}
 
 
 class KeyValueStorageInMemory(KeyValueStorage):
     def __init__(self):
-        # TODO: Most probably this will need to be replaced by SortedDict
         self._dict = {}
 
     def get(self, key):
         if isinstance(key, str):
             key = key.encode()
         return self._dict[key]
```

### Comparing `indy-plenum-1.9.2.dev879/storage/helper.py` & `indy-plenum-1.9.2rc1/storage/helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -65,20 +65,20 @@
     if keyValueType == KeyValueStorageType.Leveldb:
         return KeyValueStorageLeveldbIntKeys(dataLocation, keyValueStorageName, open, read_only)
     if keyValueType == KeyValueStorageType.Rocksdb:
         return KeyValueStorageRocksdbIntKeys(dataLocation, keyValueStorageName, open, read_only, db_config)
     return initKeyValueStorage(keyValueType, dataLocation, keyValueStorageName, open, read_only, db_config, txn_serializer)
 
 
-def initHashStore(data_dir, name, config=None, read_only=False, hs_type=None) -> HashStore:
+def initHashStore(data_dir, name, config=None, read_only=False) -> HashStore:
     """
     Create and return a hashStore implementation based on configuration
     """
     config = config or getConfig()
-    hsConfig = hs_type if hs_type is not None else config.hashStore['type'].lower()
+    hsConfig = config.hashStore['type'].lower()
     if hsConfig == HS_FILE:
         return FileHashStore(dataDir=data_dir,
                              fileNamePrefix=name)
     elif hsConfig == HS_LEVELDB or hsConfig == HS_ROCKSDB:
         return DbHashStore(dataDir=data_dir,
                            fileNamePrefix=name,
                            db_type=hsConfig,
```

### Comparing `indy-plenum-1.9.2.dev879/storage/text_file_store.py` & `indy-plenum-1.9.2rc1/storage/text_file_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/storage/binary_serializer_based_file_store.py` & `indy-plenum-1.9.2rc1/storage/binary_serializer_based_file_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/README.md` & `indy-plenum-1.9.2rc1/README.md`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/loop/eventually.py` & `indy-plenum-1.9.2rc1/stp_core/loop/eventually.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/loop/startable.py` & `indy-plenum-1.9.2rc1/stp_core/loop/startable.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/loop/motor.py` & `indy-plenum-1.9.2rc1/stp_core/loop/motor.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/loop/looper.py` & `indy-plenum-1.9.2rc1/stp_core/loop/looper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/common/util.py` & `indy-plenum-1.9.2rc1/stp_core/common/util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/common/logging/CompressingFileHandler.py` & `indy-plenum-1.9.2rc1/stp_core/common/logging/CompressingFileHandler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/common/logging/handlers.py` & `indy-plenum-1.9.2rc1/stp_core/common/logging/handlers.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/common/log.py` & `indy-plenum-1.9.2rc1/stp_core/common/log.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/network/keep_in_touch.py` & `indy-plenum-1.9.2rc1/stp_core/network/keep_in_touch.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/network/exceptions.py` & `indy-plenum-1.9.2rc1/stp_core/network/exceptions.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/network/util.py` & `indy-plenum-1.9.2rc1/stp_core/network/util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/network/port_dispenser.py` & `indy-plenum-1.9.2rc1/stp_core/network/port_dispenser.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/network/network_interface.py` & `indy-plenum-1.9.2rc1/stp_core/network/network_interface.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/config.py` & `indy-plenum-1.9.2rc1/stp_core/config.py`

 * *Files 3% similar despite different names*

```diff
@@ -31,25 +31,23 @@
 NODE_TO_NODE_STACK_SIZE = 1024 * 1024
 CLIENT_TO_NODE_STACK_SIZE = 1024 * 1024
 
 # Zeromq configuration
 DEFAULT_LISTENER_SIZE = 20 * 1024
 DEFAULT_LISTENER_QUOTA = 100
 DEFAULT_SENDER_QUOTA = 100
-KEEPALIVE_INTVL = 1  # seconds
-KEEPALIVE_IDLE = 20  # seconds
+KEEPALIVE_INTVL = 1     # seconds
+KEEPALIVE_IDLE = 20     # seconds
 KEEPALIVE_CNT = 10
 MAX_SOCKETS = 16384 if sys.platform != 'win32' else None
 ENABLE_HEARTBEATS = False
-HEARTBEAT_FREQ = 5  # seconds
+HEARTBEAT_FREQ = 5      # seconds
 ZMQ_CLIENT_QUEUE_SIZE = 100  # messages (0 - no limit)
 ZMQ_NODE_QUEUE_SIZE = 20000  # messages (0 - no limit)
 ZMQ_STASH_TO_NOT_CONNECTED_QUEUE_SIZE = 10000
 PENDING_CLIENT_LIMIT = 100
 PENDING_MESSAGES_FOR_ONE_CLIENT_LIMIT = 100
 RESEND_CLIENT_MSG_TIMEOUT = 30
 REMOVE_CLIENT_MSG_TIMEOUT = 60 * 5
 
 # All messages exceeding the limit will be rejected without processing
 MSG_LEN_LIMIT = 128 * 1024
-
-MAX_WAIT_FOR_BIND_SUCCESS = 120  # seconds
```

### Comparing `indy-plenum-1.9.2.dev879/stp_core/ratchet.py` & `indy-plenum-1.9.2rc1/stp_core/ratchet.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/test/helper.py` & `indy-plenum-1.9.2rc1/stp_core/test/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/crypto/nacl_wrappers.py` & `indy-plenum-1.9.2rc1/stp_core/crypto/nacl_wrappers.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_core/crypto/util.py` & `indy-plenum-1.9.2rc1/stp_core/crypto/util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/exceptions.py` & `indy-plenum-1.9.2rc1/common/exceptions.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/test/test_msgpack_serializer.py` & `indy-plenum-1.9.2rc1/common/test/test_msgpack_serializer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/test/test_signing_serializer.py` & `indy-plenum-1.9.2rc1/common/test/test_signing_serializer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/test/version/test_version.py` & `indy-plenum-1.9.2rc1/common/test/version/test_version.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/test/test_compact_serializer.py` & `indy-plenum-1.9.2rc1/common/test/test_compact_serializer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/test/test_exceptions.py` & `indy-plenum-1.9.2rc1/common/test/test_exceptions.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/test/test_json_serializer.py` & `indy-plenum-1.9.2rc1/common/test/test_json_serializer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/test/test_fields.py` & `indy-plenum-1.9.2rc1/common/test/test_fields.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/serializers/compact_serializer.py` & `indy-plenum-1.9.2rc1/common/serializers/compact_serializer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/serializers/msgpack_serializer.py` & `indy-plenum-1.9.2rc1/common/serializers/msgpack_serializer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/serializers/serialization.py` & `indy-plenum-1.9.2rc1/common/serializers/serialization.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/serializers/field.py` & `indy-plenum-1.9.2rc1/common/serializers/field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/serializers/json_serializer.py` & `indy-plenum-1.9.2rc1/common/serializers/json_serializer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/serializers/signing_serializer.py` & `indy-plenum-1.9.2rc1/common/serializers/signing_serializer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/common/version.py` & `indy-plenum-1.9.2rc1/common/version.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/data/domain_transactions_local_genesis` & `indy-plenum-1.9.2rc1/data/domain_transactions_local_genesis`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/data/pool_transactions_sandbox_genesis` & `indy-plenum-1.9.2rc1/data/pool_transactions_sandbox_genesis`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/data/domain_transactions_sandbox_genesis` & `indy-plenum-1.9.2rc1/data/domain_transactions_sandbox_genesis`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/data/pool_transactions_local_genesis` & `indy-plenum-1.9.2rc1/data/pool_transactions_local_genesis`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/setup.py` & `indy-plenum-1.9.2rc1/setup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/indy_plenum.egg-info/SOURCES.txt` & `indy-plenum-1.9.2rc1/indy_plenum.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -61,15 +61,14 @@
 ledger/merkle_verifier.py
 ledger/tree_hasher.py
 ledger/util.py
 ledger/genesis_txn/__init__.py
 ledger/genesis_txn/genesis_txn_file_util.py
 ledger/genesis_txn/genesis_txn_initiator.py
 ledger/genesis_txn/genesis_txn_initiator_from_file.py
-ledger/genesis_txn/genesis_txn_initiator_from_mem.py
 ledger/hash_stores/__init__.py
 ledger/hash_stores/file_hash_store.py
 ledger/hash_stores/hash_store.py
 ledger/hash_stores/memory_hash_store.py
 ledger/test/__init__.py
 ledger/test/__main__.py
 ledger/test/conftest.py
@@ -107,14 +106,15 @@
 plenum/common/constants.py
 plenum/common/did_method.py
 plenum/common/error.py
 plenum/common/event_bus.py
 plenum/common/exceptions.py
 plenum/common/gc_trackers.py
 plenum/common/has_file_storage.py
+plenum/common/hook_manager.py
 plenum/common/init_util.py
 plenum/common/jsonpickle_util.py
 plenum/common/keygen_utils.py
 plenum/common/latency_measurements.py
 plenum/common/ledger.py
 plenum/common/ledger_info.py
 plenum/common/ledger_manager.py
@@ -128,15 +128,14 @@
 plenum/common/perf_util.py
 plenum/common/pkg_util.py
 plenum/common/plenum_protocol_version.py
 plenum/common/plugin_helper.py
 plenum/common/prepare_batch.py
 plenum/common/request.py
 plenum/common/roles.py
-plenum/common/router.py
 plenum/common/script_helper.py
 plenum/common/signer_did.py
 plenum/common/signer_simple.py
 plenum/common/stack_manager.py
 plenum/common/stacks.py
 plenum/common/startable.py
 plenum/common/stashing_deque.py
@@ -188,31 +187,31 @@
 plenum/server/client_authn.py
 plenum/server/database_manager.py
 plenum/server/future_primaries_batch_handler.py
 plenum/server/has_action_queue.py
 plenum/server/inconsistency_watchers.py
 plenum/server/instances.py
 plenum/server/last_sent_pp_store_helper.py
-plenum/server/ledgers_bootstrap.py
 plenum/server/message_handlers.py
 plenum/server/message_req_processor.py
 plenum/server/models.py
 plenum/server/monitor.py
 plenum/server/msg_filter.py
 plenum/server/node.py
 plenum/server/node_bootstrap.py
 plenum/server/notifier_plugin_manager.py
 plenum/server/plugin_loader.py
 plenum/server/pool_manager.py
+plenum/server/primary_decider.py
+plenum/server/primary_selector.py
 plenum/server/propagator.py
 plenum/server/quorums.py
 plenum/server/quota_control.py
 plenum/server/replica.py
 plenum/server/replica_freshness_checker.py
-plenum/server/replica_helper.py
 plenum/server/replica_stasher.py
 plenum/server/replica_validator.py
 plenum/server/replica_validator_enums.py
 plenum/server/replicas.py
 plenum/server/req_authenticator.py
 plenum/server/router.py
 plenum/server/stats_consumer.py
@@ -233,19 +232,15 @@
 plenum/server/catchup/node_catchup_data.py
 plenum/server/catchup/node_leecher_service.py
 plenum/server/catchup/seeder_service.py
 plenum/server/catchup/utils.py
 plenum/server/consensus/__init__.py
 plenum/server/consensus/checkpoint_service.py
 plenum/server/consensus/consensus_shared_data.py
-plenum/server/consensus/metrics_decorator.py
-plenum/server/consensus/msg_validator.py
 plenum/server/consensus/ordering_service.py
-plenum/server/consensus/ordering_service_msg_validator.py
-plenum/server/consensus/primary_selector.py
 plenum/server/consensus/replica_service.py
 plenum/server/consensus/view_change_service.py
 plenum/server/general_config/__init__.py
 plenum/server/general_config/ubuntu_platform_config.py
 plenum/server/general_config/windows_platform_config.py
 plenum/server/observer/__init__.py
 plenum/server/observer/observable.py
@@ -374,14 +369,16 @@
 plenum/test/batching_3pc/catch-up/test_catchup_during_3pc.py
 plenum/test/batching_3pc/catch-up/test_clearing_requests_after_catchup.py
 plenum/test/batching_3pc/catch-up/test_freeing_forwarded_not_preprepared_request.py
 plenum/test/batching_3pc/catch-up/test_freeing_forwarded_preprepared_request.py
 plenum/test/batching_3pc/catch-up/test_state_reverted_before_catchup.py
 plenum/test/blacklist/__init__.py
 plenum/test/blacklist/test_blacklist_client.py
+plenum/test/blacklist/test_blacklist_node_on_multiple_nominations.py
+plenum/test/blacklist/test_blacklist_node_on_multiple_primary_declarations.py
 plenum/test/bls/__init__.py
 plenum/test/bls/conftest.py
 plenum/test/bls/helper.py
 plenum/test/bls/test_add_bls_key.py
 plenum/test/bls/test_add_incorrect_bls_key.py
 plenum/test/bls/test_bls_bft_factory.py
 plenum/test/bls/test_bls_bft_replica.py
@@ -431,54 +428,44 @@
 plenum/test/client/__init__.py
 plenum/test/client/test_client.py
 plenum/test/client/test_protocol_version.py
 plenum/test/common/__init__.py
 plenum/test/common/test_config_util.py
 plenum/test/common/test_database_manager.py
 plenum/test/common/test_digest_validation.py
+plenum/test/common/test_hook_mananger.py
 plenum/test/common/test_parse_ledger.py
 plenum/test/common/test_pool_file_raises_descriptive_error.py
 plenum/test/common/test_prepare_batch.py
 plenum/test/common/test_random_string.py
 plenum/test/common/test_replicas_suspicious.py
 plenum/test/common/test_roles.py
 plenum/test/common/test_signers.py
 plenum/test/common/test_splitting_large_messages.py
 plenum/test/common/test_throttler.py
 plenum/test/common/test_transactions.py
 plenum/test/common/test_verifier.py
 plenum/test/consensus/__init__.py
 plenum/test/consensus/conftest.py
-plenum/test/consensus/helper.py
 plenum/test/consensus/test_consensus_data_provider.py
 plenum/test/consensus/test_consensus_dp_batches.py
 plenum/test/consensus/test_three_pc_validator.py
-plenum/test/consensus/checkpoint_service/__init__.py
-plenum/test/consensus/checkpoint_service/conftest.py
-plenum/test/consensus/checkpoint_service/test_checkpoint_service_on_view_change.py
-plenum/test/consensus/checkpoint_service/test_checkpoint_service_unit.py
-plenum/test/consensus/checkpoint_service/test_checkpoint_validation.py
-plenum/test/consensus/checkpoint_service/test_update_watermarks_api.py
 plenum/test/consensus/order_service/__init__.py
 plenum/test/consensus/order_service/conftest.py
 plenum/test/consensus/order_service/helper.py
 plenum/test/consensus/order_service/test_buffers_cleaning.py
 plenum/test/consensus/order_service/test_can_order.py
 plenum/test/consensus/order_service/test_can_send_3pc.py
 plenum/test/consensus/order_service/test_orderer_api.py
-plenum/test/consensus/order_service/test_ordering_msg_validator.py
-plenum/test/consensus/order_service/test_ordering_process_commit.py
 plenum/test/consensus/order_service/test_ordering_process_prepare.py
 plenum/test/consensus/order_service/test_ordering_process_preprepare.py
-plenum/test/consensus/order_service/test_ordering_service_on_view_change.py
 plenum/test/consensus/order_service/test_pp_obsolescence.py
 plenum/test/consensus/view_change/__init__.py
 plenum/test/consensus/view_change/helper.py
 plenum/test/consensus/view_change/test_new_view_builder.py
-plenum/test/consensus/view_change/test_primary_selector.py
 plenum/test/consensus/view_change/test_sim_view_change.py
 plenum/test/consensus/view_change/test_view_change_msg_creation.py
 plenum/test/consensus/view_change/test_view_change_service.py
 plenum/test/forced_request/__init__.py
 plenum/test/forced_request/test_forced_request_validation.py
 plenum/test/freshness/__init__.py
 plenum/test/freshness/helper.py
@@ -511,15 +498,14 @@
 plenum/test/input_validation/test_message_base.py
 plenum/test/input_validation/test_message_factory.py
 plenum/test/input_validation/test_message_serialization.py
 plenum/test/input_validation/test_strict_schema.py
 plenum/test/input_validation/utils.py
 plenum/test/input_validation/fields_validation/__init__.py
 plenum/test/input_validation/fields_validation/test_base58_field.py
-plenum/test/input_validation/fields_validation/test_batch_id_field.py
 plenum/test/input_validation/fields_validation/test_bls_multisig_field.py
 plenum/test/input_validation/fields_validation/test_bls_multisig_value_field.py
 plenum/test/input_validation/fields_validation/test_bool_field.py
 plenum/test/input_validation/fields_validation/test_fixed_length_string_field.py
 plenum/test/input_validation/fields_validation/test_hex_field.py
 plenum/test/input_validation/fields_validation/test_identifier_field.py
 plenum/test/input_validation/fields_validation/test_iterable_field.py
@@ -547,22 +533,22 @@
 plenum/test/input_validation/message_validation/test_checkpoint_message.py
 plenum/test/input_validation/message_validation/test_client_message.py
 plenum/test/input_validation/message_validation/test_commit_message.py
 plenum/test/input_validation/message_validation/test_consistencyproof_message.py
 plenum/test/input_validation/message_validation/test_currentstate_message.py
 plenum/test/input_validation/message_validation/test_instanceChange_message.py
 plenum/test/input_validation/message_validation/test_ledgerstatus_message.py
-plenum/test/input_validation/message_validation/test_new_view_message.py
+plenum/test/input_validation/message_validation/test_nomination_message.py
 plenum/test/input_validation/message_validation/test_observed_data.py
 plenum/test/input_validation/message_validation/test_ordered_message.py
 plenum/test/input_validation/message_validation/test_prepare_message.py
 plenum/test/input_validation/message_validation/test_preprepare_message.py
+plenum/test/input_validation/message_validation/test_primary_message.py
 plenum/test/input_validation/message_validation/test_propagate_message.py
-plenum/test/input_validation/message_validation/test_view_change_message.py
-plenum/test/input_validation/message_validation/test_view_change_message_ack.py
+plenum/test/input_validation/message_validation/test_reelection_message.py
 plenum/test/input_validation/message_validation/test_viewchangedone_messsage.py
 plenum/test/instances/__init__.py
 plenum/test/instances/helper.py
 plenum/test/instances/test_instance_cannot_become_active_with_less_than_four_servers.py
 plenum/test/instances/test_msgs_from_slow_instances.py
 plenum/test/instances/test_multiple_commit.py
 plenum/test/instances/test_multiple_instance_change_msgs.py
@@ -652,14 +638,15 @@
 plenum/test/node_catchup/test_node_catchup_after_disconnect.py
 plenum/test/node_catchup/test_node_catchup_after_restart_after_txns.py
 plenum/test/node_catchup/test_node_catchup_after_restart_no_txns.py
 plenum/test/node_catchup/test_node_catchup_and_view_change_after_start.py
 plenum/test/node_catchup/test_node_catchup_causes_no_desync.py
 plenum/test/node_catchup/test_node_catchup_when_3_not_primary_node_restarted.py
 plenum/test/node_catchup/test_node_catchup_with_connection_problem.py
+plenum/test/node_catchup/test_node_catchup_with_new_ls_form.py
 plenum/test/node_catchup/test_node_ledger_manager.py
 plenum/test/node_catchup/test_node_reject_invalid_txn_during_catchup.py
 plenum/test/node_catchup/test_node_request_consistency_proof.py
 plenum/test/node_catchup/test_node_request_missing_transactions.py
 plenum/test/node_catchup/test_not_set_H_as_maxsize_for_backup_if_is_primary.py
 plenum/test/node_catchup/test_post_genesis_txn_from_catchup_added_to_ledger.py
 plenum/test/node_catchup/test_process_catchup_replies.py
@@ -847,23 +834,24 @@
 plenum/test/primary_selection/test_new_node_accepts_chosen_primary.py
 plenum/test/primary_selection/test_primary_selection.py
 plenum/test/primary_selection/test_primary_selection_after_demoted_node_promotion.py
 plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_pool_restart.py
 plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_promotion.py
 plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_view_changes.py
 plenum/test/primary_selection/test_primary_selection_pool_txn.py
+plenum/test/primary_selection/test_primary_selection_routes.py
+plenum/test/primary_selection/test_primary_selector.py
 plenum/test/primary_selection/test_promotion_leads_to_primary_inconsistency.py
 plenum/test/primary_selection/test_propagate_primary_after_primary_restart_view_0.py
 plenum/test/primary_selection/test_propagate_primary_after_primary_restart_view_1.py
 plenum/test/primary_selection/test_reconnect_primary_and_not_primary.py
 plenum/test/primary_selection/test_recover_after_demoted.py
 plenum/test/primary_selection/test_recover_more_than_f_failure.py
 plenum/test/primary_selection/test_recover_primary_no_view_change.py
 plenum/test/primary_selection/test_selection_f_plus_one_quorum.py
-plenum/test/primary_selection/test_view_changer_primary_selection.py
 plenum/test/propagate/__init__.py
 plenum/test/propagate/helper.py
 plenum/test/propagate/test_propagate_recvd_after_request.py
 plenum/test/propagate/test_propagate_recvd_before_request.py
 plenum/test/recorder/__init__.py
 plenum/test/recorder/conftest.py
 plenum/test/recorder/helper.py
@@ -880,23 +868,26 @@
 plenum/test/replica/helper.py
 plenum/test/replica/test_3pc_messages_validation.py
 plenum/test/replica/test_api.py
 plenum/test/replica/test_bitmask_apply.py
 plenum/test/replica/test_buffers_cleaning.py
 plenum/test/replica/test_catchup_after_replica_addition.py
 plenum/test/replica/test_catchup_after_replica_removing.py
-plenum/test/replica/test_consensus_dp_batches.py
+plenum/test/replica/test_consensus_data_helper.py
 plenum/test/replica/test_create_3pc_batch.py
 plenum/test/replica/test_get_last_timestamp_from_state.py
 plenum/test/replica/test_instance_faulty_processor.py
 plenum/test/replica/test_max_3pc_batches_in_flight.py
 plenum/test/replica/test_monitor_reset_after_replica_addition.py
 plenum/test/replica/test_ordered_tracker.py
 plenum/test/replica/test_primary_marked_suspicious_for_sending_prepare.py
+plenum/test/replica/test_process_prepare.py
+plenum/test/replica/test_process_preprepare.py
 plenum/test/replica/test_replica_3pc_validation.py
+plenum/test/replica/test_replica_checkpoint_validation.py
 plenum/test/replica/test_replica_clear_collections_after_view_change.py
 plenum/test/replica/test_replica_received_preprepare_with_unknown_request.py
 plenum/test/replica/test_replica_reject_same_pre_prepare.py
 plenum/test/replica/test_replicas_primary_names.py
 plenum/test/replica/test_revert_from_malicious.py
 plenum/test/replica/test_update_watermarks_api.py
 plenum/test/replica/stashing/__init__.py
@@ -1031,15 +1022,14 @@
 plenum/test/view_change/test_no_view_change_while_catchup.py
 plenum/test/view_change/test_node_detecting_lag_from_view_change_messages.py
 plenum/test/view_change/test_old_instance_change_discarding.py
 plenum/test/view_change/test_pp_seq_no_starts_from_1.py
 plenum/test/view_change/test_pre_vc_strategy_3PC_msgs.py
 plenum/test/view_change/test_prepare_in_queue_before_vc.py
 plenum/test/view_change/test_queueing_req_from_future_view.py
-plenum/test/view_change/test_re_order_pre_prepares.py
 plenum/test/view_change/test_resend_inst_ch_in_progress_v_ch.py
 plenum/test/view_change/test_resend_instance_change_messages.py
 plenum/test/view_change/test_reset_monitor_after_view_change.py
 plenum/test/view_change/test_restarted_node_not_complete_vc_before_others.py
 plenum/test/view_change/test_reverted_unordered.py
 plenum/test/view_change/test_select_primary_after_removed_backup.py
 plenum/test/view_change/test_start_view_change_ts_set.py
@@ -1097,15 +1087,14 @@
 plenum/test/wallet/__init__.py
 plenum/test/wallet/test_wallet.py
 plenum/test/wallet/test_wallet_storage_helper.py
 plenum/test/watermarks/__init__.py
 plenum/test/watermarks/test_watermarks_after_view_change.py
 plenum/test/zstack_tests/__init__.py
 plenum/test/zstack_tests/test_clientstack_restart_trigger.py
-plenum/test/zstack_tests/test_restart_client_stack.py
 plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_2_of_4_nodes.py
 plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_3_of_4_nodes.py
 plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_4_of_4_nodes.py
 plenum/test/zstack_tests/test_send_client_msgs_with_delay_reqs.py
 plenum/test/zstack_tests/test_send_too_many_reqs.py
 plenum/test/zstack_tests/test_zstack_reconnection.py
 scripts/export-gen-txns
@@ -1129,15 +1118,14 @@
 state/db/db.py
 state/db/persistent_db.py
 state/db/refcount_db.py
 state/test/__init__.py
 state/test/bench.py
 state/test/conftest.py
 state/test/test_pruning_state.py
-state/test/test_remove_from_trie.py
 state/test/test_state_proof_verification.py
 state/test/db/__init__.py
 state/test/db/test_RefcountDB.py
 state/test/trie/__init__.py
 state/test/trie/test_api.py
 state/test/trie/test_prefix_nodes.py
 state/test/trie/test_proof.py
```

### Comparing `indy-plenum-1.9.2.dev879/indy_plenum.egg-info/requires.txt` & `indy-plenum-1.9.2rc1/indy_plenum.egg-info/requires.txt`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/indy_plenum.egg-info/PKG-INFO` & `indy-plenum-1.9.2rc1/indy_plenum.egg-info/PKG-INFO`

 * *Files 19% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 Metadata-Version: 2.1
 Name: indy-plenum
-Version: 1.9.2.dev879
+Version: 1.9.2rc1
 Summary: Plenum Byzantine Fault Tolerant Protocol
 Home-page: https://github.com/hyperledger/indy-plenum
 Author: Hyperledger
 Author-email: hyperledger-indy@lists.hyperledger.org
 Maintainer: Hyperledger
 Maintainer-email: hyperledger-indy@lists.hyperledger.org
 License: Apache 2.0
-Download-URL: https://github.com/hyperledger/indy-plenum/tarball/1.9.2.dev879
+Download-URL: https://github.com/hyperledger/indy-plenum/tarball/1.9.2rc1
 Description: Plenum Byzantine Fault Tolerant Protocol
 Keywords: Byzantine Fault Tolerant Plenum
 Platform: UNKNOWN
-Provides-Extra: stats
-Provides-Extra: benchmark
 Provides-Extra: tests
+Provides-Extra: benchmark
+Provides-Extra: stats
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/recorder/simple_zstack_with_silencer.py` & `indy-plenum-1.9.2rc1/plenum/recorder/simple_zstack_with_silencer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/recorder/combined_recorder.py` & `indy-plenum-1.9.2rc1/plenum/recorder/combined_recorder.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/recorder/replayer.py` & `indy-plenum-1.9.2rc1/plenum/recorder/replayer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/recorder/recorder.py` & `indy-plenum-1.9.2rc1/plenum/recorder/recorder.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/recorder/replayable_node.py` & `indy-plenum-1.9.2rc1/plenum/recorder/replayable_node.py`

 * *Files 3% similar despite different names*

```diff
@@ -32,22 +32,22 @@
                 # No time for this PRE-PREPARE since a PRE-PREPARE with
                 # (view_no, pp_seq_no) was not sent during normal execution
                 return valid_reqs, invalid_reqs, rejects, tm
 
             req_ids, discarded = self.sent_pps[(view_no, pp_seq_no)][1:]
             fin_reqs = {}
             for key in req_ids:
-                if key not in self._ordering_service.requestQueues[ledger_id]:
+                if key not in self.requestQueues[ledger_id]:
                     # Request not available yet
                     return valid_reqs, invalid_reqs, rejects, tm
                 fin_req = self.requests[key].finalised
                 fin_reqs[key] = fin_req
 
             for key in req_ids:
-                self._ordering_service.requestQueues[ledger_id].remove(key)
+                self.requestQueues[ledger_id].remove(key)
 
             # Not entirely accurate as in the real execution invalid reqs
             # are interleaved with valid reqs but since invalid reqs are
             # never applied as dynamic validation is a read only
             # operation, it is functionally correct.
             # Can be fixed by capturing the exact order
             idx = 0
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/recorder/simple_zstack_with_recorder.py` & `indy-plenum-1.9.2rc1/plenum/recorder/simple_zstack_with_recorder.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/database_manager.py` & `indy-plenum-1.9.2rc1/plenum/server/database_manager.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 from typing import Dict, Optional
 
 from common.exceptions import LogicError
-from common.serializers.serialization import state_roots_serializer
 from plenum.common.constants import BLS_LABEL, TS_LABEL, IDR_CACHE_LABEL, ATTRIB_LABEL, SEQ_NO_DB_LABEL
 from plenum.common.ledger import Ledger
 from state.state import State
 
 
 class DatabaseManager():
     def __init__(self):
@@ -30,37 +29,19 @@
         return self.databases[lid]
 
     def get_ledger(self, lid):
         if lid not in self.databases:
             return None
         return self.databases[lid].ledger
 
-    def get_txn_root_hash(self, ledger_str, to_str=True):
-        ledger = self.get_ledger(ledger_str)
-        if ledger is None:
-            return None
-        root = ledger.uncommitted_root_hash
-        if to_str:
-            root = ledger.hashToStr(root)
-        return root
-
     def get_state(self, lid):
         if lid not in self.databases:
             return None
         return self.databases[lid].state
 
-    def get_state_root_hash(self, ledger_id, to_str=True, committed=False):
-        state = self.get_state(ledger_id)
-        if state is None:
-            return None
-        root = state.committedHeadHash if committed else state.headHash
-        if to_str:
-            root = state_roots_serializer.serialize(bytes(root))
-        return root
-
     def get_tracker(self, lid):
         if lid not in self.trackers:
             return None
         return self.trackers[lid]
 
     def register_new_store(self, label, store):
         if label in self.stores:
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/replica_stasher.py` & `indy-plenum-1.9.2rc1/plenum/server/replica_stasher.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/replica_helper.py` & `indy-plenum-1.9.2rc1/plenum/common/metrics_stats.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,146 +1,118 @@
-from _sha256 import sha256
-from collections import OrderedDict, defaultdict
-from enum import IntEnum, unique
-from typing import List
-
-from sortedcontainers import SortedListWithKey
-
-from common.exceptions import LogicError
-from plenum.common.messages.node_messages import PrePrepare, Checkpoint
-from plenum.server.consensus.consensus_shared_data import ConsensusSharedData, preprepare_to_batch_id
-
-PP_CHECK_NOT_FROM_PRIMARY = 0
-PP_CHECK_TO_PRIMARY = 1
-PP_CHECK_DUPLICATE = 2
-PP_CHECK_OLD = 3
-PP_CHECK_REQUEST_NOT_FINALIZED = 4
-PP_CHECK_NOT_NEXT = 5
-PP_CHECK_WRONG_TIME = 6
-PP_CHECK_INCORRECT_POOL_STATE_ROOT = 14
-
-PP_APPLY_REJECT_WRONG = 7
-PP_APPLY_WRONG_DIGEST = 8
-PP_APPLY_WRONG_STATE = 9
-PP_APPLY_ROOT_HASH_MISMATCH = 10
-PP_APPLY_HOOK_ERROR = 11
-PP_SUB_SEQ_NO_WRONG = 12
-PP_NOT_FINAL = 13
-PP_APPLY_AUDIT_HASH_MISMATCH = 15
-PP_REQUEST_ALREADY_ORDERED = 16
-
-
-@unique
-class TPCStat(IntEnum):  # TPC => Three-Phase Commit
-    ReqDigestRcvd = 0
-    PrePrepareSent = 1
-    PrePrepareRcvd = 2
-    PrepareRcvd = 3
-    PrepareSent = 4
-    CommitRcvd = 5
-    CommitSent = 6
-    OrderSent = 7
-
-
-class Stats:
-    def __init__(self, keys):
-        sort = sorted([k.value for k in keys])
-        self.stats = OrderedDict((s, 0) for s in sort)
-
-    def inc(self, key):
-        """
-        Increment the stat specified by key.
-        """
-        self.stats[key] += 1
+from collections import defaultdict
+from copy import deepcopy
+from datetime import datetime, timedelta
+from typing import Sequence, Union
 
-    def get(self, key):
-        return self.stats[key]
+from plenum.common.metrics_collector import MetricsName, KvStoreMetricsFormat
+from plenum.common.value_accumulator import ValueAccumulator
+from storage.kv_store import KeyValueStorage
 
-    def __repr__(self):
-        return str({TPCStat(k).name: v for k, v in self.stats.items()})
 
+def trunc_ts(ts: datetime, step: timedelta):
+    base = datetime.min.replace(year=2000)
+    step_s = step.total_seconds()
+    seconds = (ts - base).total_seconds()
+    seconds = int(seconds / step_s) * step_s
+    return (base + timedelta(seconds=seconds, milliseconds=500)).replace(microsecond=0)
 
-class IntervalList:
+
+class MetricsStatsFrame:
     def __init__(self):
-        self._intervals = []
+        self._stats = defaultdict(ValueAccumulator)
 
-    def __len__(self):
-        return sum(i[1] - i[0] + 1 for i in self._intervals)
+    def add(self, id: MetricsName, value: Union[float, ValueAccumulator]):
+        if isinstance(value, ValueAccumulator):
+            self._stats[id].merge(value)
+        else:
+            self._stats[id].add(value)
+
+    def get(self, id: MetricsName) -> ValueAccumulator:
+        return self._stats[id]
+
+    def merge(self, other):
+        for id, acc in other._stats.items():
+            self._stats[id].merge(acc)
 
     def __eq__(self, other):
-        if not isinstance(other, IntervalList):
+        if not isinstance(other, MetricsStatsFrame):
             return False
-        return self._intervals == other._intervals
-
-    def __contains__(self, item):
-        return any(i[0] <= item <= i[1] for i in self._intervals)
-
-    def add(self, item):
-        if len(self._intervals) == 0:
-            self._intervals.append([item, item])
-            return
-
-        if item < self._intervals[0][0] - 1:
-            self._intervals.insert(0, [item, item])
-            return
-
-        if item == self._intervals[0][0] - 1:
-            self._intervals[0][0] -= 1
-            return
-
-        if self._intervals[0][0] <= item <= self._intervals[0][1]:
-            return
-
-        for prev, next in zip(self._intervals, self._intervals[1:]):
-            if item == prev[1] + 1:
-                prev[1] += 1
-                if prev[1] == next[0] - 1:
-                    prev[1] = next[1]
-                    self._intervals.remove(next)
-                return
-
-            if prev[1] + 1 < item < next[0] - 1:
-                idx = self._intervals.index(next)
-                self._intervals.insert(idx, [item, item])
-                return
-
-            if item == next[0] - 1:
-                next[0] -= 1
-                return
-
-            if next[0] <= item <= next[1]:
-                return
-
-        if item == self._intervals[-1][1] + 1:
-            self._intervals[-1][1] += 1
-            return
-
-        self._intervals.append([item, item])
-
-
-class OrderedTracker:
-    def __init__(self):
-        self._batches = defaultdict(IntervalList)
-
-    def __len__(self):
-        return sum(len(il) for il in self._batches.values())
+        for k in set(self._stats.keys()).union(other._stats.keys()):
+            if self._stats[k] != other._stats[k]:
+                return False
+        return True
+
+
+class MetricsStats:
+    def __init__(self, timestep=timedelta(minutes=1)):
+        self._timestep = timestep
+        self._frames = defaultdict(MetricsStatsFrame)
+        self._total = None
+
+    def add(self, ts: datetime, name: MetricsName, value: Union[float, ValueAccumulator]):
+        ts = trunc_ts(ts, self._timestep)
+        self._frames[ts].add(name, value)
+        self._total = None
+
+    def frame(self, ts):
+        return self._frames[trunc_ts(ts, self._timestep)]
+
+    def frames(self):
+        return self._frames.items()
+
+    @property
+    def timestep(self):
+        return self._timestep
+
+    @property
+    def min_ts(self):
+        return min(k for k in self._frames.keys())
+
+    @property
+    def max_ts(self):
+        return max(k for k in self._frames.keys()) + self._timestep
+
+    @property
+    def total(self):
+        if self._total is None:
+            self._total = self.merge_all(list(self._frames.values()))
+        return self._total
 
     def __eq__(self, other):
-        if not isinstance(other, OrderedTracker):
+        if not isinstance(other, MetricsStats):
             return False
-        return self._batches == other._batches
-
-    def __contains__(self, item):
-        view_no, pp_seq_no = item
-        return pp_seq_no in self._batches[view_no]
-
-    def add(self, view_no, pp_seq_no):
-        self._batches[view_no].add(pp_seq_no)
-
-    def clear_below_view(self, view_no):
-        for v in list(self._batches.keys()):
-            if v < view_no:
-                del self._batches[v]
-
+        for k in set(self._frames.keys()).union(other._frames.keys()):
+            if self._frames[k] != other._frames[k]:
+                return False
+        return True
+
+    @staticmethod
+    def merge_all(frames: Sequence[MetricsStatsFrame]) -> MetricsStatsFrame:
+        count = len(frames)
+        if count == 0:
+            return MetricsStatsFrame()
+        if count == 1:
+            return deepcopy(frames[0])
+
+        count_2 = count // 2
+        lo = MetricsStats.merge_all(frames[:count_2])
+        hi = MetricsStats.merge_all(frames[count_2:])
+        lo.merge(hi)
+        return lo
+
+
+def load_metrics_from_kv_store(storage: KeyValueStorage,
+                               min_ts: datetime = None,
+                               max_ts: datetime = None,
+                               step: timedelta = timedelta(minutes=1)) -> MetricsStats:
+    result = MetricsStats(step)
+
+    start = KvStoreMetricsFormat.encode_key(min_ts, 0) if min_ts else None
+    for k, v in storage.iterator(start=start):
+        ev = KvStoreMetricsFormat.decode(k, v)
+        if ev is None:
+            continue
+        if max_ts is not None and ev.timestamp > max_ts:
+            break
+        result.add(ev.timestamp, ev.name, ev.value)
 
-def replica_batch_digest(reqs: List):
-    return sha256(b''.join([r.digest.encode() for r in reqs])).hexdigest()
+    return result
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/validator_info_tool.py` & `indy-plenum-1.9.2rc1/plenum/server/validator_info_tool.py`

 * *Files 0% similar despite different names*

```diff
@@ -428,17 +428,16 @@
         res = {}
         for replica in self._node.replicas.values():
             replica_stat = {}
             replica_stat["Primary"] = self._prepare_for_json(replica.primaryName)
             replica_stat["Watermarks"] = "{}:{}".format(replica.h, replica.H)
             replica_stat["Last_ordered_3PC"] = self._prepare_for_json(replica.last_ordered_3pc)
             stashed_txns = {}
-            # TODO: Rename? Remove?
-            stashed_txns["Stashed_checkpoints"] = self._prepare_for_json(len(replica._checkpointer._received_checkpoints))
-            stashed_txns["Stashed_PrePrepare"] = self._prepare_for_json(len(replica._ordering_service.prePreparesPendingPrevPP))
+            stashed_txns["Stashed_checkpoints"] = self._prepare_for_json(len(replica.stashedRecvdCheckpoints))
+            stashed_txns["Stashed_PrePrepare"] = self._prepare_for_json(len(replica.prePreparesPendingPrevPP))
             replica_stat["Stashed_txns"] = stashed_txns
             res[replica.name] = self._prepare_for_json(replica_stat)
         return res
 
     def _get_node_metrics(self):
         metrics = {}
         for metrica in self._node.monitor.metrics()[1:]:
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/blacklister.py` & `indy-plenum-1.9.2rc1/plenum/server/blacklister.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/replica_validator.py` & `indy-plenum-1.9.2rc1/plenum/server/replica_validator.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 import time
 
 from plenum.common.messages.node_messages import Commit
-from plenum.common.stashing_router import DISCARD, PROCESS
 from plenum.common.types import f
 from plenum.common.util import compare_3PC_keys
-from plenum.server.replica_validator_enums import INCORRECT_INSTANCE, ALREADY_ORDERED, FUTURE_VIEW, \
+from plenum.server.replica_validator_enums import DISCARD, INCORRECT_INSTANCE, PROCESS, ALREADY_ORDERED, FUTURE_VIEW, \
     GREATER_PREP_CERT, OLD_VIEW, CATCHING_UP, OUTSIDE_WATERMARKS, INCORRECT_PP_SEQ_NO, ALREADY_STABLE, STASH_WATERMARKS, \
     STASH_CATCH_UP, STASH_VIEW
 from stp_core.common.log import getlogger
 
 logger = getlogger()
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/quorums.py` & `indy-plenum-1.9.2rc1/plenum/server/quorums.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/replica_freshness_checker.py` & `indy-plenum-1.9.2rc1/plenum/server/replica_freshness_checker.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/catchup/cons_proof_service.py` & `indy-plenum-1.9.2rc1/plenum/server/catchup/cons_proof_service.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/catchup/catchup_rep_service.py` & `indy-plenum-1.9.2rc1/plenum/server/catchup/catchup_rep_service.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/catchup/ledger_leecher_service.py` & `indy-plenum-1.9.2rc1/plenum/server/catchup/ledger_leecher_service.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/catchup/node_catchup_data.py` & `indy-plenum-1.9.2rc1/plenum/server/catchup/node_catchup_data.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/catchup/seeder_service.py` & `indy-plenum-1.9.2rc1/plenum/server/catchup/seeder_service.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/catchup/node_leecher_service.py` & `indy-plenum-1.9.2rc1/plenum/server/catchup/node_leecher_service.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/catchup/utils.py` & `indy-plenum-1.9.2rc1/plenum/server/catchup/utils.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/client_authn.py` & `indy-plenum-1.9.2rc1/plenum/server/client_authn.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/replica.py` & `indy-plenum-1.9.2rc1/plenum/server/monitor.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,866 +1,843 @@
 import time
-from collections import deque, OrderedDict, defaultdict
-from enum import unique, IntEnum
-from functools import partial
-from hashlib import sha256
-from typing import List, Dict, Optional, Any, Set, Tuple, Callable, Iterable
-import itertools
-
-import math
-
-import sys
-
-import functools
-
-from common.exceptions import LogicError, PlenumValueError
-from common.serializers.serialization import serialize_msg_for_signing, state_roots_serializer, \
-    invalid_index_serializer
-from crypto.bls.bls_bft_replica import BlsBftReplica
-from orderedset import OrderedSet
+from datetime import datetime
+from statistics import mean
+from typing import Dict, Iterable, Optional
+from typing import List
+from typing import Tuple
+
+import psutil
 
 from plenum.common.config_util import getConfig
-from plenum.common.constants import THREE_PC_PREFIX, PREPREPARE, PREPARE, \
-    DOMAIN_LEDGER_ID, COMMIT, POOL_LEDGER_ID, AUDIT_LEDGER_ID, AUDIT_TXN_PP_SEQ_NO, AUDIT_TXN_VIEW_NO, \
-    AUDIT_TXN_PRIMARIES, TS_LABEL
-from plenum.common.event_bus import InternalBus, ExternalBus
-from plenum.common.exceptions import SuspiciousNode
-from plenum.common.message_processor import MessageProcessor
-from plenum.common.messages.internal_messages import NeedBackupCatchup, CheckpointStabilized, RaisedSuspicion
-from plenum.common.messages.message_base import MessageBase
-from plenum.common.messages.node_messages import Reject, Ordered, \
-    PrePrepare, Prepare, Commit, Checkpoint, ThreePhaseMsg, ThreePhaseKey
-from plenum.common.metrics_collector import NullMetricsCollector, MetricsCollector, MetricsName
-from plenum.common.request import Request, ReqKey
-from plenum.common.router import Subscription
-from plenum.common.stashing_router import StashingRouter
-from plenum.common.util import updateNamedTuple, compare_3PC_keys
-from plenum.server.consensus.checkpoint_service import CheckpointService
-from plenum.server.consensus.consensus_shared_data import ConsensusSharedData, preprepare_to_batch_id
-from plenum.server.consensus.ordering_service import OrderingService
-from plenum.server.has_action_queue import HasActionQueue
-from plenum.server.models import Commits, Prepares
-from plenum.server.replica_freshness_checker import FreshnessChecker
-from plenum.server.replica_helper import replica_batch_digest, TPCStat
-from plenum.server.replica_stasher import ReplicaStasher
-from plenum.server.replica_validator import ReplicaValidator
-from plenum.server.replica_validator_enums import STASH_VIEW, STASH_WATERMARKS, STASH_CATCH_UP
-from plenum.server.router import Router
-from sortedcontainers import SortedList, SortedListWithKey
+from plenum.common.constants import MONITORING_PREFIX
+from plenum.common.monitor_strategies import AccumulatingMonitorStrategy
 from stp_core.common.log import getlogger
+from plenum.common.types import EVENT_REQ_ORDERED, EVENT_NODE_STARTED, \
+    EVENT_PERIODIC_STATS_THROUGHPUT, PLUGIN_TYPE_STATS_CONSUMER, \
+    EVENT_VIEW_CHANGE, EVENT_PERIODIC_STATS_LATENCIES, \
+    EVENT_PERIODIC_STATS_NODES, EVENT_PERIODIC_STATS_TOTAL_REQUESTS, \
+    EVENT_PERIODIC_STATS_NODE_INFO, EVENT_PERIODIC_STATS_SYSTEM_PERFORMANCE_INFO
+from plenum.server.blacklister import Blacklister
+from plenum.server.has_action_queue import HasActionQueue
+from plenum.server.instances import Instances
+from plenum.server.notifier_plugin_manager import notifierPluginTriggerEvents, \
+    PluginManager
+from plenum.server.plugin.has_plugin_loader_helper import PluginLoaderHelper
+
+pluginManager = PluginManager()
+logger = getlogger()
+
+
+class RequestTimeTracker:
+    """
+    Request time tracking utility
+    """
+
+    class Request:
+        def __init__(self, timestamp, instances_ids):
+            self.ordered = dict()
+            self.timestamp = timestamp
+            for ins_id in instances_ids:
+                self.ordered[ins_id] = False
+
+            # True if request was unordered for too long and
+            # was handled by handlers on master replica
+            self.handled = False
+
+        def order(self, instId):
+            if instId in self.ordered:
+                self.ordered[instId] = True
+
+        def remove_instance(self, instId):
+            self.ordered.pop(instId, None)
+
+        @property
+        def is_ordered(self):
+            return self.ordered[0]
+
+        @property
+        def is_handled(self):
+            return self.handled
+
+        @property
+        def is_ordered_by_all(self):
+            return all(self.ordered.values())
+
+    def __init__(self, instances_ids):
+        self.instances_ids = instances_ids
+        self._requests = {}
+        self._unordered = set()
+        self._handled_unordered = set()
+
+    def __len__(self):
+        return len(self._requests)
+
+    def __contains__(self, item):
+        return item in self._requests
+
+    def started(self, key):
+        req = self._requests.get(key)
+        return req.timestamp if req is not None else None
+
+    def start(self, key, timestamp):
+        self._requests[key] = RequestTimeTracker.Request(timestamp, self.instances_ids)
+        self._unordered.add(key)
+
+    def order(self, instId, key, timestamp):
+        if key not in self._requests:
+            return 0
+        req = self._requests[key]
+        tto = timestamp - req.timestamp
+        req.order(instId)
+        if instId == 0:
+            self._handled_unordered.discard(key)
+            self._unordered.discard(key)
+        if req.is_ordered_by_all:
+            del self._requests[key]
+        return tto
+
+    def handle(self, key):
+        if key in self._requests:
+            self._requests[key].handled = True
+            self._handled_unordered.add(key)
+
+    def reset(self):
+        self._requests.clear()
+        self._unordered.clear()
+        self._handled_unordered.clear()
+
+    def unordered(self):
+        return self._unordered
+
+    def handled_unordered(self):
+        return self._handled_unordered
+
+    def unhandled_unordered(self):
+        return ((key, req.timestamp) for key, req in self._requests.items()
+                if not req.is_ordered and not req.is_handled)
+
+    def add_instance(self, inst_id):
+        self.instances_ids.add(inst_id)
+
+    def remove_instance(self, instId):
+        for req in self._requests.values():
+            req.remove_instance(instId)
+        keys_to_del = [key for key, req in self._requests.items() if req.is_ordered_by_all]
+        for key in keys_to_del:
+            self.force_req_drop(key)
+        self.instances_ids.remove(instId)
+
+    def force_req_drop(self, key):
+        if key in self._requests:
+            del self._requests[key]
+        self._unordered.discard(key)
+        self._handled_unordered.discard(key)
+
+
+class Monitor(HasActionQueue, PluginLoaderHelper):
+    """
+    Implementation of RBFT's monitoring mechanism.
+
+    The monitoring metrics are collected at the level of a node. Each node
+    monitors the performance of each instance. Throughput of requests and
+    latency per client request are measured.
+    """
+
+    def __init__(self, name: str, Delta: float, Lambda: float, Omega: float,
+                 instances: Instances, nodestack,
+                 blacklister: Blacklister, nodeInfo: Dict,
+                 notifierEventTriggeringConfig: Dict,
+                 pluginPaths: Iterable[str] = None,
+                 notifierEventsEnabled: bool = True):
+        self.name = name
+        self.instances = instances
+        self.nodestack = nodestack
+        self.blacklister = blacklister
+        self.nodeInfo = nodeInfo
+        self.notifierEventTriggeringConfig = notifierEventTriggeringConfig
+        self.notifierEventsEnabled = notifierEventsEnabled
+
+        self.Delta = Delta
+        self.Lambda = Lambda
+        self.Omega = Omega
+        self.statsConsumers = self.getPluginsByType(pluginPaths,
+                                                    PLUGIN_TYPE_STATS_CONSUMER)
+
+        self.config = getConfig()
+
+        # Number of ordered requests by each replica. The value at key `i` in
+        # the dict is a tuple of the number of ordered requests by replica and
+        # the time taken to order those requests by the replica of the `i`th
+        # protocol instance
+        self.numOrderedRequests = dict()  # type: Dict[int, Tuple[int, int]]
+
+        # Dict(instance_id, throughput) of throughputs for replicas. Key is a instId and value is a instance of
+        # ThroughputMeasurement class and provide throughputs evaluating mechanism
+        self.throughputs = dict()   # type: Dict[int, ThroughputMeasurement]
+
+        # Utility object for tracking requests order start and end
+        # TODO: Has very similar cleanup logic to propagator.Requests
+        self.requestTracker = RequestTimeTracker(instances.ids)
+
+        # Request latencies for the master protocol instances. Key of the
+        # dictionary is a tuple of client id and request id and the value is
+        # the time the master instance took for ordering it
+        self.masterReqLatencies = {}  # type: Dict[Tuple[str, int], float]
+
+        # Indicates that request latency in previous snapshot of master req
+        # latencies was too high
+        self.masterReqLatencyTooHigh = False
+
+        # Request latency(time taken to be ordered) for the client. The value
+        # at key `i` in the dict is the LatencyMeasurement object which accumulate
+        # average latency and total request for each client.
+        self.clientAvgReqLatencies = dict()  # type: Dict[int, LatencyMeasurement]
+
+        # TODO: Set this if this monitor belongs to a node which has primary
+        # of master. Will be used to set `totalRequests`
+        self.hasMasterPrimary = None
+
+        # Total requests that have been ordered since the node started
+        self.totalRequests = 0
+
+        self.started = datetime.utcnow().isoformat()
+
+        # attention: handlers will work over unordered request only once
+        self.unordered_requests_handlers = []  # type: List[Callable]
+
+        # Monitoring suspicious spikes in cluster throughput
+        self.clusterThroughputSpikeMonitorData = {
+            'value': 0,
+            'cnt': 0,
+            'accum': []
+        }
 
-import plenum.server.node
-
-LOG_TAGS = {
-    'PREPREPARE': {"tags": ["node-preprepare"]},
-    'PREPARE': {"tags": ["node-prepare"]},
-    'COMMIT': {"tags": ["node-commit"]},
-    'ORDERED': {"tags": ["node-ordered"]}
-}
-
-
-class Replica3PRouter(Router):
-    def __init__(self, replica, *args, **kwargs):
-        self.replica = replica
-        super().__init__(*args, *kwargs)
-
-    # noinspection PyCallingNonCallable
-    def handleSync(self, msg: Any) -> Any:
-        try:
-            super().handleSync(msg)
-        except SuspiciousNode as ex:
-            self.replica.report_suspicious_node(ex)
-
-
-def measure_replica_time(master_name: MetricsName, backup_name: MetricsName):
-    def decorator(f):
-        @functools.wraps(f)
-        def wrapper(self, *args, **kwargs):
-            metrics = self.metrics
-            if self.isMaster:
-                with metrics.measure_time(master_name):
-                    return f(self, *args, **kwargs)
-            else:
-                with metrics.measure_time(backup_name):
-                    return f(self, *args, **kwargs)
-
-        return wrapper
-
-    return decorator
-
-
-class Replica(HasActionQueue, MessageProcessor):
-    STASHED_CHECKPOINTS_BEFORE_CATCHUP = 1
-    HAS_NO_PRIMARY_WARN_THRESCHOLD = 10
+        psutil.cpu_percent(interval=None)
+        self.lastKnownTraffic = self.calculateTraffic()
 
-    def __init__(self, node: 'plenum.server.node.Node', instId: int,
-                 config=None,
-                 isMaster: bool = False,
-                 bls_bft_replica: BlsBftReplica = None,
-                 metrics: MetricsCollector = NullMetricsCollector(),
-                 get_current_time=None,
-                 get_time_for_3pc_batch=None):
-        """
-        Create a new replica.
-
-        :param node: Node on which this replica is located
-        :param instId: the id of the protocol instance the replica belongs to
-        :param isMaster: is this a replica of the master protocol instance
-        """
+        self.totalViewChanges = 0
+        self._lastPostedViewChange = 0
         HasActionQueue.__init__(self)
-        self.get_current_time = get_current_time or time.perf_counter
-        self.get_time_for_3pc_batch = get_time_for_3pc_batch or node.utc_epoch
-        # self.stats = Stats(TPCStat)
-        self.config = config or getConfig()
-        self.metrics = metrics
-        self.node = node
-        self.instId = instId
-        self.name = self.generateName(node.name, self.instId)
-        self.logger = getlogger(self.name)
-        self.validator = ReplicaValidator(self)
-
-        self.outBox = deque()
-        """
-        This queue is used by the replica to send messages to its node. Replica
-        puts messages that are consumed by its node
-        """
-
-        self.inBox = deque()
-        """
-        This queue is used by the replica to receive messages from its node.
-        Node puts messages that are consumed by the replica
-        """
-
-        self.inBoxStash = deque()
-        """
-        If messages need to go back on the queue, they go here temporarily and
-        are put back on the queue on a state change
-        """
-
-        self._is_master = isMaster
-
-        # Dictionary to keep track of the which replica was primary during each
-        # view. Key is the view no and value is the name of the primary
-        # replica during that view
-        self.primaryNames = OrderedDict()  # type: OrderedDict[int, str]
-
-        # Flag being used for preterm exit from the loop in the method
-        # `processStashedMsgsForNewWaterMarks`. See that method for details.
-        self.consumedAllStashedMsgs = True
 
-        self._freshness_checker = FreshnessChecker(freshness_timeout=self.config.STATE_FRESHNESS_UPDATE_INTERVAL)
-
-        self._bls_bft_replica = bls_bft_replica
-        self._state_root_serializer = state_roots_serializer
-
-        # Did we log a message about getting request while absence of primary
-        self.warned_no_primary = False
-
-        self._consensus_data = ConsensusSharedData(self.name,
-                                                   self.node.get_validators(),
-                                                   self.instId,
-                                                   self.isMaster)
-        self._internal_bus = InternalBus()
-        self._external_bus = ExternalBus(send_handler=self.send)
-        self.stasher = self._init_replica_stasher()
-        self._subscription = Subscription()
-        self._bootstrap_consensus_data()
-        self._subscribe_to_external_msgs()
-        self._subscribe_to_internal_msgs()
-        self._checkpointer = self._init_checkpoint_service()
-        self._ordering_service = self._init_ordering_service()
-        for ledger_id in self.ledger_ids:
-            self.register_ledger(ledger_id)
-
-    @property
-    def internal_bus(self):
-        return self._internal_bus
-
-    def cleanup(self):
-        # Aggregate all the currently forwarded requests
-        req_keys = set()
-        for msg in self.inBox:
-            if isinstance(msg, ReqKey):
-                req_keys.add(msg.digest)
-        for req_queue in self._ordering_service.requestQueues.values():
-            for req_key in req_queue:
-                req_keys.add(req_key)
-        for pp in self._ordering_service.sentPrePrepares.values():
-            for req_key in pp.reqIdr:
-                req_keys.add(req_key)
-        for pp in self._ordering_service.prePrepares.values():
-            for req_key in pp.reqIdr:
-                req_keys.add(req_key)
-
-        for req_key in req_keys:
-            if req_key in self.requests:
-                self.requests.ordered_by_replica(req_key)
-                self.requests.free(req_key)
-
-        self._ordering_service.cleanup()
-        self._checkpointer.cleanup()
-        self._subscription.unsubscribe_all()
-        self.stasher.unsubscribe_from_all()
-
-    @property
-    def external_bus(self):
-        return self._external_bus
-
-    @property
-    def requested_pre_prepares(self):
-        return self._ordering_service.requested_pre_prepares
-
-    @property
-    def requested_prepares(self):
-        return self._ordering_service.requested_prepares
-
-    @property
-    def isMaster(self):
-        return self._is_master
+        if self.config.SendMonitorStats:
+            self.startRepeating(self.sendPeriodicStats,
+                                self.config.DashboardUpdateFreq)
+
+        self.startRepeating(
+            self.checkPerformance,
+            self.config.notifierEventTriggeringConfig['clusterThroughputSpike']['freq'])
+
+        self.startRepeating(self.check_unordered, self.config.UnorderedCheckFreq)
+
+        if 'disable_view_change' in self.config.unsafe:
+            self.isMasterDegraded = lambda: False
+        if 'disable_monitor' in self.config.unsafe:
+            self.requestOrdered = lambda *args, **kwargs: {}
+            self.sendPeriodicStats = lambda: None
+            self.checkPerformance = lambda: None
+
+        self.latency_avg_for_backup_cls = self.config.LatencyAveragingStrategyClass
+        self.latency_measurement_cls = self.config.LatencyMeasurementCls
+        self.throughput_avg_strategy_cls = self.config.throughput_averaging_strategy_class
+        self.backup_throughput_avg_strategy_cls = self.config.backup_throughput_averaging_strategy_class
+
+        self.acc_monitor = None
+
+        if self.config.ACC_MONITOR_ENABLED:
+            self.acc_monitor = AccumulatingMonitorStrategy(
+                start_time=time.perf_counter(),
+                instances=instances.ids,
+                txn_delta_k=self.config.ACC_MONITOR_TXN_DELTA_K,
+                timeout=self.config.ACC_MONITOR_TIMEOUT,
+                input_rate_reaction_half_time=self.config.ACC_MONITOR_INPUT_RATE_REACTION_HALF_TIME)
 
-    @isMaster.setter
-    def isMaster(self, value):
-        self._is_master = value
-        self._consensus_data.is_master = value
-
-    @property
-    def requested_commits(self):
-        return self._ordering_service.requested_commits
-
-    def _bootstrap_consensus_data(self):
-        self._consensus_data.requests = self.requests
-        self._consensus_data.node_mode = self.node.mode
-        self._consensus_data.quorums = self.quorums
-
-    def set_primaries(self, primaries):
-        self._consensus_data.primaries = primaries
-
-    def _subscribe_to_external_msgs(self):
-        # self._subscription.subscribe(self._external_bus, ReqKey, self.readyFor3PC)
-        pass
-
-    def _subscribe_to_internal_msgs(self):
-        self._subscription.subscribe(self.internal_bus, Ordered, self._send_ordered)
-        self._subscription.subscribe(self.internal_bus, NeedBackupCatchup, self._caught_up_backup)
-        self._subscription.subscribe(self.internal_bus, CheckpointStabilized, self._cleanup_process)
-        self._subscription.subscribe(self.internal_bus, ReqKey, self.readyFor3PC)
-        self._subscription.subscribe(self.internal_bus, RaisedSuspicion, self._process_suspicious_node)
-
-    def register_ledger(self, ledger_id):
-        # Using ordered set since after ordering each PRE-PREPARE,
-        # the request key is removed, so fast lookup and removal of
-        # request key is needed. Need the collection to be ordered since
-        # the request key needs to be removed once its ordered
-        if ledger_id not in self._ordering_service.requestQueues:
-            self._ordering_service.requestQueues[ledger_id] = OrderedSet()
-        if ledger_id != AUDIT_LEDGER_ID:
-            self._freshness_checker.register_ledger(ledger_id=ledger_id,
-                                                    initial_time=self.get_time_for_3pc_batch())
-
-    def get_sent_preprepare(self, viewNo, ppSeqNo):
-        return self._ordering_service.get_sent_preprepare(viewNo, ppSeqNo)
-
-    def get_sent_prepare(self, viewNo, ppSeqNo):
-        return self._ordering_service.get_sent_prepare(viewNo, ppSeqNo)
-
-    def get_sent_commit(self, viewNo, ppSeqNo):
-        return self._ordering_service.get_sent_commit(viewNo, ppSeqNo)
-
-    @property
-    def last_prepared_before_view_change(self):
-        return self._consensus_data.legacy_last_prepared_before_view_change
-
-    @last_prepared_before_view_change.setter
-    def last_prepared_before_view_change(self, lst):
-        self._consensus_data.legacy_last_prepared_before_view_change = lst
-
-    @property
-    def h(self) -> int:
-        return self._consensus_data.low_watermark
-
-    @property
-    def H(self) -> int:
-        return self._consensus_data.high_watermark
-
-    @property
-    def last_ordered_3pc(self) -> tuple:
-        return self._consensus_data.last_ordered_3pc
-
-    @last_ordered_3pc.setter
-    def last_ordered_3pc(self, key3PC):
-        self._consensus_data.last_ordered_3pc = key3PC
-        self.logger.info('{} set last ordered as {}'.format(self, key3PC))
-
-    @property
-    def lastPrePrepareSeqNo(self):
-        return self._ordering_service._lastPrePrepareSeqNo
-
-    @lastPrePrepareSeqNo.setter
-    def lastPrePrepareSeqNo(self, n):
-        """
-        This will _lastPrePrepareSeqNo to values greater than its previous
-        values else it will not. To forcefully override as in case of `revert`,
-        directly set `self._lastPrePrepareSeqNo`
-        """
-        if n > self._ordering_service._lastPrePrepareSeqNo:
-            # ToDo: need to pass it into ordering service through ConsensusDataProvider
-            self._ordering_service._lastPrePrepareSeqNo = n
-        else:
-            self.logger.debug(
-                '{} cannot set lastPrePrepareSeqNo to {} as its '
-                'already {}'.format(
-                    self, n, self.lastPrePrepareSeqNo))
-
-    @property
-    def requests(self):
-        return self.node.requests
-
-    @property
-    def ledger_ids(self):
-        return self.node.ledger_ids
-
-    @property
-    def quorums(self):
-        return self.node.quorums
-
-    @property
-    def utc_epoch(self):
-        return self.node.utc_epoch()
+    def __repr__(self):
+        return self.name
 
-    @staticmethod
-    def generateName(nodeName: str, instId: int):
+    def metrics(self):
         """
-        Create and return the name for a replica using its nodeName and
-        instanceId.
-         Ex: Alpha:1
+        Calculate and return the metrics.
         """
-
-        if isinstance(nodeName, str):
-            # Because sometimes it is bytes (why?)
-            if ":" in nodeName:
-                # Because in some cases (for requested messages) it
-                # already has ':'. This should be fixed.
-                return nodeName
-        return "{}:{}".format(nodeName, instId)
+        masterThrp, backupThrp = self.getThroughputs(self.instances.masterId)
+        r = self.instance_throughput_ratio(self.instances.masterId)
+        m = [
+            ("{} Monitor metrics:".format(self), None),
+            ("Delta", self.Delta),
+            ("Lambda", self.Lambda),
+            ("Omega", self.Omega),
+            ("instances started", self.instances.started),
+            ("ordered request counts",
+             {i: r[0] for i, r in self.numOrderedRequests.items()}),
+            ("ordered request durations",
+             {i: r[1] for i, r in self.numOrderedRequests.items()}),
+            ("master request latencies", self.masterReqLatencies),
+            ("client avg request latencies", {i: self.getLatency(i)
+                                              for i in self.instances.ids}),
+            ("throughput", {i: self.getThroughput(i)
+                            for i in self.instances.ids}),
+            ("master throughput", masterThrp),
+            ("total requests", self.totalRequests),
+            ("avg backup throughput", backupThrp),
+            ("master throughput ratio", r)]
+        return m
+
+    @property
+    def prettymetrics(self) -> str:
+        """
+        Pretty printing for metrics
+        """
+        rendered = ["{}: {}".format(*m) for m in self.metrics()]
+        return "\n            ".join(rendered)
+
+    def calculateTraffic(self):
+        currNetwork = psutil.net_io_counters()
+        currNetwork = currNetwork.bytes_sent + currNetwork.bytes_recv
+        currNetwork /= 1024
+        return currNetwork
 
     @staticmethod
-    def getNodeName(replicaName: str):
-        return replicaName.split(":")[0]
-
-    @property
-    def isPrimary(self):
-        """
-        Is this node primary?
-
-        :return: True if this node is primary, False if not, None if primary status not known
-        """
-        return self._consensus_data.is_primary
-
-    @property
-    def hasPrimary(self):
-        return self.primaryName is not None
-
-    @property
-    def primaryName(self):
-        """
-        Name of the primary replica of this replica's instance
-
-        :return: Returns name if primary is known, None otherwise
-        """
-        return self._consensus_data.primary_name
-
-    @primaryName.setter
-    def primaryName(self, value: Optional[str]) -> None:
-        """
-        Set the value of isPrimary.
-
-        :param value: the value to set isPrimary to
-        """
-        if value is not None:
-            self.warned_no_primary = False
-        self.primaryNames[self.viewNo] = value
-        self.compact_primary_names()
-        if value != self._consensus_data.primary_name:
-            self._consensus_data.primary_name = value
-            self.logger.info("{} setting primaryName for view no {} to: {}".
-                             format(self, self.viewNo, value))
-            if value is None:
-                # Since the GC needs to happen after a primary has been
-                # decided.
-                return
-            self._gc_before_new_view()
-            if self._checkpointer.should_reset_watermarks_before_new_view():
-                self._checkpointer.reset_watermarks_before_new_view()
-                self._ordering_service._lastPrePrepareSeqNo = 0
-
-    def compact_primary_names(self):
-        min_allowed_view_no = self.viewNo - 1
-        views_to_remove = []
-        for view_no in self.primaryNames:
-            if view_no >= min_allowed_view_no:
-                break
-            views_to_remove.append(view_no)
-        for view_no in views_to_remove:
-            self.primaryNames.pop(view_no)
-
-    def primaryChanged(self, primaryName):
-        self._ordering_service.batches.clear()
-        if self.isMaster:
-            # Since there is no temporary state data structure and state root
-            # is explicitly set to correct value
-            for lid in self.ledger_ids:
-                try:
-                    ledger = self.node.getLedger(lid)
-                except KeyError:
-                    continue
-                ledger.reset_uncommitted()
-
-        self.primaryName = primaryName
-        self._setup_for_non_master_after_view_change(self.viewNo)
-
-    def on_view_change_start(self):
-        if self.isMaster:
-            lst = self._ordering_service.l_last_prepared_certificate_in_view()
-            self._consensus_data.legacy_last_prepared_before_view_change = lst
-            self.logger.info('{} setting last prepared for master to {}'.format(self, lst))
-
-    def on_view_change_done(self):
-        if self.isMaster:
-            self.last_prepared_before_view_change = None
-        self.stasher.process_all_stashed(STASH_VIEW)
-
-    def clear_requests_and_fix_last_ordered(self):
-        if self.isMaster:
+    def create_throughput_measurement(config, start_ts=None):
+        if start_ts is None:
+            start_ts = time.perf_counter()
+        tm = config.throughput_measurement_class(
+            **config.throughput_measurement_params)
+        tm.init_time(start_ts)
+        logger.trace("Creating throughput measurement class {} with parameters {} in start time {}"
+                     .format(str(config.throughput_measurement_class), str(config.throughput_measurement_params), start_ts))
+        return tm
+
+    def reset(self):
+        """
+        Reset the monitor. Sets all monitored values to defaults.
+        """
+        logger.debug("{}'s Monitor being reset".format(self))
+        instances_ids = self.instances.started.keys()
+        self.numOrderedRequests = {inst_id: (0, 0) for inst_id in instances_ids}
+        self.requestTracker.reset()
+        self.masterReqLatencies = {}
+        self.masterReqLatencyTooHigh = False
+        self.totalViewChanges += 1
+        self.lastKnownTraffic = self.calculateTraffic()
+        if self.acc_monitor:
+            self.acc_monitor.reset()
+        for i in instances_ids:
+            rm = self.create_throughput_measurement(self.config)
+            self.throughputs[i] = rm
+            lm = self.latency_measurement_cls(self.config)
+            self.clientAvgReqLatencies[i] = lm
+
+    def addInstance(self, inst_id):
+        """
+        Add one protocol instance for monitoring.
+        """
+        self.instances.add(inst_id)
+        self.requestTracker.add_instance(inst_id)
+        self.numOrderedRequests[inst_id] = (0, 0)
+        rm = self.create_throughput_measurement(self.config)
+
+        self.throughputs[inst_id] = rm
+        lm = self.latency_measurement_cls(self.config)
+        self.clientAvgReqLatencies[inst_id] = lm
+        if self.acc_monitor:
+            self.acc_monitor.add_instance(inst_id)
+
+    def removeInstance(self, inst_id):
+        if self.acc_monitor:
+            self.acc_monitor.remove_instance(inst_id)
+        if self.instances.count > 0:
+            self.instances.remove(inst_id)
+            self.requestTracker.remove_instance(inst_id)
+            self.numOrderedRequests.pop(inst_id, None)
+            self.clientAvgReqLatencies.pop(inst_id, None)
+            self.throughputs.pop(inst_id, None)
+
+    def requestOrdered(self, reqIdrs: List[str], instId: int,
+                       requests, byMaster: bool = False) -> Dict:
+        """
+        Measure the time taken for ordering of a request and return it. Monitor
+        might have been reset due to view change due to which this method
+        returns None
+        """
+        now = time.perf_counter()
+        if self.acc_monitor:
+            self.acc_monitor.update_time(now)
+        durations = {}
+        for key in reqIdrs:
+            if key not in self.requestTracker:
+                logger.debug("Got untracked ordered request with digest {}".
+                             format(key))
+                continue
+            if self.acc_monitor:
+                self.acc_monitor.request_ordered(key, instId)
+            if key in self.requestTracker.handled_unordered():
+                started = self.requestTracker.started(key)
+                logger.info('Consensus for ReqId: {} was achieved by {}:{} in {} seconds.'
+                            .format(key, self.name, instId, now - started))
+            duration = self.requestTracker.order(instId, key, now)
+            self.throughputs[instId].add_request(now)
+
+            if key in requests:
+                identifier = requests[key].request.identifier
+                self.clientAvgReqLatencies[instId].add_duration(identifier, duration)
+
+            durations[key] = duration
+
+        reqs, tm = self.numOrderedRequests[instId]
+        orderedNow = len(durations)
+        self.numOrderedRequests[instId] = (reqs + orderedNow,
+                                           tm + sum(durations.values()))
+
+        # TODO: Inefficient, as on every request a minimum of a large list is
+        # calculated
+        if min(r[0] for r in self.numOrderedRequests.values()) == (reqs + orderedNow):
+            # If these requests is ordered by the last instance then increment
+            # total requests, but why is this important, why cant is ordering
+            # by master not enough?
+            self.totalRequests += orderedNow
+            self.postOnReqOrdered()
+            if 0 == reqs:
+                self.postOnNodeStarted(self.started)
+
+        return durations
+
+    def requestUnOrdered(self, key: str):
+        """
+        Record the time at which request ordering started.
+        """
+        now = time.perf_counter()
+        if self.acc_monitor:
+            self.acc_monitor.update_time(now)
+            self.acc_monitor.request_received(key)
+        self.requestTracker.start(key, now)
+
+    def check_unordered(self):
+        now = time.perf_counter()
+        new_unordereds = [(req, now - started) for req, started in self.requestTracker.unhandled_unordered()
+                          if now - started > self.config.UnorderedCheckFreq]
+        if len(new_unordereds) == 0:
             return
-        reqs_for_remove = []
-        for req in self.requests.values():
-            ledger_id, seq_no = self.node.seqNoDB.get_by_payload_digest(req.request.payload_digest)
-            if seq_no is not None:
-                reqs_for_remove.append((req.request.digest, ledger_id, seq_no))
-        for key, ledger_id, seq_no in reqs_for_remove:
-            self.requests.ordered_by_replica(key)
-            self.requests.free(key)
-            self._ordering_service.requestQueues[int(ledger_id)].discard(key)
-        master_last_ordered_3pc = self.node.master_replica.last_ordered_3pc
-        if compare_3PC_keys(master_last_ordered_3pc, self.last_ordered_3pc) < 0 \
-                and self.isPrimary is False:
-            self.last_ordered_3pc = master_last_ordered_3pc
-
-    def on_propagate_primary_done(self):
-        if self.isMaster:
-            # if this is a Primary that is re-connected (that is view change is not actually changed,
-            # we just propagate it, then make sure that we did't break the sequence
-            # of ppSeqNo
-            self._checkpointer.update_watermark_from_3pc()
-            if self.isPrimary and (self.last_ordered_3pc[0] == self.viewNo):
-                self.lastPrePrepareSeqNo = self.last_ordered_3pc[1]
-        elif not self.isPrimary:
-            self._checkpointer.set_watermarks(low_watermark=0,
-                                              high_watermark=sys.maxsize)
-
-    def get_lowest_probable_prepared_certificate_in_view(
-            self, view_no) -> Optional[int]:
-        """
-        Return lowest pp_seq_no of the view for which can be prepared but
-        choose from unprocessed PRE-PREPAREs and PREPAREs.
-        """
-        # TODO: Naive implementation, dont need to iterate over the complete
-        # data structures, fix this later
-        seq_no_pp = SortedList()  # pp_seq_no of PRE-PREPAREs
-        # pp_seq_no of PREPAREs with count of PREPAREs for each
-        seq_no_p = set()
-
-        for (v, p) in self._ordering_service.prePreparesPendingPrevPP:
-            if v == view_no:
-                seq_no_pp.add(p)
-            if v > view_no:
-                break
-
-        for (v, p), pr in self._ordering_service.preparesWaitingForPrePrepare.items():
-            if v == view_no and len(pr) >= self.quorums.prepare.value:
-                seq_no_p.add(p)
-
-        for n in seq_no_pp:
-            if n in seq_no_p:
-                return n
-        return None
-
-    def _setup_for_non_master_after_view_change(self, current_view):
-        if not self.isMaster:
-            for v in list(self.stashed_out_of_order_commits.keys()):
-                if v < current_view:
-                    self.stashed_out_of_order_commits.pop(v)
-
-    def is_primary_in_view(self, viewNo: int) -> Optional[bool]:
-        """
-        Return whether this replica was primary in the given view
-        """
-        if viewNo not in self.primaryNames:
-            return False
-        return self.primaryNames[viewNo] == self.name
-
-    def isMsgForCurrentView(self, msg):
-        """
-        Return whether this request's view number is equal to the current view
-        number of this replica.
-        """
-        viewNo = getattr(msg, "viewNo", None)
-        return viewNo == self.viewNo
-
-    def isPrimaryForMsg(self, msg) -> Optional[bool]:
-        """
-        Return whether this replica is primary if the request's view number is
-        equal this replica's view number and primary has been selected for
-        the current view.
-        Return None otherwise.
-        :param msg: message
-        """
-        return self.isPrimary if self.isMsgForCurrentView(msg) \
-            else self.is_primary_in_view(msg.viewNo)
+        for handler in self.unordered_requests_handlers:
+            handler(new_unordereds)
+        for unordered in new_unordereds:
+            self.requestTracker.handle(unordered[0])
+            logger.debug('Following requests were not ordered for more than {} seconds: {}'
+                         .format(self.config.UnorderedCheckFreq, unordered[0]))
+
+    def isMasterDegraded(self):
+        """
+        Return whether the master instance is slow.
+        """
+        if self.acc_monitor:
+            self.acc_monitor.update_time(time.perf_counter())
+            return self.acc_monitor.is_master_degraded()
+        else:
+            return (self.instances.masterId is not None and
+                    (self.isMasterThroughputTooLow() or
+                     # TODO for now, view_change procedure can take more that 15 minutes
+                     # (5 minutes for catchup and 10 minutes for primary's answer).
+                     # Therefore, view_change triggering by max latency now is not indicative.
+                     # self.isMasterReqLatencyTooHigh() or
+                     self.isMasterAvgReqLatencyTooHigh()))
+
+    def areBackupsDegraded(self):
+        """
+        Return slow instance.
+        """
+        slow_instances = []
+        if self.acc_monitor:
+            for instance in self.instances.backupIds:
+                if self.acc_monitor.is_instance_degraded(instance):
+                    slow_instances.append(instance)
+        else:
+            for instance in self.instances.backupIds:
+                if self.is_instance_throughput_too_low(instance):
+                    slow_instances.append(instance)
+        return slow_instances
 
-    def isMsgFromPrimary(self, msg, sender: str) -> bool:
+    def instance_throughput_ratio(self, inst_id):
         """
-        Return whether this message was from primary replica
-        :param msg:
-        :param sender:
-        :return:
+        The relative throughput of an instance compared to the backup
+        instances.
         """
-        if self.isMsgForCurrentView(msg):
-            return self.primaryName == sender
-        try:
-            return self.primaryNames[msg.viewNo] == sender
-        except KeyError:
-            return False
+        inst_thrp, otherThrp = self.getThroughputs(inst_id)
 
-    def __repr__(self):
-        return self.name
+        # Backup throughput may be 0 so moving ahead only if it is not 0
+        r = inst_thrp / otherThrp if otherThrp and inst_thrp is not None \
+            else None
+        return r
 
-    @property
-    def f(self) -> int:
+    def isMasterThroughputTooLow(self):
         """
-        Return the number of Byzantine Failures that can be tolerated by this
-        system. Equal to (N - 1)/3, where N is the number of nodes in the
-        system.
+        Return whether the throughput of the master instance is greater than the
+        acceptable threshold
         """
-        return self.node.f
+        return self.is_instance_throughput_too_low(self.instances.masterId)
 
-    @property
-    def viewNo(self):
+    def is_instance_throughput_too_low(self, inst_id):
         """
-        Return the current view number of this replica.
+        Return whether the throughput of the master instance is greater than the
+        acceptable threshold
         """
-        return self._consensus_data.view_no
-
-    @property
-    def stashed_out_of_order_commits(self):
-        # Commits which are not being ordered since commits with lower
-        # sequence numbers have not been ordered yet. Key is the
-        # viewNo and value a map of pre-prepare sequence number to commit
-        # type: Dict[int,Dict[int,Commit]]
-        return self._ordering_service.stashed_out_of_order_commits
-
-    def send_3pc_batch(self):
-        return self._ordering_service.send_3pc_batch()
-
-    @staticmethod
-    def batchDigest(reqs):
-        return replica_batch_digest(reqs)
-
-    def readyFor3PC(self, key: ReqKey):
-        try:
-            fin_req = self.requests[key.digest].finalised
-        except KeyError:
-            # Means that this request is outdated and is dropped from the main requests queue
-            self.logger.debug('{} reports request {} is ready for 3PC but it has been dropped '
-                              'from requests queue, ignore this request'.format(self, key))
-            return
-        queue = self._ordering_service.requestQueues[self.node.ledger_id_for_request(fin_req)]
-        queue.add(key.digest)
-        if not self.hasPrimary and len(queue) >= self.HAS_NO_PRIMARY_WARN_THRESCHOLD and not self.warned_no_primary:
-            self.logger.warning('{} is getting requests but still does not have '
-                                'a primary so the replica will not process the request '
-                                'until a primary is chosen'.format(self))
-            self.warned_no_primary = True
-
-    @measure_replica_time(MetricsName.SERVICE_REPLICA_QUEUES_TIME,
-                          MetricsName.SERVICE_BACKUP_REPLICAS_QUEUES_TIME)
-    def serviceQueues(self, limit=None):
-        """
-        Process `limit` number of messages in the inBox.
-
-        :param limit: the maximum number of messages to process
-        :return: the number of messages successfully processed
-        """
-        # TODO should handle SuspiciousNode here
-        r = self.dequeue_pre_prepares()
-        # r += self.inBoxRouter.handleAllSync(self.inBox, limit)
-        r += self._handle_external_messages(self.inBox, limit)
-        r += self.send_3pc_batch()
-        r += self._serviceActions()
-        return r
-        # Messages that can be processed right now needs to be added back to the
-        # queue. They might be able to be processed later
-
-    def _handle_external_messages(self, deq: deque, limit=None) -> int:
-        """
-        Synchronously handle all items in a deque.
-
-        :param deq: a deque of items to be handled by this router
-        :param limit: the number of items in the deque to the handled
-        :return: the number of items handled successfully
-        """
-        count = 0
-        while deq and (not limit or count < limit):
-            count += 1
-            msg = deq.popleft()
-            external_msg, sender = msg if len(msg) == 2 else (msg, None)
-            sender = self.generateName(sender, self.instId)
-            self._external_bus.process_incoming(external_msg, sender)
-        return count
-
-    def _gc_before_new_view(self):
-        # Trigger GC for all batches of old view
-        # Clear any checkpoints, since they are valid only in a view
-        # ToDo: Need to send a cmd like ViewChangeStart into internal bus
-        # self._gc(self.last_ordered_3pc)
-        self._ordering_service.gc(self.last_ordered_3pc)
-        self._checkpointer.gc_before_new_view()
-        # ToDo: get rid of directly calling
-        self._ordering_service._clear_prev_view_pre_prepares()
-        # self._clear_prev_view_pre_prepares()
-
-    def has_already_ordered(self, view_no, pp_seq_no):
-        return compare_3PC_keys((view_no, pp_seq_no),
-                                self.last_ordered_3pc) >= 0
-
-    def dequeue_pre_prepares(self):
-        return self._ordering_service.dequeue_pre_prepares()
-
-    def getDigestFor3PhaseKey(self, key: ThreePhaseKey) -> Optional[str]:
-        reqKey = self.getReqKeyFrom3PhaseKey(key)
-        digest = self.requests.digest(reqKey)
-        if not digest:
-            self.logger.debug("{} could not find digest in sent or received "
-                              "PRE-PREPAREs or PREPAREs for 3 phase key {} and req "
-                              "key {}".format(self, key, reqKey))
+        r = self.instance_throughput_ratio(inst_id)
+        if r is None:
+            logger.debug("{} instance {} throughput is not "
+                         "measurable.".format(self, inst_id))
             return None
+        too_low = r < self.Delta
+        if too_low:
+            logger.display("{}{} instance {} throughput ratio {} is lower than Delta {}.".
+                           format(MONITORING_PREFIX, self, inst_id, r, self.Delta))
         else:
-            return digest
-
-    def getReqKeyFrom3PhaseKey(self, key: ThreePhaseKey):
-        reqKey = None
-        if key in self.sentPrePrepares:
-            reqKey = self.sentPrePrepares[key][0]
-        elif key in self.prePrepares:
-            reqKey = self.prePrepares[key][0]
-        elif key in self.prepares:
-            reqKey = self.prepares[key][0]
+            logger.trace("{} instance {} throughput ratio {} is acceptable.".
+                         format(self, inst_id, r))
+        return too_low
+
+    def isMasterReqLatencyTooHigh(self):
+        """
+        Return whether the request latency of the master instance is greater
+        than the acceptable threshold
+        """
+        # TODO for now, view_change procedure can take more that 15 minutes
+        # (5 minutes for catchup and 10 minutes for primary's answer).
+        # Therefore, view_change triggering by max latency is not indicative now.
+
+        r = self.masterReqLatencyTooHigh or \
+            next(((key, lat) for key, lat in self.masterReqLatencies.items() if
+                  lat > self.Lambda), None)
+        if r:
+            logger.display("{}{} found master's latency {} to be higher than the threshold for request {}.".
+                           format(MONITORING_PREFIX, self, r[1], r[0]))
         else:
-            self.logger.debug("Could not find request key for 3 phase key {}".format(key))
-        return reqKey
-
-    def _process_requested_three_phase_msg(self, msg: object,
-                                           sender: List[str],
-                                           stash: Dict[Tuple[int, int], Optional[Tuple[str, str, str]]],
-                                           get_saved: Optional[Callable[[int, int], Optional[MessageBase]]] = None):
-        if msg is None:
-            self.logger.debug('{} received null from {}'.format(self, sender))
-            return
-        key = (msg.viewNo, msg.ppSeqNo)
-        self.logger.debug('{} received requested msg ({}) from {}'.format(self, key, sender))
-
-        if key not in stash:
-            self.logger.debug('{} had either not requested this msg or already '
-                              'received the msg for {}'.format(self, key))
-            return
-        if self.has_already_ordered(*key):
-            self.logger.debug(
-                '{} has already ordered msg ({})'.format(self, key))
-            return
-        if get_saved and get_saved(*key):
-            self.logger.debug(
-                '{} has already received msg ({})'.format(self, key))
-            return
-        # There still might be stashed msg but not checking that
-        # it is expensive, also reception of msgs is idempotent
-        stashed_data = stash[key]
-        curr_data = (msg.digest, msg.stateRootHash, msg.txnRootHash) \
-            if isinstance(msg, PrePrepare) or isinstance(msg, Prepare) \
-            else None
-        if stashed_data is None or curr_data == stashed_data:
-            return self._external_bus.process_incoming(msg, sender)
-
-        self.discard(msg, reason='{} does not have expected state {}'.
-                     format(THREE_PC_PREFIX, stashed_data),
-                     logMethod=self.logger.warning)
-
-    def process_requested_pre_prepare(self, pp: PrePrepare, sender: str):
-        return self._process_requested_three_phase_msg(pp, sender, self.requested_pre_prepares,
-                                                       self._ordering_service.get_preprepare)
-
-    def process_requested_prepare(self, prepare: Prepare, sender: str):
-        return self._process_requested_three_phase_msg(prepare, sender, self.requested_prepares)
-
-    def process_requested_commit(self, commit: Commit, sender: str):
-        return self._process_requested_three_phase_msg(commit, sender, self.requested_commits)
+            logger.trace("{} found master's latency to be lower than the "
+                         "threshold for all requests.".format(self))
+        return r
 
-    def send(self, msg, to_nodes=None) -> None:
+    def isMasterAvgReqLatencyTooHigh(self):
         """
-        Send a message to the node on which this replica resides.
-
-        :param stat:
-        :param rid: remote id of one recipient (sends to all recipients if None)
-        :param msg: the message to send
+        Return whether the average request latency of the master instance is
+        greater than the acceptable threshold
         """
-        self.logger.trace("{} sending {}".format(self, msg.__class__.__name__),
-                          extra={"cli": True, "tags": ['sending']})
-        self.logger.trace("{} sending {}".format(self, msg))
-        if to_nodes:
-            self.node.sendToNodes(msg, names=to_nodes)
-            return
-        self.outBox.append(msg)
+        return self.is_instance_avg_req_latency_too_high(self.instances.masterId)
 
-    def revert_unordered_batches(self):
+    def is_instance_avg_req_latency_too_high(self, inst_id):
         """
-        Revert changes to ledger (uncommitted) and state made by any requests
-        that have not been ordered.
+        Return whether the average request latency of an instance is
+        greater than the acceptable threshold
         """
-        return self._ordering_service.revert_unordered_batches()
+        avg_lat, avg_lat_others = self.getLatencies()
+        if not avg_lat or not avg_lat_others:
+            return False
 
-    def on_catch_up_finished(self, last_caught_up_3PC=None, master_last_ordered_3PC=None):
-        if master_last_ordered_3PC and last_caught_up_3PC and \
-                compare_3PC_keys(master_last_ordered_3PC,
-                                 last_caught_up_3PC) > 0:
-            if self.isMaster:
-                self._caught_up_till_3pc(last_caught_up_3PC)
+        d = avg_lat - avg_lat_others
+        if d < self.Omega:
+            return False
+
+        if inst_id == self.instances.masterId:
+            logger.info("{}{} found difference between master's and "
+                        "backups's avg latency {} to be higher than the "
+                        "threshold".format(MONITORING_PREFIX, self, d))
+            logger.trace(
+                "{}'s master's avg request latency is {} and backup's "
+                "avg request latency is {}".format(self, avg_lat, avg_lat_others))
+        return True
+
+    def getThroughputs(self, desired_inst_id: int):
+        """
+        Return a tuple of  the throughput of the given instance and the average
+        throughput of the remaining instances.
+
+        :param instId: the id of the protocol instance
+        """
+
+        instance_thrp = self.getThroughput(desired_inst_id)
+        totalReqs, totalTm = self.getInstanceMetrics(forAllExcept=desired_inst_id)
+        # Average backup replica's throughput
+        if len(self.throughputs) > 1:
+            thrs = []
+            for inst_id, thr_obj in self.throughputs.items():
+                if inst_id == desired_inst_id:
+                    continue
+                thr = self.getThroughput(inst_id)
+                if thr is not None:
+                    thrs.append(thr)
+            if thrs:
+                if desired_inst_id == self.instances.masterId:
+                    other_thrp = self.throughput_avg_strategy_cls.get_avg(thrs)
+                else:
+                    other_thrp = self.backup_throughput_avg_strategy_cls.get_avg(thrs)
             else:
-                self._ordering_service.first_batch_after_catchup = True
-                self._catchup_clear_for_backup()
-        self.stasher.process_all_stashed(STASH_CATCH_UP)
-
-    def discard_req_key(self, ledger_id, req_key):
-        return self._ordering_service.discard_req_key(ledger_id, req_key)
-
-    def _caught_up_backup(self, msg: NeedBackupCatchup):
-        self._caught_up_till_3pc(msg.caught_up_till_3pc)
-
-    def _caught_up_till_3pc(self, last_caught_up_3PC):
-        self._ordering_service._caught_up_till_3pc(last_caught_up_3PC)
-        self._checkpointer.caught_up_till_3pc(last_caught_up_3PC)
-
-    def _catchup_clear_for_backup(self):
-        if not self.isPrimary:
-            self.outBox.clear()
-            self._checkpointer.catchup_clear_for_backup()
-            self._ordering_service.catchup_clear_for_backup()
-
-    def _remove_ordered_from_queue(self, last_caught_up_3PC=None):
-        """
-        Remove any Ordered that the replica might be sending to node which is
-        less than or equal to `last_caught_up_3PC` if `last_caught_up_3PC` is
-        passed else remove all ordered, needed in catchup
-        """
-        to_remove = []
-        for i, msg in enumerate(self.outBox):
-            if isinstance(msg, Ordered) and \
-                    (not last_caught_up_3PC or
-                     compare_3PC_keys((msg.viewNo, msg.ppSeqNo), last_caught_up_3PC) >= 0):
-                to_remove.append(i)
-
-        self.logger.trace('{} going to remove {} Ordered messages from outbox'.format(self, len(to_remove)))
-
-        # Removing Ordered from queue but returning `Ordered` in order that
-        # they should be processed.
-        removed = []
-        for i in reversed(to_remove):
-            removed.insert(0, self.outBox[i])
-            del self.outBox[i]
-        return removed
-
-    def _get_last_timestamp_from_state(self, ledger_id):
-        if ledger_id == DOMAIN_LEDGER_ID:
-            ts_store = self.node.db_manager.get_store(TS_LABEL)
-            if ts_store:
-                last_timestamp = ts_store.get_last_key()
-                if last_timestamp:
-                    last_timestamp = int(last_timestamp.decode())
-                    self.logger.debug("Last ordered timestamp from store is : {}"
-                                      "".format(last_timestamp))
-                    return last_timestamp
-        return None
-
-    def get_ledgers_last_update_time(self) -> dict:
-        if self._freshness_checker:
-            return self._freshness_checker.get_last_update_time()
-
-    def get_valid_req_ids_from_all_requests(self, reqs, invalid_indices):
-        return [req.key for idx, req in enumerate(reqs) if idx not in invalid_indices]
-
-    def report_suspicious_node(self, ex):
-        if self.isMaster:
-            self.node.reportSuspiciousNodeEx(ex)
+                other_thrp = None
         else:
-            self.warn_suspicious_backup(ex.node, ex.reason, ex.code)
-
-    def warn_suspicious_backup(self, nodeName, reason, code):
-        self.logger.warning("backup replica {} raised suspicion on node {} for {}; suspicion code "
-                            "is {}".format(self, nodeName, reason, code))
-
-    def set_validators(self, validators):
-        self._consensus_data.set_validators(validators)
-
-    def set_view_no(self, view_no):
-        self._consensus_data.view_no = view_no
-
-    def set_view_change_status(self, legacy_vc_in_progress):
-        self._consensus_data.legacy_vc_in_progress = legacy_vc_in_progress
-
-    def set_mode(self, mode):
-        self._consensus_data.node_mode = mode
-
-    def set_primaries_batch_needed(self, value):
-        self._ordering_service.primaries_batch_needed = value
+            other_thrp = None
+        if instance_thrp == 0:
+            if self.numOrderedRequests[desired_inst_id] == (0, 0):
+                avgReqsPerInst = (totalReqs or 0) / self.instances.count
+                if avgReqsPerInst <= 1:
+                    # too early to tell if we need an instance change
+                    instance_thrp = None
+        return instance_thrp, other_thrp
+
+    def getThroughput(self, instId: int) -> float:
+        """
+        Return the throughput of the specified instance.
+
+        :param instId: the id of the protocol instance
+        """
+        # We are using the instanceStarted time in the denominator instead of
+        # a time interval. This is alright for now as all the instances on a
+        # node are started at almost the same time.
+        if instId not in self.instances.ids:
+            return None
+        perf_time = time.perf_counter()
+        throughput = self.throughputs[instId].get_throughput(perf_time)
+        return throughput
+
+    def getInstanceMetrics(
+            self, forAllExcept: int) -> Tuple[Optional[int], Optional[float]]:
+        """
+        Calculate and return the average throughput of all the instances except
+        the one specified as `forAllExcept`.
+        """
+        m = [(reqs, tm) for i, (reqs, tm)
+             in self.numOrderedRequests.items()
+             if i != forAllExcept]
+        if m:
+            reqs, tm = zip(*m)
+            return sum(reqs), sum(tm)
+        else:
+            return None, None
 
-    def update_connecteds(self, connecteds: dict):
-        self._external_bus.update_connecteds(connecteds)
+    def getLatencies(self, desired_inst_id=None):
+        if desired_inst_id is None:
+            desired_inst_id = self.instances.masterId
+        avg_lat = self.getLatency(desired_inst_id)
+        avg_lat_others_by_inst = []
+        for inst_id in self.instances.ids:
+            if desired_inst_id == inst_id:
+                continue
+            lat = self.getLatency(inst_id)
+            if lat:
+                avg_lat_others_by_inst.append(lat)
+        avg_lat_others = self.latency_avg_for_backup_cls.get_avg(avg_lat_others_by_inst)\
+            if avg_lat_others_by_inst else None
+        return avg_lat, avg_lat_others
+
+    def getLatency(self, instId: int) -> float:
+        """
+        Return a dict with client identifier as a key and calculated latency as a value
+        """
+        if len(self.clientAvgReqLatencies) == 0:
+            return 0.0
+        return self.clientAvgReqLatencies[instId].get_avg_latency()
+
+    def sendPeriodicStats(self):
+        thoughputData = self.sendThroughput()
+        self.clusterThroughputSpikeMonitorData['accum'].append(
+            thoughputData['throughput'])
+        self.sendLatencies()
+        self.sendKnownNodesInfo()
+        self.sendNodeInfo()
+        self.sendSystemPerfomanceInfo()
+        self.sendTotalRequests()
 
-    def _init_replica_stasher(self):
-        return StashingRouter(self.config.REPLICA_STASH_LIMIT,
-                              buses=[self.internal_bus, self._external_bus],
-                              unstash_handler=self._add_to_inbox)
+    def checkPerformance(self):
+        self.sendClusterThroughputSpike()
 
-    def _cleanup_process(self, msg: CheckpointStabilized):
-        if msg.inst_id != self.instId:
-            return
-        self._ordering_service.gc(msg.last_stable_3pc)
-
-    def _process_suspicious_node(self, msg: RaisedSuspicion):
-        if msg.inst_id != self.instId:
-            return
-        self.report_suspicious_node(msg.ex)
+    def sendClusterThroughputSpike(self):
+        if self.instances.masterId is None:
+            return None
+        accum = 0
+        for val in self.clusterThroughputSpikeMonitorData['accum']:
+            accum += val
+        if len(self.clusterThroughputSpikeMonitorData['accum']):
+            accum /= len(self.clusterThroughputSpikeMonitorData['accum'])
+        self.clusterThroughputSpikeMonitorData['accum'] = []
+        return pluginManager.sendMessageUponSuspiciousSpike(
+            notifierPluginTriggerEvents['clusterThroughputSpike'],
+            self.clusterThroughputSpikeMonitorData,
+            accum,
+            self.notifierEventTriggeringConfig['clusterThroughputSpike'],
+            self.name,
+            self.notifierEventsEnabled
+        )
+
+    @property
+    def highResThroughput(self):
+        # TODO:KS Move these computations as well to plenum-stats project
+        return self.getThroughput(self.instances.masterId)
+
+    def sendThroughput(self):
+        logger.debug("{} sending throughput".format(self))
+
+        throughput = self.highResThroughput
+        utcTime = datetime.utcnow()
+        mtrStats = {
+            "throughput": throughput,
+            "timestamp": utcTime.isoformat(),
+            "nodeName": self.name,
+            # Multiply by 1000 for JavaScript date conversion
+            "time": time.mktime(utcTime.timetuple()) * 1000
+        }
+        self._sendStatsDataIfRequired(
+            EVENT_PERIODIC_STATS_THROUGHPUT, mtrStats)
+        return mtrStats
+
+    @property
+    def masterLatency(self):
+        master_latency, _ = self.getLatencies()
+        return master_latency
+
+    @property
+    def avgBackupLatency(self):
+        _, lat_backup = self.getLatencies()
+        return lat_backup
+
+    def sendLatencies(self):
+        logger.debug("{} sending latencies".format(self))
+        utcTime = datetime.utcnow()
+        # Multiply by 1000 to make it compatible to JavaScript Date()
+        jsTime = time.mktime(utcTime.timetuple()) * 1000
+
+        latencies = dict(
+            masterLatency=self.masterLatency,
+            averageBackupLatency=self.avgBackupLatency,
+            time=jsTime,
+            nodeName=self.name,
+            timestamp=utcTime.isoformat()
+        )
+
+        self._sendStatsDataIfRequired(
+            EVENT_PERIODIC_STATS_LATENCIES, latencies)
+
+    def sendKnownNodesInfo(self):
+        logger.debug("{} sending nodestack".format(self))
+        self._sendStatsDataIfRequired(
+            EVENT_PERIODIC_STATS_NODES, remotesInfo(
+                self.nodestack, self.blacklister))
+
+    def sendSystemPerfomanceInfo(self):
+        logger.debug("{} sending system performance".format(self))
+        self._sendStatsDataIfRequired(
+            EVENT_PERIODIC_STATS_SYSTEM_PERFORMANCE_INFO,
+            self.captureSystemPerformance())
+
+    def sendNodeInfo(self):
+        logger.debug("{} sending node info".format(self))
+        self._sendStatsDataIfRequired(
+            EVENT_PERIODIC_STATS_NODE_INFO, self.nodeInfo['data'])
+
+    def sendTotalRequests(self):
+        logger.debug("{} sending total requests".format(self))
+
+        totalRequests = dict(
+            totalRequests=self.totalRequests
+        )
+
+        self._sendStatsDataIfRequired(
+            EVENT_PERIODIC_STATS_TOTAL_REQUESTS, totalRequests)
+
+    def captureSystemPerformance(self):
+        logger.debug("{} capturing system performance".format(self))
+        timestamp = time.time()
+        cpu = psutil.cpu_percent(interval=None)
+        ram = psutil.virtual_memory()
+        curr_network = self.calculateTraffic()
+        network = curr_network - self.lastKnownTraffic
+        self.lastKnownTraffic = curr_network
+        cpu_data = {
+            'time': timestamp,
+            'value': cpu
+        }
+        ram_data = {
+            'time': timestamp,
+            'value': ram.percent
+        }
+        traffic_data = {
+            'time': timestamp,
+            'value': network
+        }
+        return {
+            'cpu': cpu_data,
+            'ram': ram_data,
+            'traffic': traffic_data
+        }
+
+    def postOnReqOrdered(self):
+        utcTime = datetime.utcnow()
+        # Multiply by 1000 to make it compatible to JavaScript Date()
+        jsTime = time.mktime(utcTime.timetuple()) * 1000
+
+        if self.totalViewChanges != self._lastPostedViewChange:
+            self._lastPostedViewChange = self.totalViewChanges
+            viewChange = dict(
+                time=jsTime,
+                viewChange=self._lastPostedViewChange
+            )
+            self._sendStatsDataIfRequired(EVENT_VIEW_CHANGE, viewChange)
+
+        reqOrderedEventDict = dict(self.metrics())
+        reqOrderedEventDict["created_at"] = utcTime.isoformat()
+        reqOrderedEventDict["nodeName"] = self.name
+        reqOrderedEventDict["time"] = jsTime
+        reqOrderedEventDict["hasMasterPrimary"] = "Y" if self.hasMasterPrimary else "N"
+        self._sendStatsDataIfRequired(EVENT_REQ_ORDERED, reqOrderedEventDict)
+        self._clearSnapshot()
+
+    def postOnNodeStarted(self, startedAt):
+        throughputData = {
+            "updateFrequency": self.config.DashboardUpdateFreq,
+            "graphDuration": self.config.ThroughputGraphDuration
+        }
+        startedAtData = {"startedAt": startedAt, "ctx": "DEMO"}
+        startedEventDict = {
+            "startedAtData": startedAtData,
+            "throughputConfig": throughputData
+        }
+        self._sendStatsDataIfRequired(EVENT_NODE_STARTED, startedEventDict)
+
+    def _clearSnapshot(self):
+        self.masterReqLatencyTooHigh = self.isMasterReqLatencyTooHigh()
+        self.masterReqLatencies = {}
+
+    def _sendStatsDataIfRequired(self, event, stats):
+        if self.config.SendMonitorStats:
+            for sc in self.statsConsumers:
+                sc.sendStats(event, stats)
 
-    def _send_ordered(self, msg: Ordered):
-        self.send(msg)
+    @staticmethod
+    def mean(data):
+        return 0 if len(data) == 0 else mean(data)
 
-    def _init_checkpoint_service(self) -> CheckpointService:
-        return CheckpointService(data=self._consensus_data,
-                                 bus=self.internal_bus,
-                                 network=self._external_bus,
-                                 stasher=self.stasher,
-                                 db_manager=self.node.db_manager,
-                                 metrics=self.metrics)
-
-    def _init_ordering_service(self) -> OrderingService:
-        return OrderingService(data=self._consensus_data,
-                               timer=self.node.timer,
-                               bus=self.internal_bus,
-                               network=self._external_bus,
-                               write_manager=self.node.write_manager,
-                               bls_bft_replica=self._bls_bft_replica,
-                               freshness_checker=self._freshness_checker,
-                               get_current_time=self.get_current_time,
-                               get_time_for_3pc_batch=self.get_time_for_3pc_batch,
-                               stasher=self.stasher,
-                               metrics=self.metrics)
 
-    def _add_to_inbox(self, message):
-        self.inBox.append(message)
+def remotesInfo(nodestack, blacklister):
+    res = {
+        'connected': [],
+        'disconnected': []
+    }
+
+    conns, disconns = nodestack.remotesByConnected()
+
+    for r in conns:
+        res['connected'].append(remoteInfo(r, nodestack, blacklister))
+    for r in disconns:
+        res['disconnected'].append(remoteInfo(r, nodestack, blacklister))
+
+    return res
+
+
+def remoteInfo(remote, nodestack, blacklister):
+    regName = nodestack.findInNodeRegByHA(remote.ha)
+    res = pickRemoteEstateFields(remote, regName)
+    res['blacklisted'] = blacklister.isBlacklisted(remote.name)
+    if not res['blacklisted'] and regName:
+        res['blacklisted'] = blacklister.isBlacklisted(regName)
+    return res
+
+
+def pickRemoteEstateFields(remote, customName=None):
+    host, port = remote.ha
+    return {
+        'name': customName or remote.name,
+        'host': host,
+        'port': port,
+        'nat': getattr(remote, 'natted', False) or False
+    }
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/plugin_loader.py` & `indy-plenum-1.9.2rc1/plenum/server/plugin_loader.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/msg_filter.py` & `indy-plenum-1.9.2rc1/plenum/server/msg_filter.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/pool_manager.py` & `indy-plenum-1.9.2rc1/plenum/server/pool_manager.py`

 * *Files 2% similar despite different names*

```diff
@@ -389,22 +389,14 @@
         if node_services is not None:
             self._ordered_node_services[node_nym] = node_services
 
     def node_ids_ordered_by_rank(self, node_reg, node_ids) -> List:
         return [nym for nym, name in node_ids.items()
                 if name in node_reg]
 
-    def node_names_ordered_by_rank(self) -> List:
-        return self.calc_node_names_ordered_by_rank(self.nodeReg, self._ordered_node_ids)
-
-    @staticmethod
-    def calc_node_names_ordered_by_rank(node_reg, node_ids) -> List:
-        return [name for nym, name in node_ids.items()
-                if name in node_reg]
-
     def get_rank_of(self, node_id, node_reg, node_ids) -> Optional[int]:
         if self.id is None:
             # This can happen if a non-genesis node starts
             return None
         return self._get_rank(node_id, self.node_ids_ordered_by_rank(node_reg, node_ids))
 
     def get_rank_by_name(self, name, node_reg, node_ids) -> Optional[int]:
@@ -426,11 +418,12 @@
                 return nym
         return None
 
     def set_validators_for_replicas(self):
         for r in self.node.replicas.values():
             # We set new list of validators for every replica,
             # cause cdp for every replica need to be independent
-            r.set_validators(self.node_names_ordered_by_rank())
+            r.set_validators(self.node_ids_ordered_by_rank(self.nodeReg,
+                                                           self._ordered_node_ids))
 
     def get_node_ids(self):
         return self._ordered_node_ids
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/quota_control.py` & `indy-plenum-1.9.2rc1/plenum/server/quota_control.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/has_action_queue.py` & `indy-plenum-1.9.2rc1/plenum/server/has_action_queue.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/last_sent_pp_store_helper.py` & `indy-plenum-1.9.2rc1/plenum/server/last_sent_pp_store_helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -72,17 +72,15 @@
     def _restore_last_stored(self, inst_id, pair_3pc):
         stored = (inst_id, pair_3pc)
         logger.info("{} restoring lastPrePrepareSeqNo from {}"
                     .format(self.node, stored))
         replica = self.node.replicas[inst_id]
         replica.lastPrePrepareSeqNo = pair_3pc[1]
         replica.last_ordered_3pc = (pair_3pc[0], pair_3pc[1])
-        # TODO: add the method update_watermark_from_3pc to replica
-        # or solve this problem better
-        replica._checkpointer.update_watermark_from_3pc()
+        replica.update_watermark_from_3pc()
 
     def _save_last_stored(self, value: Dict):
         serialized_value = node_status_db_serializer.serialize(value)
         self.node.nodeStatusDB.put(LAST_SENT_PRE_PREPARE, serialized_value)
 
     def _load_last_sent_pp_key(self) -> Optional[Dict]:
         if LAST_SENT_PRE_PREPARE not in self.node.nodeStatusDB:
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/req_authenticator.py` & `indy-plenum-1.9.2rc1/plenum/server/req_authenticator.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/propagator.py` & `indy-plenum-1.9.2rc1/plenum/server/propagator.py`

 * *Files 0% similar despite different names*

```diff
@@ -2,15 +2,14 @@
 
 from collections import OrderedDict, defaultdict
 
 from typing import Union
 
 from orderedset import OrderedSet
 from plenum.common.constants import PROPAGATE, THREE_PC_PREFIX
-from plenum.common.messages.internal_messages import RequestPropagates
 from plenum.common.messages.node_messages import Propagate
 from plenum.common.metrics_collector import MetricsCollector, NullMetricsCollector, MetricsName
 from plenum.common.request import Request, ReqKey
 from plenum.common.types import f
 from plenum.server.quorums import Quorum
 from stp_core.common.log import getlogger
 
@@ -277,15 +276,15 @@
 
         :param request: the REQUEST to propagate
         """
         key = request.key
         num_replicas = self.replicas.num_replicas
         logger.debug('{} forwarding request {} to {} replicas'
                      .format(self, key, num_replicas))
-        self.replicas.send_to_internal_bus(ReqKey(key))
+        self.replicas.pass_message(ReqKey(key))
         self.monitor.requestUnOrdered(key)
         self.requests.mark_as_forwarded(request, num_replicas)
 
     # noinspection PyUnresolvedReferences
     def recordAndPropagate(self, request: Request, clientName):
         """
         Record the request in the list of requests and propagate.
@@ -309,23 +308,22 @@
             # propagate request(PROPAGATE) but have enough propagate requests
             # to move ahead
             self.forward(request)
         else:
             logger.trace("{} not forwarding request {} to its replicas "
                          "since {}".format(self, request, cannot_reason_msg))
 
-    def request_propagates(self, request_list: RequestPropagates):
+    def request_propagates(self, req_keys):
         """
         Request PROPAGATEs for the given request keys. Since replicas can
         request PROPAGATEs independently of each other, check if it has
         been requested recently
         :param req_keys:
         :return:
         """
-        req_keys = request_list.bad_requests
         i = 0
         for digest in req_keys:
             if digest not in self.requested_propagates_for:
                 if digest not in self.requests:
                     # Request from all nodes
                     self.request_msg(PROPAGATE, {f.DIGEST.nm: digest})
                 else:
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/node.py` & `indy-plenum-1.9.2rc1/plenum/server/node.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,19 +5,14 @@
 from collections import deque
 from contextlib import closing
 from functools import partial
 from typing import Dict, Any, Mapping, Iterable, List, Optional, Set, Tuple, Callable
 
 import gc
 import psutil
-
-from plenum.common.event_bus import InternalBus
-from plenum.common.messages.internal_messages import NeedMasterCatchup, \
-    RequestPropagates, PreSigVerification
-from plenum.server.consensus.primary_selector import RoundRobinPrimariesSelector, PrimariesSelector
 from plenum.server.database_manager import DatabaseManager
 from plenum.server.node_bootstrap import NodeBootstrap
 from plenum.server.replica import Replica
 
 from common.exceptions import LogicError
 from common.serializers.serialization import state_roots_serializer
 from crypto.bls.bls_key_manager import LoadBLSKeyError
@@ -26,15 +21,14 @@
     async_measure_time, measure_time
 from plenum.common.timer import QueueTimer
 from plenum.server.backup_instance_faulty_processor import BackupInstanceFaultyProcessor
 from plenum.server.batch_handlers.three_pc_batch import ThreePcBatch
 from plenum.server.inconsistency_watchers import NetworkInconsistencyWatcher
 from plenum.server.last_sent_pp_store_helper import LastSentPpStoreHelper
 from plenum.server.quota_control import StaticQuotaControl, RequestQueueQuotaControl
-from plenum.server.replica_validator_enums import STASH_WATERMARKS, STASH_VIEW, STASH_CATCH_UP
 from plenum.server.request_handlers.utils import VALUE
 from plenum.server.request_managers.action_request_manager import ActionRequestManager
 from plenum.server.request_managers.read_request_manager import ReadRequestManager
 from plenum.server.request_managers.write_request_manager import WriteRequestManager
 from plenum.server.view_change.node_view_changer import create_view_changer
 from state.pruning_state import PruningState
 from storage.helper import initKeyValueStorage, initHashStore, initKeyValueStorageIntKeys
@@ -52,30 +46,31 @@
     CLIENT_BLACKLISTER_SUFFIX, CONFIG_LEDGER_ID, \
     NODE_BLACKLISTER_SUFFIX, NODE_PRIMARY_STORAGE_SUFFIX, \
     TXN_TYPE, LEDGER_STATUS, \
     CLIENT_STACK_SUFFIX, PRIMARY_SELECTION_PREFIX, VIEW_CHANGE_PREFIX, \
     OP_FIELD_NAME, CATCH_UP_PREFIX, NYM, \
     GET_TXN, DATA, VERKEY, \
     TARGET_NYM, ROLE, STEWARD, TRUSTEE, ALIAS, \
-    NODE_IP, BLS_PREFIX, LedgerState, CURRENT_PROTOCOL_VERSION, AUDIT_LEDGER_ID, \
+    NODE_IP, BLS_PREFIX, NodeHooks, LedgerState, CURRENT_PROTOCOL_VERSION, AUDIT_LEDGER_ID, \
     AUDIT_TXN_VIEW_NO, AUDIT_TXN_PP_SEQ_NO, \
     TXN_AUTHOR_AGREEMENT_VERSION, AML, TXN_AUTHOR_AGREEMENT_TEXT, TS_LABEL, SEQ_NO_DB_LABEL, NODE_STATUS_DB_LABEL, \
     LAST_SENT_PP_STORE_LABEL, AUDIT_TXN_PRIMARIES, MULTI_SIGNATURE
 from plenum.common.exceptions import SuspiciousNode, SuspiciousClient, \
     MissingNodeOp, InvalidNodeOp, InvalidNodeMsg, InvalidClientMsgType, \
     InvalidClientRequest, BaseExc, \
     InvalidClientMessageException, KeysNotFoundException as REx, BlowUp, SuspiciousPrePrepare, \
     TaaAmlNotSetError, InvalidClientTaaAcceptanceError, UnauthorizedClientRequest
 from plenum.common.has_file_storage import HasFileStorage
+from plenum.common.hook_manager import HookManager
 from plenum.common.keygen_utils import areKeysSetup
 from plenum.common.ledger import Ledger
 from plenum.common.message_processor import MessageProcessor
 from plenum.common.messages.node_message_factory import node_message_factory
-from plenum.common.messages.node_messages import Batch, \
-    RequestAck, RequestNack, Reject, Ordered, \
+from plenum.common.messages.node_messages import Nomination, Batch, Reelection, \
+    Primary, RequestAck, RequestNack, Reject, Ordered, \
     Propagate, PrePrepare, Prepare, Commit, Checkpoint, Reply, InstanceChange, LedgerStatus, \
     ConsistencyProof, CatchupReq, CatchupRep, ViewChangeDone, \
     MessageReq, MessageRep, ThreePhaseType, BatchCommitted, \
     ObservedData, FutureViewChangeDone, BackupInstanceFaulty
 from plenum.common.motor import Motor
 from plenum.common.plugin_helper import loadPlugins
 from plenum.common.request import Request, SafeRequest
@@ -110,29 +105,31 @@
 from plenum.server.notifier_plugin_manager import notifierPluginTriggerEvents, \
     PluginManager
 from plenum.server.observer.observable import Observable
 from plenum.server.observer.observer_node import NodeObserver
 from plenum.server.observer.observer_sync_policy import ObserverSyncPolicyType
 from plenum.server.plugin.has_plugin_loader_helper import PluginLoaderHelper
 from plenum.server.pool_manager import TxnPoolManager
+from plenum.server.primary_decider import PrimaryDecider
+from plenum.server.primary_selector import PrimarySelector
 from plenum.server.propagator import Propagator
 from plenum.server.quorums import Quorums
 from plenum.server.replicas import Replicas
 from plenum.server.req_authenticator import ReqAuthenticator
 from plenum.server.router import Router
 from plenum.server.suspicion_codes import Suspicions
 from plenum.server.validator_info_tool import ValidatorNodeInfoTool
 from plenum.server.view_change.view_changer import ViewChanger
 
 pluginManager = PluginManager()
 logger = getlogger()
 
 
 class Node(HasActionQueue, Motor, Propagator, MessageProcessor, HasFileStorage,
-           PluginLoaderHelper, MessageReqProcessor):
+           PluginLoaderHelper, MessageReqProcessor, HookManager):
     """
     A node in a plenum system.
     """
 
     suspicions = {s.code: s.reason for s in Suspicions.get_list()}
     keygenScript = "init_plenum_keys"
     client_request_class = SafeRequest
@@ -152,27 +149,32 @@
                  config_helper=None,
                  ledger_dir: str = None,
                  keys_dir: str = None,
                  genesis_dir: str = None,
                  plugins_dir: str = None,
                  node_info_dir: str = None,
                  view_changer: ViewChanger = None,
+                 primaryDecider: PrimaryDecider = None,
                  pluginPaths: Iterable[str] = None,
                  storage: Storage = None,
                  config=None,
                  seed=None,
                  bootstrap_cls=NodeBootstrap):
         """
         Create a new node.
+
+        :param clientAuthNr: client authenticator implementation to be used
+        :param primaryDecider: the mechanism to be used to decide the primary
+        of a protocol instance
         """
         self.ha = ha
         self.cliname = cliname
         self.cliha = cliha
         self.timer = QueueTimer()
-        self.poolManager = None  # type: TxnPoolManager
+        self.poolManager = None
         self.ledgerManager = None
         self.bls_bft = None
         self.write_req_validator = None
 
         self.config_and_dirs_init(name, config, config_helper, ledger_dir, keys_dir,
                                   genesis_dir, plugins_dir, node_info_dir, pluginPaths)
         self.requestExecuter = {}  # type: Dict[int, Callable]
@@ -203,29 +205,26 @@
         # Number of read requests the node has processed
         self.total_read_request_number = 0
 
         self.clientAuthNr = clientAuthNr or self.defaultAuthNr()
 
         self.addGenesisNyms()
 
-        self._mode = None  # type: Optional[Mode]
-
-        # List of current replica's primaries, used for persisting in audit ledger
-        # and restoration current primaries from audit ledger
-        self._primaries = []
+        self.mode = None  # type: Optional[Mode]
 
         self.network_stacks_init(seed)
 
         HasActionQueue.__init__(self)
 
         Propagator.__init__(self, metrics=self.metrics)
 
         MessageReqProcessor.__init__(self, metrics=self.metrics)
 
         self.view_changer = view_changer
+        self.primaryDecider = primaryDecider
 
         self.nodeInBox = deque()
         self.clientInBox = deque()
 
         # 3PC state consistency watchdog based on network events
         self.network_i3pc_watcher = NetworkInconsistencyWatcher(self.on_inconsistent_3pc_state_from_network)
 
@@ -240,19 +239,20 @@
             self.name + NODE_BLACKLISTER_SUFFIX)  # type: Blacklister
 
         self.nodeInfo = {
             'data': {}
         }
 
         self._view_changer = None  # type: ViewChanger
-        self.primaries_selector = RoundRobinPrimariesSelector()  # type: PrimariesSelector
+        self._elector = None  # type: PrimaryDecider
 
         self.instances = Instances()
 
         self.monitor_init(pluginPaths)
+
         self.replicas = self.create_replicas()
 
         # Need to keep track of the time when lost connection with primary,
         # help in voting for/against a view change on the master and removing
         # replica on a backup instance
         self.primaries_disconnection_times = []
 
@@ -334,28 +334,25 @@
         # The start time of the catch-up during view change
         self._catch_up_start_ts = 0
 
         self._last_performance_check_data = {}
 
         self.init_ledger_manager()
 
+        HookManager.__init__(self, NodeHooks.get_all_vals())
+
         self._observable = Observable()
         self._observer = NodeObserver(self)
 
-        self._subscribe_to_internal_msgs()
-
-    @property
-    def mode(self):
-        return self._mode
+        # List of current replica's primaries, used for persisting in audit ledger
+        # and restoration current primaries from audit ledger
+        self.primaries = []
 
-    @mode.setter
-    def mode(self, value):
-        self._mode = value
-        for r in self.replicas.values():
-            r.set_mode(value)
+        # Flag which node set, when it have set new primaries and need to send batch
+        self.primaries_batch_needed = False
 
     def config_and_dirs_init(self, name, config, config_helper, ledger_dir, keys_dir,
                              genesis_dir, plugins_dir, node_info_dir, pluginPaths):
         self.created = time.time()
         self.name = name
         self.last_prod_started = None
         self.config = config or getConfig()
@@ -417,14 +414,17 @@
     def white_list_init(self):
         # BE CAREFUL HERE
         # This controls which message types are excluded from signature
         # verification. Expressly prohibited from being in this is
         # ClientRequest and Propagation, which both require client
         # signature verification
         self.authnWhitelist = (
+            Nomination,
+            Primary,
+            Reelection,
             Batch,
             ViewChangeDone,
             PrePrepare,
             Prepare,
             Checkpoint,
             Commit,
             InstanceChange,
@@ -552,24 +552,14 @@
     @property
     def view_change_in_progress(self):
         if self.view_changer is None:
             return False
         return self.view_changer.view_change_in_progress
 
     @property
-    def primaries(self):
-        return self._primaries
-
-    @primaries.setter
-    def primaries(self, ps):
-        self._primaries = ps
-        for r in self.replicas.values():
-            r.set_primaries(ps)
-
-    @property
     def pre_view_change_in_progress(self):
         if self.view_changer is None:
             return False
         return self.view_changer.pre_view_change_in_progress
 
     def _add_config_ledger(self):
         self.ledgerManager.addLedger(
@@ -627,14 +617,22 @@
     def view_changer(self) -> ViewChanger:
         return self._view_changer
 
     @view_changer.setter
     def view_changer(self, value):
         self._view_changer = value
 
+    @property
+    def elector(self) -> PrimaryDecider:
+        return self._elector
+
+    @elector.setter
+    def elector(self, value):
+        self._elector = value
+
     # EXTERNAL EVENTS
 
     def on_view_change_start(self):
         """
         Notifies node about the fact that view changed to let it
         prepare for election
         """
@@ -968,14 +966,16 @@
             if self.nodeStatusDB and self.nodeStatusDB.closed:
                 self.nodeStatusDB.open()
 
             self.nodestack.start()
             self.clientstack.start()
 
             self.view_changer = self.newViewChanger()
+            self.elector = self.newPrimaryDecider()
+
             self.schedule_initial_propose_view_change()
 
             self.schedule_node_status_dump()
             self.dump_additional_info()
 
             # if first time running this node
             if not self.nodestack.remotes:
@@ -1018,14 +1018,20 @@
 
     def newViewChanger(self):
         if self.view_changer:
             return self.view_changer
         else:
             return create_view_changer(self)
 
+    def newPrimaryDecider(self):
+        if self.primaryDecider:
+            return self.primaryDecider
+        else:
+            return PrimarySelector(self)
+
     @property
     def connectedNodeCount(self) -> int:
         """
         The plus one is for this node, for example, if this node has three
         connections, then there would be four total nodes
         :return: number of connected nodes this one
         """
@@ -1077,14 +1083,15 @@
                 self, len(
                     self.aqStash)))
         self.nodestack.conns.clear()
         # TODO: Should `self.clientstack.conns` be cleared too
         # self.clientstack.conns.clear()
         self.aqStash.clear()
         self.actionQueue.clear()
+        self.elector = None
         self.view_changer = None
 
     @async_measure_time(MetricsName.NODE_PROD_TIME)
     async def prod(self, limit: int = None) -> int:
         """.opened
         This function is executed by the node each time it gets its share of
         CPU time from the event loop.
@@ -1244,14 +1251,15 @@
         if self.isGoing():
             if self.connectedNodeCount == self.totalNodes:
                 self.status = Status.started
             elif self.connectedNodeCount >= self.minimumNodes:
                 self.status = Status.started_hungry
             else:
                 self.status = Status.starting
+        self.elector.nodeCount = self.connectedNodeCount
 
         if self.master_primary_name in joined:
             self.primaries_disconnection_times[self.master_replica.instId] = None
         if self.master_primary_name in left:
             logger.display('{} lost connection to primary of master'.format(self))
             self.lost_master_primary()
         elif _prev_status == Status.starting and self.status == Status.started_hungry \
@@ -1261,15 +1269,14 @@
             Such situation may occur if the pool has come back to reachable consensus but
             primary is still disconnected, so view change proposal makes sense now.
             """
             self._schedule_view_change()
 
         for inst_id, replica in self.replicas.items():
             if not replica.isMaster and replica.primaryName is not None:
-                replica.update_connecteds(self.nodestack.connecteds)
                 primary_node_name = replica.primaryName.split(':')[0]
                 if primary_node_name in joined:
                     self.primaries_disconnection_times[inst_id] = None
                 elif primary_node_name in left:
                     self.primaries_disconnection_times[inst_id] = time.perf_counter()
                     self._schedule_replica_removal(inst_id)
 
@@ -1459,15 +1466,15 @@
         Process `limit` number of replica messages
         """
         # TODO: rewrite this using Router
 
         num_processed = 0
         for message in self.replicas.get_output(limit):
             num_processed += 1
-            if isinstance(message, (PrePrepare, Prepare, Commit, Checkpoint, MessageReq)):
+            if isinstance(message, (PrePrepare, Prepare, Commit, Checkpoint)):
                 self.send(message)
             elif isinstance(message, Ordered):
                 self.try_processing_ordered(message)
             elif isinstance(message, tuple) and isinstance(message[1], Reject):
                 with self.metrics.measure_time(MetricsName.NODE_SEND_REJECT_TIME):
                     digest, reject = message
                     result_reject = Reject(
@@ -1835,16 +1842,15 @@
         except Exception as ex:
             raise InvalidClientRequest(msg.get(f.IDENTIFIER.nm),
                                        msg.get(f.REQ_ID.nm)) from ex
 
         if needStaticValidation:
             self.doStaticValidation(cMsg)
 
-        self.replicas.send_to_internal_bus(PreSigVerification(cMsg),
-                                           self.master_replica.instId)
+        self.execute_hook(NodeHooks.PRE_SIG_VERIFICATION, cMsg)
         self.verifySignature(cMsg)
         logger.trace("{} received CLIENT message: {}".
                      format(self.clientstack.name, cMsg))
         return cMsg, frm
 
     def unpackClientMsg(self, msg, frm):
         """
@@ -2020,68 +2026,15 @@
                         extra={'cli': True})
 
             self.no_more_catchups_needed()
 
             if self.view_change_in_progress:
                 self.view_changer.on_catchup_complete()
             else:
-                self.select_primaries_on_catchup_complete()
-
-    def select_primaries_on_catchup_complete(self):
-        # Select primaries after usual catchup (not view change)
-        ledger = self.getLedger(AUDIT_LEDGER_ID)
-        self.backup_instance_faulty_processor.restore_replicas()
-        self.drop_primaries()
-        if len(ledger) == 0:
-            self.select_primaries()
-        else:
-            # Emulate view change start
-            self.view_changer.previous_view_no = self.viewNo
-            self.viewNo = get_payload_data(ledger.get_last_committed_txn())[AUDIT_TXN_VIEW_NO]
-            self.view_changer.previous_master_primary = self.master_primary_name
-            self.view_changer.set_defaults()
-
-            self.primaries = self._get_last_audited_primaries()
-            if len(self.replicas) != len(self.primaries):
-                logger.error('Audit ledger has inconsistent number of nodes. '
-                             'Node primaries = {}'.format(self.primaries))
-            if any(p not in self.nodeReg for p in self.primaries):
-                logger.error('Audit ledger has inconsistent names of primaries. '
-                             'Node primaries = {}'.format(self.primaries))
-            # Similar functionality to select_primaries
-            for instance_id, replica in self.replicas.items():
-                if instance_id == 0:
-                    self.start_participating()
-                replica.primaryChanged(
-                    Replica.generateName(self.primaries[instance_id], instance_id))
-                self.primary_selected(instance_id)
-
-        # Primary propagation
-        last_sent_pp_seq_no_restored = False
-        for replica in self.replicas.values():
-            replica.on_propagate_primary_done()
-        if self.view_changer.previous_view_no == 0:
-            last_sent_pp_seq_no_restored = \
-                self.last_sent_pp_store_helper.try_restore_last_sent_pp_seq_no()
-        if not last_sent_pp_seq_no_restored:
-            self.last_sent_pp_store_helper.erase_last_sent_pp_seq_no()
-
-        # Emulate view_change ending
-        self.on_view_propagated()
-
-    def _get_last_audited_primaries(self):
-        audit = self.getLedger(AUDIT_LEDGER_ID)
-        last_txn = audit.get_last_committed_txn()
-        last_txn_prim_value = get_payload_data(last_txn)[AUDIT_TXN_PRIMARIES]
-
-        if isinstance(last_txn_prim_value, int):
-            seq_no = get_seq_no(last_txn) - last_txn_prim_value
-            last_txn_prim_value = get_payload_data(audit.getBySeqNo(seq_no))[AUDIT_TXN_PRIMARIES]
-
-        return last_txn_prim_value
+                self.elector.on_catchup_complete()
 
     def is_catchup_needed(self) -> bool:
         # More than one catchup may be needed during the current ViewChange protocol
         if self.view_change_in_progress:
             return self.is_catchup_needed_during_view_change()
 
         # If we already have audit ledger we don't need any more catch-ups
@@ -2211,14 +2164,40 @@
             req_manager = self._get_manager_for_txn_type(txn_type)
             if req_manager is None:
                 raise InvalidClientRequest(identifier, req_id, 'invalid {}: {}'.
                                            format(TXN_TYPE, operation[TXN_TYPE]))
             else:
                 req_manager.static_validation(request)
 
+    # TODO hooks might need pp_time as well
+    def doDynamicValidation(self, request: Request, req_pp_time: int):
+        """
+        State based validation
+        """
+        # Digest validation
+        # TODO implicit caller's context: request is processed by (master) replica
+        # as part of PrePrepare 3PC batch
+        ledger_id, seq_no = self.seqNoDB.get_by_payload_digest(request.payload_digest)
+        if ledger_id is not None and seq_no is not None:
+            raise SuspiciousPrePrepare('Trying to order already ordered request')
+
+        ledger = self.getLedger(self.ledger_id_for_request(request))
+        for txn in ledger.uncommittedTxns:
+            if get_payload_digest(txn) == request.payload_digest:
+                raise SuspiciousPrePrepare('Trying to order already ordered request')
+
+        # specific validation for the request txn type
+        operation = request.operation
+        req_manager = self._get_manager_for_txn_type(txn_type=operation[TXN_TYPE])
+        # TAA validation
+        # For now, we need to call taa_validation not from dynamic_validation because
+        # req_pp_time is required
+        req_manager.do_taa_validation(request, req_pp_time, self.config)
+        req_manager.dynamic_validation(request)
+
     def applyReq(self, request: Request, cons_time: int):
         """
         Apply request to appropriate ledger and state. `cons_time` is the
         UTC epoch at which consensus was reached.
         """
         req_manager = self._get_manager_for_txn_type(txn_type=request.operation[TXN_TYPE])
         req_manager.apply_request(request, cons_time)
@@ -2507,15 +2486,15 @@
                 self.try_processing_ordered(message)
                 num_processed += 1
             logger.info('{} processed {} Ordered batches for instance {} '
                         'before starting catch up'
                         .format(self, num_processed, instance_id))
 
     def try_processing_ordered(self, msg):
-        if self.master_replica._ordering_service.can_order_commits():
+        if self.master_replica.validator.can_order():
             self.processOrdered(msg)
         else:
             logger.warning("{} can not process Ordered message {} since mode is {}".format(self, msg, self.mode))
 
     def processEscalatedException(self, ex):
         """
         Process an exception escalated from a Replica
@@ -2602,14 +2581,23 @@
 
         self.metrics.add_event(MetricsName.VIEW_CHANGER_INBOX, len(self.view_changer.inBox))
         self.metrics.add_event(MetricsName.VIEW_CHANGER_OUTBOX, len(self.view_changer.outBox))
         self.metrics.add_event(MetricsName.VIEW_CHANGER_NEXT_VIEW_INDICATIONS,
                                len(self.view_changer._next_view_indications))
         self.metrics.add_event(MetricsName.VIEW_CHANGER_VIEW_CHANGE_DONE, len(self.view_changer._view_change_done))
 
+        if self.primaryDecider:
+            self.metrics.add_event(MetricsName.PRIMARY_DECIDER_ACTION_QUEUE, len(self.primaryDecider.actionQueue))
+            self.metrics.add_event(MetricsName.PRIMARY_DECIDER_AQ_STASH, len(self.primaryDecider.aqStash))
+            self.metrics.add_event(MetricsName.PRIMARY_DECIDER_REPEATING_ACTIONS,
+                                   len(self.primaryDecider.repeatingActions))
+            self.metrics.add_event(MetricsName.PRIMARY_DECIDER_SCHEDULED, len(self.primaryDecider.scheduled))
+            self.metrics.add_event(MetricsName.PRIMARY_DECIDER_INBOX, len(self.primaryDecider.inBox))
+            self.metrics.add_event(MetricsName.PRIMARY_DECIDER_OUTBOX, len(self.primaryDecider.outBox))
+
         self.metrics.add_event(MetricsName.MSGS_FOR_FUTURE_REPLICAS, len(self.msgsForFutureReplicas))
         self.metrics.add_event(MetricsName.MSGS_TO_VIEW_CHANGER, len(self.msgsToViewChanger))
         self.metrics.add_event(MetricsName.REQUEST_SENDER, len(self.requestSender))
 
         self.metrics.add_event(MetricsName.MSGS_FOR_FUTURE_VIEWS, len(self.msgsForFutureViews))
 
         self.metrics.add_event(MetricsName.LEDGERMANAGER_POOL_UNCOMMITEDS, len(self.getLedger(0).uncommittedTxns))
@@ -2617,132 +2605,110 @@
         self.metrics.add_event(MetricsName.LEDGERMANAGER_CONFIG_UNCOMMITEDS, len(self.getLedger(2).uncommittedTxns))
 
         # REPLICAS
         self.metrics.add_event(MetricsName.REPLICA_OUTBOX_MASTER, len(self.master_replica.outBox))
         self.metrics.add_event(MetricsName.REPLICA_INBOX_MASTER, len(self.master_replica.inBox))
         self.metrics.add_event(MetricsName.REPLICA_INBOX_STASH_MASTER, len(self.master_replica.inBoxStash))
         self.metrics.add_event(MetricsName.REPLICA_PREPREPARES_PENDING_FIN_REQS_MASTER,
-                               len(self.master_replica._ordering_service.prePreparesPendingFinReqs))
+                               len(self.master_replica.prePreparesPendingFinReqs))
         self.metrics.add_event(MetricsName.REPLICA_PREPREPARES_PENDING_PREVPP_MASTER,
-                               len(self.master_replica._ordering_service.prePreparesPendingPrevPP))
+                               len(self.master_replica.prePreparesPendingPrevPP))
         self.metrics.add_event(MetricsName.REPLICA_PREPARES_WAITING_FOR_PREPREPARE_MASTER,
-                               sum_for_values(self.master_replica._ordering_service.preparesWaitingForPrePrepare))
+                               sum_for_values(self.master_replica.preparesWaitingForPrePrepare))
         self.metrics.add_event(MetricsName.REPLICA_COMMITS_WAITING_FOR_PREPARE_MASTER,
-                               sum_for_values(self.master_replica._ordering_service.commitsWaitingForPrepare))
-        self.metrics.add_event(MetricsName.REPLICA_SENT_PREPREPARES_MASTER, len(self.master_replica._ordering_service.sentPrePrepares))
-        self.metrics.add_event(MetricsName.REPLICA_PREPREPARES_MASTER, len(self.master_replica._ordering_service.prePrepares))
-        self.metrics.add_event(MetricsName.REPLICA_PREPARES_MASTER, len(self.master_replica._ordering_service.prepares))
-        self.metrics.add_event(MetricsName.REPLICA_COMMITS_MASTER, len(self.master_replica._ordering_service.commits))
+                               sum_for_values(self.master_replica.commitsWaitingForPrepare))
+        self.metrics.add_event(MetricsName.REPLICA_SENT_PREPREPARES_MASTER, len(self.master_replica.sentPrePrepares))
+        self.metrics.add_event(MetricsName.REPLICA_PREPREPARES_MASTER, len(self.master_replica.prePrepares))
+        self.metrics.add_event(MetricsName.REPLICA_PREPARES_MASTER, len(self.master_replica.prepares))
+        self.metrics.add_event(MetricsName.REPLICA_COMMITS_MASTER, len(self.master_replica.commits))
         self.metrics.add_event(MetricsName.REPLICA_PRIMARYNAMES_MASTER, len(self.master_replica.primaryNames))
         self.metrics.add_event(MetricsName.REPLICA_STASHED_OUT_OF_ORDER_COMMITS_MASTER,
                                sum_for_values(self.master_replica.stashed_out_of_order_commits))
-        self.metrics.add_event(MetricsName.REPLICA_CHECKPOINTS_MASTER,
-                               len(self.master_replica._consensus_data.checkpoints))
-        self.metrics.add_event(MetricsName.REPLICA_RECVD_CHECKPOINTS_MASTER,
-                               sum_for_values(self.master_replica._checkpointer._received_checkpoints))
+        self.metrics.add_event(MetricsName.REPLICA_CHECKPOINTS_MASTER, len(self.master_replica.checkpoints))
+        self.metrics.add_event(MetricsName.REPLICA_STASHED_RECVD_CHECKPOINTS_MASTER,
+                               sum_for_values(self.master_replica.stashedRecvdCheckpoints))
         self.metrics.add_event(MetricsName.REPLICA_STASHING_WHILE_OUTSIDE_WATERMARKS_MASTER,
-                               self.master_replica.stasher.stash_size(STASH_WATERMARKS))
+                               self.master_replica.stasher.num_stashed_watermarks)
         self.metrics.add_event(MetricsName.REPLICA_REQUEST_QUEUES_MASTER,
-                               sum_for_values(self.master_replica._ordering_service.requestQueues))
-        self.metrics.add_event(MetricsName.REPLICA_BATCHES_MASTER, len(self.master_replica._ordering_service.batches))
+                               sum_for_values(self.master_replica.requestQueues))
+        self.metrics.add_event(MetricsName.REPLICA_BATCHES_MASTER, len(self.master_replica.batches))
         self.metrics.add_event(MetricsName.REPLICA_REQUESTED_PRE_PREPARES_MASTER,
                                len(self.master_replica.requested_pre_prepares))
         self.metrics.add_event(MetricsName.REPLICA_REQUESTED_PREPARES_MASTER,
                                len(self.master_replica.requested_prepares))
         self.metrics.add_event(MetricsName.REPLICA_REQUESTED_COMMITS_MASTER, len(self.master_replica.requested_commits))
         self.metrics.add_event(MetricsName.REPLICA_PRE_PREPARES_STASHED_FOR_INCORRECT_TIME_MASTER,
-                               len(self.master_replica._ordering_service.pre_prepares_stashed_for_incorrect_time))
+                               len(self.master_replica.pre_prepares_stashed_for_incorrect_time))
 
         self.metrics.add_event(MetricsName.REPLICA_ACTION_QUEUE_MASTER, len(self.master_replica.actionQueue))
         self.metrics.add_event(MetricsName.REPLICA_AQ_STASH_MASTER, len(self.master_replica.aqStash))
         self.metrics.add_event(MetricsName.REPLICA_REPEATING_ACTIONS_MASTER, len(self.master_replica.repeatingActions))
         self.metrics.add_event(MetricsName.REPLICA_SCHEDULED_MASTER, len(self.master_replica.scheduled))
         self.metrics.add_event(MetricsName.REPLICA_STASHED_CATCHUP_MASTER,
-                               self.master_replica.stasher.stash_size(STASH_CATCH_UP))
+                               self.master_replica.stasher.num_stashed_catchup)
         self.metrics.add_event(MetricsName.REPLICA_STASHED_FUTURE_VIEW_MASTER,
-                               self.master_replica.stasher.stash_size(STASH_VIEW))
+                               self.master_replica.stasher.num_stashed_future_view)
         self.metrics.add_event(MetricsName.REPLICA_STASHED_WATERMARKS_MASTER,
-                               self.master_replica.stasher.stash_size(STASH_WATERMARKS))
+                               self.master_replica.stasher.num_stashed_watermarks)
 
         def sum_for_backups(field):
             return sum(len(getattr(r, field)) for r in self.replicas._replicas.values() if r is not self.master_replica)
 
-        def sum_for_backups_ordering_service(field):
-            return sum(len(getattr(r._ordering_service, field)) for r in self.replicas._replicas.values() if r is not self.master_replica)
-
-        def sum_for_backups_data(field):
-            return sum(len(getattr(r._consensus_data, field)) for r in self.replicas._replicas.values() if r is not self.master_replica)
-
         def sum_for_values_for_backups(field):
             return sum(sum_for_values(getattr(r, field))
                        for r in self.replicas._replicas.values() if r is not self.master_replica)
 
-        def sum_for_values_for_backups_ordering_service(field):
-            return sum(sum_for_values(getattr(r._ordering_service, field))
-                       for r in self.replicas._replicas.values() if r is not self.master_replica)
-
-        def sum_for_values_for_backups_checkpointer(field):
-            return sum(sum_for_values(getattr(r._checkpointer, field))
-                       for r in self.replicas._replicas.values() if r is not self.master_replica)
-
         self.metrics.add_event(MetricsName.REPLICA_OUTBOX_BACKUP, sum_for_backups('outBox'))
         self.metrics.add_event(MetricsName.REPLICA_INBOX_BACKUP, sum_for_backups('inBox'))
         self.metrics.add_event(MetricsName.REPLICA_INBOX_STASH_BACKUP, sum_for_backups('inBoxStash'))
         self.metrics.add_event(MetricsName.REPLICA_PREPREPARES_PENDING_FIN_REQS_BACKUP,
-                               sum_for_backups_ordering_service('prePreparesPendingFinReqs'))
+                               sum_for_backups('prePreparesPendingFinReqs'))
         self.metrics.add_event(MetricsName.REPLICA_PREPREPARES_PENDING_PREVPP_BACKUP,
-                               sum_for_backups_ordering_service('prePreparesPendingPrevPP'))
+                               sum_for_backups('prePreparesPendingPrevPP'))
         self.metrics.add_event(MetricsName.REPLICA_PREPARES_WAITING_FOR_PREPREPARE_BACKUP,
-                               sum_for_values_for_backups_ordering_service('preparesWaitingForPrePrepare'))
+                               sum_for_values_for_backups('preparesWaitingForPrePrepare'))
         self.metrics.add_event(MetricsName.REPLICA_COMMITS_WAITING_FOR_PREPARE_BACKUP,
-                               sum_for_values_for_backups_ordering_service('commitsWaitingForPrepare'))
-        self.metrics.add_event(MetricsName.REPLICA_SENT_PREPREPARES_BACKUP,
-                               sum_for_backups_ordering_service('sentPrePrepares'))
-        self.metrics.add_event(MetricsName.REPLICA_PREPREPARES_BACKUP,
-                               sum_for_backups_ordering_service('prePrepares'))
-        self.metrics.add_event(MetricsName.REPLICA_PREPARES_BACKUP,
-                               sum_for_backups_ordering_service('prepares'))
-        self.metrics.add_event(MetricsName.REPLICA_COMMITS_BACKUP,
-                               sum_for_backups_ordering_service('commits'))
-        self.metrics.add_event(MetricsName.REPLICA_PRIMARYNAMES_BACKUP,
-                               sum_for_backups('primaryNames'))
+                               sum_for_values_for_backups('commitsWaitingForPrepare'))
+        self.metrics.add_event(MetricsName.REPLICA_SENT_PREPREPARES_BACKUP, sum_for_backups('sentPrePrepares'))
+        self.metrics.add_event(MetricsName.REPLICA_PREPREPARES_BACKUP, sum_for_backups('prePrepares'))
+        self.metrics.add_event(MetricsName.REPLICA_PREPARES_BACKUP, sum_for_backups('prepares'))
+        self.metrics.add_event(MetricsName.REPLICA_COMMITS_BACKUP, sum_for_backups('commits'))
+        self.metrics.add_event(MetricsName.REPLICA_PRIMARYNAMES_BACKUP, sum_for_backups('primaryNames'))
         self.metrics.add_event(MetricsName.REPLICA_STASHED_OUT_OF_ORDER_COMMITS_BACKUP,
-                               sum_for_values_for_backups_ordering_service('stashed_out_of_order_commits'))
-        self.metrics.add_event(MetricsName.REPLICA_CHECKPOINTS_BACKUP, sum_for_backups_data('checkpoints'))
-        self.metrics.add_event(MetricsName.REPLICA_RECVD_CHECKPOINTS_BACKUP,
-                               sum_for_values_for_backups_checkpointer('_received_checkpoints'))
+                               sum_for_values_for_backups('stashed_out_of_order_commits'))
+        self.metrics.add_event(MetricsName.REPLICA_CHECKPOINTS_BACKUP, sum_for_backups('checkpoints'))
+        self.metrics.add_event(MetricsName.REPLICA_STASHED_RECVD_CHECKPOINTS_BACKUP,
+                               sum_for_values_for_backups('stashedRecvdCheckpoints'))
         self.metrics.add_event(MetricsName.REPLICA_STASHING_WHILE_OUTSIDE_WATERMARKS_BACKUP,
-                               sum(r.stasher.stash_size(STASH_WATERMARKS) for r in self.replicas.values()))
+                               sum(r.stasher.num_stashed_watermarks for r in self.replicas.values()))
         self.metrics.add_event(MetricsName.REPLICA_REQUEST_QUEUES_BACKUP,
-                               sum_for_values_for_backups_ordering_service('requestQueues'))
-        self.metrics.add_event(MetricsName.REPLICA_BATCHES_BACKUP, sum_for_backups_ordering_service('batches'))
+                               sum_for_values_for_backups('requestQueues'))
+        self.metrics.add_event(MetricsName.REPLICA_BATCHES_BACKUP, sum_for_backups('batches'))
         self.metrics.add_event(MetricsName.REPLICA_REQUESTED_PRE_PREPARES_BACKUP,
-                               sum_for_backups_ordering_service('requested_pre_prepares'))
-        self.metrics.add_event(MetricsName.REPLICA_REQUESTED_PREPARES_BACKUP,
-                               sum_for_backups_ordering_service('requested_prepares'))
-        self.metrics.add_event(MetricsName.REPLICA_REQUESTED_COMMITS_BACKUP,
-                               sum_for_backups_ordering_service('requested_commits'))
+                               sum_for_backups('requested_pre_prepares'))
+        self.metrics.add_event(MetricsName.REPLICA_REQUESTED_PREPARES_BACKUP, sum_for_backups('requested_prepares'))
+        self.metrics.add_event(MetricsName.REPLICA_REQUESTED_COMMITS_BACKUP, sum_for_backups('requested_commits'))
         self.metrics.add_event(MetricsName.REPLICA_PRE_PREPARES_STASHED_FOR_INCORRECT_TIME_BACKUP,
-                               sum_for_backups_ordering_service('pre_prepares_stashed_for_incorrect_time'))
+                               sum_for_backups('pre_prepares_stashed_for_incorrect_time'))
         self.metrics.add_event(MetricsName.REPLICA_ACTION_QUEUE_BACKUP, sum_for_backups('actionQueue'))
         self.metrics.add_event(MetricsName.REPLICA_AQ_STASH_BACKUP, sum_for_backups('aqStash'))
         self.metrics.add_event(MetricsName.REPLICA_REPEATING_ACTIONS_BACKUP, sum_for_backups('repeatingActions'))
         self.metrics.add_event(MetricsName.REPLICA_SCHEDULED_BACKUP, sum_for_backups('scheduled'))
 
         # Stashed msgs
-        def sum_stashed_for_backups(stash_type):
-            return sum(r.stasher.stash_size(stash_type)
+        def sum_stashed_for_backups(field):
+            return sum(getattr(r.stasher, field)
                        for r in self.replicas._replicas.values() if r is not self.master_replica)
 
         self.metrics.add_event(MetricsName.REPLICA_STASHED_CATCHUP_BACKUP,
-                               sum_stashed_for_backups(STASH_CATCH_UP))
+                               sum_stashed_for_backups('num_stashed_catchup'))
         self.metrics.add_event(MetricsName.REPLICA_STASHED_FUTURE_VIEW_BACKUP,
-                               sum_stashed_for_backups(STASH_VIEW))
+                               sum_stashed_for_backups('num_stashed_future_view'))
         self.metrics.add_event(MetricsName.REPLICA_STASHED_WATERMARKS_BACKUP,
-                               sum_stashed_for_backups(STASH_WATERMARKS))
+                               sum_stashed_for_backups('num_stashed_watermarks'))
 
         def store_rocksdb_metrics(name, storage):
             if not hasattr(storage, '_db'):
                 return
             if not hasattr(storage._db, 'get_property'):
                 return
             self.metrics.add_event(name, int(storage._db.get_property(b"rocksdb.estimate-table-readers-mem")))
@@ -2901,27 +2867,25 @@
         """
         Schedule an primary connection check which in turn can send a view
         change message
         """
         self.primaries_disconnection_times[self.master_replica.instId] = time.perf_counter()
         self._schedule_view_change()
 
-    def get_primaries_for_current_view(self):
-        return self.primaries_selector.select_primaries(view_no=self.viewNo,
-                                                        instance_count=self.requiredNumberOfInstances,
-                                                        validators=self.poolManager.node_names_ordered_by_rank())
-
     def select_primaries(self):
         # If you want to refactor primaries selection,
         # please take a look at https://jira.hyperledger.org/browse/INDY-1946
 
         self.backup_instance_faulty_processor.restore_replicas()
         self.ensure_primaries_dropped()
 
-        self.primaries = self.get_primaries_for_current_view()
+        self.primaries = self.elector.process_selection(
+            self.requiredNumberOfInstances,
+            self.nodeReg, self.poolManager._ordered_node_ids)
+
         pc = len(self.primaries)
         rc = len(self.replicas)
         if pc != rc:
             raise LogicError('Inconsistent number or primaries ({}) and replicas ({})'
                              .format(pc, rc))
 
         for i, primary_name in enumerate(self.primaries):
@@ -2948,23 +2912,23 @@
                                    instance_name,
                                    self.ledger_summary),
                            extra={"cli": "ANNOUNCE",
                                   "tags": ["node-election"]})
 
         # Notify replica, that we need to send batch with new primaries
         if self.viewNo != 0:
-            for r in self.replicas.values():
-                r.set_primaries_batch_needed(True)
+            self.primaries_batch_needed = True
 
     def _do_start_catchup(self, just_started: bool):
         # Process any already Ordered requests by the replica
         self.force_process_ordered()
 
         # # revert uncommitted txns and state for unordered requests
-        self.master_replica.revert_unordered_batches()
+        r = self.master_replica.revert_unordered_batches()
+        logger.info('{} reverted {} batches before starting catch up'.format(self, r))
 
         self.mode = Mode.starting
         self.ledgerManager.start_catchup(is_initial=just_started)
 
     def start_catchup(self, just_started=False):
         if not self.is_synced and not just_started:
             logger.info('{} does not start the catchup procedure '
@@ -3515,32 +3479,16 @@
 
     def init_req_managers(self):
         self.write_manager = WriteRequestManager(self.db_manager)
         self.read_manager = ReadRequestManager()
         self.action_manager = ActionRequestManager()
 
     def _bootstrap_node(self, bootstrap_cls, storage):
-        bootstrap_cls(self).init(domain_storage=storage)
+        bootstrap_cls(self).init_node(storage)
 
     def get_validators(self):
         return self.poolManager.node_ids_ordered_by_rank(
             self.nodeReg, self.poolManager.get_node_ids())
 
     def set_view_for_replicas(self, view_no):
         for r in self.replicas.values():
             r.set_view_no(view_no)
-
-    def _process_start_master_catchup_msg(self, msg: NeedMasterCatchup):
-        self.start_catchup()
-
-    def _subscribe_to_internal_msgs(self):
-        self.replicas.subscribe_to_internal_bus(RequestPropagates, self.request_propagates)
-        self.replicas.subscribe_to_internal_bus(NeedMasterCatchup,
-                                                self._process_start_master_catchup_msg,
-                                                self.master_replica.instId)
-
-    def set_view_change_status(self, value: bool):
-        """
-        Remove this method after a PBFT ViewChange integration
-        """
-        for r in self.replicas.values():
-            r.set_view_change_status(value)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/view_change/view_change_msg_filter.py` & `indy-plenum-1.9.2rc1/plenum/server/view_change/view_change_msg_filter.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/view_change/node_view_changer.py` & `indy-plenum-1.9.2rc1/plenum/server/view_change/node_view_changer.py`

 * *Files 6% similar despite different names*

```diff
@@ -34,15 +34,16 @@
     def is_node_synced(self) -> bool:
         return self._node.is_synced
 
     def node_mode(self) -> Mode:
         return self._node.mode
 
     def next_primary_name(self) -> str:
-        return self._node.get_primaries_for_current_view()[0]
+        return self._node.elector._next_primary_node_name_for_master(
+            self._node.nodeReg, self._node.nodeIds)
 
     def current_primary_name(self) -> str:
         return self._node.master_primary_name
 
     def has_primary(self) -> bool:
         return self._node.master_replica.hasPrimary
 
@@ -87,17 +88,14 @@
 
     def ensure_primaries_dropped(self):
         self._node.ensure_primaries_dropped()
 
     def discard(self, msg, reason, logMethod=logging.error, cliOutput=False):
         self._node.discard(msg, reason, logMethod, cliOutput)
 
-    def set_view_change_status(self, value: bool):
-        self._node.set_view_change_status(value)
-
     @property
     def node_status_db(self) -> KeyValueStorage:
         return self._node.nodeStatusDB
 
     def view_setting_handler(self, view_no):
         self._node.set_view_for_replicas(view_no)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/view_change/view_changer.py` & `indy-plenum-1.9.2rc1/plenum/server/view_change/view_changer.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import logging
 from abc import ABC, abstractmethod
 from collections import deque
 from typing import List, Optional, Tuple, Set
 from functools import partial
 
 from common.exceptions import LogicError
-from plenum.common.event_bus import InternalBus
 from plenum.common.startable import Mode
 from plenum.common.timer import TimerService, RepeatingTimer
 from plenum.server.quorums import Quorums
 from plenum.server.view_change.instance_change_provider import InstanceChangeProvider
 from storage.kv_store import KeyValueStorage
 from stp_core.common.log import getlogger
 
@@ -133,18 +132,14 @@
     def view_setting_handler(self, view_no):
         pass
 
     @abstractmethod
     def schedule_resend_inst_chng(self):
         pass
 
-    @abstractmethod
-    def set_view_change_status(self, value: bool):
-        pass
-
 
 class ViewChanger():
 
     def __init__(self, provider: ViewChangerDataProvider, timer: TimerService):
         self.provider = provider
         self._timer = timer
         self.pre_vc_strategy = None
@@ -231,15 +226,14 @@
     @property
     def view_change_in_progress(self) -> bool:
         return self._view_change_in_progress
 
     @view_change_in_progress.setter
     def view_change_in_progress(self, value: bool):
         self._view_change_in_progress = value
-        self.provider.set_view_change_status(value)
 
     @property
     def quorum(self) -> int:
         return self.quorums.view_change_done.value
 
     @property
     def _hasViewChangeQuorum(self):
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/view_change/pre_view_change_strategies.py` & `indy-plenum-1.9.2rc1/plenum/server/view_change/pre_view_change_strategies.py`

 * *Files 3% similar despite different names*

```diff
@@ -100,15 +100,15 @@
             logger.info("VCStartMsgStrategy: Got {} messages from nodestack".format(msgs_count))
             strategy.stashedNodeInBox = await VCStartMsgStrategy._process_node_inbox_3PC(node)
             logger.info("VCStartMsgStrategy: {} not 3PC msgs was stashed".format(len(strategy.stashedNodeInBox)))
             node.master_replica.inBox.append(vcc_msg)
 
     """Handler for processing ViewChangeStart message on replica's inBoxRouter"""
     @staticmethod
-    def on_view_change_continued(replica, msg: ViewChangeContinueMessage, sender: str=None):
+    def on_view_change_continued(replica, msg: ViewChangeContinueMessage):
         strategy = replica.node.view_changer.pre_vc_strategy
         proposed_view_no = msg.proposed_view_no
         replica.logger.info("VCStartMsgStrategy: got ViewChangeContinueMessage with proposed_view_no: {}".format(proposed_view_no))
         if proposed_view_no > replica.node.viewNo:
             """
             Return stashed not 3PC msgs to nodeInBox queue and start ViewChange
             Critical assumption: All 3PC msgs passed from node already processed
@@ -121,22 +121,23 @@
     def unstash_messages(self):
         logger.info("VCStartMsgStrategy: unstash all not 3PC msgs to nodeInBox queue")
         while self.stashedNodeInBox:
             self.node.nodeInBox.appendleft(self.stashedNodeInBox.pop())
 
     def _set_req_handlers(self):
         node_msg_router = self.node.nodeMsgRouter
-        replicas_external_bus = self.replica._external_bus
+        replica_msg_router = self.replica.inBoxRouter
 
         if ViewChangeStartMessage not in node_msg_router.routes:
             processor = partial(VCStartMsgStrategy.on_view_change_started,
                                 self.node)
             node_msg_router.add((ViewChangeStartMessage, processor))
 
-        processor = partial(VCStartMsgStrategy.on_view_change_continued,
-                            self.replica)
-        replicas_external_bus.subscribe(ViewChangeContinueMessage, processor)
+        if ViewChangeContinueMessage not in replica_msg_router.routes:
+            processor = partial(VCStartMsgStrategy.on_view_change_continued,
+                                self.replica)
+            replica_msg_router.add((ViewChangeContinueMessage, processor))
 
 
 preVCStrategies = {
     PreVCStrategies.VC_START_MSG_STRATEGY: VCStartMsgStrategy
 }
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/view_change/instance_change_provider.py` & `indy-plenum-1.9.2rc1/plenum/server/view_change/instance_change_provider.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/router.py` & `indy-plenum-1.9.2rc1/plenum/server/router.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/observer/observable_sync_policy_each_batch.py` & `indy-plenum-1.9.2rc1/plenum/server/observer/observable_sync_policy_each_batch.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/observer/observer_sync_policy_each_batch.py` & `indy-plenum-1.9.2rc1/plenum/server/observer/observer_sync_policy_each_batch.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/observer/observable_sync_policy.py` & `indy-plenum-1.9.2rc1/plenum/server/observer/observable_sync_policy.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/observer/observable.py` & `indy-plenum-1.9.2rc1/plenum/server/observer/observable.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/observer/observer.py` & `indy-plenum-1.9.2rc1/plenum/server/observer/observer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/observer/observer_sync_policy.py` & `indy-plenum-1.9.2rc1/plenum/server/observer/observer_sync_policy.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/observer/observer_node.py` & `indy-plenum-1.9.2rc1/plenum/server/observer/observer_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/consensus/ordering_service_msg_validator.py` & `indy-plenum-1.9.2rc1/ledger/hash_stores/file_hash_store.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,109 +1,120 @@
-from typing import Tuple, Optional
-
-from common.exceptions import LogicError
-from plenum.common.messages.internal_messages import NewViewCheckpointsApplied
-from plenum.common.messages.node_messages import PrePrepare, Commit, Prepare
-from plenum.common.stashing_router import DISCARD, PROCESS
-from plenum.common.types import f
-from plenum.common.util import compare_3PC_keys
-from plenum.server.consensus.consensus_shared_data import ConsensusSharedData
-from plenum.server.replica_validator_enums import STASH_WAITING_NEW_VIEW, STASH_WATERMARKS, STASH_VIEW, STASH_CATCH_UP, \
-    ALREADY_ORDERED, OUTSIDE_WATERMARKS, CATCHING_UP, FUTURE_VIEW, OLD_VIEW, WAITING_FOR_NEW_VIEW
-
-
-class OrderingServiceMsgValidator:
-
-    def __init__(self, data: ConsensusSharedData):
-        self._data = data
-
-    # TODO: this method is created for compatibility adn tests only
-    # validate_XXX methods will be used later
-    def validate(self, msg) -> Tuple[int, Optional[str]]:
-        if isinstance(msg, PrePrepare):
-            return self.validate_pre_prepare(msg)
-        if isinstance(msg, Prepare):
-            return self.validate_prepare(msg)
-        if isinstance(msg, Commit):
-            return self.validate_commit(msg)
-        if isinstance(msg, NewViewCheckpointsApplied):
-            return self.validate_new_view(msg)
-        raise LogicError("Unknown message type")
-
-    def validate_pre_prepare(self, msg: PrePrepare) -> Tuple[int, Optional[str]]:
-        # we discard already ordered PrePrepares
-        # check for discarded first of all
-        view_no = getattr(msg, f.VIEW_NO.nm, None)
-        pp_seq_no = getattr(msg, f.PP_SEQ_NO.nm, None)
-        if self.has_already_ordered(view_no, pp_seq_no):
-            return DISCARD, ALREADY_ORDERED
-
-        return self._validate_3pc(msg)
-
-    def validate_prepare(self, msg: Prepare) -> Tuple[int, Optional[str]]:
-        # process Prepares that have been already ordered
-        # to re-order batches from NEW_VIEW msg
-        # prepares below low watermark have been discarded anyway
-        return self._validate_3pc(msg)
-
-    def validate_commit(self, msg: Commit) -> Tuple[int, Optional[str]]:
-        # process Prepares that have been already ordered
-        # to re-order batches from NEW_VIEW msg
-        # prepares below low watermark have been discarded anyway
-        return self._validate_3pc(msg)
-
-    def validate_new_view(self, msg: NewViewCheckpointsApplied) -> Tuple[int, Optional[str]]:
-        # View Change service has already validated NewView
-        # so basic validation here is sufficient
-        return self._validate_base(msg, msg.view_no)
-
-    def _validate_3pc(self, msg) -> Tuple[int, Optional[str]]:
-        pp_seq_no = getattr(msg, f.PP_SEQ_NO.nm, None)
-        view_no = getattr(msg, f.VIEW_NO.nm, None)
-
-        # DISCARD CHECKS first
-
-        # Check if below lower watermark (meaning it's already ordered)
-        if pp_seq_no <= self._data.low_watermark:
-            return DISCARD, ALREADY_ORDERED
-
-        # Default checks next
-        res, reason = self._validate_base(msg, view_no)
-        if res != PROCESS:
-            return res, reason
-
-        # STASH CHECKS finally
-
-        # Check if waiting for new view
-        if self._data.waiting_for_new_view:
-            return STASH_WAITING_NEW_VIEW, WAITING_FOR_NEW_VIEW
-
-        # Check if above high watermarks
-        if pp_seq_no is not None and pp_seq_no > self._data.high_watermark:
-            return STASH_WATERMARKS, OUTSIDE_WATERMARKS
-
-        # PROCESS
-        return PROCESS, None
-
-    def _validate_base(self, msg, view_no) -> Tuple[int, Optional[str]]:
-        # DISCARD CHECKS
-
-        # Check if from old view
-        if view_no < self._data.view_no:
-            return DISCARD, OLD_VIEW
-
-        # STASH CHECKS
-
-        # Check if from future view
-        if view_no > self._data.view_no:
-            return STASH_VIEW, FUTURE_VIEW
-
-        # Check if catchup is in progress
-        if not self._data.is_participating:
-            return STASH_CATCH_UP, CATCHING_UP
-
-        # PROCESS
-        return PROCESS, None
-
-    def has_already_ordered(self, view_no, pp_seq_no):
-        return compare_3PC_keys((view_no, pp_seq_no),
-                                self._data.last_ordered_3pc) >= 0
+from ledger.hash_stores.hash_store import HashStore
+from storage.binary_file_store import BinaryFileStore
+from storage.kv_store_file import KeyValueStorageFile
+
+
+class FileHashStore(HashStore):
+    # Hashes are stored as raw bytes. By default each leaf hash is of 32 bytes
+    # and each node hash is too of 32 bytes. The extra 5 bytes in for each node
+    # are used to store the `start` and `height`. `start` takes 4 bytes so it
+    # can support upto 1 billion nodes and height takes 1 byte so it can store
+    # a tree upto the height of 255
+    def __init__(self, dataDir, fileNamePrefix="", leafSize=32, nodeSize=32):
+        self.dataDir = dataDir
+        self.fileNamePrefix = fileNamePrefix
+        nodesFileName = fileNamePrefix + "_merkleNodes"
+        leavesFileName = fileNamePrefix + "_merkleLeaves"
+
+        self.nodesFile = BinaryFileStore(self.dataDir, nodesFileName,
+                                         isLineNoKey=True,
+                                         storeContentHash=False)
+        self.leavesFile = BinaryFileStore(self.dataDir, leavesFileName,
+                                          isLineNoKey=True,
+                                          storeContentHash=False)
+
+        # Do not need line separators since each entry is of fixed size
+        self.nodesFile.lineSep = b''
+        self.leavesFile.lineSep = b''
+        self.nodeSize = nodeSize
+        self.leafSize = leafSize
+
+    @property
+    def is_persistent(self) -> bool:
+        return True
+
+    @staticmethod
+    def write(data, store, size):
+        if not isinstance(data, bytes):
+            data = data.encode()
+        dataSize = len(data)
+        if dataSize != size:
+            raise ValueError(
+                "Data size not allowed. Size of the data should be "
+                "{} but instead was {}".format(
+                    size, dataSize))
+        store.put(key=None, value=data)
+
+    @staticmethod
+    def read(store: KeyValueStorageFile, entryNo, size):
+        store.db_file.seek((entryNo - 1) * size)
+        return store.db_file.read(size)
+
+    @staticmethod
+    def dataGen(dataFactory, startpos, endpos):
+        i = startpos
+        while True:
+            data = dataFactory(i)
+            yield data
+            i += 1
+            if i <= endpos:
+                break
+
+    def writeNode(self, node):
+        # TODO: Need to have some exception handling around converting to bytes
+        # since they can result in `OverflowError`
+        # start, height, nodeHash = node
+        # start = start.to_bytes(4, byteorder='little')
+        # height = height.to_bytes(1, byteorder='little')
+        # data = start + height + nodeHash
+        data = node[2]
+        self.write(data, self.nodesFile, self.nodeSize)
+
+    def writeLeaf(self, leafHash):
+        self.write(leafHash, self.leavesFile, self.leafSize)
+
+    def readNode(self, pos):
+        data = self.read(self.nodesFile, pos, self.nodeSize)
+        if len(data) < self.nodeSize:
+            raise IndexError("No node at given position")
+        # start = int.from_bytes(data[:4], byteorder='little')
+        # height = int.from_bytes(data[4:5], byteorder='little')
+        # nodeHash = data[5:]
+        # return start, height, nodeHash
+        return data
+
+    def readLeaf(self, pos):
+        data = self.read(self.leavesFile, pos, self.leafSize)
+        if len(data) < self.leafSize:
+            raise IndexError("No leaf at given position")
+        return data
+
+    def readLeafs(self, startpos, endpos):
+        return self.dataGen(self.readLeaf, startpos, endpos)
+
+    def readNodes(self, startpos, endpos):
+        return self.dataGen(self.readNode, startpos, endpos)
+
+    @property
+    def leafCount(self) -> int:
+        return self.leavesFile.db_file.seek(0, 2) // self.leafSize
+
+    @property
+    def nodeCount(self) -> int:
+        return self.nodesFile.db_file.seek(0, 2) // self.nodeSize
+
+    @property
+    def closed(self):
+        return self.nodesFile.closed and self.leavesFile.closed
+
+    def open(self):
+        self.nodesFile.open()
+        self.leavesFile.open()
+
+    def close(self):
+        self.nodesFile.close()
+        self.leavesFile.close()
+
+    def reset(self):
+        self.nodesFile.reset()
+        self.leavesFile.reset()
+        return True
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/consensus/replica_service.py` & `indy-plenum-1.9.2rc1/plenum/server/consensus/replica_service.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,49 +1,36 @@
 from typing import List
 
-from plenum.server.replica_freshness_checker import FreshnessChecker
-
 from crypto.bls.bls_bft_replica import BlsBftReplica
-from plenum.common.config_util import getConfig
 from plenum.common.event_bus import InternalBus, ExternalBus
 from plenum.common.messages.node_messages import Checkpoint
-from plenum.common.stashing_router import StashingRouter
 from plenum.common.timer import TimerService
 from plenum.server.consensus.checkpoint_service import CheckpointService
 from plenum.server.consensus.consensus_shared_data import ConsensusSharedData
 from plenum.server.consensus.ordering_service import OrderingService
 from plenum.server.consensus.view_change_service import ViewChangeService
 from plenum.server.request_managers.write_request_manager import WriteRequestManager
-from plenum.test.testing_utils import FakeSomething
 
 
 class ReplicaService:
     """
     This is a wrapper consensus-related services. Now it is intended mostly for
     simulation tests, however in future it can replace actual Replica in plenum.
     """
 
     def __init__(self, name: str, validators: List[str], primary_name: str,
-                 timer: TimerService, bus: InternalBus, network: ExternalBus,
-                 write_manager: WriteRequestManager,
+                 timer: TimerService, bus: InternalBus, network: ExternalBus, write_manager: WriteRequestManager=None,
                  bls_bft_replica: BlsBftReplica=None):
         self._data = ConsensusSharedData(name, validators, 0)
         self._data.primary_name = primary_name
-        config = getConfig()
-        stasher = StashingRouter(config.REPLICA_STASH_LIMIT, buses=[bus, network])
         self._orderer = OrderingService(data=self._data,
                                         timer=timer,
                                         bus=bus,
                                         network=network,
                                         write_manager=write_manager,
-                                        bls_bft_replica=bls_bft_replica,
-                                        freshness_checker=FreshnessChecker(
-                                            freshness_timeout=config.STATE_FRESHNESS_UPDATE_INTERVAL),
-                                        stasher=stasher)
-        self._checkpointer = CheckpointService(self._data, bus, network, stasher,
-                                               write_manager.database_manager)
-        self._view_changer = ViewChangeService(self._data, timer, bus, network, stasher)
+                                        bls_bft_replica=bls_bft_replica)
+        self._checkpointer = CheckpointService(self._data, bus, network)
+        self._view_changer = ViewChangeService(self._data, timer, bus, network)
 
         # TODO: This is just for testing purposes only
         self._data.checkpoints.append(
-            Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0,
-                       digest='4F7BsTMVPKFshM1MwLf6y23cid6fL3xMpazVoF9krzUw'))
+            Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty'))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/consensus/checkpoint_service.py` & `indy-plenum-1.9.2rc1/plenum/server/consensus/view_change_service.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,304 +1,443 @@
+from _sha256 import sha256
 from collections import defaultdict
+from functools import partial
+from typing import List, Optional, Union, NamedTuple, Dict, Any, Tuple
 
-import sys
-from typing import Tuple, NamedTuple, List, Set
-
-from common.exceptions import LogicError
+from common.serializers.json_serializer import JsonSerializer
 from plenum.common.config_util import getConfig
 from plenum.common.event_bus import InternalBus, ExternalBus
-from plenum.common.messages.internal_messages import NeedMasterCatchup, NeedBackupCatchup, CheckpointStabilized, \
-    BackupSetupLastOrdered, NewViewAccepted, NewViewCheckpointsApplied
-from plenum.common.messages.node_messages import Checkpoint, Ordered
-from plenum.common.metrics_collector import MetricsName, MetricsCollector, NullMetricsCollector
-from plenum.common.router import Subscription
-from plenum.common.stashing_router import StashingRouter, PROCESS
-from plenum.common.util import compare_3PC_keys
+from plenum.common.messages.node_messages import ViewChange, ViewChangeAck, NewView, PrePrepare, Checkpoint
+from plenum.common.stashing_router import StashingRouter, PROCESS, DISCARD, STASH
+from plenum.common.timer import TimerService
 from plenum.server.consensus.consensus_shared_data import ConsensusSharedData
-from plenum.server.consensus.metrics_decorator import measure_consensus_time
-from plenum.server.consensus.msg_validator import CheckpointMsgValidator
-from plenum.server.database_manager import DatabaseManager
-from plenum.server.replica_validator_enums import STASH_WATERMARKS
+from plenum.server.quorums import Quorums
 from stp_core.common.log import getlogger
 
+BatchID = NamedTuple('BatchID', [('view_no', int), ('pp_seq_no', int), ('pp_digest', str)])
 
-class CheckpointService:
-    STASHED_CHECKPOINTS_BEFORE_CATCHUP = 1
 
-    # TODO: Remove view_no from key after implementing INDY-1336
-    CheckpointKey = NamedTuple('CheckpointKey',
-                               [('view_no', int), ('pp_seq_no', int), ('digest', str)])
-
-    def __init__(self, data: ConsensusSharedData, bus: InternalBus, network: ExternalBus,
-                 stasher: StashingRouter, db_manager: DatabaseManager,
-                 metrics: MetricsCollector = NullMetricsCollector(),):
-        self._data = data
-        self._bus = bus
-        self._network = network
-        self._stasher = stasher
-        self._subscription = Subscription()
-        self._validator = CheckpointMsgValidator(self._data)
-        self._db_manager = db_manager
-        self.metrics = metrics
+def view_change_digest(msg: ViewChange) -> str:
+    msg_as_dict = msg.__dict__
+    msg_as_dict['checkpoints'] = [cp.__dict__ for cp in msg_as_dict['checkpoints']]
+    serialized = JsonSerializer().dumps(msg_as_dict)
+    return sha256(serialized).hexdigest()
+
+
+class ViewChangeVotesForNode:
+    """
+    Storage for view change vote from some node for some view + corresponding acks
+    """
+
+    def __init__(self, quorums: Quorums):
+        self._quorums = quorums
+        self._view_change = None
+        self._digest = None
+        self._acks = defaultdict(set)  # Dict[str, Set[str]]
 
-        # Received checkpoints, mapping CheckpointKey -> List(node_alias)
-        self._received_checkpoints = defaultdict(set)  # type: Dict[CheckpointService.CheckpointKey, Set[str]]
+    @property
+    def digest(self) -> Optional[str]:
+        """
+        Returns digest of received view change message
+        """
+        return self._digest
 
-        self._config = getConfig()
-        self._logger = getlogger()
+    @property
+    def view_change(self) -> Optional[ViewChange]:
+        """
+        Returns received view change
+        """
+        return self._view_change
 
-        self._subscription.subscribe(stasher, Checkpoint, self.process_checkpoint)
+    @property
+    def is_confirmed(self) -> bool:
+        """
+        Returns True if received view change message and enough corresponding acks
+        """
+        if self._digest is None:
+            return False
 
-        self._subscription.subscribe(bus, Ordered, self.process_ordered)
-        self._subscription.subscribe(bus, BackupSetupLastOrdered, self.process_backup_setup_last_ordered)
-        self._subscription.subscribe(bus, NewViewAccepted, self.process_new_view_accepted)
+        return self._quorums.view_change_ack.is_reached(len(self._acks[self._digest]))
 
-    def cleanup(self):
-        self._subscription.unsubscribe_all()
+    def add_view_change(self, msg: ViewChange) -> bool:
+        """
+        Adds view change vote and returns boolean indicating if it found node suspicios
+        """
+        if self._view_change is None:
+            self._view_change = msg
+            self._digest = view_change_digest(msg)
+            return self._validate_acks()
 
-    @property
-    def view_no(self):
-        return self._data.view_no
+        return self._digest == view_change_digest(msg)
 
-    @property
-    def is_master(self):
-        return self._data.is_master
+    def add_view_change_ack(self, msg: ViewChangeAck, frm: str) -> bool:
+        """
+        Adds view change ack and returns boolean indicating if it found node suspicios
+        """
+        self._acks[msg.digest].add(frm)
+        return self._validate_acks()
+
+    def _validate_acks(self) -> bool:
+        digests = [digest for digest, acks in self._acks.items()
+                   if self._quorums.weak.is_reached(len(acks))]
+
+        if len(digests) > 1:
+            return False
+
+        if len(digests) < 1 or self._digest is None:
+            return True
+
+        return self._digest == digests[0]
+
+
+class ViewChangeVotesForView:
+    """
+    Storage for view change votes for some view + corresponding acks
+    """
+
+    def __init__(self, quorums: Quorums):
+        self._quorums = quorums
+        self._votes = defaultdict(partial(ViewChangeVotesForNode, quorums))
 
     @property
-    def last_ordered_3pc(self):
-        return self._data.last_ordered_3pc
+    def confirmed_votes(self) -> List[Tuple[str, str]]:
+        return [(frm, node_votes.digest) for frm, node_votes in self._votes.items()
+                if node_votes.is_confirmed]
+
+    def get_view_change(self, frm: str, digest: str) -> Optional[ViewChange]:
+        vc = self._votes[frm].view_change
+        if vc is not None and view_change_digest(vc) == digest:
+            return vc
 
-    @measure_consensus_time(MetricsName.PROCESS_CHECKPOINT_TIME,
-                            MetricsName.BACKUP_PROCESS_CHECKPOINT_TIME)
-    def process_checkpoint(self, msg: Checkpoint, sender: str) -> (bool, str):
+    def add_view_change(self, msg: ViewChange, frm: str) -> bool:
         """
-        Process checkpoint messages
-        :return: whether processed (True) or stashed (False)
+        Adds view change ack and returns boolean indicating if it found node suspicios
         """
-        self._logger.info('{} processing checkpoint {} from {}'.format(self, msg, sender))
-        result, reason = self._validator.validate(msg)
+        return self._votes[frm].add_view_change(msg)
+
+    def add_view_change_ack(self, msg: ViewChangeAck, frm: str) -> bool:
+        """
+        Adds view change ack and returns boolean indicating if it found node suspicios
+        """
+        return self._votes[msg.name].add_view_change_ack(msg, frm)
+
+    def clear(self):
+        self._votes.clear()
+
+
+class ViewChangeService:
+    def __init__(self, data: ConsensusSharedData, timer: TimerService, bus: InternalBus, network: ExternalBus):
+        self._config = getConfig()
+        self._logger = getlogger()
+
+        self._data = data
+        self._new_view_builder = NewViewBuilder(self._data)
+        self._timer = timer
+        self._bus = bus
+        self._network = network
+        self._router = StashingRouter(self._config.VIEW_CHANGE_SERVICE_STASH_LIMIT)
+        self._votes = ViewChangeVotesForView(self._data.quorums)
+        self._new_view = None   # type: Optional[NewView]
+
+        self._router.subscribe(ViewChange, self.process_view_change_message)
+        self._router.subscribe(ViewChangeAck, self.process_view_change_ack_message)
+        self._router.subscribe(NewView, self.process_new_view_message)
+        self._router.subscribe_to(network)
+
+        self._old_prepared = {}     # type: Dict[int, BatchID]
+        self._old_preprepared = {}  # type: Dict[int, List[BatchID]]
+
+    def __repr__(self):
+        return self._data.name
+
+    def start_view_change(self, view_no: Optional[int] = None):
+        if view_no is None:
+            view_no = self._data.view_no + 1
+
+        self._clear_old_batches(self._old_prepared)
+        self._clear_old_batches(self._old_preprepared)
+
+        for pp in self._data.prepared:
+            self._old_prepared[pp.ppSeqNo] = self.batch_id(pp)
+        prepared = sorted([tuple(bid) for bid in self._old_prepared.values()])
+
+        for pp in self._data.preprepared:
+            new_bid = self.batch_id(pp)
+            pretenders = self._old_preprepared.get(pp.ppSeqNo, [])
+            pretenders = [bid for bid in pretenders
+                          if bid.pp_digest != new_bid.pp_digest]
+            pretenders.append(new_bid)
+            self._old_preprepared[pp.ppSeqNo] = pretenders
+        preprepared = sorted([tuple(bid) for bids in self._old_preprepared.values() for bid in bids])
+
+        self._data.view_no = view_no
+        self._data.waiting_for_new_view = True
+        self._data.primary_name = self._find_primary(self._data.validators, self._data.view_no)
+        self._data.preprepared.clear()
+        self._data.prepared.clear()
+        self._votes.clear()
+        self._new_view = None
+
+        vc = ViewChange(
+            viewNo=self._data.view_no,
+            stableCheckpoint=self._data.stable_checkpoint,
+            prepared=prepared,
+            preprepared=preprepared,
+            checkpoints=list(self._data.checkpoints)
+        )
+        self._network.send(vc)
+        self._votes.add_view_change(vc, self._data.name)
+
+        self._router.process_all_stashed()
+
+    def process_view_change_message(self, msg: ViewChange, frm: str):
+        result = self._validate(msg, frm)
         if result != PROCESS:
-            return result, reason
+            return result
 
-        key = self._checkpoint_key(msg)
-        self._received_checkpoints[key].add(sender)
-        self._try_to_stabilize_checkpoint(key)
-        self._start_catchup_if_needed(key)
+        self._votes.add_view_change(msg, frm)
 
-    def process_backup_setup_last_ordered(self, msg: BackupSetupLastOrdered):
-        if msg.inst_id != self._data.inst_id:
+        if self._data.is_primary:
+            self._send_new_view_if_needed()
             return
-        self.update_watermark_from_3pc()
 
-    def process_ordered(self, ordered: Ordered):
-        for batch_id in reversed(self._data.preprepared):
-            if batch_id.pp_seq_no == ordered.ppSeqNo:
-                self._add_to_checkpoint(batch_id.pp_seq_no,
-                                        batch_id.view_no,
-                                        ordered.auditTxnRootHash)
-                return
-        raise LogicError("CheckpointService | Can't process Ordered msg because "
-                         "ppSeqNo {} not in preprepared".format(ordered.ppSeqNo))
+        vca = ViewChangeAck(
+            viewNo=msg.viewNo,
+            name=frm,
+            digest=view_change_digest(msg)
+        )
+        self._network.send(vca, self._data.primary_name)
 
-    def _start_catchup_if_needed(self, key: CheckpointKey):
-        if self._have_own_checkpoint(key):
-            return
+        self._finish_view_change_if_needed()
+
+    def process_view_change_ack_message(self, msg: ViewChangeAck, frm: str):
+        result = self._validate(msg, frm)
+        if result != PROCESS:
+            return result
 
-        unknown_stabilized = self._unknown_stabilized_checkpoints()
-        lag_in_checkpoints = len(unknown_stabilized)
-        if lag_in_checkpoints <= self.STASHED_CHECKPOINTS_BEFORE_CATCHUP:
+        if not self._data.is_primary:
             return
 
-        last_key = sorted(unknown_stabilized, key=lambda v: (v.view_no, v.pp_seq_no))[-1]
+        self._votes.add_view_change_ack(msg, frm)
+        self._send_new_view_if_needed()
 
-        if self.is_master:
-            # TODO: This code doesn't seem to be needed, but it was there. Leaving just in case
-            #  tests explain why it was really needed.
-            # self._logger.display(
-            #     '{} has lagged for {} checkpoints so updating watermarks to {}'.format(
-            #         self, lag_in_checkpoints, last_key.pp_seq_no))
-            # self.set_watermarks(low_watermark=last_key.pp_seq_no)
-
-            if not self._data.is_primary:
-                self._logger.display('{} has lagged for {} checkpoints so the catchup procedure starts'.
-                                     format(self, lag_in_checkpoints))
-                self._bus.send(NeedMasterCatchup())
-        else:
-            self._logger.info('{} has lagged for {} checkpoints so adjust last_ordered_3pc to {}, '
-                              'shift watermarks and clean collections'.
-                              format(self, lag_in_checkpoints, last_key.pp_seq_no))
-            # Adjust last_ordered_3pc, shift watermarks, clean operational
-            # collections and process stashed messages which now fit between
-            # watermarks
-            # TODO: Actually we might need to process view_no from last_key as well, however
-            #  it wasn't processed before, and it will go away when INDY-1336 gets implemented
-            key_3pc = (self.view_no, last_key.pp_seq_no)
-            self._bus.send(NeedBackupCatchup(inst_id=self._data.inst_id,
-                                             caught_up_till_3pc=key_3pc))
-            self.caught_up_till_3pc(key_3pc)
-
-    def gc_before_new_view(self):
-        self._reset_checkpoints()
-        self._remove_received_checkpoints(till_3pc_key=(self.view_no, 0))
-
-    def caught_up_till_3pc(self, caught_up_till_3pc):
-        # TODO: Implement it like this
-        # cp_seq_no = caught_up_till_3pc[1] // self._config.CHK_FREQ * self._config.CHK_FREQ
-        # self._mark_checkpoint_stable(cp_seq_no)
-
-        self._reset_checkpoints()
-        self._remove_received_checkpoints(till_3pc_key=caught_up_till_3pc)
-        self.update_watermark_from_3pc()
-
-    def catchup_clear_for_backup(self):
-        self._reset_checkpoints()
-        self._remove_received_checkpoints()
-        self.set_watermarks(low_watermark=0,
-                            high_watermark=sys.maxsize)
+    def process_new_view_message(self, msg: NewView, frm: str):
+        result = self._validate(msg, frm)
+        if result != PROCESS:
+            return result
 
-    def _add_to_checkpoint(self, ppSeqNo, view_no, audit_txn_root_hash):
-        if ppSeqNo % self._config.CHK_FREQ != 0:
-            return
+        self._new_view = msg
+
+        self._finish_view_change_if_needed()
+
+    @staticmethod
+    def _find_primary(validators: List[str], view_no: int) -> str:
+        return validators[view_no % len(validators)]
+
+    def _validate(self, msg: Union[ViewChange, ViewChangeAck, NewView], frm: str) -> int:
+        # TODO: Proper validation
+
+        if msg.viewNo < self._data.view_no:
+            return DISCARD
+
+        if msg.viewNo == self._data.view_no and not self._data.waiting_for_new_view:
+            return DISCARD
+
+        if msg.viewNo > self._data.view_no:
+            return STASH
 
-        key = self.CheckpointKey(view_no=view_no,
-                                 pp_seq_no=ppSeqNo,
-                                 digest=audit_txn_root_hash)
-
-        self._do_checkpoint(ppSeqNo, view_no, audit_txn_root_hash)
-        self._try_to_stabilize_checkpoint(key)
-
-    @measure_consensus_time(MetricsName.SEND_CHECKPOINT_TIME,
-                            MetricsName.BACKUP_SEND_CHECKPOINT_TIME)
-    def _do_checkpoint(self, pp_seq_no, view_no, audit_txn_root_hash):
-        self._logger.info("{} sending Checkpoint {} view {} audit txn root hash {}".
-                          format(self, pp_seq_no, view_no, audit_txn_root_hash))
-
-        checkpoint = Checkpoint(self._data.inst_id, view_no, 0, pp_seq_no, audit_txn_root_hash)
-        self._network.send(checkpoint)
-        self._data.checkpoints.add(checkpoint)
+        return PROCESS
 
-    def _try_to_stabilize_checkpoint(self, key: CheckpointKey):
-        if not self._have_quorum_on_received_checkpoint(key):
+    def _send_new_view_if_needed(self):
+        confirmed_votes = self._votes.confirmed_votes
+        if not self._data.quorums.view_change.is_reached(len(confirmed_votes)):
             return
 
-        if not self._have_own_checkpoint(key):
+        view_changes = [self._votes.get_view_change(*v) for v in confirmed_votes]
+        cp = self._new_view_builder.calc_checkpoint(view_changes)
+        if cp is None:
             return
 
-        self._mark_checkpoint_stable(key.pp_seq_no)
+        batches = self._new_view_builder.calc_batches(cp, view_changes)
+        if batches is None:
+            return
 
-    def _mark_checkpoint_stable(self, pp_seq_no):
-        self._data.stable_checkpoint = pp_seq_no
+        nv = NewView(
+            viewNo=self._data.view_no,
+            viewChanges=confirmed_votes,
+            checkpoint=cp,
+            batches=batches
+        )
+        self._network.send(nv)
+        self._new_view = nv
+        self._finish_view_change(cp, batches)
 
-        for cp in self._data.checkpoints.copy():
-            # TODO: Change to < to make checkpoints compatible with PBFT view change
-            if cp.seqNoEnd <= pp_seq_no:
-                self._logger.trace("{} removing previous checkpoint {}".format(self, cp))
-                self._data.checkpoints.remove(cp)
-
-        self.set_watermarks(low_watermark=pp_seq_no)
-
-        self._remove_received_checkpoints(till_3pc_key=(self.view_no, pp_seq_no))
-        self._bus.send(CheckpointStabilized(self._data.inst_id, (self.view_no, pp_seq_no)))  # call OrderingService.l_gc()
-        self._logger.info("{} marked stable checkpoint {}".format(self, pp_seq_no))
-
-    def reset_watermarks_before_new_view(self):
-        # Reset any previous view watermarks since for view change to
-        # successfully complete, the node must have reached the same state
-        # as other nodes
-        self.set_watermarks(low_watermark=0)
+    def _finish_view_change_if_needed(self):
+        if self._new_view is None:
+            return
 
-    def should_reset_watermarks_before_new_view(self):
-        if self.view_no <= 0:
-            return False
-        if self.last_ordered_3pc[0] == self.view_no and self.last_ordered_3pc[1] > 0:
-            return False
-        return True
+        view_changes = []
+        for name, vc_digest in self._new_view.viewChanges:
+            vc = self._votes.get_view_change(name, vc_digest)
+            # We don't have needed ViewChange, so we cannot validate NewView
+            if vc is None:
+                return
+            view_changes.append(vc)
 
-    def set_watermarks(self, low_watermark: int, high_watermark: int = None):
-        self._data.low_watermark = low_watermark
-        self._data.high_watermark = self._data.low_watermark + self._config.LOG_SIZE \
-            if high_watermark is None else \
-            high_watermark
-
-        self._logger.info('{} set watermarks as {} {}'.format(self,
-                                                              self._data.low_watermark,
-                                                              self._data.high_watermark))
-        self._stasher.process_all_stashed(STASH_WATERMARKS)
-
-    def update_watermark_from_3pc(self):
-        last_ordered_3pc = self.last_ordered_3pc
-        if (last_ordered_3pc is not None) and (last_ordered_3pc[0] == self.view_no):
-            self._logger.info("update_watermark_from_3pc to {}".format(last_ordered_3pc))
-            self.set_watermarks(last_ordered_3pc[1])
-        else:
-            self._logger.info("try to update_watermark_from_3pc but last_ordered_3pc is None")
-
-    def _remove_received_checkpoints(self, till_3pc_key=None):
-        """
-        Remove received checkpoints up to `till_3pc_key` if provided,
-        otherwise remove all received checkpoints
-        """
-        if till_3pc_key is None:
-            self._received_checkpoints.clear()
-            self._logger.info('{} removing all received checkpoints'.format(self))
+        cp = self._new_view_builder.calc_checkpoint(view_changes)
+        if cp is None or cp != self._new_view.checkpoint:
+            # New primary is malicious
+            self.start_view_change()
+            assert False  # TODO: Test debugging purpose
             return
 
-        for cp in list(self._received_checkpoints.keys()):
-            if self._is_below_3pc_key(cp, till_3pc_key):
-                self._logger.info('{} removing received checkpoints: {}'.format(self, cp))
-                del self._received_checkpoints[cp]
-
-    def _reset_checkpoints(self):
-        # That function most probably redundant in PBFT approach,
-        # because according to paper, checkpoints cleared only when next stabilized.
-        # Avoid using it while implement other services.
-        self._data.checkpoints.clear()
-        # TODO: change to = 1 in ViewChangeService integration.
-        self._data.stable_checkpoint = 0
-
-    def __str__(self) -> str:
-        return "{} - checkpoint_service".format(self._data.name)
-
-    def discard(self, msg, reason, sender):
-        self._logger.trace("{} discard message {} from {} "
-                           "with the reason: {}".format(self, msg, sender, reason))
-
-    def _have_own_checkpoint(self, key: CheckpointKey) -> bool:
-        own_checkpoints = self._data.checkpoints.irange_key(min_key=key.pp_seq_no, max_key=key.pp_seq_no)
-        return any(cp.viewNo == key.view_no and cp.digest == key.digest for cp in own_checkpoints)
-
-    def _have_quorum_on_received_checkpoint(self, key: CheckpointKey) -> bool:
-        votes = self._received_checkpoints[key]
-        return self._data.quorums.checkpoint.is_reached(len(votes))
-
-    def _unknown_stabilized_checkpoints(self) -> List[CheckpointKey]:
-        return [key for key in self._received_checkpoints
-                if self._have_quorum_on_received_checkpoint(key) and
-                not self._have_own_checkpoint(key) and
-                not self._is_below_3pc_key(key, self.last_ordered_3pc)]
+        batches = self._new_view_builder.calc_batches(cp, view_changes)
+        if batches != self._new_view.batches:
+            # New primary is malicious
+            self.start_view_change()
+            assert False  # TODO: Test debugging purpose
+            return
 
-    @staticmethod
-    def _is_below_3pc_key(cp: CheckpointKey, key: Tuple[int, int]) -> bool:
-        return compare_3PC_keys((cp.view_no, cp.pp_seq_no), key) >= 0
+        self._finish_view_change(cp, batches)
+
+    def _finish_view_change(self, cp: Checkpoint, batches: List[BatchID]):
+        # Update checkpoint
+        self._data.stable_checkpoint = cp.seqNoEnd
+        self._data.checkpoints = [old_cp for old_cp in self._data.checkpoints if old_cp.seqNoEnd > cp.seqNoEnd]
+        self._data.checkpoints.append(cp)
+
+        # Update batches
+        # TODO: Actually we'll need to retrieve preprepares by ID from somewhere
+        self._data.preprepared = batches
+
+        # We finished a view change!
+        self._data.waiting_for_new_view = False
+
+    def _clear_old_batches(self, batches: Dict[int, Any]):
+        for pp_seq_no in list(batches.keys()):
+            if pp_seq_no <= self._data.stable_checkpoint:
+                del batches[pp_seq_no]
 
     @staticmethod
-    def _checkpoint_key(checkpoint: Checkpoint) -> CheckpointKey:
-        return CheckpointService.CheckpointKey(
-            view_no=checkpoint.viewNo,
-            pp_seq_no=checkpoint.seqNoEnd,
-            digest=checkpoint.digest
-        )
+    def batch_id(batch: PrePrepare):
+        return BatchID(batch.viewNo, batch.ppSeqNo, batch.digest)
+
+
+class NewViewBuilder:
+
+    def __init__(self, data: ConsensusSharedData) -> None:
+        self._data = data
+
+    def calc_checkpoint(self, vcs: List[ViewChange]) -> Optional[Checkpoint]:
+        checkpoints = []
+        for cur_vc in vcs:
+            for cur_cp in cur_vc.checkpoints:
+                # Don't add checkpoint to pretending ones if it is already there
+                if cur_cp in checkpoints:
+                    continue
+
+                # Don't add checkpoint to pretending ones if too many nodes already stabilized it
+                # TODO: Should we take into account view_no as well?
+                stable_checkpoint_not_higher = [vc for vc in vcs if cur_cp.seqNoEnd >= vc.stableCheckpoint]
+                if not self._data.quorums.strong.is_reached(len(stable_checkpoint_not_higher)):
+                    continue
+
+                # Don't add checkpoint to pretending ones if not enough nodes have it
+                have_checkpoint = [vc for vc in vcs if cur_cp in vc.checkpoints]
+                if not self._data.quorums.weak.is_reached(len(have_checkpoint)):
+                    continue
+
+                # All checks passed, this is a valid candidate checkpoint
+                checkpoints.append(cur_cp)
+
+        highest_cp = None
+        for cp in checkpoints:
+            # TODO: Should we take into account view_no as well?
+            if highest_cp is None or cp.seqNoEnd > highest_cp.seqNoEnd:
+                highest_cp = cp
+
+        return highest_cp
+
+    def calc_batches(self, cp: Checkpoint, vcs: List[ViewChange]) -> Optional[List[BatchID]]:
+        # TODO: Optimize this
+        batches = set()
+        pp_seq_no = cp.seqNoEnd + 1
+        while pp_seq_no <= cp.seqNoEnd + self._data.log_size:
+            bid = self._try_find_batch_for_pp_seq_no(vcs, pp_seq_no)
+            if bid:
+                batches.add(bid)
+                pp_seq_no += 1
+                continue
+
+            if self._check_null_batch(vcs, pp_seq_no):
+                # TODO: the protocol says to do the loop for all pp_seq_no till h+L (apply NULL batches)
+                # Since we require sequential applying of PrePrepares, we can stop on the first non-found (NULL) batch
+                # Double-check this!
+                break
+
+            # not enough quorums yet
+            return None
+
+        return sorted(batches)
+
+    def _try_find_batch_for_pp_seq_no(self, vcs, pp_seq_no):
+        for vc in vcs:
+            for _bid in vc.prepared:
+                bid = BatchID(*_bid)
+                if bid.pp_seq_no != pp_seq_no:
+                    continue
+                if not self._is_batch_prepared(bid, vcs):
+                    continue
+                if not self._is_batch_preprepared(bid, vcs):
+                    continue
+                return bid
+
+        return None
+
+    def _is_batch_prepared(self, bid: BatchID, vcs: List[ViewChange]) -> bool:
+        def check(vc: ViewChange):
+            if bid.pp_seq_no <= vc.stableCheckpoint:
+                return False
+
+            for _some_bid in vc.prepared:
+                some_bid = BatchID(*_some_bid)
+                if some_bid.pp_seq_no != bid.pp_seq_no:
+                    continue
+                # not ( (v' < v) OR (v'==v and d'==d) )
+                if some_bid.view_no > bid.view_no:
+                    return False
+                if some_bid.view_no >= bid.view_no and some_bid.pp_digest != bid.pp_digest:
+                    return False
+            return True
+
+        prepared_witnesses = sum(1 for vc in vcs if check(vc))
+        return self._data.quorums.strong.is_reached(prepared_witnesses)
+
+    def _is_batch_preprepared(self, bid: BatchID, vcs: List[ViewChange]) -> bool:
+        def check(vc: ViewChange):
+            for _some_bid in vc.preprepared:
+                some_bid = BatchID(*_some_bid)
+                if some_bid.pp_seq_no != bid.pp_seq_no:
+                    continue
+                if some_bid.pp_digest != bid.pp_digest:
+                    continue
+                if some_bid.view_no >= bid.view_no:
+                    return True
+
+            return False
+
+        preprepared_witnesses = sum(1 for vc in vcs if check(vc))
+        return self._data.quorums.weak.is_reached(preprepared_witnesses)
+
+    def _check_null_batch(self, vcs, pp_seq_no):
+        def check(vc: ViewChange):
+            if pp_seq_no <= vc.stableCheckpoint:
+                return False
+
+            for _some_bid in vc.prepared:
+                some_bid = BatchID(*_some_bid)
+                if some_bid.pp_seq_no == pp_seq_no:
+                    return False
+            return True
 
-    def process_new_view_accepted(self, msg: NewViewAccepted):
-        # 1. update shared data
-        cp = msg.checkpoint
-        if cp not in self._data.checkpoints:
-            self._data.checkpoints.append(cp)
-        self._mark_checkpoint_stable(cp.seqNoEnd)
-        self.set_watermarks(low_watermark=cp.seqNoEnd)
-
-        # 2. send NewViewCheckpointsApplied
-        self._bus.send(NewViewCheckpointsApplied(view_no=msg.view_no,
-                                                 view_changes=msg.view_changes,
-                                                 checkpoint=msg.checkpoint,
-                                                 batches=msg.batches))
-        return PROCESS, None
+        null_batch_witnesses = sum(1 for vc in vcs if check(vc))
+        return self._data.quorums.strong.is_reached(null_batch_witnesses)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/consensus/ordering_service.py` & `indy-plenum-1.9.2rc1/plenum/server/consensus/ordering_service.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,114 +1,188 @@
 import itertools
-import logging
 import time
 from collections import defaultdict, OrderedDict, deque
 from functools import partial
 from typing import Tuple, List, Set, Optional, Dict, Iterable
 
 import math
 from orderedset._orderedset import OrderedSet
 from sortedcontainers import SortedList
 
-from common.exceptions import PlenumValueError, LogicError
 from common.serializers.serialization import state_roots_serializer, invalid_index_serializer
 from crypto.bls.bls_bft_replica import BlsBftReplica
 from plenum.common.config_util import getConfig
-from plenum.common.constants import POOL_LEDGER_ID, SEQ_NO_DB_LABEL, AUDIT_LEDGER_ID, TXN_TYPE, \
-    LAST_SENT_PP_STORE_LABEL, AUDIT_TXN_PP_SEQ_NO, AUDIT_TXN_VIEW_NO, AUDIT_TXN_PRIMARIES, PREPREPARE, PREPARE, COMMIT, \
-    DOMAIN_LEDGER_ID, TS_LABEL
+from plenum.common.constants import POOL_LEDGER_ID, SEQ_NO_DB_LABEL, ReplicaHooks, AUDIT_LEDGER_ID, TXN_TYPE, \
+    LAST_SENT_PP_STORE_LABEL, AUDIT_TXN_PP_SEQ_NO, AUDIT_TXN_VIEW_NO, AUDIT_TXN_PRIMARIES, PREPREPARE, PREPARE, COMMIT
 from plenum.common.event_bus import InternalBus, ExternalBus
 from plenum.common.exceptions import SuspiciousNode, InvalidClientMessageException, SuspiciousPrePrepare, \
     UnknownIdentifier
 from plenum.common.ledger import Ledger
-from plenum.common.messages.internal_messages import RequestPropagates, BackupSetupLastOrdered, \
-    RaisedSuspicion, ViewChangeStarted, NewViewCheckpointsApplied
+from plenum.common.messages.internal_messages import HookMessage, OutboxMessage, DoCheckpointMessage, \
+    RemoveStashedCheckpoints, RequestPropagates
 from plenum.common.messages.node_messages import PrePrepare, Prepare, Commit, Reject, ThreePhaseKey, Ordered, \
-    MessageReq
-from plenum.common.metrics_collector import MetricsName, MetricsCollector, NullMetricsCollector, measure_time
+    CheckpointState, MessageReq
+from plenum.common.metrics_collector import MetricsName
 from plenum.common.request import Request
-from plenum.common.router import Subscription
-from plenum.common.stashing_router import StashingRouter, PROCESS, DISCARD
+from plenum.common.stashing_router import StashingRouter
 from plenum.common.timer import TimerService, RepeatingTimer
 from plenum.common.txn_util import get_payload_digest, get_payload_data, get_seq_no
 from plenum.common.types import f
 from plenum.common.util import compare_3PC_keys, updateNamedTuple, SortedDict, getMaxFailures, mostCommonElement, \
-    get_utc_epoch, max_3PC_key
+    get_utc_epoch
 from plenum.server.batch_handlers.three_pc_batch import ThreePcBatch
-from plenum.server.consensus.consensus_shared_data import ConsensusSharedData, BatchID, preprepare_to_batch_id
-from plenum.server.consensus.metrics_decorator import measure_consensus_time
-from plenum.server.consensus.msg_validator import ThreePCMsgValidator
+from plenum.server.consensus.consensus_shared_data import ConsensusSharedData
 from plenum.server.models import Prepares, Commits
-from plenum.server.replica_helper import PP_APPLY_REJECT_WRONG, PP_APPLY_WRONG_DIGEST, PP_APPLY_WRONG_STATE, \
+from plenum.server.propagator import Requests
+from plenum.server.replica import PP_APPLY_REJECT_WRONG, PP_APPLY_WRONG_DIGEST, PP_APPLY_WRONG_STATE, \
     PP_APPLY_ROOT_HASH_MISMATCH, PP_APPLY_HOOK_ERROR, PP_SUB_SEQ_NO_WRONG, PP_NOT_FINAL, PP_APPLY_AUDIT_HASH_MISMATCH, \
     PP_REQUEST_ALREADY_ORDERED, PP_CHECK_NOT_FROM_PRIMARY, PP_CHECK_TO_PRIMARY, PP_CHECK_DUPLICATE, \
     PP_CHECK_INCORRECT_POOL_STATE_ROOT, PP_CHECK_OLD, PP_CHECK_REQUEST_NOT_FINALIZED, PP_CHECK_NOT_NEXT, \
-    PP_CHECK_WRONG_TIME, Stats, OrderedTracker, TPCStat
+    PP_CHECK_WRONG_TIME, Replica, TPCStat, Stats, ConsensusDataHelper, OrderedTracker
 from plenum.server.replica_freshness_checker import FreshnessChecker
-from plenum.server.replica_helper import replica_batch_digest
+from plenum.server.replica_validator_enums import INCORRECT_INSTANCE, DISCARD, INCORRECT_PP_SEQ_NO, ALREADY_ORDERED, \
+    STASH_VIEW, FUTURE_VIEW, OLD_VIEW, PROCESS, GREATER_PREP_CERT, STASH_CATCH_UP, CATCHING_UP, OUTSIDE_WATERMARKS, \
+    STASH_WATERMARKS
 from plenum.server.request_managers.write_request_manager import WriteRequestManager
 from plenum.server.suspicion_codes import Suspicions
 from stp_core.common.log import getlogger
 
 
+class ThreePCMsgValidator:
+    def __init__(self, data: ConsensusSharedData):
+        self._data = data
+
+    @property
+    def view_no(self):
+        return self._data.view_no
+
+    @property
+    def low_watermark(self):
+        return self._data.low_watermark
+
+    @property
+    def high_watermark(self):
+        return self._data.high_watermark
+
+    @property
+    def legacy_last_prepared_sertificate(self):
+        """
+        We assume, that prepared list is an ordered list, and the last element is
+        the last quorumed Prepared
+        """
+        if self._data.prepared:
+            last_prepared = self._data.prepared[-1]
+            return last_prepared.view_no, last_prepared.pp_seq_no
+        return self.last_ordered_3pc
+
+    @property
+    def last_ordered_3pc(self):
+        return self._data.last_ordered_3pc
+
+    @property
+    def is_participating(self):
+        return self._data.is_participating
+
+    @property
+    def legacy_vc_in_progress(self):
+        return self._data.legacy_vc_in_progress
+
+    def has_already_ordered(self, view_no, pp_seq_no):
+        return compare_3PC_keys((view_no, pp_seq_no),
+                                self.last_ordered_3pc) >= 0
+
+    def validate(self, msg):
+        view_no = getattr(msg, f.VIEW_NO.nm, None)
+        pp_seq_no = getattr(msg, f.PP_SEQ_NO.nm, None)
+
+        # ToDO: this checks should be performed in previous level (ReplicaService)
+        # 1. Check INSTANCE_ID
+        # if inst_id is None or inst_id != self.replica.instId:
+        #     return DISCARD, INCORRECT_INSTANCE
+
+        # 2. Check pp_seq_no
+        if pp_seq_no == 0:
+            # should start with 1
+            return DISCARD, INCORRECT_PP_SEQ_NO
+
+        # 3. Check already ordered
+        if self.has_already_ordered(view_no, pp_seq_no):
+            return DISCARD, ALREADY_ORDERED
+
+        # 4. Check viewNo
+        if view_no > self.view_no:
+            return STASH_VIEW, FUTURE_VIEW
+        if view_no < self.view_no - 1:
+            return DISCARD, OLD_VIEW
+        if view_no == self.view_no - 1:
+            if not isinstance(msg, Commit):
+                return DISCARD, OLD_VIEW
+            if not self.legacy_vc_in_progress:
+                return DISCARD, OLD_VIEW
+            if self._data.legacy_last_prepared_before_view_change is None:
+                return DISCARD, OLD_VIEW
+            if compare_3PC_keys((view_no, pp_seq_no), self._data.legacy_last_prepared_before_view_change) < 0:
+                return DISCARD, GREATER_PREP_CERT
+        if view_no == self.view_no and self.legacy_vc_in_progress:
+            return STASH_VIEW, FUTURE_VIEW
+
+        # ToDo: we assume, that only is_participating needs checking orderability
+        # If Catchup in View Change finished then process Commit messages
+        if self._data.is_synced and self.legacy_vc_in_progress:
+            return PROCESS, None
+
+        # 5. Check if Participating
+        if not self.is_participating:
+            return STASH_CATCH_UP, CATCHING_UP
+
+        # 6. Check watermarks
+        if not (self.low_watermark < pp_seq_no <= self.high_watermark):
+            return STASH_WATERMARKS, OUTSIDE_WATERMARKS
+
+        return PROCESS, None
+
+
 class OrderingService:
 
     def __init__(self,
                  data: ConsensusSharedData,
                  timer: TimerService,
                  bus: InternalBus,
                  network: ExternalBus,
                  write_manager: WriteRequestManager,
                  bls_bft_replica: BlsBftReplica,
-                 freshness_checker: FreshnessChecker,
-                 stasher=None,
+                 is_master=True,
                  get_current_time=None,
-                 get_time_for_3pc_batch=None,
-                 metrics: MetricsCollector = NullMetricsCollector()):
-        self.metrics = metrics
+                 get_time_for_3pc_batch=None):
         self._data = data
         self._requests = self._data.requests
         self._timer = timer
         self._bus = bus
         self._network = network
         self._write_manager = write_manager
+        self._is_master = is_master
         self._name = self._data.name
         self.get_time_for_3pc_batch = get_time_for_3pc_batch or get_utc_epoch
-        # Flag which node set, when it have set new primaries and need to send batch
-        self.primaries_batch_needed = False
 
         self._config = getConfig()
         self._logger = getlogger()
-        # TODO: Change just to self._stasher = stasher
-        self._stasher = stasher
-        self._subscription = Subscription()
+        self._stasher = StashingRouter(self._config.REPLICA_STASH_LIMIT)
         self._validator = ThreePCMsgValidator(self._data)
         self.get_current_time = get_current_time or self._timer.get_current_time
         self._out_of_order_repeater = RepeatingTimer(self._timer,
                                                      self._config.PROCESS_STASHED_OUT_OF_ORDER_COMMITS_INTERVAL,
-                                                     self._process_stashed_out_of_order_commits,
+                                                     self.l_process_stashed_out_of_order_commits,
                                                      active=False)
 
         """
         Maps from legacy replica code
         """
         self._state_root_serializer = state_roots_serializer
-
-        # Keeps a map of PRE-PREPAREs which did not satisfy timestamp
-        # criteria, they can be accepted if >f PREPAREs are encountered.
-        # This is emptied on view change. With each PRE-PREPARE, a flag is
-        # stored which indicates whether there are sufficient acceptable
-        # PREPAREs or not
         self.pre_prepares_stashed_for_incorrect_time = {}
-
-        # Time of the last PRE-PREPARE which satisfied all validation rules
-        # (time, digest, roots were all correct). This time is not to be
-        # reverted even if the PRE-PREPAREs are not ordered. This implies that
-        # the next primary would have seen all accepted PRE-PREPAREs or another
-        # view change will happen
+        self.legacy_preprepares = SortedDict(lambda k: (k[0], k[1]))
         self.last_accepted_pre_prepare_time = None
         # Tracks for which keys PRE-PREPAREs have been requested.
         # Cleared in `gc`
         # type: Dict[Tuple[int, int], Optional[Tuple[str, str, str]]]
         self.requested_pre_prepares = {}
 
         # Tracks for which keys PREPAREs have been requested.
@@ -152,15 +226,15 @@
         # This deque is attempted to be flushed on receiving every
         # PRE-PREPARE request.
         self.preparesWaitingForPrePrepare = {}
 
         # Defines if there was a batch after last catchup
         self.first_batch_after_catchup = False
 
-        self._lastPrePrepareSeqNo = self._data.low_watermark  # type: int
+        self._lastPrePrepareSeqNo = self._data.low_watermark
 
         # COMMITs that are stored for which there are no PRE-PREPARE or PREPARE
         # received
         self.commitsWaitingForPrepare = {}
         # type: Dict[Tuple[int, int], deque]
 
         # Dictionary of sent PRE-PREPARE that are stored by primary replica
@@ -196,172 +270,153 @@
         # None in case the replica does not know who the primary of the
         # instance is
         self._primary_name = None  # type: Optional[str]
 
         # Did we log a message about getting request while absence of primary
         self.warned_no_primary = False
 
+        # Queues used in PRE-PREPARE for each ledger,
         self.requestQueues = {}  # type: Dict[int, OrderedSet]
 
+        self.batches = OrderedDict()  # type: OrderedDict[Tuple[int, int]]
+
         self.stats = Stats(TPCStat)
 
-        self.batches = OrderedDict()  # type: OrderedDict[Tuple[int, int]]
+        self.l_batches = OrderedDict()  # type: OrderedDict[Tuple[int, int]]
 
         self.l_bls_bft_replica = bls_bft_replica
 
         # Set of tuples to keep track of ordered requests. Each tuple is
         # (viewNo, ppSeqNo).
         self.ordered = OrderedTracker()
 
-        self.lastBatchCreated = self.get_current_time()
-
         # Commits which are not being ordered since commits with lower
         # sequence numbers have not been ordered yet. Key is the
         # viewNo and value a map of pre-prepare sequence number to commit
         # type: Dict[int,Dict[int,Commit]]
         self.stashed_out_of_order_commits = {}
 
-        self._freshness_checker = freshness_checker
+        self._freshness_checker = FreshnessChecker(freshness_timeout=self._config.STATE_FRESHNESS_UPDATE_INTERVAL)
         self._skip_send_3pc_ts = None
 
-        self._subscription.subscribe(self._stasher, PrePrepare, self.process_preprepare)
-        self._subscription.subscribe(self._stasher, Prepare, self.process_prepare)
-        self._subscription.subscribe(self._stasher, Commit, self.process_commit)
-        self._subscription.subscribe(self._stasher, NewViewCheckpointsApplied, self.process_new_view_checkpoints_applied)
-        self._subscription.subscribe(self._bus, ViewChangeStarted, self.process_view_change_started)
-
-        # Dict to keep PrePrepares from old view to be re-ordered in the new view
-        # key is (viewNo, ppDigest) tuple, and value is a PrePrepare
-        self.old_view_preprepares = {}
-
-    def cleanup(self):
-        self._subscription.unsubscribe_all()
+        self._consensus_data_helper = ConsensusDataHelper(self._data)
 
-    def __repr__(self):
-        return self.name
+        self._stasher.subscribe(PrePrepare, self.process_preprepare)
+        self._stasher.subscribe(Prepare, self.process_prepare)
+        self._stasher.subscribe(Commit, self.process_commit)
+        self._stasher.subscribe_to(network)
 
-    @measure_consensus_time(MetricsName.PROCESS_PREPARE_TIME,
-                            MetricsName.BACKUP_PROCESS_PREPARE_TIME)
     def process_prepare(self, prepare: Prepare, sender: str):
         """
         Validate and process the PREPARE specified.
         If validation is successful, create a COMMIT and broadcast it.
 
         :param prepare: a PREPARE msg
         :param sender: name of the node that sent the PREPARE
         """
         result, reason = self._validate(prepare)
         if result != PROCESS:
-            return result, reason
+            return result
 
         key = (prepare.viewNo, prepare.ppSeqNo)
         self._logger.debug("{} received PREPARE{} from {}".format(self, key, sender))
 
         # TODO move this try/except up higher
         try:
-            if self._validate_prepare(prepare, sender):
-                self._add_to_prepares(prepare, sender)
+            if self.l_validatePrepare(prepare, sender):
+                self.l_addToPrepares(prepare, sender)
                 self.stats.inc(TPCStat.PrepareRcvd)
                 self._logger.debug("{} processed incoming PREPARE {}".format(
                     self, (prepare.viewNo, prepare.ppSeqNo)))
             else:
                 # TODO let's have isValidPrepare throw an exception that gets
                 # handled and possibly logged higher
                 self._logger.trace("{} cannot process incoming PREPARE".format(self))
         except SuspiciousNode as ex:
             self.report_suspicious_node(ex)
-        return None, None
 
-    def _validate_prepare(self, prepare: Prepare, sender: str) -> bool:
+    def l_validatePrepare(self, prepare: Prepare, sender: str) -> bool:
         """
         Return whether the PREPARE specified is valid.
 
         :param prepare: the PREPARE to validate
         :param sender: the name of the node that sent the PREPARE
         :return: True if PREPARE is valid, False otherwise
         """
         key = (prepare.viewNo, prepare.ppSeqNo)
-        primaryStatus = self._is_primary_for_msg(prepare)
+        primaryStatus = self.l_isPrimaryForMsg(prepare)
 
-        ppReq = self.get_preprepare(*key)
+        ppReq = self.l_getPrePrepare(*key)
 
         # If a non primary replica and receiving a PREPARE request before a
         # PRE-PREPARE request, then proceed
 
         # PREPARE should not be sent from primary
-        if self._is_msg_from_primary(prepare, sender):
-            self.report_suspicious_node(SuspiciousNode(sender, Suspicions.PR_FRM_PRIMARY, prepare))
-            return False
+        if self.l_isMsgFromPrimary(prepare, sender):
+            raise SuspiciousNode(sender, Suspicions.PR_FRM_PRIMARY, prepare)
 
         # If non primary replica
         if primaryStatus is False:
             if self.prepares.hasPrepareFrom(prepare, sender):
-                self.report_suspicious_node(SuspiciousNode(
-                    sender, Suspicions.DUPLICATE_PR_SENT, prepare))
-                return False
+                raise SuspiciousNode(
+                    sender, Suspicions.DUPLICATE_PR_SENT, prepare)
             # If PRE-PREPARE not received for the PREPARE, might be slow
             # network
             if not ppReq:
-                self._enqueue_prepare(prepare, sender)
+                self.l_enqueue_prepare(prepare, sender)
                 self.l_setup_last_ordered_for_non_master()
                 return False
         # If primary replica
         if primaryStatus is True:
             if self.prepares.hasPrepareFrom(prepare, sender):
-                self.report_suspicious_node(SuspiciousNode(
-                    sender, Suspicions.DUPLICATE_PR_SENT, prepare))
-                return False
+                raise SuspiciousNode(
+                    sender, Suspicions.DUPLICATE_PR_SENT, prepare)
             # If PRE-PREPARE was not sent for this PREPARE, certainly
             # malicious behavior
             elif not ppReq:
-                self.report_suspicious_node(SuspiciousNode(
-                    sender, Suspicions.UNKNOWN_PR_SENT, prepare))
-                return False
+                raise SuspiciousNode(
+                    sender, Suspicions.UNKNOWN_PR_SENT, prepare)
 
         if primaryStatus is None and not ppReq:
-            self._enqueue_prepare(prepare, sender)
+            self.l_enqueue_prepare(prepare, sender)
             self.l_setup_last_ordered_for_non_master()
             return False
 
         if prepare.digest != ppReq.digest:
-            self.report_suspicious_node(SuspiciousNode(sender, Suspicions.PR_DIGEST_WRONG, prepare))
-            return False
+            raise SuspiciousNode(sender, Suspicions.PR_DIGEST_WRONG, prepare)
         elif prepare.stateRootHash != ppReq.stateRootHash:
-            self.report_suspicious_node(SuspiciousNode(sender, Suspicions.PR_STATE_WRONG,
-                                                       prepare))
-            return False
+            raise SuspiciousNode(sender, Suspicions.PR_STATE_WRONG,
+                                 prepare)
         elif prepare.txnRootHash != ppReq.txnRootHash:
-            self.report_suspicious_node(SuspiciousNode(sender, Suspicions.PR_TXN_WRONG,
-                                                       prepare))
-            return False
+            raise SuspiciousNode(sender, Suspicions.PR_TXN_WRONG,
+                                 prepare)
         elif prepare.auditTxnRootHash != ppReq.auditTxnRootHash:
-            self.report_suspicious_node(SuspiciousNode(sender, Suspicions.PR_AUDIT_TXN_ROOT_HASH_WRONG,
-                                                       prepare))
-            return False
+            raise SuspiciousNode(sender, Suspicions.PR_AUDIT_TXN_ROOT_HASH_WRONG,
+                                 prepare)
 
         # BLS multi-sig:
         self.l_bls_bft_replica.validate_prepare(prepare, sender)
 
         return True
 
     """Method from legacy code"""
-    def _enqueue_prepare(self, pMsg: Prepare, sender: str):
+    def l_enqueue_prepare(self, pMsg: Prepare, sender: str):
         key = (pMsg.viewNo, pMsg.ppSeqNo)
         self._logger.debug("{} queueing prepare due to unavailability of PRE-PREPARE. "
                            "Prepare {} for key {} from {}".format(self, pMsg, key, sender))
         if key not in self.preparesWaitingForPrePrepare:
             self.preparesWaitingForPrePrepare[key] = deque()
         self.preparesWaitingForPrePrepare[key].append((pMsg, sender))
         if key not in self.pre_prepares_stashed_for_incorrect_time:
             if self.is_master or self.last_ordered_3pc[1] != 0:
-                self._request_pre_prepare_for_prepare(key)
+                self.l_request_pre_prepare_for_prepare(key)
         else:
-            self._process_stashed_pre_prepare_for_time_if_possible(key)
+            self.l_process_stashed_pre_prepare_for_time_if_possible(key)
 
-    def _process_stashed_pre_prepare_for_time_if_possible(
+    def l_process_stashed_pre_prepare_for_time_if_possible(
             self, key: Tuple[int, int]):
         """
         Check if any PRE-PREPAREs that were stashed since their time was not
         acceptable, can now be accepted since enough PREPAREs are received
         """
         self._logger.debug('{} going to process stashed PRE-PREPAREs with '
                            'incorrect times'.format(self))
@@ -377,19 +432,20 @@
                 pp, sender, done = stashed_pp[key]
                 if done:
                     self._logger.debug('{} already processed PRE-PREPARE{}'.format(self, key))
                     return True
                 # True is set since that will indicate to `is_pre_prepare_time_acceptable`
                 # that sufficient PREPAREs are received
                 stashed_pp[key] = (pp, sender, True)
-                self._network.process_incoming(pp, sender)
+                self.process_preprepare(pp, sender)
                 return True
         return False
 
-    def _request_pre_prepare_for_prepare(self, three_pc_key) -> bool:
+    """Method from legacy code"""
+    def l_request_pre_prepare_for_prepare(self, three_pc_key) -> bool:
         """
         Check if has an acceptable PRE_PREPARE already stashed, if not then
         check count of PREPAREs, make sure >f consistent PREPAREs are found,
         store the acceptable PREPARE state (digest, roots) for verification of
         the received PRE-PREPARE
         """
 
@@ -403,122 +459,120 @@
             self._logger.debug(
                 '{} not requesting a PRE-PREPARE because does not have'
                 ' sufficient PREPAREs for {}'.format(
                     self, three_pc_key))
             return False
 
         digest, state_root, txn_root, _ = \
-            self._get_acceptable_stashed_prepare_state(three_pc_key)
+            self.l_get_acceptable_stashed_prepare_state(three_pc_key)
 
         # Choose a better data structure for `prePreparesPendingFinReqs`
         pre_prepares = [pp for pp, _, _ in self.prePreparesPendingFinReqs
                         if (pp.viewNo, pp.ppSeqNo) == three_pc_key]
         if pre_prepares:
             if [pp for pp in pre_prepares if (pp.digest, pp.stateRootHash, pp.txnRootHash) == (digest, state_root, txn_root)]:
                 self._logger.debug('{} not requesting a PRE-PREPARE since already '
                                    'found stashed for {}'.format(self, three_pc_key))
                 return False
 
         self._request_pre_prepare(three_pc_key,
                                   stash_data=(digest, state_root, txn_root))
         return True
 
-    def _get_acceptable_stashed_prepare_state(self, three_pc_key):
+    """Method from legacy code"""
+    def l_get_acceptable_stashed_prepare_state(self, three_pc_key):
         prepares = {s: (m.digest, m.stateRootHash, m.txnRootHash) for m, s in
                     self.preparesWaitingForPrePrepare[three_pc_key]}
         acceptable, freq = mostCommonElement(prepares.values())
         return (*acceptable, {s for s, state in prepares.items()
                               if state == acceptable})
 
-    def _is_primary_for_msg(self, msg) -> Optional[bool]:
+    """Method from legacy code"""
+    def l_isPrimaryForMsg(self, msg) -> Optional[bool]:
         """
         Return whether this replica is primary if the request's view number is
         equal this replica's view number and primary has been selected for
         the current view.
         Return None otherwise.
         :param msg: message
         """
-        return self._data.is_primary if self._is_msg_for_current_view(msg) \
-            else self._is_primary_in_view(msg.viewNo)
+        return self._data.is_primary if self.l_isMsgForCurrentView(msg) \
+            else self.l_is_primary_in_view(msg.viewNo)
 
-    def _is_primary_in_view(self, viewNo: int) -> Optional[bool]:
+    """Method from legacy code"""
+    def l_is_primary_in_view(self, viewNo: int) -> Optional[bool]:
         """
         Return whether this replica was primary in the given view
         """
         if viewNo not in self.primary_names:
             return False
         return self.primary_names[viewNo] == self.name
 
-    @measure_consensus_time(MetricsName.PROCESS_COMMIT_TIME,
-                            MetricsName.BACKUP_PROCESS_COMMIT_TIME)
     def process_commit(self, commit: Commit, sender: str):
         """
         Validate and process the COMMIT specified.
         If validation is successful, return the message to the node.
 
         :param commit: an incoming COMMIT message
         :param sender: name of the node that sent the COMMIT
         """
         result, reason = self._validate(commit)
         if result != PROCESS:
-            return result, reason
+            return result
 
         self._logger.debug("{} received COMMIT{} from {}".format(
             self, (commit.viewNo, commit.ppSeqNo), sender))
 
-        if self._validate_commit(commit, sender):
+        if self.l_validateCommit(commit, sender):
             self.stats.inc(TPCStat.CommitRcvd)
-            self._add_to_commits(commit, sender)
+            self.l_addToCommits(commit, sender)
             self._logger.debug("{} processed incoming COMMIT{}".format(
                 self, (commit.viewNo, commit.ppSeqNo)))
-        return result, reason
+        return result
 
-    def _validate_commit(self, commit: Commit, sender: str) -> bool:
+    """Method from legacy code"""
+    def l_validateCommit(self, commit: Commit, sender: str) -> bool:
         """
         Return whether the COMMIT specified is valid.
 
         :param commit: the COMMIT to validate
         :return: True if `request` is valid, False otherwise
         """
         key = (commit.viewNo, commit.ppSeqNo)
-        if not self._has_prepared(key):
-            self._enqueue_commit(commit, sender)
+        if not self.l_has_prepared(key):
+            self.l_enqueue_commit(commit, sender)
             return False
 
         if self.commits.hasCommitFrom(commit, sender):
-            self.report_suspicious_node(SuspiciousNode(sender, Suspicions.DUPLICATE_CM_SENT, commit))
-            return False
+            raise SuspiciousNode(sender, Suspicions.DUPLICATE_CM_SENT, commit)
 
         # BLS multi-sig:
-        pre_prepare = self.get_preprepare(commit.viewNo, commit.ppSeqNo)
+        pre_prepare = self.l_getPrePrepare(commit.viewNo, commit.ppSeqNo)
         why_not = self.l_bls_bft_replica.validate_commit(commit, sender, pre_prepare)
 
         if why_not == BlsBftReplica.CM_BLS_SIG_WRONG:
             self._logger.warning("{} discard Commit message from "
                                  "{}:{}".format(self, sender, commit))
-            self.report_suspicious_node(SuspiciousNode(sender,
-                                                       Suspicions.CM_BLS_SIG_WRONG,
-                                                       commit))
-            return False
+            raise SuspiciousNode(sender,
+                                 Suspicions.CM_BLS_SIG_WRONG,
+                                 commit)
         elif why_not is not None:
             self._logger.warning("Unknown error code returned for bls commit "
                                  "validation {}".format(why_not))
 
         return True
 
-    def _enqueue_commit(self, request: Commit, sender: str):
+    def l_enqueue_commit(self, request: Commit, sender: str):
         key = (request.viewNo, request.ppSeqNo)
         self._logger.debug("{} - Queueing commit due to unavailability of PREPARE. "
                            "Request {} with key {} from {}".format(self, request, key, sender))
         if key not in self.commitsWaitingForPrepare:
             self.commitsWaitingForPrepare[key] = deque()
         self.commitsWaitingForPrepare[key].append((request, sender))
 
-    @measure_consensus_time(MetricsName.PROCESS_PREPREPARE_TIME,
-                            MetricsName.BACKUP_PROCESS_PREPREPARE_TIME)
     def process_preprepare(self, pre_prepare: PrePrepare, sender: str):
         """
         Validate and process provided PRE-PREPARE, create and
         broadcast PREPARE for it.
 
         :param pre_prepare: message
         :param sender: name of the node that sent this message
@@ -528,32 +582,32 @@
         if (pp_key and (pre_prepare, sender) not in self.pre_prepare_tss[pp_key]):
             # TODO more clean solution would be to set timestamps
             # earlier (e.g. in zstack)
             self.pre_prepare_tss[pp_key][pre_prepare, sender] = self.get_time_for_3pc_batch()
 
         result, reason = self._validate(pre_prepare)
         if result != PROCESS:
-            return result, reason
+            return result
 
         key = (pre_prepare.viewNo, pre_prepare.ppSeqNo)
         self._logger.debug("{} received PRE-PREPARE{} from {}".format(self, key, sender))
 
         # TODO: should we still do it?
         # Converting each req_idrs from list to tuple
         req_idrs = {f.REQ_IDR.nm: [key for key in pre_prepare.reqIdr]}
         pre_prepare = updateNamedTuple(pre_prepare, **req_idrs)
 
         def report_suspicious(reason):
             ex = SuspiciousNode(sender, reason, pre_prepare)
             self.report_suspicious_node(ex)
 
-        why_not = self._can_process_pre_prepare(pre_prepare, sender)
+        why_not = self.l_can_process_pre_prepare(pre_prepare, sender)
         if why_not is None:
             why_not_applied = \
-                self._process_valid_preprepare(pre_prepare, sender)
+                self.l_process_valid_preprepare(pre_prepare, sender)
             if why_not_applied is not None:
                 if why_not_applied == PP_APPLY_REJECT_WRONG:
                     report_suspicious(Suspicions.PPR_REJECT_WRONG)
                 elif why_not_applied == PP_APPLY_WRONG_DIGEST:
                     report_suspicious(Suspicions.PPR_DIGEST_WRONG)
                 elif why_not_applied == PP_APPLY_WRONG_STATE:
                     report_suspicious(Suspicions.PPR_STATE_WRONG)
@@ -561,15 +615,15 @@
                     report_suspicious(Suspicions.PPR_TXN_WRONG)
                 elif why_not_applied == PP_APPLY_HOOK_ERROR:
                     report_suspicious(Suspicions.PPR_PLUGIN_EXCEPTION)
                 elif why_not_applied == PP_SUB_SEQ_NO_WRONG:
                     report_suspicious(Suspicions.PPR_SUB_SEQ_NO_WRONG)
                 elif why_not_applied == PP_NOT_FINAL:
                     # this is fine, just wait for another
-                    return None, None
+                    return
                 elif why_not_applied == PP_APPLY_AUDIT_HASH_MISMATCH:
                     report_suspicious(Suspicions.PPR_AUDIT_TXN_ROOT_HASH_WRONG)
                 elif why_not_applied == PP_REQUEST_ALREADY_ORDERED:
                     report_suspicious(Suspicions.PPR_WITH_ORDERED_REQUEST)
         elif why_not == PP_CHECK_NOT_FROM_PRIMARY:
             report_suspicious(Suspicions.PPR_FRM_NON_PRIMARY)
         elif why_not == PP_CHECK_TO_PRIMARY:
@@ -595,143 +649,189 @@
             absent_str = ', '.join(str(key) for key in absents)
             non_fin_str = ', '.join(
                 '{} ({} : {})'.format(str(key),
                                       str(len(self._requests[key].propagates)),
                                       ', '.join(self._requests[key].propagates.keys())) for key in non_fin)
             self._logger.warning(
                 "{} found requests in the incoming pp, of {} ledger, that are not finalized. "
-                "{} of them don't have propagates: [{}]. "
-                "{} of them don't have enough propagates: [{}].".format(self, pre_prepare.ledgerId,
-                                                                        len(absents), absent_str,
-                                                                        len(non_fin), non_fin_str))
+                "{} of them don't have propagates: {}."
+                "{} of them don't have enough propagates: {}.".format(self, pre_prepare.ledgerId,
+                                                                      len(absents), absent_str,
+                                                                      len(non_fin), non_fin_str))
 
             def signal_suspicious(req):
                 self._logger.info("Request digest {} already ordered. Discard {} "
                                   "from {}".format(req, pre_prepare, sender))
                 report_suspicious(Suspicions.PPR_WITH_ORDERED_REQUEST)
 
             # checking for payload digest is more effective
             for payload_key in non_fin_payload:
                 if self.db_manager.get_store(SEQ_NO_DB_LABEL).get_by_payload_digest(payload_key) != (None, None):
                     signal_suspicious(payload_key)
-                    return None, None
+                    return
 
             # for absents we can only check full digest
             for full_key in absents:
                 if self.db_manager.get_store(SEQ_NO_DB_LABEL).get_by_full_digest(full_key) is not None:
                     signal_suspicious(full_key)
-                    return None, None
+                    return
 
             bad_reqs = absents | non_fin
-            self._enqueue_pre_prepare(pre_prepare, sender, bad_reqs)
+            self.l_enqueue_pre_prepare(pre_prepare, sender, bad_reqs)
             # TODO: An optimisation might be to not request PROPAGATEs
             # if some PROPAGATEs are present or a client request is
             # present and sufficient PREPAREs and PRE-PREPARE are present,
             # then the digest can be compared but this is expensive as the
             # PREPARE and PRE-PREPARE contain a combined digest
-            self._schedule(partial(self._request_propagates_if_needed, bad_reqs, pre_prepare),
+            self._schedule(partial(self.l_request_propagates_if_needed, bad_reqs, pre_prepare),
                            self._config.PROPAGATE_REQUEST_DELAY)
         elif why_not == PP_CHECK_NOT_NEXT:
             pp_view_no = pre_prepare.viewNo
             pp_seq_no = pre_prepare.ppSeqNo
             last_pp_view_no, last_pp_seq_no = self.__last_pp_3pc
             if pp_view_no >= last_pp_view_no and (
                     self.is_master or self.last_ordered_3pc[1] != 0):
                 seq_frm = last_pp_seq_no + 1 if pp_view_no == last_pp_view_no else 1
                 seq_to = pp_seq_no - 1
                 if seq_to >= seq_frm >= pp_seq_no - self._config.CHK_FREQ + 1:
                     self._logger.warning(
                         "{} missing PRE-PREPAREs from {} to {}, "
                         "going to request".format(self, seq_frm, seq_to))
-                    self._request_missing_three_phase_messages(
+                    self.l_request_missing_three_phase_messages(
                         pp_view_no, seq_frm, seq_to)
-            self._enqueue_pre_prepare(pre_prepare, sender)
+            self.l_enqueue_pre_prepare(pre_prepare, sender)
             self.l_setup_last_ordered_for_non_master()
         elif why_not == PP_CHECK_WRONG_TIME:
             key = (pre_prepare.viewNo, pre_prepare.ppSeqNo)
             item = (pre_prepare, sender, False)
             self.pre_prepares_stashed_for_incorrect_time[key] = item
             report_suspicious(Suspicions.PPR_TIME_WRONG)
         elif why_not == BlsBftReplica.PPR_BLS_MULTISIG_WRONG:
             report_suspicious(Suspicions.PPR_BLS_MULTISIG_WRONG)
         else:
             self._logger.warning("Unknown PRE-PREPARE check status: {}".format(why_not))
-        return None, None
 
     """Properties from legacy code"""
 
     @property
     def view_no(self):
         return self._data.view_no
 
     @property
     def last_ordered_3pc(self):
         return self._data.last_ordered_3pc
 
     @last_ordered_3pc.setter
     def last_ordered_3pc(self, lo_tuple):
         self._data.last_ordered_3pc = lo_tuple
-        self._logger.info('{} set last ordered as {}'.format(self, lo_tuple))
 
     @property
-    def last_preprepare(self):
+    def l_lastPrePrepare(self):
         last_3pc = (0, 0)
         lastPp = None
         if self.sentPrePrepares:
             (v, s), pp = self.sentPrePrepares.peekitem(-1)
             last_3pc = (v, s)
             lastPp = pp
         if self.prePrepares:
             (v, s), pp = self.prePrepares.peekitem(-1)
             if compare_3PC_keys(last_3pc, (v, s)) > 0:
                 lastPp = pp
         return lastPp
 
     @property
     def __last_pp_3pc(self):
-        last_pp = self.last_preprepare
+        last_pp = self.l_lastPrePrepare
         if not last_pp:
             return self.last_ordered_3pc
 
         last_3pc = (last_pp.viewNo, last_pp.ppSeqNo)
         if compare_3PC_keys(self.last_ordered_3pc, last_3pc) > 0:
             return last_3pc
 
-        return self.last_ordered_3pc
-
     @property
     def db_manager(self):
         return self._write_manager.database_manager
 
     @property
     def is_master(self):
-        return self._data.is_master
+        return self._is_master
 
     @property
     def primary_name(self):
         """
         Name of the primary replica of this replica's instance
 
         :return: Returns name if primary is known, None otherwise
         """
         return self._data.primary_name
 
+    @primary_name.setter
+    def primary_name(self, value: Optional[str]) -> None:
+        """
+        Set the value of isPrimary.
+
+        :param value: the value to set isPrimary to
+        """
+        if value is not None:
+            self.warned_no_primary = False
+        self.primary_names[self.view_no] = value
+        self.l_compact_primary_names()
+        if value != self._data.primary_name:
+            self._data.primary_name = value
+            self._logger.info("{} setting primaryName for view no {} to: {}".
+                              format(self, self.view_no, value))
+            if value is None:
+                # Since the GC needs to happen after a primary has been
+                # decided.
+                return
+            self.l_gc_before_new_view()
+            if self.l__should_reset_watermarks_before_new_view():
+                self.l_reset_watermarks_before_new_view()
+
     @property
     def name(self):
+        # ToDo: Change to real name
         return self._data.name
 
     @name.setter
     def name(self, n):
         self._data._name = n
 
     @property
     def f(self):
         return getMaxFailures(self._data.total_nodes)
 
-    def gc(self, till3PCKey):
+    """Method from legacy code"""
+    def l_reset_watermarks_before_new_view(self):
+        # Reset any previous view watermarks since for view change to
+        # successfully complete, the node must have reached the same state
+        # as other nodes
+        self._data.low_watermark = 0
+        self._lastPrePrepareSeqNo = self._data.low_watermark
+
+    """Method from legacy code"""
+    def l__should_reset_watermarks_before_new_view(self):
+        if self.view_no <= 0:
+            return False
+        if self.last_ordered_3pc[0] == self.view_no and self.last_ordered_3pc[1] > 0:
+            return False
+        return True
+
+    """Method from legacy code"""
+    def l_gc_before_new_view(self):
+        # Trigger GC for all batches of old view
+        # Clear any checkpoints, since they are valid only in a view
+        self.l_gc(self.last_ordered_3pc)
+        self._data.checkpoints.clear()
+        self.l_remove_stashed_checkpoints(view_no=self.view_no)
+        self._clear_prev_view_pre_prepares()
+
+    """Method from legacy code"""
+    def l_gc(self, till3PCKey):
         self._logger.info("{} cleaning up till {}".format(self, till3PCKey))
         tpcKeys = set()
         reqKeys = set()
 
         for key3PC, pp in itertools.chain(
             self.sentPrePrepares.items(),
             self.prePrepares.items()
@@ -761,53 +861,85 @@
             self.prePrepares,
             self.prepares,
             self.commits,
             self.batches,
             self.requested_pre_prepares,
             self.requested_prepares,
             self.requested_commits,
-            self.pre_prepares_stashed_for_incorrect_time,
-            self.old_view_preprepares
+            self.pre_prepares_stashed_for_incorrect_time
         )
         for request_key in tpcKeys:
             for coll in to_clean_up:
                 coll.pop(request_key, None)
 
         for request_key in reqKeys:
             self._requests.free(request_key)
             for ledger_id, keys in self.requestQueues.items():
                 if request_key in keys:
-                    self.discard_req_key(ledger_id, request_key)
+                    self.l_discard_req_key(ledger_id, request_key)
             self._logger.trace('{} freed request {} from previous checkpoints'
                                .format(self, request_key))
 
         # ToDo: do we need ordered messages there?
         self.ordered.clear_below_view(self.view_no - 1)
 
         # BLS multi-sig:
         self.l_bls_bft_replica.gc(till3PCKey)
 
-    def discard_req_key(self, ledger_id, req_key):
+    """Method from legacy code"""
+    def l_discard_req_key(self, ledger_id, req_key):
         self.requestQueues[ledger_id].discard(req_key)
 
+    """Method from legacy code"""
+    def l_remove_stashed_checkpoints(self, view_no=None):
+        """
+        Remove stashed received checkpoints up to `till_3pc_key` if provided,
+        otherwise remove all stashed received checkpoints
+        """
+        # ToDo: Maybe need to change message format?
+        self._bus.send(RemoveStashedCheckpoints(view_no=view_no,
+                                                all=True,
+                                                start_no=0,
+                                                end_no=0))
+        # ToDo: Should we send some notification for checkpoint service?
+        # if till_3pc_key is None:
+        #     self.stashedRecvdCheckpoints.clear()
+        #     self._logger.info('{} removing all stashed checkpoints'.format(self))
+        #     return
+        #
+        # for view_no in list(self.stashedRecvdCheckpoints.keys()):
+        #
+        #     if view_no < till_3pc_key[0]:
+        #         self._logger.info('{} removing stashed checkpoints for view {}'.format(self, view_no))
+        #         del self.stashedRecvdCheckpoints[view_no]
+        #
+        #     elif view_no == till_3pc_key[0]:
+        #         for (s, e) in list(self.stashedRecvdCheckpoints[view_no].keys()):
+        #             if e <= till_3pc_key[1]:
+        #                 self._logger.info('{} removing stashed checkpoints: '
+        #                                  'viewNo={}, seqNoStart={}, seqNoEnd={}'.
+        #                                  format(self, view_no, s, e))
+        #                 del self.stashedRecvdCheckpoints[view_no][(s, e)]
+        #         if len(self.stashedRecvdCheckpoints[view_no]) == 0:
+        #             del self.stashedRecvdCheckpoints[view_no]
+
     def _clear_prev_view_pre_prepares(self):
         to_remove = []
         for idx, (pp, _, _) in enumerate(self.prePreparesPendingFinReqs):
             if pp.viewNo < self.view_no:
                 to_remove.insert(0, idx)
         for idx in to_remove:
             self.prePreparesPendingFinReqs.pop(idx)
 
         for (v, p) in list(self.prePreparesPendingPrevPP.keys()):
             if v < self.view_no:
                 self.prePreparesPendingPrevPP.pop((v, p))
 
     def report_suspicious_node(self, ex: SuspiciousNode):
-        self._bus.send(RaisedSuspicion(inst_id=self._data.inst_id,
-                                       ex=ex))
+        self._bus.send(ex)
 
     def _validate(self, msg):
         return self._validator.validate(msg)
 
     """Method from legacy code"""
     def l_compact_primary_names(self):
         min_allowed_view_no = self.view_no - 1
@@ -815,135 +947,151 @@
         for view_no in self.primary_names:
             if view_no >= min_allowed_view_no:
                 break
             views_to_remove.append(view_no)
         for view_no in views_to_remove:
             self.primary_names.pop(view_no)
 
-    def _can_process_pre_prepare(self, pre_prepare: PrePrepare, sender: str):
+    """Method from legacy code"""
+    def l_can_process_pre_prepare(self, pre_prepare: PrePrepare, sender: str):
         """
         Decide whether this replica is eligible to process a PRE-PREPARE.
 
         :param pre_prepare: a PRE-PREPARE msg to process
         :param sender: the name of the node that sent the PRE-PREPARE msg
         """
         # TODO: Check whether it is rejecting PRE-PREPARE from previous view
 
         # PRE-PREPARE should not be sent from non primary
-        if not self._is_msg_from_primary(pre_prepare, sender):
+        if not self.l_isMsgFromPrimary(pre_prepare, sender):
             return PP_CHECK_NOT_FROM_PRIMARY
 
         # Already has a PRE-PREPARE with same 3 phase key
-        if (pre_prepare.viewNo, pre_prepare.ppSeqNo) in self.prePrepares:
+        if (pre_prepare.viewNo, pre_prepare.ppSeqNo) in self.legacy_preprepares:
             return PP_CHECK_DUPLICATE
 
-        if not self._is_pre_prepare_time_acceptable(pre_prepare, sender):
+        if not self.l_is_pre_prepare_time_acceptable(pre_prepare, sender):
             return PP_CHECK_WRONG_TIME
 
         if compare_3PC_keys((pre_prepare.viewNo, pre_prepare.ppSeqNo),
                             self.__last_pp_3pc) > 0:
             return PP_CHECK_OLD  # ignore old pre-prepare
 
-        if self._non_finalised_reqs(pre_prepare.reqIdr):
+        if self.l_nonFinalisedReqs(pre_prepare.reqIdr):
             return PP_CHECK_REQUEST_NOT_FINALIZED
 
-        if not self._is_next_pre_prepare(pre_prepare.viewNo,
-                                         pre_prepare.ppSeqNo):
+        if not self.l__is_next_pre_prepare(pre_prepare.viewNo,
+                                           pre_prepare.ppSeqNo):
             return PP_CHECK_NOT_NEXT
 
         if f.POOL_STATE_ROOT_HASH.nm in pre_prepare and \
-                pre_prepare.poolStateRootHash != self.get_state_root_hash(POOL_LEDGER_ID):
+                pre_prepare.poolStateRootHash != self.l_stateRootHash(POOL_LEDGER_ID):
             return PP_CHECK_INCORRECT_POOL_STATE_ROOT
 
         # BLS multi-sig:
         status = self.l_bls_bft_replica.validate_pre_prepare(pre_prepare,
                                                              sender)
         if status is not None:
             return status
         return None
 
     def _schedule(self, func, delay):
         self._timer.schedule(delay, func)
 
-    def _process_valid_preprepare(self, pre_prepare: PrePrepare, sender: str):
+    """Method from legacy code"""
+    def l_process_valid_preprepare(self, pre_prepare: PrePrepare, sender: str):
         self.first_batch_after_catchup = False
-        old_state_root = self.get_state_root_hash(pre_prepare.ledgerId, to_str=False)
-        old_txn_root = self.get_txn_root_hash(pre_prepare.ledgerId)
+        old_state_root = self.l_stateRootHash(pre_prepare.ledgerId, to_str=False)
+        old_txn_root = self.l_txnRootHash(pre_prepare.ledgerId)
         if self.is_master:
             self._logger.debug('{} state root before processing {} is {}, {}'.format(
                 self,
                 pre_prepare,
                 old_state_root,
                 old_txn_root))
 
         # 1. APPLY
-        reqs, invalid_indices, rejects, suspicious = self._apply_pre_prepare(pre_prepare)
+        reqs, invalid_indices, rejects, suspicious = self.l_apply_pre_prepare(pre_prepare)
 
         # 2. CHECK IF MORE CHUNKS NEED TO BE APPLIED FURTHER BEFORE VALIDATION
         if pre_prepare.sub_seq_no != 0:
             return PP_SUB_SEQ_NO_WRONG
 
         if not pre_prepare.final:
             return PP_NOT_FINAL
 
         # 3. VALIDATE APPLIED
         invalid_from_pp = invalid_index_serializer.deserialize(pre_prepare.discarded)
         if suspicious:
             why_not_applied = PP_REQUEST_ALREADY_ORDERED
         else:
-            why_not_applied = self._validate_applied_pre_prepare(pre_prepare,
-                                                                 reqs, invalid_indices, invalid_from_pp)
+            why_not_applied = self.l_validate_applied_pre_prepare(pre_prepare,
+                                                                  reqs, invalid_indices, invalid_from_pp)
 
         # 4. IF NOT VALID AFTER APPLYING - REVERT
         if why_not_applied is not None:
             if self.is_master:
-                self._revert(pre_prepare.ledgerId,
-                             old_state_root,
-                             len(pre_prepare.reqIdr) - len(invalid_indices))
+                self.l_revert(pre_prepare.ledgerId,
+                              old_state_root,
+                              len(pre_prepare.reqIdr) - len(invalid_indices))
             return why_not_applied
 
-        # 5. TRACK APPLIED
-        if rejects:
-            for reject in rejects:
-                self._network.send(reject)
-        self._add_to_pre_prepares(pre_prepare)
+        # 5. EXECUTE HOOK
+        if self.is_master:
+            try:
+                self.l_execute_hook(ReplicaHooks.APPLY_PPR, pre_prepare)
+            except Exception as ex:
+                self._logger.warning('{} encountered exception in replica '
+                                     'hook {} : {}'.
+                                     format(self, ReplicaHooks.APPLY_PPR, ex))
+                self.l_revert(pre_prepare.ledgerId,
+                              old_state_root,
+                              len(pre_prepare.reqIdr) - len(invalid_from_pp))
+                return PP_APPLY_HOOK_ERROR
+
+        # 6. TRACK APPLIED
+        self.send_outbox(rejects)
+        self.l_addToPrePrepares(pre_prepare)
 
         if self.is_master:
             # BLS multi-sig:
             self.l_bls_bft_replica.process_pre_prepare(pre_prepare, sender)
             self._logger.trace("{} saved shared multi signature for "
                                "root".format(self, old_state_root))
 
         if not self.is_master:
             self.db_manager.get_store(LAST_SENT_PP_STORE_LABEL).store_last_sent_pp_seq_no(
                 self._data.inst_id, pre_prepare.ppSeqNo)
-        self._track_batches(pre_prepare, old_state_root)
+        self.l_trackBatches(pre_prepare, old_state_root)
         key = (pre_prepare.viewNo, pre_prepare.ppSeqNo)
         self._logger.debug("{} processed incoming PRE-PREPARE{}".format(self, key),
                            extra={"tags": ["processing"]})
         return None
 
-    def _enqueue_pre_prepare(self, pre_prepare: PrePrepare, sender: str,
-                             nonFinReqs: Set = None):
+    """Method from legacy code"""
+    def l_enqueue_pre_prepare(self, pre_prepare: PrePrepare, sender: str,
+                              nonFinReqs: Set = None):
         if nonFinReqs:
             self._logger.info("{} - Queueing pre-prepares due to unavailability of finalised "
                               "requests. PrePrepare {} from {}".format(self, pre_prepare, sender))
             self.prePreparesPendingFinReqs.append((pre_prepare, sender, nonFinReqs))
         else:
             # Possible exploit, an malicious party can send an invalid
             # pre-prepare and over-write the correct one?
             self._logger.info("Queueing pre-prepares due to unavailability of previous pre-prepares. {} from {}".
                               format(pre_prepare, sender))
             self.prePreparesPendingPrevPP[pre_prepare.viewNo, pre_prepare.ppSeqNo] = (pre_prepare, sender)
 
-    def _request_propagates_if_needed(self, bad_reqs: list, pre_prepare: PrePrepare):
+    """Method from legacy code"""
+    def l_request_propagates_if_needed(self, bad_reqs: list, pre_prepare: PrePrepare):
         if any(pre_prepare is pended[0] for pended in self.prePreparesPendingFinReqs):
             self._bus.send(RequestPropagates(bad_reqs))
 
-    def _request_missing_three_phase_messages(self, view_no: int, seq_frm: int, seq_to: int) -> None:
+    """Method from legacy code"""
+    def l_request_missing_three_phase_messages(self, view_no: int, seq_frm: int, seq_to: int) -> None:
         for pp_seq_no in range(seq_frm, seq_to + 1):
             key = (view_no, pp_seq_no)
             self._request_pre_prepare(key)
             self._request_prepare(key)
             self._request_commit(key)
 
     def _request_three_phase_msg(self, three_pc_key: Tuple[int, int],
@@ -988,29 +1136,26 @@
         """
         Request preprepare
         """
         if recipients is None:
             recipients = self._network.connecteds.copy()
             primary_name = self.primary_name[:self.primary_name.rfind(":")]
             if primary_name in recipients:
-                recipients.remove(primary_name)
+                del recipients[primary_name]
         return self._request_three_phase_msg(three_pc_key, self.requested_prepares, PREPARE, recipients, stash_data)
 
     def _request_commit(self, three_pc_key: Tuple[int, int],
                         recipients: List[str] = None) -> bool:
         """
         Request commit
         """
-        if recipients is None:
-            recipients = self._network.connecteds.copy()
         return self._request_three_phase_msg(three_pc_key, self.requested_commits, COMMIT, recipients)
 
-    @measure_time(MetricsName.SEND_MESSAGE_REQ_TIME)
     def _request_msg(self, typ, params: Dict, frm: List[str] = None):
-        self._send(MessageReq(**{
+        self._network.send(MessageReq(**{
             f.MSG_TYPE.nm: typ,
             f.PARAMS.nm: params
         }), dst=frm)
 
     """Method from legacy code"""
     def l_setup_last_ordered_for_non_master(self):
         """
@@ -1027,50 +1172,53 @@
                 self.view_no)
             if lowest_prepared is not None:
                 # now after catch up we have in last_ordered_3pc[1] value 0
                 # it value should change last_ordered_3pc to lowest_prepared - 1
                 self._logger.info('{} Setting last ordered for non-master as {}'.
                                   format(self, self.last_ordered_3pc))
                 self.last_ordered_3pc = (self.view_no, lowest_prepared - 1)
-                self._bus.send(BackupSetupLastOrdered(inst_id=self._data.inst_id))
+                self.l_update_watermark_from_3pc()
                 self.first_batch_after_catchup = False
 
-    def get_state_root_hash(self, ledger_id: str, to_str=True, committed=False):
-        return self.db_manager.get_state_root_hash(ledger_id, to_str, committed) \
-            if self.is_master \
-            else None
-
-    def get_txn_root_hash(self, ledger_id: str, to_str=True):
-        return self.db_manager.get_txn_root_hash(ledger_id, to_str) \
-            if self.is_master \
-            else None
+    """Method from legacy code"""
+    def l_stateRootHash(self, ledger_id, to_str=True, committed=False):
+        if not self.is_master:
+            return None
+        state = self.db_manager.get_state(ledger_id)
+        root = state.committedHeadHash if committed else state.headHash
+        if to_str:
+            root = self._state_root_serializer.serialize(bytes(root))
+        return root
 
-    def _is_msg_from_primary(self, msg, sender: str) -> bool:
+    """Method from legacy code"""
+    def l_isMsgFromPrimary(self, msg, sender: str) -> bool:
         """
         Return whether this message was from primary replica
         :param msg:
         :param sender:
         :return:
         """
-        if self._is_msg_for_current_view(msg):
+        if self.l_isMsgForCurrentView(msg):
             return self.primary_name == sender
         try:
             return self.primary_names[msg.viewNo] == sender
         except KeyError:
             return False
 
-    def _is_msg_for_current_view(self, msg):
+    """Method from legacy code"""
+    def l_isMsgForCurrentView(self, msg):
         """
         Return whether this request's view number is equal to the current view
         number of this replica.
         """
         viewNo = getattr(msg, "viewNo", None)
         return viewNo == self.view_no
 
-    def _is_pre_prepare_time_correct(self, pp: PrePrepare, sender: str) -> bool:
+    """Method from legacy code"""
+    def l_is_pre_prepare_time_correct(self, pp: PrePrepare, sender: str) -> bool:
         """
         Check if this PRE-PREPARE is not older than (not checking for greater
         than since batches maybe sent in less than 1 second) last PRE-PREPARE
         and in a sufficient range of local clock's UTC time.
         :param pp:
         :return:
         """
@@ -1084,121 +1232,138 @@
             return False
         else:
             return (
                 abs(pp.ppTime - self.pre_prepare_tss[tpcKey][pp, sender]) <=
                 self._config.ACCEPTABLE_DEVIATION_PREPREPARE_SECS
             )
 
-    def _is_pre_prepare_time_acceptable(self, pp: PrePrepare, sender: str) -> bool:
+    """Method from legacy code"""
+    def l_is_pre_prepare_time_acceptable(self, pp: PrePrepare, sender: str) -> bool:
         """
         Returns True or False depending on the whether the time in PRE-PREPARE
         is acceptable. Can return True if time is not acceptable but sufficient
         PREPAREs are found to support the PRE-PREPARE
         :param pp:
         :return:
         """
         key = (pp.viewNo, pp.ppSeqNo)
         if key in self.requested_pre_prepares:
             # Special case for requested PrePrepares
             return True
-        correct = self._is_pre_prepare_time_correct(pp, sender)
+        correct = self.l_is_pre_prepare_time_correct(pp, sender)
         if not correct:
             if key in self.pre_prepares_stashed_for_incorrect_time and \
                     self.pre_prepares_stashed_for_incorrect_time[key][-1]:
                 self._logger.debug('{} marking time as correct for {}'.format(self, pp))
                 correct = True
             else:
                 self._logger.warning('{} found {} to have incorrect time.'.format(self, pp))
         return correct
 
-    def _non_finalised_reqs(self, reqKeys: List[Tuple[str, int]]):
+    """Method from legacy code"""
+    def l_nonFinalisedReqs(self, reqKeys: List[Tuple[str, int]]):
         """
         Check if there are any requests which are not finalised, i.e for
         which there are not enough PROPAGATEs
         """
         return {key for key in reqKeys if not self._requests.is_finalised(key)}
 
-    def _is_next_pre_prepare(self, view_no: int, pp_seq_no: int):
+    """Method from legacy code"""
+    def l__is_next_pre_prepare(self, view_no: int, pp_seq_no: int):
         if view_no == self.view_no and pp_seq_no == 1:
             # First PRE-PREPARE in a new view
             return True
         (last_pp_view_no, last_pp_seq_no) = self.__last_pp_3pc
         if last_pp_view_no > view_no:
             return False
         if last_pp_view_no < view_no:
             if view_no != self.view_no:
                 return False
             last_pp_seq_no = 0
         if pp_seq_no - last_pp_seq_no > 1:
             return False
         return True
 
-    def _apply_pre_prepare(self, pre_prepare: PrePrepare):
+    """Method from legacy code"""
+    def l_txnRootHash(self, ledger_str, to_str=True):
+        if not self.is_master:
+            return None
+        ledger = self.db_manager.get_ledger(ledger_str)
+        root = ledger.uncommitted_root_hash
+        if to_str:
+            root = ledger.hashToStr(root)
+        return root
+
+    """Method from legacy code"""
+    def l_apply_pre_prepare(self, pre_prepare: PrePrepare):
         """
         Applies (but not commits) requests of the PrePrepare
         to the ledger and state
         """
+
         reqs = []
         idx = 0
         rejects = []
         invalid_indices = []
         suspicious = False
 
         # 1. apply each request
         for req_key in pre_prepare.reqIdr:
             req = self._requests[req_key].finalised
             try:
-                self._process_req_during_batch(req,
-                                               pre_prepare.ppTime)
+                self.l_processReqDuringBatch(req,
+                                             pre_prepare.ppTime)
             except (InvalidClientMessageException, UnknownIdentifier, SuspiciousPrePrepare) as ex:
                 self._logger.warning('{} encountered exception {} while processing {}, '
                                      'will reject'.format(self, ex, req))
                 rejects.append((req.key, Reject(req.identifier, req.reqId, ex)))
                 invalid_indices.append(idx)
                 if isinstance(ex, SuspiciousPrePrepare):
                     suspicious = True
             finally:
                 reqs.append(req)
             idx += 1
 
         # 2. call callback for the applied batch
         if self.is_master:
             three_pc_batch = ThreePcBatch.from_pre_prepare(pre_prepare,
-                                                           state_root=self.get_state_root_hash(pre_prepare.ledgerId,
-                                                                                               to_str=False),
-                                                           txn_root=self.get_txn_root_hash(pre_prepare.ledgerId,
+                                                           state_root=self.l_stateRootHash(pre_prepare.ledgerId,
                                                                                            to_str=False),
+                                                           txn_root=self.l_txnRootHash(pre_prepare.ledgerId,
+                                                                                       to_str=False),
                                                            primaries=[],
-                                                           valid_digests=self._get_valid_req_ids_from_all_requests(
+                                                           valid_digests=self.l_get_valid_req_ids_from_all_requests(
                                                                reqs, invalid_indices))
             self.post_batch_creation(three_pc_batch)
 
         return reqs, invalid_indices, rejects, suspicious
 
-    def _get_valid_req_ids_from_all_requests(self, reqs, invalid_indices):
+    """Method from legacy code"""
+    def l_get_valid_req_ids_from_all_requests(self, reqs, invalid_indices):
         return [req.key for idx, req in enumerate(reqs) if idx not in invalid_indices]
 
-    def _validate_applied_pre_prepare(self, pre_prepare: PrePrepare,
-                                      reqs, invalid_indices, invalid_from_pp) -> Optional[int]:
+    """Method from legacy code"""
+    def l_validate_applied_pre_prepare(self, pre_prepare: PrePrepare,
+                                       reqs, invalid_indices, invalid_from_pp) -> Optional[int]:
         if len(invalid_indices) != len(invalid_from_pp):
             return PP_APPLY_REJECT_WRONG
 
-        digest = self.replica_batch_digest(reqs)
+        digest = Replica.batchDigest(reqs)
         if digest != pre_prepare.digest:
             return PP_APPLY_WRONG_DIGEST
 
         if self.is_master:
-            if pre_prepare.stateRootHash != self.get_state_root_hash(pre_prepare.ledgerId):
+            if pre_prepare.stateRootHash != self.l_stateRootHash(pre_prepare.ledgerId):
                 return PP_APPLY_WRONG_STATE
 
-            if pre_prepare.txnRootHash != self.get_txn_root_hash(pre_prepare.ledgerId):
+            if pre_prepare.txnRootHash != self.l_txnRootHash(pre_prepare.ledgerId):
                 return PP_APPLY_ROOT_HASH_MISMATCH
 
             # TODO: move this kind of validation to batch handlers
-            if f.AUDIT_TXN_ROOT_HASH.nm in pre_prepare and pre_prepare.auditTxnRootHash != self.get_txn_root_hash(AUDIT_LEDGER_ID):
+            if f.AUDIT_TXN_ROOT_HASH.nm in pre_prepare and pre_prepare.auditTxnRootHash != self.l_txnRootHash(AUDIT_LEDGER_ID):
                 return PP_APPLY_AUDIT_HASH_MISMATCH
 
         return None
 
     """Method from legacy code"""
     def l_get_lowest_probable_prepared_certificate_in_view(
             self, view_no) -> Optional[int]:
@@ -1223,39 +1388,48 @@
                 seq_no_p.add(p)
 
         for n in seq_no_pp:
             if n in seq_no_p:
                 return n
         return None
 
-    def _revert(self, ledgerId, stateRootHash, reqCount):
+    """Method from legacy code"""
+    def l_revert(self, ledgerId, stateRootHash, reqCount):
         # A batch should only be reverted if all batches that came after it
         # have been reverted
         ledger = self.db_manager.get_ledger(ledgerId)
         state = self.db_manager.get_state(ledgerId)
         self._logger.info('{} reverting {} txns and state root from {} to {} for ledger {}'
                           .format(self, reqCount, Ledger.hashToStr(state.headHash),
                                   Ledger.hashToStr(stateRootHash), ledgerId))
         state.revertToHead(stateRootHash)
         ledger.discardTxns(reqCount)
         self.post_batch_rejection(ledgerId)
 
-    def _track_batches(self, pp: PrePrepare, prevStateRootHash):
+    """Method from legacy code"""
+    def l_execute_hook(self, hook_id, *args):
+        # ToDo: need to receive results from hooks
+        self._bus.send(HookMessage(hook=hook_id,
+                                   args=args))
+
+    """Method from legacy code"""
+    def l_trackBatches(self, pp: PrePrepare, prevStateRootHash):
         # pp.discarded indicates the index from where the discarded requests
         #  starts hence the count of accepted requests, prevStateRoot is
         # tracked to revert this PRE-PREPARE
         self._logger.trace('{} tracking batch for {} with state root {}'.format(
             self, pp, prevStateRootHash))
-        if self.is_master:
-            self.metrics.add_event(MetricsName.THREE_PC_BATCH_SIZE, len(pp.reqIdr))
-        else:
-            self.metrics.add_event(MetricsName.BACKUP_THREE_PC_BATCH_SIZE, len(pp.reqIdr))
+        # ToDo: for first stage we will exclude metrics
+        # if self.is_master:
+        #     self._metrics.add_event(MetricsName.THREE_PC_BATCH_SIZE, len(pp.reqIdr))
+        # else:
+        #     self._metrics.add_event(MetricsName.BACKUP_THREE_PC_BATCH_SIZE, len(pp.reqIdr))
 
-        self.batches[(pp.viewNo, pp.ppSeqNo)] = [pp.ledgerId, pp.discarded,
-                                                 pp.ppTime, prevStateRootHash, len(pp.reqIdr)]
+        self.l_batches[(pp.viewNo, pp.ppSeqNo)] = [pp.ledgerId, pp.discarded,
+                                                   pp.ppTime, prevStateRootHash, len(pp.reqIdr)]
 
     @property
     def lastPrePrepareSeqNo(self):
         return self._lastPrePrepareSeqNo
 
     @lastPrePrepareSeqNo.setter
     def lastPrePrepareSeqNo(self, n):
@@ -1268,92 +1442,99 @@
             self._lastPrePrepareSeqNo = n
         else:
             self._logger.debug(
                 '{} cannot set lastPrePrepareSeqNo to {} as its '
                 'already {}'.format(
                     self, n, self._lastPrePrepareSeqNo))
 
-    def _add_to_pre_prepares(self, pp: PrePrepare) -> None:
+    """Method from legacy code"""
+    def l_addToPrePrepares(self, pp: PrePrepare) -> None:
         """
         Add the specified PRE-PREPARE to this replica's list of received
         PRE-PREPAREs and try sending PREPARE
 
         :param pp: the PRE-PREPARE to add to the list
         """
         key = (pp.viewNo, pp.ppSeqNo)
         # ToDo:
         self.prePrepares[key] = pp
-        self._preprepare_batch(pp)
         self.lastPrePrepareSeqNo = pp.ppSeqNo
         self.last_accepted_pre_prepare_time = pp.ppTime
-        self._dequeue_prepares(*key)
-        self._dequeue_commits(*key)
+        self.l_dequeue_prepares(*key)
+        self.l_dequeue_commits(*key)
         self.stats.inc(TPCStat.PrePrepareRcvd)
-        self.try_prepare(pp)
+        self.l_tryPrepare(pp)
 
-    def _dequeue_prepares(self, viewNo: int, ppSeqNo: int):
+    """Method from legacy code"""
+    def l_dequeue_prepares(self, viewNo: int, ppSeqNo: int):
         key = (viewNo, ppSeqNo)
         if key in self.preparesWaitingForPrePrepare:
             i = 0
             # Keys of pending prepares that will be processed below
             while self.preparesWaitingForPrePrepare[key]:
                 prepare, sender = self.preparesWaitingForPrePrepare[
                     key].popleft()
                 self._logger.debug("{} popping stashed PREPARE{}".format(self, key))
-                self._network.process_incoming(prepare, sender)
+                self.process_prepare(prepare, sender)
                 i += 1
             self.preparesWaitingForPrePrepare.pop(key)
             self._logger.debug("{} processed {} PREPAREs waiting for PRE-PREPARE for"
                                " view no {} and seq no {}".format(self, i, viewNo, ppSeqNo))
 
-    def _dequeue_commits(self, viewNo: int, ppSeqNo: int):
+    """Method from legacy code"""
+    def l_dequeue_commits(self, viewNo: int, ppSeqNo: int):
         key = (viewNo, ppSeqNo)
         if key in self.commitsWaitingForPrepare:
-            if not self._has_prepared(key):
+            if not self.l_has_prepared(key):
                 self._logger.debug('{} has not pre-prepared {}, will dequeue the '
                                    'COMMITs later'.format(self, key))
                 return
             i = 0
             # Keys of pending prepares that will be processed below
             while self.commitsWaitingForPrepare[key]:
                 commit, sender = self.commitsWaitingForPrepare[
                     key].popleft()
                 self._logger.debug("{} popping stashed COMMIT{}".format(self, key))
-                self._network.process_incoming(commit, sender)
+                self.process_commit(commit, sender)
 
                 i += 1
             self.commitsWaitingForPrepare.pop(key)
             self._logger.debug("{} processed {} COMMITs waiting for PREPARE for"
                                " view no {} and seq no {}".format(self, i, viewNo, ppSeqNo))
 
-    def try_prepare(self, pp: PrePrepare):
+    """Method from legacy code"""
+    def l_tryPrepare(self, pp: PrePrepare):
         """
         Try to send the Prepare message if the PrePrepare message is ready to
         be passed into the Prepare phase.
         """
-        rv, msg = self._can_prepare(pp)
+        rv, msg = self.l_canPrepare(pp)
         if rv:
-            self._do_prepare(pp)
+            self.l_doPrepare(pp)
         else:
             self._logger.debug("{} cannot send PREPARE since {}".format(self, msg))
 
-    def _can_prepare(self, ppReq) -> (bool, str):
+    """Method from legacy code"""
+    def l_canPrepare(self, ppReq) -> (bool, str):
         """
         Return whether the batch of requests in the PRE-PREPARE can
         proceed to the PREPARE step.
 
         :param ppReq: any object with identifier and requestId attributes
         """
-        if self.prepares.hasPrepareFrom(ppReq, self.name):
+        if self.l_has_sent_prepare(ppReq):
             return False, 'has already sent PREPARE for {}'.format(ppReq)
         return True, ''
 
-    @measure_consensus_time(MetricsName.SEND_PREPARE_TIME,
-                            MetricsName.BACKUP_SEND_PREPARE_TIME)
-    def _do_prepare(self, pp: PrePrepare):
+    """Method from legacy code"""
+    def l_has_sent_prepare(self, request) -> bool:
+        return self.prepares.hasPrepareFrom(request, self.name)
+
+    """Method from legacy code"""
+    def l_doPrepare(self, pp: PrePrepare):
         self._logger.debug("{} Sending PREPARE{} at {}".format(
             self, (pp.viewNo, pp.ppSeqNo), self.get_current_time()))
         params = [self._data.inst_id,
                   pp.viewNo,
                   pp.ppSeqNo,
                   pp.ppTime,
                   pp.digest,
@@ -1362,131 +1543,146 @@
         if f.AUDIT_TXN_ROOT_HASH.nm in pp:
             params.append(pp.auditTxnRootHash)
 
         # BLS multi-sig:
         params = self.l_bls_bft_replica.update_prepare(params, pp.ledgerId)
 
         prepare = Prepare(*params)
-        self._send(prepare, stat=TPCStat.PrepareSent)
-        self._add_to_prepares(prepare, self.name)
+        if self.is_master:
+            rv = self.l_execute_hook(ReplicaHooks.CREATE_PR, prepare, pp)
+            prepare = rv if rv is not None else prepare
+        self._network.send(prepare, TPCStat.PrepareSent)
+        self.l_addToPrepares(prepare, self.name)
 
-    def _has_prepared(self, key):
-        if not self.get_preprepare(*key):
+    """Method from legacy code"""
+    def l_update_watermark_from_3pc(self):
+        if (self.last_ordered_3pc is not None) and (self.last_ordered_3pc[0] == self.view_no):
+            self._logger.info("update_watermark_from_3pc to {}".format(self.last_ordered_3pc))
+            self.h = self.last_ordered_3pc[1]
+        else:
+            self._logger.info("try to update_watermark_from_3pc but last_ordered_3pc is None")
+
+    """Method from legacy code"""
+    def l_has_prepared(self, key):
+        if not self.l_getPrePrepare(*key):
             return False
         if ((key not in self.prepares and key not in self.sentPrePrepares) and
                 (key not in self.preparesWaitingForPrePrepare)):
             return False
         return True
 
-    def get_preprepare(self, viewNo, ppSeqNo):
+    """Method from legacy code"""
+    def l_getPrePrepare(self, viewNo, ppSeqNo):
         key = (viewNo, ppSeqNo)
         if key in self.sentPrePrepares:
             return self.sentPrePrepares[key]
         if key in self.prePrepares:
             return self.prePrepares[key]
         return None
 
-    def _add_to_prepares(self, prepare: Prepare, sender: str):
+    """Method from legacy code"""
+    def l_addToPrepares(self, prepare: Prepare, sender: str):
         """
         Add the specified PREPARE to this replica's list of received
         PREPAREs and try sending COMMIT
 
         :param prepare: the PREPARE to add to the list
         """
         # BLS multi-sig:
         self.l_bls_bft_replica.process_prepare(prepare, sender)
 
         self.prepares.addVote(prepare, sender)
-        self._dequeue_commits(prepare.viewNo, prepare.ppSeqNo)
-        self._try_commit(prepare)
+        self.l_dequeue_commits(prepare.viewNo, prepare.ppSeqNo)
+        self.l_tryCommit(prepare)
 
-    def _try_commit(self, prepare: Prepare):
+    """Method from legacy code"""
+    def l_tryCommit(self, prepare: Prepare):
         """
         Try to commit if the Prepare message is ready to be passed into the
         commit phase.
         """
-        rv, reason = self._can_commit(prepare)
+        rv, reason = self.l_canCommit(prepare)
         if rv:
-            pp = self.get_preprepare(prepare.viewNo, prepare.ppSeqNo)
-            self._prepare_batch(pp)
-            self._do_commit(prepare)
+            pp = self.l_getPrePrepare(prepare.viewNo, prepare.ppSeqNo)
+            self._consensus_data_helper.prepare_batch(pp)
+            self.l_doCommit(prepare)
         else:
             self._logger.debug("{} cannot send COMMIT since {}".format(self, reason))
 
-    @measure_consensus_time(MetricsName.SEND_COMMIT_TIME,
-                            MetricsName.BACKUP_SEND_COMMIT_TIME)
-    def _do_commit(self, p: Prepare):
+    def l_doCommit(self, p: Prepare):
         """
         Create a commit message from the given Prepare message and trigger the
         commit phase
         :param p: the prepare message
         """
         key_3pc = (p.viewNo, p.ppSeqNo)
         self._logger.debug("{} Sending COMMIT{} at {}".format(self, key_3pc, self.get_current_time()))
 
         params = [
             self._data.inst_id, p.viewNo, p.ppSeqNo
         ]
 
-        pre_prepare = self.get_preprepare(*key_3pc)
+        pre_prepare = self.l_getPrePrepare(*key_3pc)
 
         # BLS multi-sig:
         if p.stateRootHash is not None:
-            pre_prepare = self.get_preprepare(*key_3pc)
+            pre_prepare = self.l_getPrePrepare(*key_3pc)
             params = self.l_bls_bft_replica.update_commit(params, pre_prepare)
 
         commit = Commit(*params)
 
-        self._send(commit, stat=TPCStat.CommitSent)
-        self._add_to_commits(commit, self.name)
+        self._network.send(commit, TPCStat.CommitSent)
+        self.l_addToCommits(commit, self.name)
 
-    def _add_to_commits(self, commit: Commit, sender: str):
+    """Method from legacy code"""
+    def l_addToCommits(self, commit: Commit, sender: str):
         """
         Add the specified COMMIT to this replica's list of received
         commit requests.
 
         :param commit: the COMMIT to add to the list
         :param sender: the name of the node that sent the COMMIT
         """
         # BLS multi-sig:
         self.l_bls_bft_replica.process_commit(commit, sender)
 
         self.commits.addVote(commit, sender)
-        self._try_order(commit)
+        self.l_tryOrder(commit)
 
-    def _try_order(self, commit: Commit):
+    """Method from legacy code"""
+    def l_tryOrder(self, commit: Commit):
         """
         Try to order if the Commit message is ready to be ordered.
         """
-        canOrder, reason = self._can_order(commit)
+        canOrder, reason = self.l_canOrder(commit)
         if canOrder:
             self._logger.trace("{} returning request to node".format(self))
-            self._do_order(commit)
+            self.l_doOrder(commit)
         else:
             self._logger.debug("{} cannot return request to node: {}".format(self, reason))
         return canOrder
 
-    def _do_order(self, commit: Commit):
+    """Method from legacy code"""
+    def l_doOrder(self, commit: Commit):
         key = (commit.viewNo, commit.ppSeqNo)
         self._logger.debug("{} ordering COMMIT {}".format(self, key))
-        return self._order_3pc_key(key)
+        return self.l_order_3pc_key(key)
 
-    @measure_consensus_time(MetricsName.ORDER_3PC_BATCH_TIME,
-                            MetricsName.BACKUP_ORDER_3PC_BATCH_TIME)
-    def _order_3pc_key(self, key):
-        pp = self.get_preprepare(*key)
+    """Method from legacy code"""
+    def l_order_3pc_key(self, key):
+        pp = self.l_getPrePrepare(*key)
         if pp is None:
             raise ValueError(
                 "{} no PrePrepare with a 'key' {} found".format(self, key)
             )
 
         self._freshness_checker.update_freshness(ledger_id=pp.ledgerId,
                                                  ts=pp.ppTime)
 
-        self._add_to_ordered(*key)
+        self.l_addToOrdered(*key)
         invalid_indices = invalid_index_serializer.deserialize(pp.discarded)
         invalid_reqIdr = []
         valid_reqIdr = []
         for ind, reqIdr in enumerate(pp.reqIdr):
             if ind in invalid_indices:
                 invalid_reqIdr.append(reqIdr)
             else:
@@ -1500,39 +1696,133 @@
                           pp.ppSeqNo,
                           pp.ppTime,
                           pp.ledgerId,
                           pp.stateRootHash,
                           pp.txnRootHash,
                           pp.auditTxnRootHash if f.AUDIT_TXN_ROOT_HASH.nm in pp else None,
                           self._get_primaries_for_ordered(pp))
-        self._discard_ordered_req_keys(pp)
+        if self.is_master:
+            rv = self.l_execute_hook(ReplicaHooks.CREATE_ORD, ordered, pp)
+            ordered = rv if rv is not None else ordered
+
+        self.l_discard_ordered_req_keys(pp)
 
-        self._bus.send(ordered)
+        self._bus.send(ordered, TPCStat.OrderSent)
 
         ordered_msg = "{} ordered batch request, view no {}, ppSeqNo {}, ledger {}, " \
                       "state root {}, txn root {}, audit root {}".format(self, pp.viewNo, pp.ppSeqNo, pp.ledgerId,
                                                                          pp.stateRootHash, pp.txnRootHash,
                                                                          pp.auditTxnRootHash)
         self._logger.debug("{}, requests ordered {}, discarded {}".
                            format(ordered_msg, valid_reqIdr, invalid_reqIdr))
         self._logger.info("{}, requests ordered {}, discarded {}".
                           format(ordered_msg, len(valid_reqIdr), len(invalid_reqIdr)))
 
-        if self.is_master:
-            self.metrics.add_event(MetricsName.ORDERED_BATCH_SIZE, len(valid_reqIdr) + len(invalid_reqIdr))
-            self.metrics.add_event(MetricsName.ORDERED_BATCH_INVALID_COUNT, len(invalid_reqIdr))
-        else:
-            self.metrics.add_event(MetricsName.BACKUP_ORDERED_BATCH_SIZE, len(valid_reqIdr))
+        # ToDo: add metrics in integration phase
+        # if self.isMaster:
+        #     self.metrics.add_event(MetricsName.ORDERED_BATCH_SIZE, len(valid_reqIdr) + len(invalid_reqIdr))
+        #     self.metrics.add_event(MetricsName.ORDERED_BATCH_INVALID_COUNT, len(invalid_reqIdr))
+        # else:
+        #     self.metrics.add_event(MetricsName.BACKUP_ORDERED_BATCH_SIZE, len(valid_reqIdr))
+
+        self.l_addToCheckpoint(pp.ppSeqNo, pp.digest, pp.ledgerId, pp.viewNo)
 
         # BLS multi-sig:
         self.l_bls_bft_replica.process_order(key, self._data.quorums, pp)
 
         return True
 
-    def _add_to_ordered(self, view_no: int, pp_seq_no: int):
+    """Method from legacy code"""
+    def l_addToCheckpoint(self, ppSeqNo, digest, ledger_id, view_no):
+        for (s, e) in self._data.checkpoints.keys():
+            if s <= ppSeqNo <= e:
+                state = self._data.checkpoints[s, e]  # type: CheckpointState
+                state.digests.append(digest)
+                state = updateNamedTuple(state, seqNo=ppSeqNo)
+                self._data.checkpoints[s, e] = state
+                break
+        else:
+            s, e = ppSeqNo, math.ceil(ppSeqNo / self._config.CHK_FREQ) * self._config.CHK_FREQ
+            self._logger.debug("{} adding new checkpoint state for {}".format(self, (s, e)))
+            state = CheckpointState(ppSeqNo, [digest, ], None, {}, False)
+            self._data.checkpoints[s, e] = state
+
+        if state.seqNo == e:
+            if len(state.digests) == self._config.CHK_FREQ:
+                self.do_checkpoint(state, s, e, ledger_id, view_no)
+            self.process_stashed_checkpoints(start_no=s,
+                                             end_no=e,
+                                             view_no=view_no)
+
+    """Method from legacy code"""
+    def process_stashed_checkpoints(self, start_no, end_no, view_no):
+
+        self._bus.send(RemoveStashedCheckpoints(start_no=start_no,
+                                                end_no=end_no,
+                                                view_no=view_no,
+                                                all=False))
+        # ToDo: remove the next code after integration phase
+        # # Remove all checkpoints from previous views if any
+        # self._remove_stashed_checkpoints(till_3pc_key=(self.viewNo, 0))
+        #
+        # if key not in self.stashedRecvdCheckpoints.get(view_no, {}):
+        #     self.logger.trace("{} have no stashed checkpoints for {}")
+        #     return
+        #
+        # # Get a snapshot of all the senders of stashed checkpoints for `key`
+        # senders = list(self.stashedRecvdCheckpoints[view_no][key].keys())
+        # total_processed = 0
+        # consumed = 0
+        #
+        # for sender in senders:
+        #     # Check if the checkpoint from `sender` is still in
+        #     # `stashedRecvdCheckpoints` because it might be removed from there
+        #     # in case own checkpoint was stabilized when we were processing
+        #     # stashed checkpoints from previous senders in this loop
+        #     if view_no in self.stashedRecvdCheckpoints \
+        #             and key in self.stashedRecvdCheckpoints[view_no] \
+        #             and sender in self.stashedRecvdCheckpoints[view_no][key]:
+        #         if self.process_checkpoint(
+        #                 self.stashedRecvdCheckpoints[view_no][key].pop(sender),
+        #                 sender):
+        #             consumed += 1
+        #         # Note that if `process_checkpoint` returned False then the
+        #         # checkpoint from `sender` was re-stashed back to
+        #         # `stashedRecvdCheckpoints`
+        #         total_processed += 1
+        #
+        # # If we have consumed stashed checkpoints for `key` from all the
+        # # senders then remove entries which have become empty
+        # if view_no in self.stashedRecvdCheckpoints \
+        #         and key in self.stashedRecvdCheckpoints[view_no] \
+        #         and len(self.stashedRecvdCheckpoints[view_no][key]) == 0:
+        #     del self.stashedRecvdCheckpoints[view_no][key]
+        #     if len(self.stashedRecvdCheckpoints[view_no]) == 0:
+        #         del self.stashedRecvdCheckpoints[view_no]
+        #
+        # restashed = total_processed - consumed
+        # self.logger.info('{} processed {} stashed checkpoints for {}, '
+        #                  '{} of them were stashed again'.
+        #                  format(self, total_processed, key, restashed))
+        #
+        # return total_processed
+
+    def do_checkpoint(self, state: CheckpointState,
+                      start_no: int,
+                      end_no: int,
+                      ledger_id: int,
+                      view_no: int):
+        self._bus.send(DoCheckpointMessage(state,
+                                           start_no,
+                                           end_no,
+                                           ledger_id,
+                                           view_no))
+
+    """Method from legacy code"""
+    def l_addToOrdered(self, view_no: int, pp_seq_no: int):
         self.ordered.add(view_no, pp_seq_no)
         self.last_ordered_3pc = (view_no, pp_seq_no)
 
         self.requested_pre_prepares.pop((view_no, pp_seq_no), None)
         self.requested_prepares.pop((view_no, pp_seq_no), None)
         self.requested_commits.pop((view_no, pp_seq_no), None)
 
@@ -1547,26 +1837,28 @@
                     return txn_primaries
                 elif isinstance(txn_primaries, int):
                     last_primaries_seq_no = get_seq_no(txn) - txn_primaries
                     return get_payload_data(
                         ledger.get_by_seq_no_uncommitted(last_primaries_seq_no))[AUDIT_TXN_PRIMARIES]
                 break
         else:
-            return self._data.primaries
+            return self.db_manager.primaries
 
-    def _discard_ordered_req_keys(self, pp: PrePrepare):
+    """Method from legacy code"""
+    def l_discard_ordered_req_keys(self, pp: PrePrepare):
         for k in pp.reqIdr:
             # Using discard since the key may not be present as in case of
             # primary, the key was popped out while creating PRE-PREPARE.
             # Or in case of node catching up, it will not validate
             # PRE-PREPAREs or PREPAREs but will only validate number of COMMITs
             #  and their consistency with PRE-PREPARE of PREPAREs
-            self.discard_req_key(pp.ledgerId, k)
+            self.l_discard_req_key(pp.ledgerId, k)
 
-    def _can_order(self, commit: Commit) -> Tuple[bool, Optional[str]]:
+    """Method from legacy code"""
+    def l_canOrder(self, commit: Commit) -> Tuple[bool, Optional[str]]:
         """
         Return whether the specified commitRequest can be returned to the node.
 
         Decision criteria:
 
         - If have got just n-f Commit requests then return request to node
         - If less than n-f of commit requests then probably don't have
@@ -1581,31 +1873,32 @@
             return False, "no quorum ({}): {} commits where f is {}". \
                 format(quorum, commit, self.f)
 
         key = (commit.viewNo, commit.ppSeqNo)
         if self._validator.has_already_ordered(*key):
             return False, "already ordered"
 
-        if commit.ppSeqNo > 1 and not self._all_prev_ordered(commit):
+        if commit.ppSeqNo > 1 and not self.l_all_prev_ordered(commit):
             viewNo, ppSeqNo = commit.viewNo, commit.ppSeqNo
             if viewNo not in self.stashed_out_of_order_commits:
                 self.stashed_out_of_order_commits[viewNo] = {}
             self.stashed_out_of_order_commits[viewNo][ppSeqNo] = commit
             self._out_of_order_repeater.start()
             return False, "stashing {} since out of order". \
                 format(commit)
 
         return True, None
 
-    def _process_stashed_out_of_order_commits(self):
+    """Method from legacy code"""
+    def l_process_stashed_out_of_order_commits(self):
         # This method is called periodically to check for any commits that
         # were stashed due to lack of commits before them and orders them if it
         # can
 
-        if not self.can_order_commits():
+        if not self.can_order():
             return
 
         self._logger.debug('{} trying to order from out of order commits. '
                            'Len(stashed_out_of_order_commits) == {}'
                            .format(self, len(self.stashed_out_of_order_commits)))
         if self.last_ordered_3pc:
             lastOrdered = self.last_ordered_3pc
@@ -1622,17 +1915,17 @@
                 pToRemove = set()
                 for p, commit in self.stashed_out_of_order_commits[v].items():
                     if (v, p) in self.ordered or \
                             self._validator.has_already_ordered(*(commit.viewNo, commit.ppSeqNo)):
                         pToRemove.add(p)
                         continue
                     if (v == lastOrdered[0] and lastOrdered == (v, p - 1)) or \
-                            (v > lastOrdered[0] and self._is_lowest_commit_in_view(commit)):
+                            (v > lastOrdered[0] and self.l_isLowestCommitInView(commit)):
                         self._logger.debug("{} ordering stashed commit {}".format(self, commit))
-                        if self._try_order(commit):
+                        if self.l_tryOrder(commit):
                             lastOrdered = (v, p)
                             pToRemove.add(p)
 
                 for p in pToRemove:
                     del self.stashed_out_of_order_commits[v][p]
                 if not self.stashed_out_of_order_commits[v]:
                     vToRemove.add(v)
@@ -1643,22 +1936,23 @@
             if not self.stashed_out_of_order_commits:
                 self._out_of_order_repeater.stop()
         else:
             self._logger.debug('{} last_ordered_3pc if False. '
                                'Len(stashed_out_of_order_commits) == {}'
                                .format(self, len(self.stashed_out_of_order_commits)))
 
-    def _is_lowest_commit_in_view(self, commit):
+    def l_isLowestCommitInView(self, commit):
         view_no = commit.viewNo
         if view_no > self.view_no:
             self._logger.debug('{} encountered {} which belongs to a later view'.format(self, commit))
             return False
         return commit.ppSeqNo == 1
 
-    def _all_prev_ordered(self, commit: Commit):
+    """Method from legacy code"""
+    def l_all_prev_ordered(self, commit: Commit):
         """
         Return True if all previous COMMITs have been ordered
         """
         # TODO: This method does a lot of work, choose correct data
         # structures to make it efficient.
 
         viewNo, ppSeqNo = commit.viewNo, commit.ppSeqNo
@@ -1680,15 +1974,16 @@
             if v == viewNo and p < ppSeqNo and (v, p) not in self.ordered:
                 # If unordered commits are found with lower ppSeqNo then this
                 # cannot be ordered.
                 return False
 
         return True
 
-    def _can_commit(self, prepare: Prepare) -> (bool, str):
+    """Method from legacy code"""
+    def l_canCommit(self, prepare: Prepare) -> (bool, str):
         """
         Return whether the specified PREPARE can proceed to the Commit
         step.
 
         Decision criteria:
 
         - If this replica has got just n-f-1 PREPARE requests then commit request.
@@ -1697,19 +1992,20 @@
         - If more than n-f-1 then already sent COMMIT; don't commit
 
         :param prepare: the PREPARE
         """
         quorum = self._data.quorums.prepare.value
         if not self.prepares.hasQuorum(prepare, quorum):
             return False, 'does not have prepare quorum for {}'.format(prepare)
-        if self._has_committed(prepare):
+        if self.l_hasCommitted(prepare):
             return False, 'has already sent COMMIT for {}'.format(prepare)
         return True, ''
 
-    def _has_committed(self, request) -> bool:
+    """Method from legacy code"""
+    def l_hasCommitted(self, request) -> bool:
         return self.commits.hasCommitFrom(ThreePhaseKey(
             request.viewNo, request.ppSeqNo), self.name)
 
     def post_batch_creation(self, three_pc_batch: ThreePcBatch):
         """
         A batch of requests has been created and has been applied but
         committed to ledger and state.
@@ -1721,69 +2017,73 @@
         if ledger_id != POOL_LEDGER_ID and not three_pc_batch.primaries:
             three_pc_batch.primaries = self._write_manager.future_primary_handler.get_last_primaries() or self._data.primaries
         if self._write_manager.is_valid_ledger_id(ledger_id):
             self._write_manager.post_apply_batch(three_pc_batch)
         else:
             self._logger.debug('{} did not know how to handle for ledger {}'.format(self, ledger_id))
 
+    def send_outbox(self, msg):
+        self._bus.send(OutboxMessage(msg=msg))
+
     def post_batch_rejection(self, ledger_id):
         """
         A batch of requests has been rejected, if stateRoot is None, reject
         the current batch.
         :param ledger_id:
         :param stateRoot: state root after the batch was created
         :return:
         """
         if self._write_manager.is_valid_ledger_id(ledger_id):
             self._write_manager.post_batch_rejected(ledger_id)
         else:
             self._logger.debug('{} did not know how to handle for ledger {}'.format(self, ledger_id))
 
-    def _ledger_id_for_request(self, request: Request):
+    """Method from legacy code"""
+    def l_ledger_id_for_request(self, request: Request):
         if request.operation.get(TXN_TYPE) is None:
             raise ValueError(
                 "{} TXN_TYPE is not defined for request {}".format(self, request)
             )
 
         typ = request.operation[TXN_TYPE]
         return self._write_manager.type_to_ledger_id[typ]
 
-    def _do_dynamic_validation(self, request: Request, req_pp_time: int):
+    """Method from legacy code"""
+    def l_do_dynamic_validation(self, request: Request, req_pp_time: int):
         """
                 State based validation
                 """
         # Digest validation
         # TODO implicit caller's context: request is processed by (master) replica
         # as part of PrePrepare 3PC batch
         ledger_id, seq_no = self.db_manager.get_store(SEQ_NO_DB_LABEL).get_by_payload_digest(request.payload_digest)
         if ledger_id is not None and seq_no is not None:
             raise SuspiciousPrePrepare('Trying to order already ordered request')
 
-        ledger = self.db_manager.get_ledger(self._ledger_id_for_request(request))
+        ledger = self.db_manager.get_ledger(self.l_ledger_id_for_request(request))
         for txn in ledger.uncommittedTxns:
             if get_payload_digest(txn) == request.payload_digest:
                 raise SuspiciousPrePrepare('Trying to order already ordered request')
 
         # TAA validation
         # For now, we need to call taa_validation not from dynamic_validation because
         # req_pp_time is required
         self._write_manager.do_taa_validation(request, req_pp_time, self._config)
         self._write_manager.dynamic_validation(request)
 
-    @measure_consensus_time(MetricsName.REQUEST_PROCESSING_TIME,
-                            MetricsName.BACKUP_REQUEST_PROCESSING_TIME)
-    def _process_req_during_batch(self,
-                                  req: Request,
-                                  cons_time: int):
+    """Method from legacy code"""
+    def l_processReqDuringBatch(self,
+                                req: Request,
+                                cons_time: int):
         """
                 This method will do dynamic validation and apply requests.
                 If there is any errors during validation it would be raised
                 """
         if self.is_master:
-            self._do_dynamic_validation(req, cons_time)
+            self.l_do_dynamic_validation(req, cons_time)
             self._write_manager.apply_request(req, cons_time)
 
     def can_send_3pc_batch(self):
         if not self._data.is_primary:
             return False
         if not self._data.is_participating:
             return False
@@ -1797,35 +2097,35 @@
                 return False
             # This check is done for current view only to simplify logic and avoid
             # edge cases between views, especially taking into account that we need
             # to send a batch in new view as soon as possible
             if self._config.Max3PCBatchesInFlight is not None:
                 batches_in_flight = self._lastPrePrepareSeqNo - self.last_ordered_3pc[1]
                 if batches_in_flight >= self._config.Max3PCBatchesInFlight:
-                    if self._can_log_skip_send_3pc():
+                    if self.l_can_log_skip_send_3pc():
                         self._logger.info("{} not creating new batch because there already {} in flight out of {} allowed".
                                           format(self.name, batches_in_flight, self._config.Max3PCBatchesInFlight))
                     return False
 
         self._skip_send_3pc_ts = None
         return True
 
-    def _can_log_skip_send_3pc(self):
+    def l_can_log_skip_send_3pc(self):
         current_time = time.perf_counter()
         if self._skip_send_3pc_ts is None:
             self._skip_send_3pc_ts = current_time
             return True
 
         if current_time - self._skip_send_3pc_ts > self._config.Max3PCBatchWait:
             self._skip_send_3pc_ts = current_time
             return True
 
         return False
 
-    def can_order_commits(self):
+    def can_order(self):
         if self._data.is_participating:
             return True
         if self._data.is_synced and self._data.legacy_vc_in_progress:
             return True
         return False
 
     @staticmethod
@@ -1839,483 +2139,7 @@
         if isinstance(node_name, str):
             # Because sometimes it is bytes (why?)
             if ":" in node_name:
                 # Because in some cases (for requested messages) it
                 # already has ':'. This should be fixed.
                 return node_name
         return "{}:{}".format(node_name, inst_id)
-
-    def dequeue_pre_prepares(self):
-        """
-        Dequeue any received PRE-PREPAREs that did not have finalized requests
-        or the replica was missing any PRE-PREPAREs before it
-        :return:
-        """
-        ppsReady = []
-        # Check if any requests have become finalised belonging to any stashed
-        # PRE-PREPAREs.
-        for i, (pp, sender, reqIds) in enumerate(
-                self.prePreparesPendingFinReqs):
-            finalised = set()
-            for r in reqIds:
-                if self._requests.is_finalised(r):
-                    finalised.add(r)
-            diff = reqIds.difference(finalised)
-            # All requests become finalised
-            if not diff:
-                ppsReady.append(i)
-            self.prePreparesPendingFinReqs[i] = (pp, sender, diff)
-
-        for i in sorted(ppsReady, reverse=True):
-            pp, sender, _ = self.prePreparesPendingFinReqs.pop(i)
-            self.prePreparesPendingPrevPP[pp.viewNo, pp.ppSeqNo] = (pp, sender)
-
-        r = 0
-        while self.prePreparesPendingPrevPP and self._is_next_pre_prepare(
-                *self.prePreparesPendingPrevPP.iloc[0]):
-            _, (pp, sender) = self.prePreparesPendingPrevPP.popitem(last=False)
-            if not self._can_pp_seq_no_be_in_view(pp.viewNo, pp.ppSeqNo):
-                self._discard(pp, "Pre-Prepare from a previous view",
-                              self._logger.debug)
-                continue
-            self._logger.info("{} popping stashed PREPREPARE{} "
-                              "from sender {}".format(self, (pp.viewNo, pp.ppSeqNo), sender))
-            self._network.process_incoming(pp, sender)
-            r += 1
-        return r
-
-    # TODO: Convert this into a free function?
-    def _discard(self, msg, reason, logMethod=logging.error, cliOutput=False):
-        """
-        Discard a message and log a reason using the specified `logMethod`.
-
-        :param msg: the message to discard
-        :param reason: the reason why this message is being discarded
-        :param logMethod: the logging function to be used
-        :param cliOutput: if truthy, informs a CLI that the logged msg should
-        be printed
-        """
-        reason = "" if not reason else " because {}".format(reason)
-        logMethod("{} discarding message {}{}".format(self, msg, reason),
-                  extra={"cli": cliOutput})
-
-    def _can_pp_seq_no_be_in_view(self, view_no, pp_seq_no):
-        """
-        Checks if the `pp_seq_no` could have been in view `view_no`. It will
-        return False when the `pp_seq_no` belongs to a later view than
-        `view_no` else will return True
-        :return:
-        """
-        if view_no > self.view_no:
-            raise PlenumValueError(
-                'view_no', view_no,
-                "<= current view_no {}".format(self.view_no),
-                prefix=self
-            )
-
-        return view_no == self.view_no or (view_no < self.view_no and self._data.legacy_last_prepared_before_view_change and
-                                           compare_3PC_keys((view_no, pp_seq_no),
-                                                            self._data.legacy_last_prepared_before_view_change) >= 0)
-
-    def send_3pc_batch(self):
-        if not self.can_send_3pc_batch():
-            return 0
-
-        sent_batches = set()
-
-        # 1. send 3PC batches with requests for every ledger
-        self._send_3pc_batches_for_ledgers(sent_batches)
-
-        # 2. for every ledger we haven't just sent a 3PC batch check if it's not fresh enough,
-        # and send an empty 3PC batch to update the state if needed
-        self._send_3pc_freshness_batch(sent_batches)
-
-        # 3. send 3PC batch if new primaries elected
-        self.l_send_3pc_primaries_batch(sent_batches)
-
-        # 4. update ts of last sent 3PC batch
-        if len(sent_batches) > 0:
-            self.lastBatchCreated = self.get_current_time()
-
-        return len(sent_batches)
-
-    def l_send_3pc_primaries_batch(self, sent_batches):
-        # As we've selected new primaries, we need to send 3pc batch,
-        # so this primaries can be saved in audit ledger
-        if not sent_batches and self.primaries_batch_needed:
-            self._logger.debug("Sending a 3PC batch to propagate newly selected primaries")
-            self.primaries_batch_needed = False
-            sent_batches.add(self._do_send_3pc_batch(ledger_id=DOMAIN_LEDGER_ID))
-
-    def _send_3pc_freshness_batch(self, sent_batches):
-        if not self._config.UPDATE_STATE_FRESHNESS:
-            return
-
-        if not self.is_master:
-            return
-
-        # Update freshness for all outdated ledgers sequentially without any waits
-        # TODO: Consider sending every next update in Max3PCBatchWait only
-        outdated_ledgers = self._freshness_checker.check_freshness(self.get_time_for_3pc_batch())
-        for ledger_id, ts in outdated_ledgers.items():
-            if ledger_id in sent_batches:
-                self._logger.debug("Ledger {} is not updated for {} seconds, "
-                                   "but a 3PC for this ledger has been just sent".format(ledger_id, ts))
-                continue
-
-            self._logger.info("Ledger {} is not updated for {} seconds, "
-                              "so its freshness state is going to be updated now".format(ledger_id, ts))
-            sent_batches.add(
-                self._do_send_3pc_batch(ledger_id=ledger_id))
-
-    def _send_3pc_batches_for_ledgers(self, sent_batches):
-        # TODO: Consider sending every next update in Max3PCBatchWait only
-        for ledger_id, q in self.requestQueues.items():
-            if len(q) == 0:
-                continue
-
-            queue_full = len(q) >= self._config.Max3PCBatchSize
-            timeout = self.lastBatchCreated + self._config.Max3PCBatchWait < self.get_current_time()
-            if not queue_full and not timeout:
-                continue
-
-            sent_batches.add(
-                self._do_send_3pc_batch(ledger_id=ledger_id))
-
-    def _do_send_3pc_batch(self, ledger_id):
-        oldStateRootHash = self.get_state_root_hash(ledger_id, to_str=False)
-        pre_prepare = self.create_3pc_batch(ledger_id)
-        self.send_pre_prepare(pre_prepare)
-        if not self.is_master:
-            self.db_manager.get_store(LAST_SENT_PP_STORE_LABEL).store_last_sent_pp_seq_no(
-                self._data.inst_id, pre_prepare.ppSeqNo)
-
-        self._preprepare_batch(pre_prepare)
-        self._track_batches(pre_prepare, oldStateRootHash)
-        return ledger_id
-
-    @measure_consensus_time(MetricsName.CREATE_3PC_BATCH_TIME,
-                            MetricsName.BACKUP_CREATE_3PC_BATCH_TIME)
-    def create_3pc_batch(self, ledger_id):
-        pp_seq_no = self.lastPrePrepareSeqNo + 1
-        pool_state_root_hash = self.get_state_root_hash(POOL_LEDGER_ID)
-        self._logger.debug("{} creating batch {} for ledger {} with state root {}".format(
-            self, pp_seq_no, ledger_id,
-            self.get_state_root_hash(ledger_id, to_str=False)))
-
-        if self.last_accepted_pre_prepare_time is None:
-            last_ordered_ts = self._get_last_timestamp_from_state(ledger_id)
-            if last_ordered_ts:
-                self.last_accepted_pre_prepare_time = last_ordered_ts
-
-        # DO NOT REMOVE `view_no` argument, used while replay
-        # tm = self.utc_epoch
-        tm = self._get_utc_epoch_for_preprepare(self._data.inst_id, self.view_no,
-                                                pp_seq_no)
-
-        reqs, invalid_indices, rejects = self._consume_req_queue_for_pre_prepare(
-            ledger_id, tm, self.view_no, pp_seq_no)
-        if self.is_master:
-            three_pc_batch = ThreePcBatch(ledger_id=ledger_id,
-                                          inst_id=self._data.inst_id,
-                                          view_no=self.view_no,
-                                          pp_seq_no=pp_seq_no,
-                                          pp_time=tm,
-                                          state_root=self.get_state_root_hash(ledger_id, to_str=False),
-                                          txn_root=self.get_txn_root_hash(ledger_id, to_str=False),
-                                          primaries=[],
-                                          valid_digests=self._get_valid_req_ids_from_all_requests(
-                                              reqs, invalid_indices))
-            self.post_batch_creation(three_pc_batch)
-
-        digest = self.replica_batch_digest(reqs)
-        state_root_hash = self.get_state_root_hash(ledger_id)
-        audit_txn_root_hash = self.get_txn_root_hash(AUDIT_LEDGER_ID)
-
-        """TODO: for now default value for fields sub_seq_no is 0 and for final is True"""
-        params = [
-            self._data.inst_id,
-            self.view_no,
-            pp_seq_no,
-            tm,
-            [req.digest for req in reqs],
-            invalid_index_serializer.serialize(invalid_indices, toBytes=False),
-            digest,
-            ledger_id,
-            state_root_hash,
-            self.get_txn_root_hash(ledger_id),
-            0,
-            True,
-            pool_state_root_hash,
-            audit_txn_root_hash
-        ]
-
-        # BLS multi-sig:
-        params = self.l_bls_bft_replica.update_pre_prepare(params, ledger_id)
-
-        pre_prepare = PrePrepare(*params)
-
-        self._logger.trace('{} created a PRE-PREPARE with {} requests for ledger {}'.format(
-            self, len(reqs), ledger_id))
-        self.lastPrePrepareSeqNo = pp_seq_no
-        self.last_accepted_pre_prepare_time = tm
-        if self.is_master and rejects:
-            for reject in rejects:
-                self._network.send(reject)
-        return pre_prepare
-
-    def _get_last_timestamp_from_state(self, ledger_id):
-        if ledger_id == DOMAIN_LEDGER_ID:
-            ts_store = self.db_manager.get_store(TS_LABEL)
-            if ts_store:
-                last_timestamp = ts_store.get_last_key()
-                if last_timestamp:
-                    last_timestamp = int(last_timestamp.decode())
-                    self._logger.debug("Last ordered timestamp from store is : {}"
-                                       "".format(last_timestamp))
-                    return last_timestamp
-        return None
-
-    # This is to enable replaying, inst_id, view_no and pp_seq_no are used
-    # while replaying
-    def _get_utc_epoch_for_preprepare(self, inst_id, view_no, pp_seq_no):
-        tm = self.get_time_for_3pc_batch()
-        if self.last_accepted_pre_prepare_time and \
-                tm < self.last_accepted_pre_prepare_time:
-            tm = self.last_accepted_pre_prepare_time
-        return tm
-
-    def _consume_req_queue_for_pre_prepare(self, ledger_id, tm,
-                                           view_no, pp_seq_no):
-        reqs = []
-        rejects = []
-        invalid_indices = []
-        idx = 0
-        while len(reqs) < self._config.Max3PCBatchSize \
-                and self.requestQueues[ledger_id]:
-            key = self.requestQueues[ledger_id].pop(0)
-            if key in self._requests:
-                fin_req = self._requests[key].finalised
-                malicious_req = False
-                try:
-                    self._process_req_during_batch(fin_req,
-                                                   tm)
-
-                except (
-                        InvalidClientMessageException,
-                        UnknownIdentifier
-                ) as ex:
-                    self._logger.warning('{} encountered exception {} while processing {}, '
-                                         'will reject'.format(self, ex, fin_req))
-                    rejects.append((fin_req.key, Reject(fin_req.identifier, fin_req.reqId, ex)))
-                    invalid_indices.append(idx)
-                except SuspiciousPrePrepare:
-                    malicious_req = True
-                finally:
-                    if not malicious_req:
-                        reqs.append(fin_req)
-                if not malicious_req:
-                    idx += 1
-            else:
-                self._logger.debug('{} found {} in its request queue but the '
-                                   'corresponding request was removed'.format(self, key))
-
-        return reqs, invalid_indices, rejects
-
-    @measure_consensus_time(MetricsName.SEND_PREPREPARE_TIME,
-                            MetricsName.BACKUP_SEND_PREPREPARE_TIME)
-    def send_pre_prepare(self, ppReq: PrePrepare):
-        self.sentPrePrepares[ppReq.viewNo, ppReq.ppSeqNo] = ppReq
-        self._send(ppReq, stat=TPCStat.PrePrepareSent)
-
-    def _send(self, msg, dst=None, stat=None) -> None:
-        """
-        Send a message to the node on which this replica resides.
-
-        :param stat:
-        :param rid: remote id of one recipient (sends to all recipients if None)
-        :param msg: the message to send
-        """
-        if stat:
-            self.stats.inc(stat)
-        self._network.send(msg, dst=dst)
-
-    def revert_unordered_batches(self):
-        """
-        Revert changes to ledger (uncommitted) and state made by any requests
-        that have not been ordered.
-        """
-        i = 0
-        for key in sorted(self.batches.keys(), reverse=True):
-            if compare_3PC_keys(self.last_ordered_3pc, key) > 0:
-                ledger_id, discarded, _, prevStateRoot, len_reqIdr = self.batches.pop(key)
-                discarded = invalid_index_serializer.deserialize(discarded)
-                self._logger.debug('{} reverting 3PC key {}'.format(self, key))
-                self._revert(ledger_id, prevStateRoot, len_reqIdr - len(discarded))
-                i += 1
-            else:
-                break
-        self._logger.info('{} reverted {} batches before starting catch up'.format(self, i))
-        return i
-
-    def l_last_prepared_certificate_in_view(self) -> Optional[Tuple[int, int]]:
-        # Pick the latest sent COMMIT in the view.
-        # TODO: Consider stashed messages too?
-        if not self.is_master:
-            raise LogicError("{} is not a master".format(self))
-        keys = []
-        quorum = self._data.quorums.prepare.value
-        for key in self.prepares.keys():
-            if self.prepares.hasQuorum(ThreePhaseKey(*key), quorum):
-                keys.append(key)
-        return max_3PC_key(keys) if keys else None
-
-    def _caught_up_till_3pc(self, last_caught_up_3PC):
-        self.last_ordered_3pc = last_caught_up_3PC
-        self._remove_till_caught_up_3pc(last_caught_up_3PC)
-
-    def catchup_clear_for_backup(self):
-        if not self._data.is_primary:
-            self.last_ordered_3pc = (self._data.view_no, 0)
-            self.batches.clear()
-            self.sentPrePrepares.clear()
-            self.prePrepares.clear()
-            self.prepares.clear()
-            self.commits.clear()
-            self._data.prepared.clear()
-            self._data.preprepared.clear()
-            self.first_batch_after_catchup = True
-
-    def _remove_till_caught_up_3pc(self, last_caught_up_3PC):
-        """
-        Remove any 3 phase messages till the last ordered key and also remove
-        any corresponding request keys
-        """
-        outdated_pre_prepares = {}
-        for key, pp in self.prePrepares.items():
-            if compare_3PC_keys(key, last_caught_up_3PC) >= 0:
-                outdated_pre_prepares[key] = pp
-        for key, pp in self.sentPrePrepares.items():
-            if compare_3PC_keys(key, last_caught_up_3PC) >= 0:
-                outdated_pre_prepares[key] = pp
-
-        self._logger.trace('{} going to remove messages for {} 3PC keys'.format(
-            self, len(outdated_pre_prepares)))
-
-        for key, pp in outdated_pre_prepares.items():
-            self.batches.pop(key, None)
-            self.sentPrePrepares.pop(key, None)
-            self.prePrepares.pop(key, None)
-            self.prepares.pop(key, None)
-            self.commits.pop(key, None)
-            self._discard_ordered_req_keys(pp)
-            self._clear_batch(pp)
-
-    def get_sent_preprepare(self, viewNo, ppSeqNo):
-        key = (viewNo, ppSeqNo)
-        return self.sentPrePrepares.get(key)
-
-    def get_sent_prepare(self, viewNo, ppSeqNo):
-        key = (viewNo, ppSeqNo)
-        if key in self.prepares:
-            prepare = self.prepares[key].msg
-            if self.prepares.hasPrepareFrom(prepare, self.name):
-                return prepare
-        return None
-
-    def get_sent_commit(self, viewNo, ppSeqNo):
-        key = (viewNo, ppSeqNo)
-        if key in self.commits:
-            commit = self.commits[key].msg
-            if self.commits.hasCommitFrom(commit, self.name):
-                return commit
-        return None
-
-    def replica_batch_digest(self, reqs):
-        return replica_batch_digest(reqs)
-
-    def process_view_change_started(self, msg: ViewChangeStarted):
-        # 1. update shared data
-        self._data.preprepared = []
-        self._data.prepared = []
-
-        # 2. save existing PrePrepares
-        new_old_view_preprepares = {(pp.ppSeqNo, pp.digest): pp
-                                    for pp in itertools.chain(self.prePrepares.values(), self.sentPrePrepares.values())}
-        self.old_view_preprepares.update(new_old_view_preprepares)
-
-        # 3. revert unordered transactions
-        if self.is_master:
-            self.revert_unordered_batches()
-
-        # 4. Clear the 3PC log
-        self.prePrepares.clear()
-        self.prepares.clear()
-        self.commits.clear()
-
-        self.requested_pre_prepares.clear()
-        self.requested_prepares.clear()
-        self.requested_commits.clear()
-
-        self.pre_prepare_tss.clear()
-        self.prePreparesPendingFinReqs.clear()
-        self.prePreparesPendingPrevPP.clear()
-        self.sentPrePrepares.clear()
-        self.batches.clear()
-        self.ordered.clear_below_view(msg.view_no)
-        return PROCESS, None
-
-    def process_new_view_checkpoints_applied(self, msg: NewViewCheckpointsApplied):
-        result, reason = self._validate(msg)
-        if result != PROCESS:
-            return result, reason
-
-        if not self.is_master:
-            return DISCARD, "not master"
-
-        for batch_id in msg.batches:
-            # TODO: take into account original view no
-            pp = self.old_view_preprepares.get((batch_id.pp_seq_no, batch_id.pp_digest))
-            if pp is None:
-                # TODO: implement correct re-sending logic
-                # self._request_pre_prepare(three_pc_key=(batch_id.view_no, batch_id.pp_seq_no))
-                continue
-            if self._validator.has_already_ordered(batch_id.view_no, batch_id.pp_seq_no):
-                self._add_to_pre_prepares(pp)
-            else:
-                sender = self.generateName(self._data.primary_name, self._data.inst_id)
-                # TODO: route it through the bus?
-                self.process_preprepare(pp, sender)
-
-        # TODO: this needs to be removed
-        self._data.preprepared = [BatchID(view_no=msg.view_no, pp_seq_no=batch_id.pp_seq_no,
-                                          pp_digest=batch_id.pp_digest)
-                                  for batch_id in msg.batches]
-        self._data.prepared = []
-
-        return PROCESS, None
-
-    def _preprepare_batch(self, pp: PrePrepare):
-        """
-        After pp had validated, it placed into _preprepared list
-        """
-        if preprepare_to_batch_id(pp) in self._data.preprepared:
-            raise LogicError('New pp cannot be stored in preprepared')
-        if self._data.checkpoints and pp.ppSeqNo < self._data.last_checkpoint.seqNoEnd:
-            raise LogicError('ppSeqNo cannot be lower than last checkpoint')
-        self._data.preprepared.append(preprepare_to_batch_id(pp))
-
-    def _prepare_batch(self, pp: PrePrepare):
-        """
-        After prepared certificate for pp had collected,
-        it removed from _preprepared and placed into _prepared list
-        """
-        self._data.prepared.append(preprepare_to_batch_id(pp))
-
-    def _clear_batch(self, pp: PrePrepare):
-        """
-        When 3pc batch processed, it removed from _prepared list
-        """
-        if preprepare_to_batch_id(pp) in self._data.preprepared:
-            self._data.preprepared.remove(preprepare_to_batch_id(pp))
-        if preprepare_to_batch_id(pp) in self._data.prepared:
-            self._data.prepared.remove(preprepare_to_batch_id(pp))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_managers/action_request_manager.py` & `indy-plenum-1.9.2rc1/plenum/server/request_managers/action_request_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_managers/request_manager.py` & `indy-plenum-1.9.2rc1/plenum/server/request_managers/request_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_managers/write_request_manager.py` & `indy-plenum-1.9.2rc1/plenum/server/request_managers/write_request_manager.py`

 * *Files 3% similar despite different names*

```diff
@@ -45,22 +45,14 @@
     def register_req_handler(self, handler: WriteRequestHandler, ledger_id=None, typ=None):
         if not isinstance(handler, WriteRequestHandler):
             raise LogicError
         self._register_req_handler(handler, ledger_id=ledger_id, typ=typ)
 
     def register_batch_handler(self, handler: BatchRequestHandler,
                                ledger_id=None, add_to_begin=False):
-        # TODO: Probably it would be a good idea to improve this a bit
-        #  - allow to register batch handler for ALL ledgers (including ones yet unknown)
-        #    with just one function call
-        #  - instead of add_to_begin flag allow handlers to order themselves by
-        #    adding method order to BatchRequestHandler which takes other batch handler
-        #    and returns one of BEFORE, AFTER, DONTCARE
-        #  With these improvements both NodeBootstrap and plugins registration will
-        #  become simpler and more robust
         if not isinstance(handler, BatchRequestHandler):
             raise LogicError
         ledger_id = ledger_id if ledger_id is not None else handler.ledger_id
         handler_list = self.batch_handlers.setdefault(ledger_id, [])
         if handler in handler_list:
             return
         if add_to_begin:
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_managers/read_request_manager.py` & `indy-plenum-1.9.2rc1/plenum/server/request_managers/read_request_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/nym_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/nym_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/get_txn_author_agreement_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/get_txn_author_agreement_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/get_txn_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/get_txn_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/txn_author_agreement_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/txn_author_agreement_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/get_txn_author_agreement_aml_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/get_txn_author_agreement_aml_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/node_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/node_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/handler_interfaces/action_request_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/handler_interfaces/action_request_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/handler_interfaces/write_request_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/handler_interfaces/write_request_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/handler_interfaces/request_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/handler_interfaces/request_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/handler_interfaces/read_request_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/handler_interfaces/read_request_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/audit_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/audit_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/static_taa_helper.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/static_taa_helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/txn_author_agreement_aml_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/txn_author_agreement_aml_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/request_handlers/utils.py` & `indy-plenum-1.9.2rc1/plenum/server/request_handlers/utils.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/notifier_plugin_manager.py` & `indy-plenum-1.9.2rc1/plenum/server/notifier_plugin_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/instances.py` & `indy-plenum-1.9.2rc1/plenum/server/instances.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/pool_batch_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/batch_handlers/pool_batch_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/batch_request_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/batch_handlers/batch_request_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/three_pc_batch.py` & `indy-plenum-1.9.2rc1/plenum/server/batch_handlers/three_pc_batch.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/config_batch_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/batch_handlers/config_batch_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/ts_store_batch_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/batch_handlers/ts_store_batch_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/domain_batch_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/batch_handlers/domain_batch_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/batch_handlers/audit_batch_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/batch_handlers/audit_batch_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/message_handlers.py` & `indy-plenum-1.9.2rc1/plenum/server/message_handlers.py`

 * *Files 2% similar despite different names*

```diff
@@ -142,15 +142,16 @@
         if pp.instId != kwargs['inst_id'] \
                 or pp.viewNo != kwargs['view_no'] \
                 or pp.ppSeqNo != kwargs['pp_seq_no']:
             raise MismatchedMessageReplyException
         return pp
 
     def requestor(self, params: Dict[str, Any]) -> Optional[PrePrepare]:
-        return self.node.replicas[params['inst_id']].get_sent_preprepare(params['view_no'], params['pp_seq_no'])
+        return self.node.replicas[params['inst_id']].sentPrePrepares.get((
+            params['view_no'], params['pp_seq_no']))
 
     def processor(self, validated_msg: PrePrepare, params: Dict[str, Any], frm: str) -> None:
         inst_id = params['inst_id']
         frm = replica.Replica.generateName(frm, inst_id)
         self.node.replicas[inst_id].process_requested_pre_prepare(validated_msg,
                                                                   sender=frm)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/models.py` & `indy-plenum-1.9.2rc1/plenum/server/models.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/backup_instance_faulty_processor.py` & `indy-plenum-1.9.2rc1/plenum/server/backup_instance_faulty_processor.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/suspicion_codes.py` & `indy-plenum-1.9.2rc1/plenum/server/suspicion_codes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/message_req_processor.py` & `indy-plenum-1.9.2rc1/plenum/server/message_req_processor.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/future_primaries_batch_handler.py` & `indy-plenum-1.9.2rc1/plenum/server/future_primaries_batch_handler.py`

 * *Files 4% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 from typing import Tuple, Dict
 
 from common.exceptions import LogicError
 from plenum.common.constants import TXN_TYPE, NODE, TARGET_NYM, ALIAS, SERVICES, VALIDATOR, DATA, POOL_LEDGER_ID
 from plenum.common.util import getMaxFailures
 from plenum.server.batch_handlers.batch_request_handler import BatchRequestHandler
 from plenum.server.batch_handlers.three_pc_batch import ThreePcBatch
-from plenum.server.pool_manager import TxnPoolManager
 
 
 class NodeState:
     def __init__(self, node_reg, node_ids, primaries):
         self.node_reg = node_reg
 
         # Nodes are never delete from here, so they are in a sequential order
@@ -63,19 +62,17 @@
                     last_state.node_reg.append(node_name)
                 elif serv == [] and node_name in last_state.node_reg:
                     last_state.node_reg.remove(node_name)
 
                 count = self.get_required_number_of_instances(len(last_state.node_reg))
                 if last_state.number_of_inst != count:
                     last_state.number_of_inst = count
-                    new_validators = TxnPoolManager.calc_node_names_ordered_by_rank(last_state.node_reg,
-                                                                                    last_state.node_ids)
-                    last_state.primaries = self.node.primaries_selector.select_primaries(view_no=self.node.viewNo,
-                                                                                         instance_count=last_state.number_of_inst,
-                                                                                         validators=new_validators)
+                    last_state.primaries = self.node.elector.process_selection(
+                        last_state.number_of_inst,
+                        last_state.node_reg, last_state.node_ids)
 
         # We will save node state at every pool batch, so we could revert it correctly
         self.node_states.append(last_state)
         three_pc_batch.primaries = last_state.primaries
         return last_state.primaries
 
     def post_batch_rejected(self, ledger_id, prev_handler_result=None):
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/plugin/has_plugin_loader_helper.py` & `indy-plenum-1.9.2rc1/plenum/server/plugin/has_plugin_loader_helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/plugin/stats_consumer/stats_publisher.py` & `indy-plenum-1.9.2rc1/plenum/server/plugin/stats_consumer/stats_publisher.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/plugin/stats_consumer/plugin_firebase_stats_consumer.py` & `indy-plenum-1.9.2rc1/plenum/server/plugin/stats_consumer/plugin_firebase_stats_consumer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/inconsistency_watchers.py` & `indy-plenum-1.9.2rc1/plenum/server/inconsistency_watchers.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/ledgers_bootstrap.py` & `indy-plenum-1.9.2rc1/plenum/server/node_bootstrap.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,251 +1,326 @@
-from typing import Any, List, Optional, NamedTuple, Dict
-
-from common.exceptions import LogicError
 from common.serializers.serialization import state_roots_serializer
-from crypto.bls.bls_bft import BlsBft
 from ledger.compact_merkle_tree import CompactMerkleTree
-from ledger.genesis_txn.genesis_txn_initiator import GenesisTxnInitiator
 from ledger.genesis_txn.genesis_txn_initiator_from_file import GenesisTxnInitiatorFromFile
-from ledger.genesis_txn.genesis_txn_initiator_from_mem import GenesisTxnInitiatorFromMem
-from plenum.common.constants import AUDIT_LEDGER_ID, POOL_LEDGER_ID, CONFIG_LEDGER_ID, DOMAIN_LEDGER_ID, \
-    NODE_PRIMARY_STORAGE_SUFFIX, BLS_LABEL, HS_MEMORY
+from plenum.bls.bls_bft_factory import create_default_bls_bft_factory
 from plenum.common.ledger import Ledger
+from plenum.common.ledger_manager import LedgerManager
 from plenum.persistence.storage import initStorage
 from plenum.server.batch_handlers.audit_batch_handler import AuditBatchHandler
 from plenum.server.batch_handlers.config_batch_handler import ConfigBatchHandler
 from plenum.server.batch_handlers.domain_batch_handler import DomainBatchHandler
 from plenum.server.batch_handlers.pool_batch_handler import PoolBatchHandler
+from plenum.server.batch_handlers.ts_store_batch_handler import TsStoreBatchHandler
+from plenum.server.future_primaries_batch_handler import FuturePrimariesBatchHandler
+from plenum.server.last_sent_pp_store_helper import LastSentPpStoreHelper
 from plenum.server.request_handlers.audit_handler import AuditTxnHandler
 from plenum.server.request_handlers.get_txn_author_agreement_aml_handler import GetTxnAuthorAgreementAmlHandler
 from plenum.server.request_handlers.get_txn_author_agreement_handler import GetTxnAuthorAgreementHandler
+from plenum.server.request_handlers.get_txn_handler import GetTxnHandler
 from plenum.server.request_handlers.node_handler import NodeHandler
 from plenum.server.request_handlers.nym_handler import NymHandler
+
+from plenum.common.constants import POOL_LEDGER_ID, AUDIT_LEDGER_ID, DOMAIN_LEDGER_ID, CONFIG_LEDGER_ID, \
+    NODE_PRIMARY_STORAGE_SUFFIX, BLS_PREFIX, BLS_LABEL, TS_LABEL, SEQ_NO_DB_LABEL, NODE_STATUS_DB_LABEL, \
+    LAST_SENT_PP_STORE_LABEL
+from plenum.server.pool_manager import TxnPoolManager
 from plenum.server.request_handlers.txn_author_agreement_aml_handler import TxnAuthorAgreementAmlHandler
 from plenum.server.request_handlers.txn_author_agreement_handler import TxnAuthorAgreementHandler
-from plenum.server.request_managers.action_request_manager import ActionRequestManager
-from plenum.server.request_managers.read_request_manager import ReadRequestManager
-from plenum.server.request_managers.write_request_manager import WriteRequestManager
 from state.pruning_state import PruningState
-
-from storage.helper import initHashStore, initKeyValueStorage
-from storage.kv_in_memory import KeyValueStorageInMemory
+from state.state import State
+from storage.helper import initKeyValueStorage
 from stp_core.common.log import getlogger
 
+
 logger = getlogger()
 
 
-class LedgersBootstrap:
-    # TODO: Create ReqManagers from inside and make them properties just like bls_bft?
+class NodeBootstrap:
 
-    def __init__(self,
-                 write_req_manager: WriteRequestManager,
-                 read_req_manager: ReadRequestManager,
-                 action_req_manager: ActionRequestManager,
-                 name: str,
-                 config: Any,
-                 ledger_ids: List[int]):
-        self.write_manager = write_req_manager
-        self.read_manager = read_req_manager
-        self.action_manager = action_req_manager
-        self.db_manager = write_req_manager.database_manager
-        self._bls_bft = None  # type: Optional[BlsBft]
-        # TODO: vvv Move into some node config container class? vvv
-        self.name = name
-        self.config = config
-        self.ledger_ids = ledger_ids
-        self.data_location = None
-        self.pool_genesis = None  # type: Optional[GenesisTxnInitiator]
-        self.domain_genesis = None  # type: Optional[GenesisTxnInitiator]
-        # TODO: ^^^
-
-    def set_data_location(self, data_location: str):
-        self.data_location = data_location
-
-    def set_genesis_location(self, genesis_dir: str):
-        pool_genesis_file = getattr(self.config, "poolTransactionsFile")
-        self.pool_genesis = GenesisTxnInitiatorFromFile(genesis_dir, pool_genesis_file)
-
-        domain_genesis_file = getattr(self.config, "domainTransactionsFile")
-        self.domain_genesis = GenesisTxnInitiatorFromFile(genesis_dir, domain_genesis_file)
-
-    def set_genesis_transactions(self, pool_txns: List, domain_txns: List):
-        self.pool_genesis = GenesisTxnInitiatorFromMem(pool_txns)
-        self.domain_genesis = GenesisTxnInitiatorFromMem(domain_txns)
-
-    def init(self, domain_storage=None):
-        self._init_storages(domain_storage=domain_storage)
-        self._init_bls_bft()
-        self._init_common_managers()
-        self._init_write_request_validator()
-        self._register_req_handlers()
-        self._register_batch_handlers()
-        self._register_common_handlers()
-        self._upload_states()
-
-    @property
-    def bls_bft(self) -> BlsBft:
-        if self._bls_bft is None:
-            raise LogicError("Tried to access BlsBft before initialization")
-        return self._bls_bft
-
-    def _create_bls_bft(self) -> BlsBft:
-        raise NotImplemented
-
-    def _update_txn_with_extra_data(self, txn):
-        raise NotImplemented
-
-    def _init_storages(self, domain_storage):
-        self.db_manager.register_new_database(CONFIG_LEDGER_ID,
-                                              self._create_ledger('config'),
-                                              self._create_state('config'),
-                                              taa_acceptance_required=False)
-
-        self.db_manager.register_new_database(POOL_LEDGER_ID,
-                                              self._create_ledger('pool', self.pool_genesis),
-                                              self._create_state('pool'),
-                                              taa_acceptance_required=False)
-
-        self.db_manager.register_new_database(DOMAIN_LEDGER_ID,
-                                              domain_storage or self._create_domain_ledger(),
-                                              self._create_state('domain'),
-                                              taa_acceptance_required=True)
-
-        self.db_manager.register_new_database(AUDIT_LEDGER_ID,
-                                              self._create_ledger('audit'),
-                                              taa_acceptance_required=False)
-
-    def _init_bls_bft(self):
-        self._bls_bft = self._create_bls_bft()
-        self.db_manager.register_new_store(BLS_LABEL, self.bls_bft.bls_store)
+    def __init__(self, node):
+        self.node = node
 
-    def _init_common_managers(self):
-        pass
+    def init_node(self, storage):
+        self.init_storages(storage=storage)
+        self.init_bls_bft()
+        self.init_common_managers()
+        self._init_write_request_validator()
+        self.register_req_handlers()
+        self.register_batch_handlers()
+        self.register_common_handlers()
+        self.upload_states()
+
+    def init_state_ts_db_storage(self):
+        ts_storage = self.node._get_state_ts_db_storage()
+        self.node.db_manager.register_new_store(TS_LABEL, ts_storage)
+
+    def init_seq_no_db_storage(self):
+        seq_no_db_storage = self.node.loadSeqNoDB()
+        self.node.db_manager.register_new_store(SEQ_NO_DB_LABEL, seq_no_db_storage)
+
+    def init_node_status_db_storage(self):
+        node_status_db = self.node.loadNodeStatusDB()
+        self.node.db_manager.register_new_store(NODE_STATUS_DB_LABEL, node_status_db)
+
+    def init_last_sent_pp_store(self):
+        last_sent_pp_store = LastSentPpStoreHelper(self.node)
+        self.node.db_manager.register_new_store(LAST_SENT_PP_STORE_LABEL, last_sent_pp_store)
+
+    def init_storages(self, storage=None):
+        # Config ledger and state init
+        self.node.db_manager.register_new_database(CONFIG_LEDGER_ID,
+                                                   self.init_config_ledger(),
+                                                   self.init_config_state(),
+                                                   taa_acceptance_required=False)
+
+        # Pool ledger init
+        self.node.db_manager.register_new_database(POOL_LEDGER_ID,
+                                                   self.init_pool_ledger(),
+                                                   self.init_pool_state(),
+                                                   taa_acceptance_required=False)
+
+        # Domain ledger init
+        self.node.db_manager.register_new_database(DOMAIN_LEDGER_ID,
+                                                   storage or self.init_domain_ledger(),
+                                                   self.init_domain_state(),
+                                                   taa_acceptance_required=True)
+
+        # Audit ledger init
+        self.node.db_manager.register_new_database(AUDIT_LEDGER_ID,
+                                                   self.init_audit_ledger(),
+                                                   taa_acceptance_required=False)
+        # StateTsDbStorage
+        self.init_state_ts_db_storage()
+
+        # seqNoDB storage
+        self.init_seq_no_db_storage()
+
+        # nodeStatusDB
+        self.init_node_status_db_storage()
+
+        # last_sent_pp_store
+        self.init_last_sent_pp_store()
+
+    def init_bls_bft(self):
+        self.node.bls_bft = self._create_bls_bft()
+        self.node.db_manager.register_new_store(BLS_LABEL, self.node.bls_bft.bls_store)
+
+    def init_common_managers(self):
+        # Pool manager init
+        self.node.poolManager = TxnPoolManager(self.node,
+                                               self.node.poolLedger,
+                                               self.node.states[POOL_LEDGER_ID],
+                                               self.node.write_manager,
+                                               self.node.ha,
+                                               self.node.cliname,
+                                               self.node.cliha)
+
+        # Ledger manager init
+        ledger_sync_order = self.node.ledger_ids
+        self.node.ledgerManager = LedgerManager(self.node,
+                                                postAllLedgersCaughtUp=self.node.allLedgersCaughtUp,
+                                                preCatchupClbk=self.node.preLedgerCatchUp,
+                                                postCatchupClbk=self.node.postLedgerCatchUp,
+                                                ledger_sync_order=ledger_sync_order,
+                                                metrics=self.node.metrics)
+
+    def register_req_handlers(self):
+        self.register_pool_req_handlers()
+        self.register_domain_req_handlers()
+        self.register_config_req_handlers()
+        self.register_audit_req_handlers()
+        self.register_action_req_handlers()
+
+    def register_audit_req_handlers(self):
+        audit_handler = AuditTxnHandler(database_manager=self.node.db_manager)
+        self.node.write_manager.register_req_handler(audit_handler)
+
+    def register_domain_req_handlers(self):
+        nym_handler = NymHandler(self.node.config, self.node.db_manager)
+        self.node.write_manager.register_req_handler(nym_handler)
+
+    def register_pool_req_handlers(self):
+        node_handler = NodeHandler(self.node.db_manager, self.node.bls_bft.bls_crypto_verifier)
+        self.node.write_manager.register_req_handler(node_handler)
+
+    def register_config_req_handlers(self):
+        taa_aml_handler = TxnAuthorAgreementAmlHandler(database_manager=self.node.db_manager)
+        taa_handler = TxnAuthorAgreementHandler(database_manager=self.node.db_manager)
+        get_taa_aml_handler = GetTxnAuthorAgreementAmlHandler(database_manager=self.node.db_manager)
+        get_taa_handler = GetTxnAuthorAgreementHandler(database_manager=self.node.db_manager)
 
-    def _init_write_request_validator(self):
-        pass
+        self.node.write_manager.register_req_handler(taa_aml_handler)
+        self.node.write_manager.register_req_handler(taa_handler)
 
-    def _register_req_handlers(self):
-        self._register_pool_req_handlers()
-        self._register_domain_req_handlers()
-        self._register_config_req_handlers()
-        self._register_audit_req_handlers()
-        self._register_action_req_handlers()
-
-    def _register_pool_req_handlers(self):
-        node_handler = NodeHandler(self.db_manager, self.bls_bft.bls_crypto_verifier)
-        self.write_manager.register_req_handler(node_handler)
-
-    def _register_domain_req_handlers(self):
-        nym_handler = NymHandler(self.config, self.db_manager)
-        self.write_manager.register_req_handler(nym_handler)
-
-    def _register_config_req_handlers(self):
-        taa_aml_handler = TxnAuthorAgreementAmlHandler(database_manager=self.db_manager)
-        taa_handler = TxnAuthorAgreementHandler(database_manager=self.db_manager)
-        get_taa_aml_handler = GetTxnAuthorAgreementAmlHandler(database_manager=self.db_manager)
-        get_taa_handler = GetTxnAuthorAgreementHandler(database_manager=self.db_manager)
-
-        self.write_manager.register_req_handler(taa_aml_handler)
-        self.write_manager.register_req_handler(taa_handler)
-
-        self.read_manager.register_req_handler(get_taa_aml_handler)
-        self.read_manager.register_req_handler(get_taa_handler)
-
-    def _register_audit_req_handlers(self):
-        audit_handler = AuditTxnHandler(database_manager=self.db_manager)
-        self.write_manager.register_req_handler(audit_handler)
+        self.node.read_manager.register_req_handler(get_taa_aml_handler)
+        self.node.read_manager.register_req_handler(get_taa_handler)
 
-    def _register_action_req_handlers(self):
+    def register_action_req_handlers(self):
         pass
 
-    def _register_batch_handlers(self):
-        self._register_pool_batch_handlers()
-        self._register_domain_batch_handlers()
-        self._register_config_batch_handlers()
+    def register_pool_batch_handlers(self):
+        pool_b_h = PoolBatchHandler(self.node.db_manager)
+        future_primaries_handler = FuturePrimariesBatchHandler(self.node.db_manager, self.node)
+        self.node.write_manager.register_batch_handler(pool_b_h)
+        self.node.write_manager.register_batch_handler(future_primaries_handler)
+
+    def register_domain_batch_handlers(self):
+        domain_b_h = DomainBatchHandler(self.node.db_manager)
+        self.node.write_manager.register_batch_handler(domain_b_h)
+
+    def register_config_batch_handlers(self):
+        config_b_h = ConfigBatchHandler(self.node.db_manager)
+        self.node.write_manager.register_batch_handler(config_b_h)
+
+    def register_audit_batch_handlers(self):
+        audit_b_h = AuditBatchHandler(self.node.db_manager)
+        for lid in self.node.ledger_ids:
+            self.node.write_manager.register_batch_handler(audit_b_h, ledger_id=lid)
+
+    def register_ts_store_batch_handlers(self):
+        ts_store_b_h = TsStoreBatchHandler(self.node.db_manager)
+        for lid in [DOMAIN_LEDGER_ID, CONFIG_LEDGER_ID]:
+            self.node.write_manager.register_batch_handler(ts_store_b_h, ledger_id=lid)
+
+    def register_common_handlers(self):
+        get_txn_handler = GetTxnHandler(self.node, self.node.db_manager)
+        for lid in self.node.ledger_ids:
+            self.node.read_manager.register_req_handler(get_txn_handler, ledger_id=lid)
+        self.register_ts_store_batch_handlers()
+
+    def register_batch_handlers(self):
+        self.register_pool_batch_handlers()
+        self.register_domain_batch_handlers()
+        self.register_config_batch_handlers()
         # Audit batch handler should be initiated the last
-        self._register_audit_batch_handlers()
+        self.register_audit_batch_handlers()
 
-    def _register_pool_batch_handlers(self):
-        pool_b_h = PoolBatchHandler(self.db_manager)
-        self.write_manager.register_batch_handler(pool_b_h)
-
-    def _register_domain_batch_handlers(self):
-        domain_b_h = DomainBatchHandler(self.db_manager)
-        self.write_manager.register_batch_handler(domain_b_h)
-
-    def _register_config_batch_handlers(self):
-        config_b_h = ConfigBatchHandler(self.db_manager)
-        self.write_manager.register_batch_handler(config_b_h)
-
-    def _register_audit_batch_handlers(self):
-        audit_b_h = AuditBatchHandler(self.db_manager)
-        for lid in self.ledger_ids:
-            self.write_manager.register_batch_handler(audit_b_h, ledger_id=lid)
-
-    def _register_common_handlers(self):
+    def _init_write_request_validator(self):
         pass
 
-    def _upload_states(self):
-        self._init_state_from_ledger(POOL_LEDGER_ID)
-        self._init_state_from_ledger(CONFIG_LEDGER_ID)
-        self._init_state_from_ledger(DOMAIN_LEDGER_ID)
-
-    def _create_ledger(self, name: str, genesis: Optional[GenesisTxnInitiator] = None) -> Ledger:
-        hs_type = HS_MEMORY if self.data_location is None else None
-        hash_store = initHashStore(self.data_location, name, self.config, hs_type=hs_type)
-        txn_file_name = getattr(self.config, "{}TransactionsFile".format(name))
-
-        txn_log_storage = None
-        if self.data_location is None:
-            txn_log_storage = KeyValueStorageInMemory()
-
-        return Ledger(CompactMerkleTree(hashStore=hash_store),
-                      dataDir=self.data_location,
-                      fileName=txn_file_name,
-                      transactionLogStore=txn_log_storage,
-                      ensureDurability=self.config.EnsureLedgerDurability,
-                      genesis_txn_initiator=genesis)
+    def init_pool_ledger(self):
+        genesis_txn_initiator = GenesisTxnInitiatorFromFile(
+            self.node.genesis_dir, self.node.config.poolTransactionsFile)
+        tree = CompactMerkleTree(hashStore=self.node.getHashStore('pool'))
+        return Ledger(tree,
+                      dataDir=self.node.dataLocation,
+                      fileName=self.node.config.poolTransactionsFile,
+                      ensureDurability=self.node.config.EnsureLedgerDurability,
+                      genesis_txn_initiator=genesis_txn_initiator)
 
-    def _create_domain_ledger(self) -> Ledger:
-        if self.config.primaryStorage is None:
+    def init_domain_ledger(self):
+        """
+        This is usually an implementation of Ledger
+        """
+        if self.node.config.primaryStorage is None:
             # TODO: add a place for initialization of all ledgers, so it's
             # clear what ledgers we have and how they are initialized
-            return self._create_ledger('domain', self.domain_genesis)
+            genesis_txn_initiator = GenesisTxnInitiatorFromFile(
+                self.node.genesis_dir, self.node.config.domainTransactionsFile)
+            tree = CompactMerkleTree(hashStore=self.node.getHashStore('domain'))
+            return Ledger(tree,
+                          dataDir=self.node.dataLocation,
+                          fileName=self.node.config.domainTransactionsFile,
+                          ensureDurability=self.node.config.EnsureLedgerDurability,
+                          genesis_txn_initiator=genesis_txn_initiator)
         else:
             # TODO: we need to rethink this functionality
-            return initStorage(self.config.primaryStorage,
-                               name=self.name + NODE_PRIMARY_STORAGE_SUFFIX,
-                               dataDir=self.data_location,
-                               config=self.config)
-
-    def _create_state(self, name: str) -> PruningState:
-        storage_name = getattr(self.config, "{}StateStorage".format(name))
-        db_name = getattr(self.config, "{}StateDbName".format(name))
-        if self.data_location is not None:
-            return PruningState(
-                initKeyValueStorage(
-                    storage_name,
-                    self.data_location,
-                    db_name,
-                    db_config=self.config.db_state_config))
-        else:
-            return PruningState(KeyValueStorageInMemory())
+            return initStorage(self.node.config.primaryStorage,
+                               name=self.node.name + NODE_PRIMARY_STORAGE_SUFFIX,
+                               dataDir=self.node.dataLocation,
+                               config=self.node.config)
+
+    def init_config_ledger(self):
+        return Ledger(CompactMerkleTree(hashStore=self.node.getHashStore('config')),
+                      dataDir=self.node.dataLocation,
+                      fileName=self.node.config.configTransactionsFile,
+                      ensureDurability=self.node.config.EnsureLedgerDurability)
+
+    def init_audit_ledger(self):
+        return Ledger(CompactMerkleTree(hashStore=self.node.getHashStore('audit')),
+                      dataDir=self.node.dataLocation,
+                      fileName=self.node.config.auditTransactionsFile,
+                      ensureDurability=self.node.config.EnsureLedgerDurability)
+
+    # STATES
+    def init_pool_state(self):
+        return PruningState(
+            initKeyValueStorage(
+                self.node.config.poolStateStorage,
+                self.node.dataLocation,
+                self.node.config.poolStateDbName,
+                db_config=self.node.config.db_state_config)
+        )
+
+    def init_domain_state(self):
+        return PruningState(
+            initKeyValueStorage(
+                self.node.config.domainStateStorage,
+                self.node.dataLocation,
+                self.node.config.domainStateDbName,
+                db_config=self.node.config.db_state_config)
+        )
+
+    def init_config_state(self):
+        return PruningState(
+            initKeyValueStorage(
+                self.node.config.configStateStorage,
+                self.node.dataLocation,
+                self.node.config.configStateDbName,
+                db_config=self.node.config.db_state_config)
+        )
 
-    def _init_state_from_ledger(self, ledger_id: int):
+    # STATES INIT
+    def init_state_from_ledger(self, state: State, ledger: Ledger):
         """
         If the trie is empty then initialize it by applying
         txns from ledger.
         """
-        state = self.db_manager.get_state(ledger_id)
         if state.isEmpty:
-            logger.info('{} found state to be empty, recreating from ledger {}'.format(self, ledger_id))
-            ledger = self.db_manager.get_ledger(ledger_id)
+            logger.info('{} found state to be empty, recreating from '
+                        'ledger'.format(self))
             for seq_no, txn in ledger.getAllTxn():
-                txn = self._update_txn_with_extra_data(txn)
-                self.write_manager.update_state(txn, isCommitted=True)
+                txn = self.node.update_txn_with_extra_data(txn)
+                self.node.write_manager.update_state(txn, isCommitted=True)
                 state.commit(rootHash=state.headHash)
 
+    def upload_pool_state(self):
+        self.init_state_from_ledger(self.node.states[POOL_LEDGER_ID],
+                                    self.node.poolLedger)
+        logger.info(
+            "{} initialized pool state: state root {}".format(
+                self, state_roots_serializer.serialize(
+                    bytes(self.node.states[POOL_LEDGER_ID].committedHeadHash))))
+
+    def upload_domain_state(self):
+        self.init_state_from_ledger(self.node.states[DOMAIN_LEDGER_ID],
+                                    self.node.domainLedger)
         logger.info(
-            "{} initialized state for ledger {}: state root {}".format(
-                self, ledger_id,
-                state_roots_serializer.serialize(bytes(state.committedHeadHash))))
+            "{} initialized domain state: state root {}".format(
+                self, state_roots_serializer.serialize(
+                    bytes(self.node.states[DOMAIN_LEDGER_ID].committedHeadHash))))
+
+    def upload_config_state(self):
+        self.init_state_from_ledger(self.node.states[CONFIG_LEDGER_ID],
+                                    self.node.configLedger)
+        logger.info(
+            "{} initialized config state: state root {}".format(
+                self, state_roots_serializer.serialize(
+                    bytes(self.node.states[CONFIG_LEDGER_ID].committedHeadHash))))
+
+    def upload_states(self):
+        self.upload_pool_state()
+        self.upload_config_state()
+        self.upload_domain_state()
+
+    def _create_bls_bft(self):
+        bls_factory = create_default_bls_bft_factory(self.node)
+        bls_bft = bls_factory.create_bls_bft()
+        if bls_bft.can_sign_bls():
+            logger.display("{}BLS Signatures will be used for Node {}".format(BLS_PREFIX, self.node.name))
+        else:
+            # TODO: for now we allow that BLS is optional, so that we don't require it
+            logger.warning(
+                '{}Transactions will not be BLS signed by this Node, since BLS keys were not found. '
+                'Please make sure that a script to init BLS keys was called (init_bls_keys),'
+                ' and NODE txn was sent with BLS public keys.'.format(BLS_PREFIX))
+        return bls_bft
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/server/replicas.py` & `indy-plenum-1.9.2rc1/plenum/server/replicas.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 from collections import deque
-from typing import Generator, Type, Callable
+from typing import Generator
 
 from common.exceptions import PlenumTypeError
 from crypto.bls.bls_bft import BlsBft
 from crypto.bls.bls_key_manager import LoadBLSKeyError
 from plenum.bls.bls_bft_factory import create_default_bls_bft_factory
 from plenum.common.constants import BLS_PREFIX
 from plenum.common.metrics_collector import MetricsCollector, NullMetricsCollector
@@ -50,44 +50,41 @@
         logger.info('reset monitor due to replica addition')
         self._monitor.reset()
 
     def remove_replica(self, inst_id: int):
         if inst_id not in self._replicas:
             return
         replica = self._replicas.pop(inst_id)
-        replica.cleanup()
+
+        # Aggregate all the currently forwarded requests
+        req_keys = set()
+        for msg in replica.inBox:
+            if isinstance(msg, ReqKey):
+                req_keys.add(msg.digest)
+        for req_queue in replica.requestQueues.values():
+            for req_key in req_queue:
+                req_keys.add(req_key)
+        for pp in replica.sentPrePrepares.values():
+            for req_key in pp.reqIdr:
+                req_keys.add(req_key)
+        for pp in replica.prePrepares.values():
+            for req_key in pp.reqIdr:
+                req_keys.add(req_key)
+
+        for req_key in req_keys:
+            if req_key in replica.requests:
+                replica.requests.ordered_by_replica(req_key)
+                replica.requests.free(req_key)
 
         self._messages_to_replicas.pop(inst_id, None)
         self._monitor.removeInstance(inst_id)
         logger.display("{} removed replica {} from instance {}".
                        format(self._node.name, replica, replica.instId),
                        extra={"tags": ["node-replica"]})
 
-    def send_to_internal_bus(self, msg, inst_id: int=None):
-        if inst_id is None:
-            for replica in self._replicas.values():
-                replica.internal_bus.send(msg)
-        else:
-            if inst_id in self._replicas:
-                self._replicas[inst_id].internal_bus.send(msg)
-            else:
-                logger.info("Cannot send msg ({}) to the replica {} "
-                            "because it does not exist.".format(msg, inst_id))
-
-    def subscribe_to_internal_bus(self, message_type: Type, handler: Callable, inst_id: int=None):
-        if inst_id is None:
-            for replica in self._replicas.values():
-                replica.internal_bus.subscribe(message_type, handler)
-        else:
-            if inst_id in self._replicas:
-                self._replicas[inst_id].internal_bus.subscribe(message_type, handler)
-            else:
-                logger.info("Cannot subscribe for {} for the replica {} "
-                            "because it does not exist.".format(message_type, inst_id))
-
     # TODO unit test
     @property
     def some_replica_is_primary(self) -> bool:
         return any([r.isPrimary for r in self._replicas.values()])
 
     @property
     def master_replica_is_primary(self):
@@ -183,43 +180,43 @@
 
     def unordered_request_handler_logging(self, unordereds):
         replica = self._master_replica
         for unordered in unordereds:
             reqId, duration = unordered
 
             # get ppSeqNo and viewNo
-            preprepares = replica._ordering_service.sentPrePrepares if replica.isPrimary else replica._ordering_service.prePrepares
+            preprepares = replica.sentPrePrepares if replica.isPrimary else replica.prePrepares
             ppSeqNo = None
             viewNo = None
             for key in preprepares:
                 if any([pre_pre_req == reqId for pre_pre_req in preprepares[key].reqIdr]):
                     ppSeqNo = preprepares[key].ppSeqNo
                     viewNo = preprepares[key].viewNo
                     break
             if ppSeqNo is None or viewNo is None:
                 logger.warning('Unordered request with reqId: {} was not found in prePrepares. '
                                'Prepares count: {}, Commits count: {}'.format(reqId,
-                                                                              len(replica._ordering_service.prepares),
-                                                                              len(replica._ordering_service.commits)))
+                                                                              len(replica.prepares),
+                                                                              len(replica.commits)))
                 continue
 
             # get pre-prepare sender
             prepre_sender = replica.primaryNames.get(viewNo, 'UNKNOWN')
 
             # get prepares info
-            prepares = replica._ordering_service.prepares[(viewNo, ppSeqNo)][0] \
-                if (viewNo, ppSeqNo) in replica._ordering_service.prepares else []
+            prepares = replica.prepares[(viewNo, ppSeqNo)][0] \
+                if (viewNo, ppSeqNo) in replica.prepares else []
             n_prepares = len(prepares)
             str_prepares = 'noone'
             if n_prepares:
                 str_prepares = ', '.join(prepares)
 
             # get commits info
-            commits = replica._ordering_service.commits[(viewNo, ppSeqNo)][0] \
-                if (viewNo, ppSeqNo) in replica._ordering_service.commits else []
+            commits = replica.commits[(viewNo, ppSeqNo)][0] \
+                if (viewNo, ppSeqNo) in replica.commits else []
             n_commits = len(commits)
             str_commits = 'noone'
             if n_commits:
                 str_commits = ', '.join(commits)
 
             # get txn content
             content = replica.requests[reqId].finalised.as_dict \
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/client/wallet.py` & `indy-plenum-1.9.2rc1/plenum/client/wallet.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/bls/bls_key_register_pool_ledger.py` & `indy-plenum-1.9.2rc1/plenum/bls/bls_key_register_pool_ledger.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/bls/bls_crypto_factory.py` & `indy-plenum-1.9.2rc1/plenum/bls/bls_crypto_factory.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/bls/bls_bft_factory.py` & `indy-plenum-1.9.2rc1/plenum/bls/bls_bft_factory.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/bls/bls_key_manager_file.py` & `indy-plenum-1.9.2rc1/plenum/bls/bls_key_manager_file.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/bls/bls_store.py` & `indy-plenum-1.9.2rc1/plenum/bls/bls_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/bls/bls_key_register_pool_manager.py` & `indy-plenum-1.9.2rc1/plenum/bls/bls_key_register_pool_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/bls/bls_bft_replica_plenum.py` & `indy-plenum-1.9.2rc1/plenum/bls/bls_bft_replica_plenum.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/config_helper.py` & `indy-plenum-1.9.2rc1/plenum/common/config_helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/transaction_store.py` & `indy-plenum-1.9.2rc1/plenum/common/transaction_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/batched.py` & `indy-plenum-1.9.2rc1/plenum/common/batched.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/jsonpickle_util.py` & `indy-plenum-1.9.2rc1/plenum/common/jsonpickle_util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/types.py` & `indy-plenum-1.9.2rc1/plenum/common/types.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/prepare_batch.py` & `indy-plenum-1.9.2rc1/plenum/common/prepare_batch.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/ledger_info.py` & `indy-plenum-1.9.2rc1/plenum/common/ledger_info.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/did_method.py` & `indy-plenum-1.9.2rc1/plenum/common/did_method.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/script_helper.py` & `indy-plenum-1.9.2rc1/plenum/common/script_helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/latency_measurements.py` & `indy-plenum-1.9.2rc1/plenum/common/latency_measurements.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/exceptions.py` & `indy-plenum-1.9.2rc1/plenum/common/exceptions.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/gc_trackers.py` & `indy-plenum-1.9.2rc1/plenum/common/gc_trackers.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/throttler.py` & `indy-plenum-1.9.2rc1/plenum/common/throttler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/txn_util.py` & `indy-plenum-1.9.2rc1/plenum/common/txn_util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/keygen_utils.py` & `indy-plenum-1.9.2rc1/plenum/common/keygen_utils.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/signer_simple.py` & `indy-plenum-1.9.2rc1/plenum/common/signer_simple.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/sys_util.py` & `indy-plenum-1.9.2rc1/plenum/common/sys_util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/signer_did.py` & `indy-plenum-1.9.2rc1/plenum/common/signer_did.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/stashing_deque.py` & `indy-plenum-1.9.2rc1/plenum/common/stashing_deque.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/metrics_stats.py` & `indy-plenum-1.9.2rc1/plenum/persistence/db_hash_store.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,118 +1,111 @@
-from collections import defaultdict
-from copy import deepcopy
-from datetime import datetime, timedelta
-from typing import Sequence, Union
-
-from plenum.common.metrics_collector import MetricsName, KvStoreMetricsFormat
-from plenum.common.value_accumulator import ValueAccumulator
-from storage.kv_store import KeyValueStorage
-
-
-def trunc_ts(ts: datetime, step: timedelta):
-    base = datetime.min.replace(year=2000)
-    step_s = step.total_seconds()
-    seconds = (ts - base).total_seconds()
-    seconds = int(seconds / step_s) * step_s
-    return (base + timedelta(seconds=seconds, milliseconds=500)).replace(microsecond=0)
-
-
-class MetricsStatsFrame:
-    def __init__(self):
-        self._stats = defaultdict(ValueAccumulator)
-
-    def add(self, id: MetricsName, value: Union[float, ValueAccumulator]):
-        if isinstance(value, ValueAccumulator):
-            self._stats[id].merge(value)
-        else:
-            self._stats[id].add(value)
-
-    def get(self, id: MetricsName) -> ValueAccumulator:
-        return self._stats[id]
-
-    def merge(self, other):
-        for id, acc in other._stats.items():
-            self._stats[id].merge(acc)
-
-    def __eq__(self, other):
-        if not isinstance(other, MetricsStatsFrame):
-            return False
-        for k in set(self._stats.keys()).union(other._stats.keys()):
-            if self._stats[k] != other._stats[k]:
-                return False
-        return True
+import storage.helper
 
+from common.exceptions import PlenumValueError
+from ledger.hash_stores.hash_store import HashStore
+from plenum.common.config_util import getConfig
+from stp_core.common.log import getlogger
+from plenum.common.constants import KeyValueStorageType, HS_LEVELDB, HS_ROCKSDB
+
+logger = getlogger()
+
+
+class DbHashStore(HashStore):
+    def __init__(self, dataDir, fileNamePrefix="", db_type=HS_LEVELDB, read_only=False, config=None):
+        self.dataDir = dataDir
+        if db_type not in (HS_ROCKSDB, HS_LEVELDB):
+            raise PlenumValueError(
+                'db_type', db_type, "one of {}".format((HS_ROCKSDB, HS_LEVELDB))
+            )
+        self.db_type = KeyValueStorageType.Leveldb if db_type == HS_LEVELDB \
+            else KeyValueStorageType.Rocksdb
+        self.config = config or getConfig()
+        self.nodesDb = None
+        self.leavesDb = None
+        self._leafCount = 0
+        self._read_only = read_only
+        self.nodes_db_name = fileNamePrefix + '_merkleNodes'
+        self.leaves_db_name = fileNamePrefix + '_merkleLeaves'
+        self.open()
 
-class MetricsStats:
-    def __init__(self, timestep=timedelta(minutes=1)):
-        self._timestep = timestep
-        self._frames = defaultdict(MetricsStatsFrame)
-        self._total = None
-
-    def add(self, ts: datetime, name: MetricsName, value: Union[float, ValueAccumulator]):
-        ts = trunc_ts(ts, self._timestep)
-        self._frames[ts].add(name, value)
-        self._total = None
+    @property
+    def is_persistent(self) -> bool:
+        return True
 
-    def frame(self, ts):
-        return self._frames[trunc_ts(ts, self._timestep)]
+    @property
+    def read_only(self) -> bool:
+        return self._read_only
 
-    def frames(self):
-        return self._frames.items()
+    def writeLeaf(self, leafHash):
+        self.leavesDb.put(str(self.leafCount + 1), leafHash)
+        self.leafCount += 1
+
+    def writeNode(self, node):
+        start, height, nodeHash = node
+        seqNo = self.getNodePosition(start, height)
+        self.nodesDb.put(str(seqNo), nodeHash)
+
+    def readLeaf(self, seqNo):
+        return self._readOne(seqNo, self.leavesDb)
+
+    def readNode(self, seqNo):
+        return self._readOne(seqNo, self.nodesDb)
+
+    def _readOne(self, pos, db):
+        self._validatePos(pos)
+        try:
+            # Converting any bytearray to bytes
+            return bytes(db.get(str(pos)))
+        except KeyError:
+            logger.error("{} does not have position {}".format(db, pos))
+
+    def readLeafs(self, start, end):
+        return self._readMultiple(start, end, self.leavesDb)
+
+    def readNodes(self, start, end):
+        return self._readMultiple(start, end, self.nodesDb)
+
+    def _readMultiple(self, start, end, db):
+        """
+        Returns a list of hashes with serial numbers between start
+         and end, both inclusive.
+         """
+        self._validatePos(start, end)
+        # Converting any bytearray to bytes
+        return [bytes(db.get(str(pos))) for pos in range(start, end + 1)]
 
     @property
-    def timestep(self):
-        return self._timestep
+    def leafCount(self) -> int:
+        return self._leafCount
 
     @property
-    def min_ts(self):
-        return min(k for k in self._frames.keys())
+    def nodeCount(self) -> int:
+        return self.nodesDb.size
 
-    @property
-    def max_ts(self):
-        return max(k for k in self._frames.keys()) + self._timestep
+    @leafCount.setter
+    def leafCount(self, count: int) -> None:
+        self._leafCount = count
 
     @property
-    def total(self):
-        if self._total is None:
-            self._total = self.merge_all(list(self._frames.values()))
-        return self._total
-
-    def __eq__(self, other):
-        if not isinstance(other, MetricsStats):
-            return False
-        for k in set(self._frames.keys()).union(other._frames.keys()):
-            if self._frames[k] != other._frames[k]:
-                return False
+    def closed(self):
+        return (self.nodesDb is None and self.leavesDb is None) \
+            or \
+               (self.nodesDb.closed and self.leavesDb.closed)
+
+    def open(self):
+        self.nodesDb = storage.helper.initKeyValueStorage(
+            self.db_type, self.dataDir, self.nodes_db_name,
+            read_only=self._read_only, db_config=self.config.db_merkle_nodes_config)
+        self.leavesDb = storage.helper.initKeyValueStorage(
+            self.db_type, self.dataDir, self.leaves_db_name,
+            read_only=self._read_only, db_config=self.config.db_merkle_leaves_config)
+        self._leafCount = self.leavesDb.size
+
+    def close(self):
+        self.nodesDb.close()
+        self.leavesDb.close()
+
+    def reset(self) -> bool:
+        self.nodesDb.reset()
+        self.leavesDb.reset()
+        self.leafCount = 0
         return True
-
-    @staticmethod
-    def merge_all(frames: Sequence[MetricsStatsFrame]) -> MetricsStatsFrame:
-        count = len(frames)
-        if count == 0:
-            return MetricsStatsFrame()
-        if count == 1:
-            return deepcopy(frames[0])
-
-        count_2 = count // 2
-        lo = MetricsStats.merge_all(frames[:count_2])
-        hi = MetricsStats.merge_all(frames[count_2:])
-        lo.merge(hi)
-        return lo
-
-
-def load_metrics_from_kv_store(storage: KeyValueStorage,
-                               min_ts: datetime = None,
-                               max_ts: datetime = None,
-                               step: timedelta = timedelta(minutes=1)) -> MetricsStats:
-    result = MetricsStats(step)
-
-    start = KvStoreMetricsFormat.encode_key(min_ts, 0) if min_ts else None
-    for k, v in storage.iterator(start=start):
-        ev = KvStoreMetricsFormat.decode(k, v)
-        if ev is None:
-            continue
-        if max_ts is not None and ev.timestamp > max_ts:
-            break
-        result.add(ev.timestamp, ev.name, ev.value)
-
-    return result
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/verifier.py` & `indy-plenum-1.9.2rc1/plenum/common/verifier.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/util.py` & `indy-plenum-1.9.2rc1/plenum/common/util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/bitmask_helper.py` & `indy-plenum-1.9.2rc1/plenum/common/bitmask_helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/timer.py` & `indy-plenum-1.9.2rc1/plenum/common/timer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/tools.py` & `indy-plenum-1.9.2rc1/plenum/common/tools.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/pkg_util.py` & `indy-plenum-1.9.2rc1/plenum/common/pkg_util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/moving_average.py` & `indy-plenum-1.9.2rc1/plenum/common/moving_average.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/value_accumulator.py` & `indy-plenum-1.9.2rc1/plenum/common/value_accumulator.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/messages/node_message_factory.py` & `indy-plenum-1.9.2rc1/plenum/common/messages/node_message_factory.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/messages/fields.py` & `indy-plenum-1.9.2rc1/plenum/common/messages/fields.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,22 +12,21 @@
 from common.version import (
     InvalidVersionError, VersionBase, GenericVersion
 )
 from crypto.bls.bls_multi_signature import MultiSignatureValue
 from plenum import PLUGIN_LEDGER_IDS
 from plenum.common.constants import VALID_LEDGER_IDS, CURRENT_PROTOCOL_VERSION
 from plenum.common.plenum_protocol_version import PlenumProtocolVersion
-from plenum.config import BLS_MULTI_SIG_LIMIT, DATETIME_LIMIT, VERSION_FIELD_LIMIT, DIGEST_FIELD_LIMIT
+from plenum.config import BLS_MULTI_SIG_LIMIT, DATETIME_LIMIT, VERSION_FIELD_LIMIT
 
 
 class FieldValidator(metaclass=ABCMeta):
     """"
     Interface for field validators
     """
-
     # TODO INDY-2072 test optional
     def __init__(self, optional: bool = False):
         self.optional = optional
 
     @abstractmethod
     def validate(self, val):
         """
@@ -122,15 +121,15 @@
 
     def _specific_validation(self, val):
         if not val:
             return 'empty string'
 
 
 class LimitedLengthStringField(FieldBase):
-    _base_types = (str,)
+    _base_types = (str, )
 
     def __init__(self, max_length: int, can_be_empty=False, **kwargs):
         if not max_length > 0:
             raise PlenumValueError('max_length', max_length, '> 0')
         super().__init__(**kwargs)
         self._max_length = max_length
         self._can_be_empty = can_be_empty
@@ -143,15 +142,15 @@
             return '{} is longer than {} symbols'.format(val, self._max_length)
 
 
 class DatetimeStringField(FieldBase):
     _base_types = (str,)
     _exceptional_values = []
 
-    def __init__(self, exceptional_values: Iterable[str] = [], **kwargs):
+    def __init__(self, exceptional_values: Iterable[str]=[], **kwargs):
         super().__init__(**kwargs)
         if exceptional_values is not None:
             self._exceptional_values = exceptional_values
 
     def _specific_validation(self, val):
         if len(val) > DATETIME_LIMIT:
             val = val[:100] + ('...' if len(val) > 100 else '')
@@ -160,15 +159,15 @@
             try:
                 dateutil.parser.parse(val)
             except Exception:
                 return "datetime {} is not valid".format(val)
 
 
 class FixedLengthField(FieldBase):
-    _base_types = (str,)
+    _base_types = (str, )
 
     def __init__(self, length: int, **kwargs):
         if not isinstance(length, int):
             error('length should be integer', TypeError)
         if length < 1:
             error('should be greater than 0', ValueError)
         self.length = length
@@ -197,15 +196,16 @@
     # TODO implement
 
     def _specific_validation(self, val):
         return
 
 
 class NonNegativeNumberField(FieldBase):
-    _base_types = (int,)
+
+    _base_types = (int, )
 
     def _specific_validation(self, val):
         if val < 0:
             return 'negative value'
 
 
 class ConstantField(FieldBase):
@@ -471,15 +471,15 @@
     _b58full = FullVerkeyField()
 
     def _specific_validation(self, val):
         err_ab = self._b58abbreviated.validate(val)
         err_fl = self._b58full.validate(val)
         if err_ab and err_fl:
             return 'Neither a full verkey nor an abbreviated one. One of ' \
-                   'these errors should be resolved:\n {}\n{}'. \
+                   'these errors should be resolved:\n {}\n{}'.\
                 format(err_ab, err_fl)
 
 
 class HexField(FieldBase):
     _base_types = (str,)
 
     def __init__(self, length=None, **kwargs):
@@ -697,38 +697,7 @@
                    'and `set_protocol_version({})` is called' \
                 .format(CURRENT_PROTOCOL_VERSION)
         if val != CURRENT_PROTOCOL_VERSION:
             return 'Message version ({}) differs from current protocol version. ' \
                    'Make sure that the latest LibIndy is used ' \
                    'and `set_protocol_version({})` is called' \
                 .format(val, CURRENT_PROTOCOL_VERSION)
-
-
-class BatchIDField(FieldBase):
-    _base_types = (list, tuple)
-
-    def _specific_validation(self, val):
-        if len(val) != 3:
-            return 'should have size of 3'
-
-        view_no, pp_seq_no, pp_digest = val
-        for validator, value in ((NonNegativeNumberField().validate, view_no),
-                                 (NonNegativeNumberField().validate, pp_seq_no),
-                                 (NonEmptyStringField().validate, pp_digest)):
-            err = validator(value)
-            if err:
-                return err
-
-
-class ViewChangeField(FieldBase):
-    _base_types = (list, tuple)
-
-    def _specific_validation(self, val):
-        if len(val) != 2:
-            return 'should have size of 2'
-
-        frm, digest = val
-        for validator, value in ((NonEmptyStringField().validate, frm),
-                                 (LimitedLengthStringField(max_length=DIGEST_FIELD_LIMIT).validate, digest)):
-            err = validator(value)
-            if err:
-                return err
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/messages/client_request.py` & `indy-plenum-1.9.2rc1/plenum/common/messages/client_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/messages/node_messages.py` & `indy-plenum-1.9.2rc1/plenum/common/messages/node_messages.py`

 * *Files 6% similar despite different names*

```diff
@@ -10,34 +10,66 @@
     BACKUP_INSTANCE_FAULTY, VIEW_CHANGE_START, PROPOSED_VIEW_NO, VIEW_CHANGE_CONTINUE, VIEW_CHANGE, VIEW_CHANGE_ACK, \
     NEW_VIEW
 from plenum.common.messages.client_request import ClientMessageValidator
 from plenum.common.messages.fields import NonNegativeNumberField, IterableField, \
     SerializedValueField, SignatureField, TieAmongField, AnyValueField, TimestampField, \
     LedgerIdField, MerkleRootField, Base58Field, LedgerInfoField, AnyField, ChooseField, AnyMapField, \
     LimitedLengthStringField, BlsMultiSignatureField, ProtocolVersionField, BooleanField, \
-    IntegerField, BatchIDField, ViewChangeField
+    IntegerField
 from plenum.common.messages.message_base import \
     MessageBase
 from plenum.common.types import f
 from plenum.config import NAME_FIELD_LIMIT, DIGEST_FIELD_LIMIT, SENDER_CLIENT_FIELD_LIMIT, HASH_FIELD_LIMIT, \
     SIGNATURE_FIELD_LIMIT, TIE_IDR_FIELD_LIMIT, BLS_SIG_LIMIT
 
 
 # TODO set of classes are not hashable but MessageBase expects that
 
+class Nomination(MessageBase):
+    typename = NOMINATE
+
+    schema = (
+        (f.NAME.nm, LimitedLengthStringField(max_length=NAME_FIELD_LIMIT)),
+        (f.INST_ID.nm, NonNegativeNumberField()),
+        (f.VIEW_NO.nm, NonNegativeNumberField()),
+        (f.ORD_SEQ_NO.nm, NonNegativeNumberField()),
+    )
+
 
 class Batch(MessageBase):
     typename = BATCH
 
     schema = (
         (f.MSGS.nm, IterableField(SerializedValueField())),
         (f.SIG.nm, SignatureField(max_length=SIGNATURE_FIELD_LIMIT)),
     )
 
 
+class Reelection(MessageBase):
+    typename = REELECTION
+
+    schema = (
+        (f.INST_ID.nm, NonNegativeNumberField()),
+        (f.ROUND.nm, NonNegativeNumberField()),
+        (f.TIE_AMONG.nm, IterableField(TieAmongField(max_length=TIE_IDR_FIELD_LIMIT))),
+        (f.VIEW_NO.nm, NonNegativeNumberField()),
+    )
+
+
+class Primary(MessageBase):
+    typename = PRIMARY
+
+    schema = (
+        (f.NAME.nm, LimitedLengthStringField(max_length=NAME_FIELD_LIMIT)),
+        (f.INST_ID.nm, NonNegativeNumberField()),
+        (f.VIEW_NO.nm, NonNegativeNumberField()),
+        (f.ORD_SEQ_NO.nm, NonNegativeNumberField()),
+    )
+
+
 # TODO implement actual rules
 class BlacklistMsg(MessageBase):
     typename = BLACKLIST
     schema = (
         (f.SUSP_CODE.nm, AnyValueField()),
         (f.NODE_NAME.nm, AnyValueField()),
     )
@@ -177,18 +209,31 @@
     )
 
 
 class Checkpoint(MessageBase):
     typename = CHECKPOINT
     schema = (
         (f.INST_ID.nm, NonNegativeNumberField()),
-        (f.VIEW_NO.nm, NonNegativeNumberField()),          # This will no longer be used soon
-        (f.SEQ_NO_START.nm, NonNegativeNumberField()),     # This is no longer used and must always be 0
+        (f.VIEW_NO.nm, NonNegativeNumberField()),
+        (f.SEQ_NO_START.nm, NonNegativeNumberField()),
         (f.SEQ_NO_END.nm, NonNegativeNumberField()),
-        (f.DIGEST.nm, MerkleRootField(nullable=True)),     # This is actually audit ledger merkle root
+        # TODO: Should this be root of audit ledger instead of pre-prepare digest?
+        (f.DIGEST.nm, LimitedLengthStringField(max_length=DIGEST_FIELD_LIMIT)),
+    )
+
+
+# TODO implement actual rules
+class CheckpointState(MessageBase):
+    typename = CHECKPOINT_STATE
+    schema = (
+        (f.SEQ_NO.nm, AnyValueField()),
+        (f.DIGESTS.nm, AnyValueField()),
+        (f.DIGEST.nm, AnyValueField()),
+        (f.RECEIVED_DIGESTS.nm, AnyValueField()),
+        (f.IS_STABLE.nm, AnyValueField())
     )
 
 
 # TODO implement actual rules
 class Reply(MessageBase):
     typename = REPLY
     schema = (
@@ -214,17 +259,17 @@
 
 
 class ViewChange(MessageBase):
     typename = VIEW_CHANGE
     schema = (
         (f.VIEW_NO.nm, NonNegativeNumberField()),
         (f.STABLE_CHECKPOINT.nm, NonNegativeNumberField()),
-        (f.PREPARED.nm, IterableField(BatchIDField())),  # list of tuples (view_no, pp_seq_no, pp_digest)
-        (f.PREPREPARED.nm, IterableField(BatchIDField())),  # list of tuples (view_no, pp_seq_no, pp_digest)
-        (f.CHECKPOINTS.nm, IterableField(AnyField()))  # list of Checkpoints TODO: should we change to tuples?
+        (f.PREPARED.nm, IterableField(AnyField())),           # list of tuples (view_no, pp_seq_no, pp_digest)
+        (f.PREPREPARED.nm, IterableField(AnyField())),        # list of tuples (view_no, pp_seq_no, pp_digest)
+        (f.CHECKPOINTS.nm, IterableField(AnyField()))         # list of Checkpoints TODO: should we change to tuples?
     )
 
 
 class ViewChangeAck(MessageBase):
     typename = VIEW_CHANGE_ACK
     schema = (
         (f.VIEW_NO.nm, NonNegativeNumberField()),
@@ -233,18 +278,18 @@
     )
 
 
 class NewView(MessageBase):
     typename = NEW_VIEW
     schema = (
         (f.VIEW_NO.nm, NonNegativeNumberField()),
-        (f.VIEW_CHANGES.nm, IterableField(ViewChangeField())),  # list of tuples (node_name, view_change_digest)
-        (f.CHECKPOINT.nm, AnyField()),  # Checkpoint to be selected as stable (TODO: or tuple?)
-        (f.BATCHES.nm, IterableField(BatchIDField()))  # list of tuples (view_no, pp_seq_no, pp_digest)
-        # that should get into new view
+        (f.VIEW_CHANGES.nm, IterableField(AnyField())),       # list of tuples (node_name, view_change_digest)
+        (f.CHECKPOINT.nm, AnyField()),                        # Checkpoint to be selected as stable (TODO: or tuple?)
+        (f.BATCHES.nm, IterableField(AnyField()))             # list of tuples (view_no, pp_seq_no, pp_digest)
+                                                              # that should get into new view
     )
 
 
 class LedgerStatus(MessageBase):
     """
     Purpose: spread status of ledger copy on a specific node.
     When node receives this message and see that it has different
@@ -364,14 +409,17 @@
         (f.MSG.nm, AnyField())
     )
 
 
 ThreePhaseType = (PrePrepare, Prepare, Commit)
 ThreePhaseMsg = TypeVar("3PhaseMsg", *ThreePhaseType)
 
+ElectionType = (Nomination, Primary, Reelection)
+ElectionMsg = TypeVar("ElectionMsg", *ElectionType)
+
 ThreePhaseKey = NamedTuple("ThreePhaseKey", [
     f.VIEW_NO,
     f.PP_SEQ_NO
 ])
 
 
 class BatchCommitted(MessageBase):
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/messages/message_base.py` & `indy-plenum-1.9.2rc1/plenum/common/messages/message_base.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/stashing_router.py` & `indy-plenum-1.9.2rc1/plenum/common/stashing_router.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 from abc import ABC, abstractmethod
 from functools import partial
-from typing import Callable, Any, Dict, Type, Optional, Iterable, Tuple, List
+from typing import Callable, Any, Dict, Type, Optional, Iterable, Tuple
 
 from sortedcontainers import SortedListWithKey
 
 from common.exceptions import LogicError
-from plenum.common.router import Router, Subscription
 from stp_core.common.log import getlogger
 
 DISCARD = -1
 PROCESS = 0
 STASH = 1
 
 
@@ -58,17 +57,14 @@
         return self._data.pop(0)
 
     def pop_all(self) -> Iterable[Tuple]:
         data = self._data
         self._data = []
         return data
 
-    def __iter__(self):
-        return self._data.__iter__()
-
 
 class SortedStash(StashingQueue):
     def __init__(self, limit: int, key: Callable):
         self._limit = limit
         self._key = lambda v: key(v[0])
         self._data = SortedListWithKey(key=self._key)
 
@@ -86,42 +82,36 @@
 
     def pop_all(self) -> Iterable[Tuple]:
         data = self._data
         self._data = SortedListWithKey(key=self._key)
         return data
 
 
-class StashingRouter(Router):
-    Handler = Callable[..., Optional[Tuple[int, str]]]
+class StashingRouter:
+    Handler = Callable[..., Optional[int]]
 
-    def __init__(self, limit: int, buses: List[Router], unstash_handler: Callable = None):
-        super().__init__()
+    def __init__(self, limit: int):
         self._limit = limit
         self._logger = getlogger()
+        self._handlers = {}  # type: Dict[Type, StashingRouter.Handler]
         self._queues = {}  # type: Dict[int, StashingQueue]
-        # TODO: This call has been added to saving the old message order in the list.
-        # This is a replica's method that moves the message to the inBox, rather than
-        # calling the handler immediately, as the default router does.
-        self._unstash_handler = unstash_handler
-        self._subscriptions = Subscription()
-        self._buses = buses
 
     def set_sorted_stasher(self, code: int, key: Callable):
         self._queues[code] = SortedStash(self._limit, key)
 
-    def subscribe(self, message_type: Type, handler: Handler) -> Router.SubscriptionID:
-        # TODO: subscribe to one bus only
-        for bus in self._buses:
-            self._subscriptions.subscribe(bus,
-                                          message_type,
-                                          partial(self._process, handler))
-        return super().subscribe(message_type, handler)
-
-    def unsubscribe_from_all(self):
-        self._subscriptions.unsubscribe_all()
+    def subscribe(self, message_type: Type, handler: Handler):
+        if message_type in self._handlers:
+            raise LogicError("Trying to assign handler {} for message type {}, "
+                             "but another handler is already assigned {}".
+                             format(handler, message_type, self._handlers[message_type]))
+        self._handlers[message_type] = handler
+
+    def subscribe_to(self, bus: Any):
+        for message_type, handler in self._handlers.items():
+            bus.subscribe(message_type, partial(self._process, handler))
 
     def process_all_stashed(self, code: Optional[int] = None):
         """
         Try to process all stashed messages, re-stashing some of them if needed
 
         :param code: stash code, None if we need to unstash all
         """
@@ -164,47 +154,33 @@
         return len(queue) if queue else 0
 
     def _process(self, handler: Handler, message: Any, *args) -> bool:
         """
         Tries to process message using given handler. Returns True if message
         was stashed for reprocessing in future, False otherwise.
         """
-        result = handler(message, *args)
-        code, reason = result if result else (None, None)
+        code = handler(message, *args)
 
         # If handler returned either None or PROCESS we assume it successfully processed message
         # and no further action is needed
         if not code:
             return True
 
         if code == DISCARD:
-            self.discard(message, args, reason)
+            self._logger.trace("Discarded message {} with metadata {}".format(message, args))
             return True
 
-        self._stash(code, reason, message, *args)
+        self._stash(code, message, *args)
         return False
 
     def _resolve_and_process(self, message: Any, *args) -> bool:
-        handlers = self.handlers(type(message))
-        if len(handlers) == 0:
-            raise LogicError("Handler for message {} not found".format(message))
-        return self._unstash(handlers[0], message, *args)
-
-    def _unstash(self, handler: Handler, message: Any, *args) -> bool:
-        if self._unstash_handler is None:
-            return self._process(handler, message, *args)
-        else:
-            self._unstash_handler((message, *args))
-
-    def _stash(self, code: int, reason: str, message: Any, *args):
-        self._logger.trace("Stashing message {} with metadata {} "
-                           "with the reason {}".format(message, args, reason))
+        handler = self._handlers[type(message)]
+        return self._process(handler, message, *args)
+
+    def _stash(self, code: int, message: Any, *args):
+        self._logger.trace("Stashing message {} with metadata {}".format(message, args))
 
         queue = self._queues.setdefault(code, UnsortedStash(self._limit))
         if not queue.push(message, *args):
             # TODO: This is actually better be logged on info level with some throttling applied,
             #  however this cries for some generic easy to use solution, which we don't have yet.
             self._logger.debug("Cannot stash message {} with metadata {} - queue is full".format(message, args))
-
-    def discard(self, msg, args, reason):
-        self._logger.trace("Discarded message {} with metadata {} "
-                           "with the reason {}".format(msg, args, reason))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/transactions.py` & `indy-plenum-1.9.2rc1/plenum/common/transactions.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/constants.py` & `indy-plenum-1.9.2rc1/plenum/common/constants.py`

 * *Files 18% similar despite different names*

```diff
@@ -258,7 +258,39 @@
 
 VALID_LEDGER_IDS = (POOL_LEDGER_ID, DOMAIN_LEDGER_ID, CONFIG_LEDGER_ID, AUDIT_LEDGER_ID)
 
 CURRENT_PROTOCOL_VERSION = PlenumProtocolVersion.TXN_FORMAT_1_0_SUPPORT.value
 
 OPERATION_SCHEMA_IS_STRICT = False
 SCHEMA_IS_STRICT = False
+
+
+class NodeHooks(UniqueSet):
+    PRE_STATIC_VALIDATION = 1
+    POST_STATIC_VALIDATION = 2
+    PRE_SIG_VERIFICATION = 3
+    POST_SIG_VERIFICATION = 4
+    PRE_DYNAMIC_VALIDATION = 5
+    POST_DYNAMIC_VALIDATION = 6
+    PRE_REQUEST_APPLICATION = 7
+    POST_REQUEST_APPLICATION = 8
+    PRE_REQUEST_COMMIT = 9
+    POST_REQUEST_COMMIT = 10
+    PRE_SEND_REPLY = 11
+    POST_SEND_REPLY = 12
+    POST_BATCH_CREATED = 13
+    POST_BATCH_REJECTED = 14
+    PRE_BATCH_COMMITTED = 15
+    POST_BATCH_COMMITTED = 16
+    POST_NODE_STOPPED = 17
+
+
+class ReplicaHooks(UniqueSet):
+    CREATE_PPR = 1
+    CREATE_PR = 2
+    CREATE_CM = 3
+    CREATE_ORD = 4
+    APPLY_PPR = 5
+    VALIDATE_PR = 6
+    VALIDATE_CM = 7
+    BATCH_CREATED = 8
+    BATCH_REJECTED = 9
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/perf_util.py` & `indy-plenum-1.9.2rc1/plenum/common/perf_util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/metrics_collector.py` & `indy-plenum-1.9.2rc1/plenum/common/metrics_collector.py`

 * *Files 1% similar despite different names*

```diff
@@ -231,15 +231,15 @@
     REPLICA_SENT_PREPREPARES_MASTER = TMP_METRIC + 1009
     REPLICA_PREPREPARES_MASTER = TMP_METRIC + 1010
     REPLICA_PREPARES_MASTER = TMP_METRIC + 1011
     REPLICA_COMMITS_MASTER = TMP_METRIC + 1012
     REPLICA_PRIMARYNAMES_MASTER = TMP_METRIC + 1014
     REPLICA_STASHED_OUT_OF_ORDER_COMMITS_MASTER = TMP_METRIC + 1015
     REPLICA_CHECKPOINTS_MASTER = TMP_METRIC + 1016
-    REPLICA_RECVD_CHECKPOINTS_MASTER = TMP_METRIC + 1017
+    REPLICA_STASHED_RECVD_CHECKPOINTS_MASTER = TMP_METRIC + 1017
     REPLICA_STASHING_WHILE_OUTSIDE_WATERMARKS_MASTER = TMP_METRIC + 1018
     REPLICA_REQUEST_QUEUES_MASTER = TMP_METRIC + 1019
     REPLICA_BATCHES_MASTER = TMP_METRIC + 1020
     REPLICA_REQUESTED_PRE_PREPARES_MASTER = TMP_METRIC + 1021
     REPLICA_REQUESTED_PREPARES_MASTER = TMP_METRIC + 1022
     REPLICA_REQUESTED_COMMITS_MASTER = TMP_METRIC + 1023
     REPLICA_PRE_PREPARES_STASHED_FOR_INCORRECT_TIME_MASTER = TMP_METRIC + 1024
@@ -261,15 +261,15 @@
     REPLICA_SENT_PREPREPARES_BACKUP = TMP_METRIC + 2009
     REPLICA_PREPREPARES_BACKUP = TMP_METRIC + 2010
     REPLICA_PREPARES_BACKUP = TMP_METRIC + 2011
     REPLICA_COMMITS_BACKUP = TMP_METRIC + 2012
     REPLICA_PRIMARYNAMES_BACKUP = TMP_METRIC + 2014
     REPLICA_STASHED_OUT_OF_ORDER_COMMITS_BACKUP = TMP_METRIC + 2015
     REPLICA_CHECKPOINTS_BACKUP = TMP_METRIC + 2016
-    REPLICA_RECVD_CHECKPOINTS_BACKUP = TMP_METRIC + 2017
+    REPLICA_STASHED_RECVD_CHECKPOINTS_BACKUP = TMP_METRIC + 2017
     REPLICA_STASHING_WHILE_OUTSIDE_WATERMARKS_BACKUP = TMP_METRIC + 2018
     REPLICA_REQUEST_QUEUES_BACKUP = TMP_METRIC + 2019
     REPLICA_BATCHES_BACKUP = TMP_METRIC + 2020
     REPLICA_REQUESTED_PRE_PREPARES_BACKUP = TMP_METRIC + 2021
     REPLICA_REQUESTED_PREPARES_BACKUP = TMP_METRIC + 2022
     REPLICA_REQUESTED_COMMITS_BACKUP = TMP_METRIC + 2023
     REPLICA_PRE_PREPARES_STASHED_FOR_INCORRECT_TIME_BACKUP = TMP_METRIC + 2024
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/startable.py` & `indy-plenum-1.9.2rc1/plenum/common/startable.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/plugin_helper.py` & `indy-plenum-1.9.2rc1/plenum/common/plugin_helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/monitor_strategies.py` & `indy-plenum-1.9.2rc1/plenum/common/monitor_strategies.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/ledger_manager.py` & `indy-plenum-1.9.2rc1/plenum/common/ledger_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/throughput_measurements.py` & `indy-plenum-1.9.2rc1/plenum/common/throughput_measurements.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/test_network_setup.py` & `indy-plenum-1.9.2rc1/plenum/common/test_network_setup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/ledger_uncommitted_tracker.py` & `indy-plenum-1.9.2rc1/plenum/common/ledger_uncommitted_tracker.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/request.py` & `indy-plenum-1.9.2rc1/plenum/common/request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/message_processor.py` & `indy-plenum-1.9.2rc1/plenum/common/message_processor.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/average_strategies.py` & `indy-plenum-1.9.2rc1/plenum/common/average_strategies.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/motor.py` & `indy-plenum-1.9.2rc1/plenum/common/motor.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/has_file_storage.py` & `indy-plenum-1.9.2rc1/plenum/common/has_file_storage.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/channel.py` & `indy-plenum-1.9.2rc1/plenum/common/channel.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/ledger.py` & `indy-plenum-1.9.2rc1/plenum/common/ledger.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/stacks.py` & `indy-plenum-1.9.2rc1/plenum/common/stacks.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/stack_manager.py` & `indy-plenum-1.9.2rc1/plenum/common/stack_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/member/steward.py` & `indy-plenum-1.9.2rc1/plenum/common/member/steward.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/member/member.py` & `indy-plenum-1.9.2rc1/plenum/common/member/member.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/init_util.py` & `indy-plenum-1.9.2rc1/plenum/common/init_util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/common/config_util.py` & `indy-plenum-1.9.2rc1/plenum/common/config_util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/config.py` & `indy-plenum-1.9.2rc1/plenum/config.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/__init__.py` & `indy-plenum-1.9.2rc1/plenum/__init__.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/__metadata__.py` & `indy-plenum-1.9.2rc1/plenum/__metadata__.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_safe_request.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_safe_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_message_serialization.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_message_serialization.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_strict_schema.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_strict_schema.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_message_factory.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_message_factory.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_node_op.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_node_op.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_taa.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_taa.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_sha256_hex_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_sha256_hex_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_merkle_tree_root_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_merkle_tree_root_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_fixed_length_string_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_fixed_length_string_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_bls_multisig_value_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_bls_multisig_value_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_request_identifier_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_request_identifier_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_base58_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_base58_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_iterable_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_iterable_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_protocol_version_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_protocol_version_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_verkey_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_verkey_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_limited_length_string_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_limited_length_string_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_bls_multisig_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_bls_multisig_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_version_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_version_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/fields_validation/test_identifier_field.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/fields_validation/test_identifier_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_handle_one_node_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_handle_one_node_message.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_message_base.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_message_base.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_request.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/constants.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/constants.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_taa_aml_op.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_taa_aml_op.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_catchuprep_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_catchuprep_message.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_observed_data.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_observed_data.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_client_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_client_message.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_instanceChange_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_instanceChange_message.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_checkpoint_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_checkpoint_message.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,19 @@
+import pytest
 from plenum.common.messages.node_messages import Checkpoint
 from collections import OrderedDict
 from plenum.common.messages.fields import \
-    NonNegativeNumberField, MerkleRootField
+    NonNegativeNumberField, LimitedLengthStringField
 
 EXPECTED_ORDERED_FIELDS = OrderedDict([
     ("instId", NonNegativeNumberField),
     ("viewNo", NonNegativeNumberField),
     ("seqNoStart", NonNegativeNumberField),
     ("seqNoEnd", NonNegativeNumberField),
-    ("digest", MerkleRootField),
+    ("digest", LimitedLengthStringField),
 ])
 
 
 def test_hash_expected_type():
     assert Checkpoint.typename == "CHECKPOINT"
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_prepare_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_prepare_message.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_ordered_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_ordered_message.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_commit_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_commit_message.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,12 @@
+import pytest
 from plenum.common.messages.node_messages import Commit
 from collections import OrderedDict
 from plenum.common.messages.fields import NonNegativeNumberField, \
-    LimitedLengthStringField, AnyMapField
+    LimitedLengthStringField, MerkleRootField, AnyMapField
 
 EXPECTED_ORDERED_FIELDS = OrderedDict([
     ("instId", NonNegativeNumberField),
     ("viewNo", NonNegativeNumberField),
     ("ppSeqNo", NonNegativeNumberField),
     ("blsSig", LimitedLengthStringField),
     ('plugin_fields', AnyMapField)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_batch_committed.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_batch_committed.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_catchupreq_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_catchupreq_message.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_viewchangedone_messsage.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_viewchangedone_messsage.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_new_view_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_primary_message.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,26 @@
-from plenum.common.messages.node_messages import NewView
 from collections import OrderedDict
-from plenum.common.messages.fields import NonNegativeNumberField, IterableField, AnyField
+from plenum.common.messages.fields import NonNegativeNumberField, \
+    LimitedLengthStringField
+from plenum.common.messages.node_messages import Primary
 
 EXPECTED_ORDERED_FIELDS = OrderedDict([
+    ("name", LimitedLengthStringField),
+    ("instId", NonNegativeNumberField),
     ("viewNo", NonNegativeNumberField),
-    ("viewChanges", IterableField),
-    ("checkpoint", AnyField),
-    ("batches", IterableField)
+    ("ordSeqNo", NonNegativeNumberField),
 ])
 
 
 def test_hash_expected_type():
-    assert NewView.typename == "NEW_VIEW"
+    assert Primary.typename == "PRIMARY"
 
 
 def test_has_expected_fields():
-    actual_field_names = OrderedDict(NewView.schema).keys()
+    actual_field_names = OrderedDict(Primary.schema).keys()
     assert list(actual_field_names) == list(EXPECTED_ORDERED_FIELDS.keys())
 
 
 def test_has_expected_validators():
-    schema = dict(NewView.schema)
+    schema = dict(Primary.schema)
     for field, validator in EXPECTED_ORDERED_FIELDS.items():
         assert isinstance(schema[field], validator)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_view_change_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_currentstate_message.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,26 +1,25 @@
-from plenum.common.messages.node_messages import ViewChange
+import pytest
+
 from collections import OrderedDict
 from plenum.common.messages.fields import NonNegativeNumberField, IterableField
+from plenum.common.messages.node_messages import CurrentState
 
 EXPECTED_ORDERED_FIELDS = OrderedDict([
     ("viewNo", NonNegativeNumberField),
-    ("stableCheckpoint", NonNegativeNumberField),
-    ("prepared", IterableField),
-    ("preprepared", IterableField),
-    ("checkpoints", IterableField)
+    ("primary", IterableField),
 ])
 
 
 def test_hash_expected_type():
-    assert ViewChange.typename == "VIEW_CHANGE"
+    assert CurrentState.typename == "CURRENT_STATE"
 
 
 def test_has_expected_fields():
-    actual_field_names = OrderedDict(ViewChange.schema).keys()
+    actual_field_names = OrderedDict(CurrentState.schema).keys()
     assert list(actual_field_names) == list(EXPECTED_ORDERED_FIELDS.keys())
 
 
 def test_has_expected_validators():
-    schema = dict(ViewChange.schema)
+    schema = dict(CurrentState.schema)
     for field, validator in EXPECTED_ORDERED_FIELDS.items():
         assert isinstance(schema[field], validator)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_preprepare_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_preprepare_message.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_consistencyproof_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_consistencyproof_message.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_currentstate_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_propagate_message.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,25 +1,24 @@
-import pytest
-
 from collections import OrderedDict
-from plenum.common.messages.fields import NonNegativeNumberField, IterableField
-from plenum.common.messages.node_messages import CurrentState
+from plenum.common.messages.fields import LimitedLengthStringField
+from plenum.common.messages.client_request import ClientMessageValidator
+from plenum.common.messages.node_messages import Propagate
 
 EXPECTED_ORDERED_FIELDS = OrderedDict([
-    ("viewNo", NonNegativeNumberField),
-    ("primary", IterableField),
+    ("request", ClientMessageValidator),
+    ("senderClient", LimitedLengthStringField),
 ])
 
 
 def test_hash_expected_type():
-    assert CurrentState.typename == "CURRENT_STATE"
+    assert Propagate.typename == "PROPAGATE"
 
 
 def test_has_expected_fields():
-    actual_field_names = OrderedDict(CurrentState.schema).keys()
+    actual_field_names = OrderedDict(Propagate.schema).keys()
     assert list(actual_field_names) == list(EXPECTED_ORDERED_FIELDS.keys())
 
 
 def test_has_expected_validators():
-    schema = dict(CurrentState.schema)
+    schema = dict(Propagate.schema)
     for field, validator in EXPECTED_ORDERED_FIELDS.items():
         assert isinstance(schema[field], validator)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_batch_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_batch_message.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_ledgerstatus_message.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_ledgerstatus_message.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/message_validation/test_view_change_message_ack.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/message_validation/test_nomination_message.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,24 +1,28 @@
-from plenum.common.messages.node_messages import ViewChangeAck
+import pytest
+
 from collections import OrderedDict
-from plenum.common.messages.fields import NonNegativeNumberField, LimitedLengthStringField
+from plenum.common.messages.fields import NonNegativeNumberField, \
+    LimitedLengthStringField
+from plenum.common.messages.node_messages import Nomination
 
 EXPECTED_ORDERED_FIELDS = OrderedDict([
-    ("viewNo", NonNegativeNumberField),
     ("name", LimitedLengthStringField),
-    ("digest", LimitedLengthStringField),
+    ("instId", NonNegativeNumberField),
+    ("viewNo", NonNegativeNumberField),
+    ("ordSeqNo", NonNegativeNumberField),
 ])
 
 
 def test_hash_expected_type():
-    assert ViewChangeAck.typename == "VIEW_CHANGE_ACK"
+    assert Nomination.typename == "NOMINATE"
 
 
 def test_has_expected_fields():
-    actual_field_names = OrderedDict(ViewChangeAck.schema).keys()
+    actual_field_names = OrderedDict(Nomination.schema).keys()
     assert list(actual_field_names) == list(EXPECTED_ORDERED_FIELDS.keys())
 
 
 def test_has_expected_validators():
-    schema = dict(ViewChangeAck.schema)
+    schema = dict(Nomination.schema)
     for field, validator in EXPECTED_ORDERED_FIELDS.items():
         assert isinstance(schema[field], validator)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_taa_op.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_taa_op.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_get_txn_op.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_get_txn_op.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/stub_messages.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/stub_messages.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/input_validation/test_client_nym_op.py` & `indy-plenum-1.9.2rc1/plenum/test/input_validation/test_client_nym_op.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/storage/test_hash_stores.py` & `indy-plenum-1.9.2rc1/plenum/test/storage/test_hash_stores.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 import pytest
 
 from ledger.compact_merkle_tree import CompactMerkleTree
-from ledger.hash_stores.memory_hash_store import MemoryHashStore
-from ledger.test.test_file_hash_store import nodesLeaves
 from ledger.ledger import Ledger
-from plenum.common.constants import HS_LEVELDB, HS_ROCKSDB, HS_MEMORY
+from plenum.common.constants import HS_LEVELDB, HS_ROCKSDB
+from ledger.test.test_file_hash_store import nodesLeaves
 from plenum.persistence.db_hash_store import DbHashStore
 
 
-@pytest.yield_fixture(scope="module", params=[HS_MEMORY, HS_ROCKSDB, HS_LEVELDB])
+@pytest.yield_fixture(scope="module", params=[HS_ROCKSDB, HS_LEVELDB])
 def hashStore(request, tmpdir_factory):
-    if request.param == HS_MEMORY:
-        yield MemoryHashStore()
-    else:
-        hs = DbHashStore(tmpdir_factory.mktemp('').strpath, db_type=request.param)
-        hs.reset()
-        yield hs
-        hs.close()
+    hs = DbHashStore(tmpdir_factory.mktemp('').strpath, db_type=request.param)
+    cleanup(hs)
+    yield hs
+    hs.close()
+
+
+def cleanup(hs):
+    hs.reset()
+    hs.leafCount = 0
 
 
 def testInvalidDBType(tmpdir_factory):
     HS_WRONGDB = 'somedb'
     assert HS_WRONGDB not in (HS_LEVELDB, HS_ROCKSDB)
     with pytest.raises(ValueError) as excinfo:
         DbHashStore('', db_type=HS_WRONGDB)
@@ -41,15 +42,15 @@
     onebyone = [hashStore.readLeaf(i + 1) for i in range(10)]
     multiple = hashStore.readLeafs(1, 10)
     assert onebyone == leaves
     assert onebyone == multiple
 
 
 def testRecoverLedgerFromHashStore(hashStore, tconf, tdir):
-    hashStore.reset()
+    cleanup(hashStore)
     tree = CompactMerkleTree(hashStore=hashStore)
     ledger = Ledger(tree=tree, dataDir=tdir)
     for d in range(10):
         ledger.add(str(d).encode())
     updatedTree = ledger.tree
     ledger.stop()
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/signing/test_signing_without_identifier.py` & `indy-plenum-1.9.2rc1/plenum/test/signing/test_signing_without_identifier.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/signing/test_signing.py` & `indy-plenum-1.9.2rc1/plenum/test/signing/test_signing.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 import pytest
 import sys
 
-from plenum.test.delayers import ppDelay, req_delay
+from plenum.test.delayers import ppDelay, ppgDelay, nom_delay, req_delay
 from plenum.test.helper import sdk_json_to_request_object, \
     sdk_send_random_requests
 from stp_core.loop.eventually import eventually
 from plenum.common.exceptions import InsufficientCorrectSignatures
 from stp_core.common.log import getlogger
+from stp_core.common.util import adict
 from plenum.test import waits
-from plenum.test.malicious_behaviors_node import changesRequest, makeNodeFaulty
+from plenum.test.malicious_behaviors_node import changesRequest, makeNodeFaulty, \
+    delaysPrePrepareProcessing
 from plenum.test.node_request.node_request_helper import checkPropagated
 from plenum.test.test_node import TestNode
 
 logger = getlogger()
 whitelist = ['doing nothing for now',
              'InvalidSignature']
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/signing/test_create_did_without_endorser.py` & `indy-plenum-1.9.2rc1/plenum/test/signing/test_create_did_without_endorser.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_unit_setup_for_non_master.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_unit_setup_for_non_master.py`

 * *Files 20% similar despite different names*

```diff
@@ -15,137 +15,137 @@
 logger = getlogger()
 
 
 def test_setup_last_ordered_for_non_master_after_catchup(txnPoolNodeSet,
                                                          sdk_wallet_client):
     inst_id = 1
     replica = getNonPrimaryReplicas(txnPoolNodeSet, inst_id)[-1]
-    replica._ordering_service.preparesWaitingForPrePrepare.clear()
-    replica._ordering_service.prePreparesPendingPrevPP.clear()
+    replica.preparesWaitingForPrePrepare.clear()
+    replica.prePreparesPendingPrevPP.clear()
     replica.last_ordered_3pc = (0, 0)
     timestamp = time.time()
     ppSeqNo = 5
     preprepare, prepare = \
         _create_prepare_and_preprepare(inst_id,
                                        replica.viewNo,
                                        ppSeqNo,
                                        timestamp,
                                        sdk_wallet_client)
-    replica._ordering_service.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] = deque()
-    replica._ordering_service.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] \
+    replica.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] = deque()
+    replica.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] \
         .append((preprepare, replica.primaryName))
-    replica._ordering_service.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] = deque()
+    replica.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] = deque()
     for node in txnPoolNodeSet:
-        replica._ordering_service.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] \
+        replica.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] \
             .append((prepare, node.name))
-    replica._ordering_service.first_batch_after_catchup = True
-    replica._ordering_service.l_setup_last_ordered_for_non_master()
+    replica.first_batch_after_catchup = True
+    replica._setup_last_ordered_for_non_master()
     assert replica.last_ordered_3pc == (replica.viewNo, ppSeqNo - 1)
 
 
 def test_setup_last_ordered_for_non_master_without_preprepare(txnPoolNodeSet,
                                                               sdk_wallet_client):
     inst_id = 1
     replica = getNonPrimaryReplicas(txnPoolNodeSet, inst_id)[-1]
-    replica._ordering_service.preparesWaitingForPrePrepare.clear()
-    replica._ordering_service.prePreparesPendingPrevPP.clear()
+    replica.preparesWaitingForPrePrepare.clear()
+    replica.prePreparesPendingPrevPP.clear()
     replica.last_ordered_3pc = (0, 0)
     timestamp = time.time()
     ppSeqNo = 5
     preprepare, prepare = \
         _create_prepare_and_preprepare(inst_id,
                                        replica.viewNo,
                                        ppSeqNo,
                                        timestamp,
                                        sdk_wallet_client)
-    replica._ordering_service.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] = deque()
+    replica.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] = deque()
     for node in txnPoolNodeSet:
-        replica._ordering_service.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] \
+        replica.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] \
             .append((prepare, node.name))
-    replica._ordering_service.l_setup_last_ordered_for_non_master()
+    replica._setup_last_ordered_for_non_master()
     assert replica.last_ordered_3pc == (0, 0)
 
 
 def test_setup_last_ordered_for_non_master_without_quorum_of_prepares(
         txnPoolNodeSet,
         sdk_wallet_client):
     inst_id = 1
     replica = getNonPrimaryReplicas(txnPoolNodeSet, inst_id)[-1]
-    replica._ordering_service.preparesWaitingForPrePrepare.clear()
-    replica._ordering_service.prePreparesPendingPrevPP.clear()
+    replica.preparesWaitingForPrePrepare.clear()
+    replica.prePreparesPendingPrevPP.clear()
     replica.last_ordered_3pc = (0, 0)
     timestamp = time.time()
     ppSeqNo = 5
     preprepare, prepare = \
         _create_prepare_and_preprepare(inst_id,
                                        replica.viewNo,
                                        ppSeqNo,
                                        timestamp,
                                        sdk_wallet_client)
-    replica._ordering_service.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] = deque()
-    replica._ordering_service.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] \
+    replica.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] = deque()
+    replica.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] \
         .append((preprepare, replica.primaryName))
-    replica._ordering_service.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] = deque()
-    replica._ordering_service.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] \
+    replica.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] = deque()
+    replica.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] \
         .append((prepare, txnPoolNodeSet[-1].name))
-    replica._ordering_service.l_setup_last_ordered_for_non_master()
+    replica._setup_last_ordered_for_non_master()
     assert replica.last_ordered_3pc == (0, 0)
 
 
 def test_setup_last_ordered_for_non_master_for_master(txnPoolNodeSet,
                                                       sdk_wallet_client):
     inst_id = 0
     replica = getNonPrimaryReplicas(txnPoolNodeSet, inst_id)[-1]
-    replica._ordering_service.preparesWaitingForPrePrepare.clear()
-    replica._ordering_service.prePreparesPendingPrevPP.clear()
+    replica.preparesWaitingForPrePrepare.clear()
+    replica.prePreparesPendingPrevPP.clear()
     replica.last_ordered_3pc = (0, 0)
     timestamp = time.time()
     ppSeqNo = 5
     preprepare, prepare = \
         _create_prepare_and_preprepare(inst_id,
                                        replica.viewNo,
                                        ppSeqNo,
                                        timestamp,
                                        sdk_wallet_client)
-    replica._ordering_service.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] = deque()
-    replica._ordering_service.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] \
+    replica.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] = deque()
+    replica.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] \
         .append((preprepare, replica.primaryName))
-    replica._ordering_service.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] = deque()
+    replica.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] = deque()
     for node in txnPoolNodeSet:
-        replica._ordering_service.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] \
+        replica.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] \
             .append((prepare, node.name))
-    replica._ordering_service.l_setup_last_ordered_for_non_master()
+    replica._setup_last_ordered_for_non_master()
     assert replica.last_ordered_3pc == (0, 0)
 
 
 def test_setup_last_ordered_for_non_master_without_catchup(txnPoolNodeSet,
                                                            sdk_wallet_client):
     inst_id = 1
     last_ordered_3pc = (0, 12)
     timestamp = time.time()
     ppSeqNo = 16
     replica = getNonPrimaryReplicas(txnPoolNodeSet, inst_id)[-1]
     replica.last_ordered_3pc = last_ordered_3pc
-    replica._ordering_service.preparesWaitingForPrePrepare.clear()
-    replica._ordering_service.prePreparesPendingPrevPP.clear()
+    replica.preparesWaitingForPrePrepare.clear()
+    replica.prePreparesPendingPrevPP.clear()
     preprepare, prepare = \
         _create_prepare_and_preprepare(inst_id,
                                        replica.viewNo,
                                        ppSeqNo,
                                        timestamp,
                                        sdk_wallet_client)
-    replica._ordering_service.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] = deque()
-    replica._ordering_service.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] \
+    replica.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] = deque()
+    replica.prePreparesPendingPrevPP[replica.viewNo, ppSeqNo] \
         .append((preprepare, replica.primaryName))
-    replica._ordering_service.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] = deque()
+    replica.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] = deque()
     for node in txnPoolNodeSet:
-        replica._ordering_service.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] \
+        replica.preparesWaitingForPrePrepare[replica.viewNo, ppSeqNo] \
             .append((prepare, node.name))
 
-    replica._ordering_service.l_setup_last_ordered_for_non_master()
+    replica._setup_last_ordered_for_non_master()
     assert replica.last_ordered_3pc == last_ordered_3pc
 
 
 def _create_prepare_and_preprepare(inst_id, pp_sq_no, view_no, timestamp,
                                    sdk_wallet_client):
     time = int(timestamp)
     req_idr = ["random request digest"]
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_node_got_no_preprepare.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_node_got_no_preprepare.py`

 * *Files 1% similar despite different names*

```diff
@@ -49,15 +49,15 @@
     reset_router_accepting(behind_node)
 
     # Send txns
     sdk_send_batches_of_random_and_check(
         looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, num_of_batches, num_of_batches)
 
     # behind_node stashing new 3pc messages and not ordering and not participating in consensus
-    assert len(behind_node.master_replica._ordering_service.prePreparesPendingPrevPP) == 1
+    assert len(behind_node.master_replica.prePreparesPendingPrevPP) == 1
     with pytest.raises(AssertionError):
         nodes_last_ordered_equal(behind_node, master_node)
 
     # After achieving stable checkpoint, behind_node start ordering
     sdk_send_batches_of_random_and_check(
         looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, delta, delta)
 
@@ -98,15 +98,15 @@
     reset_router_accepting(behind_nodes[0])
 
     # Send txns
     sdk_send_batches_of_random_and_check(
         looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, num_of_batches, num_of_batches)
 
     # behind_node stashing new 3pc messages and not ordering and not participating in consensus
-    assert len(behind_nodes[0].master_replica._ordering_service.prePreparesPendingPrevPP) == 1
+    assert len(behind_nodes[0].master_replica.prePreparesPendingPrevPP) == 1
     with pytest.raises(AssertionError):
         nodes_last_ordered_equal(behind_nodes[0], master_node)
 
     # Emulate connection problems, behind_node doesnt receive pre-prepares
     router_dont_accept_messages_from(behind_nodes[1], master_node.name)
 
     # Send some txns and behind_node cant order them while pool is working
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_setup_for_non_master.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_setup_for_non_master.py`

 * *Files 8% similar despite different names*

```diff
@@ -44,9 +44,8 @@
     sdk_send_random_and_check(looper, txnPoolNodeSet,
                               sdk_pool_handle, sdk_wallet_client, 1)
     looper.run(eventually(replicas_synced, new_node))
     for node in txnPoolNodeSet:
         for replica in node.replicas.values():
             assert replica.last_ordered_3pc == (0, 4)
             if not replica.isMaster:
-                assert get_count(replica._ordering_service,
-                                 replica._ordering_service._request_three_phase_msg) == 0
+                assert get_count(replica, replica._request_three_phase_msg) == 0
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_2_nodes_got_only_preprepare.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_2_nodes_got_only_preprepare.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 import pytest
 
-from plenum.test.checkpoints.helper import check_num_received_checkpoints, check_received_checkpoint_votes, \
-    check_for_nodes
+from plenum.test.checkpoints.helper import check_stashed_chekpoints
 from plenum.test.node_catchup.helper import waitNodeDataEquality
 from plenum.test.node_request.helper import nodes_last_ordered_equal
 
 from plenum.test.helper import sdk_send_batches_of_random_and_check
 from plenum.test.malicious_behaviors_node import dont_send_prepare_and_commit_to, reset_sending
 
 from plenum.test.checkpoints.conftest import chkFreqPatched
@@ -42,16 +41,15 @@
     # Send some txns and 1st behind_node cant order them while pool is working
     sdk_send_batches_of_random_and_check(
         looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, 1, 1)
     assert behind_nodes[0].master_last_ordered_3PC[1] + 1 == \
            master_node.master_last_ordered_3PC[1]
 
     # 1st behind got 1 stashed checkpoint from each node
-    check_num_received_checkpoints(behind_nodes[0].master_replica, 1)
-    check_received_checkpoint_votes(behind_nodes[0].master_replica, 2, 3)
+    check_stashed_chekpoints(behind_nodes[0], 3)
 
     # Remove connection problems
     reset_sending(txnPoolNodeSet[:-2])
 
     # Send txns
     sdk_send_batches_of_random_and_check(
         looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, 1, 1)
@@ -67,16 +65,15 @@
     # Send some txns and 2nd behind_node cant order them while pool is working
     sdk_send_batches_of_random_and_check(
         looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, 1, 1)
     assert behind_nodes[1].master_last_ordered_3PC[1] + 1 == \
            master_node.master_last_ordered_3PC[1]
 
     # 2d behind got 1 stashed checkpoint from all nodes except 1st behind
-    check_num_received_checkpoints(behind_nodes[1].master_replica, 1)
-    check_received_checkpoint_votes(behind_nodes[1].master_replica, 4, 2)
+    check_stashed_chekpoints(behind_nodes[1], 2)
 
     # 1st behind got another stashed checkpoint, so should catch-up now
     waitNodeDataEquality(looper, master_node, behind_nodes[0], customTimeout=60,
                          exclude_from_check=['check_last_ordered_3pc_backup'])
 
     # Remove connection problems
     reset_sending(txnPoolNodeSet[:-2])
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_send_node_with_invalid_verkey.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_send_node_with_invalid_verkey.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_request_schema_validation.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_request_schema_validation.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/test_3pc_timestamp.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/test_3pc_timestamp.py`

 * *Files 9% similar despite different names*

```diff
@@ -40,21 +40,21 @@
         looper.runFor(1)
 
     for node in txnPoolNodeSet:
         for r in node.replicas.values():
             rec_prps = defaultdict(list)
             for p in recvd_prepares(r):
                 rec_prps[(p.viewNo, p.ppSeqNo)].append(p)
-            pp_coll = r._ordering_service.sentPrePrepares if r.isPrimary else r._ordering_service.prePrepares
+            pp_coll = r.sentPrePrepares if r.isPrimary else r.prePrepares
             for key, pp in pp_coll.items():
                 for p in rec_prps[key]:
                     assert pp.ppTime == p.ppTime
 
             # `last_accepted_pre_prepare_time` is the time of the last PRE-PREPARE
-            assert r._ordering_service.last_accepted_pre_prepare_time == pp_coll.peekitem(-1)[
+            assert r.last_accepted_pre_prepare_time == pp_coll.peekitem(-1)[
                 1].ppTime
 
             # The ledger should store time for each txn and it should be same
             # as the time for that PRE-PREPARE
             if not r.isMaster:
                 continue
 
@@ -82,22 +82,22 @@
     # send_reqs_to_nodes_and_verify_all_replies(looper, wallet1, client1, 2)
     # The replica having the bad clock
     confused_npr = getNonPrimaryReplicas(txnPoolNodeSet, 0)[-1]
 
     make_clock_faulty(confused_npr.node)
 
     old_acceptable_rvs = getAllReturnVals(
-        confused_npr._ordering_service, confused_npr._ordering_service._is_pre_prepare_time_acceptable)
+        confused_npr, confused_npr.is_pre_prepare_time_acceptable)
     old_susp_count = get_timestamp_suspicion_count(confused_npr.node)
     sdk_send_random_and_check(looper,
                               txnPoolNodeSet,
                               sdk_pool_handle,
                               sdk_wallet_client,
                               count=2)
 
     assert get_timestamp_suspicion_count(confused_npr.node) > old_susp_count
 
     new_acceptable_rvs = getAllReturnVals(
-        confused_npr._ordering_service, confused_npr._ordering_service._is_pre_prepare_time_acceptable)
+        confused_npr, confused_npr.is_pre_prepare_time_acceptable)
 
     # `is_pre_prepare_time_acceptable` first returned False then returned True
     assert [True, False, *old_acceptable_rvs] == new_acceptable_rvs
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/test_timestamp_post_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/test_timestamp_post_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/test_timestamp_new_node.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/test_timestamp_new_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/test_clock_disruption.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/test_clock_disruption.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_timestamp/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_timestamp/helper.py`

 * *Files 6% similar despite different names*

```diff
@@ -15,16 +15,13 @@
 
     def utc_epoch(self) -> int:
         return get_utc_epoch() - clock_slow_by_sec
 
     # slow_utc_epoch = types.MethodType(utc_epoch, node)
     # setattr(node, 'utc_epoch', property(slow_utc_epoch))
     node.utc_epoch = types.MethodType(utc_epoch, node)
-    node.master_replica.get_time_for_3pc_batch = types.MethodType(utc_epoch,
-                                                                  node.master_replica)
-    node.master_replica._ordering_service.get_time_for_3pc_batch = types.MethodType(utc_epoch,
-                                                                                    node.master_replica)
+    node.master_replica.get_time_for_3pc_batch = types.MethodType(utc_epoch, node.master_replica)
 
     if ppr_always_wrong:
         for repl in node.replicas.values():
-            repl._ordering_service._is_pre_prepare_time_correct = types.MethodType(
+            repl.is_pre_prepare_time_correct = types.MethodType(
                 lambda *x, **y: False, repl)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_quorum_f_plus_2_nodes_including_primary_off_and_on.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_quorum_f_plus_2_nodes_including_primary_off_and_on.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_discard_3pc_for_ordered.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_discard_3pc_for_ordered.py`

 * *Files 3% similar despite different names*

```diff
@@ -46,20 +46,20 @@
 
     # send_reqs_batches_and_get_suff_replies(looper, wallet1, client1,
     #                                        2 * sent_batches, sent_batches)
 
     def chk(node, inst_id, p_count, c_count):
         # A node will still record PREPRAREs even if more than n-f-1, till the
         # request is not ordered
-        assert len(node.replicas[inst_id]._ordering_service.prepares) >= p_count
-        assert len(node.replicas[inst_id]._ordering_service.commits) == c_count
+        assert len(node.replicas[inst_id].prepares) >= p_count
+        assert len(node.replicas[inst_id].commits) == c_count
 
     def count_discarded(inst_id, count):
         for node in other_nodes:
-            assert countDiscarded(node.replicas[inst_id].stasher,
+            assert countDiscarded(node.replicas[inst_id],
                                   'already ordered 3 phase message') == count
 
     # `slow_node` did not receive any PREPAREs or COMMITs
     chk(slow_node, 0, 0, 0)
 
     # `other_nodes` have not discarded any 3PC message
     count_discarded(0, 0)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_belated_request_not_processed.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_belated_request_not_processed.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_f_plus_one_faults.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_f_plus_one_faults.py`

 * *Files 14% similar despite different names*

```diff
@@ -32,14 +32,16 @@
     for node in A, B, G:
         makeNodeFaulty(
             node,
             changesRequest,
             partial(
                 delaysPrePrepareProcessing,
                 delay=delayPrePrepareSec))
+        # Delaying nomination to avoid becoming primary
+        # node.delaySelfNomination(10)
     return adict(faulties=(A, B, G))
 
 
 @pytest.fixture(scope="module")
 def afterElection(setup):
     for n in setup.faulties:
         for r in n.replicas.values():
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_num_of_sufficient_preprepare.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_num_of_sufficient_preprepare.py`

 * *Files 6% similar despite different names*

```diff
@@ -14,14 +14,15 @@
 @pytest.fixture(scope="module")
 def setup(txnPoolNodeSet):
     A = txnPoolNodeSet[-2]
     B = txnPoolNodeSet[-1]
     for node in A, B:
         makeNodeFaulty(node,
                        partial(delaysPrePrepareProcessing, delay=60))
+        # node.delaySelfNomination(10)
     return adict(faulties=(A, B))
 
 
 @pytest.fixture(scope="module")
 def afterElection(setup):
     for n in setup.faulties:
         for r in n.replicas.values():
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_one_fault.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_one_fault.py`

 * *Files 19% similar despite different names*

```diff
@@ -12,14 +12,15 @@
 
 
 @pytest.fixture(scope="module")
 def setup(txnPoolNodeSet):
     A = txnPoolNodeSet[-1]
     makeNodeFaulty(A,
                    partial(delaysPrePrepareProcessing, delay=60))
+    # A.delaySelfNomination(10)
     return adict(faulties=A)
 
 
 @pytest.fixture(scope="module")
 def afterElection(setup):
     for r in setup.faulties.replicas.values():
         assert not r.isPrimary
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_primary_sends_preprepare_of_high_num.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_primary_sends_preprepare_of_high_num.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,15 +19,15 @@
     def chk():
         for r in getNonPrimaryReplicas(txnPoolNodeSet, instId):
             nodeSuspicions = len(getNodeSuspicions(
                 r.node, Suspicions.WRONG_PPSEQ_NO.code))
             assert nodeSuspicions == 1
 
     def checkPreprepare(replica, viewNo, ppSeqNo, req, numOfPrePrepares):
-        assert (replica._ordering_service.prePrepares[viewNo, ppSeqNo][0]) == \
+        assert (replica.prePrepares[viewNo, ppSeqNo][0]) == \
                (req.identifier, req.reqId, req.digest)
 
     primary = getPrimaryReplica(txnPoolNodeSet, instId)
     nonPrimaryReplicas = getNonPrimaryReplicas(txnPoolNodeSet, instId)
     req = propagated1.reqDigest
     primary.doPrePrepare(req)
     timeout = waits.expectedPrePrepareTime(len(txnPoolNodeSet))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_ignore_pre_prepare_pp_seq_no_less_than_expected.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_ignore_pre_prepare_pp_seq_no_less_than_expected.py`

 * *Files 14% similar despite different names*

```diff
@@ -20,9 +20,9 @@
     replica.last_ordered_3pc = (replica.viewNo, 10)
 
     sdk_send_random_and_check(looper,
                               txnPoolNodeSet,
                               sdk_pool_handle,
                               sdk_wallet_client,
                               count=1)
-    assert len(replica._ordering_service.prePreparesPendingPrevPP) == 0, \
+    assert len(replica.prePreparesPendingPrevPP) == 0, \
         "the pending request buffer is empty"
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_non_primary_sends_a_pre_prepare.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_non_primary_sends_a_pre_prepare.py`

 * *Files 8% similar despite different names*

```diff
@@ -28,26 +28,26 @@
     def dontSendPrePrepareRequest(self, pp_req: PrePrepare):
         logger.debug("EVIL: {} not sending pre-prepare message for request {}".
                      format(self.name, pp_req))
         return
 
     pr = getPrimaryReplica(txnPoolNodeSet, instId)
     evilMethod = types.MethodType(dontSendPrePrepareRequest, pr)
-    pr._ordering_service.send_pre_prepare = evilMethod
+    pr.sendPrePrepare = evilMethod
 
 
 def testNonPrimarySendsAPrePrepare(looper, txnPoolNodeSet, setup, propagated1):
     nonPrimaryReplicas = getNonPrimaryReplicas(txnPoolNodeSet, instId)
     firstNpr = nonPrimaryReplicas[0]
     remainingNpr = nonPrimaryReplicas[1:]
 
     def sendPrePrepareFromNonPrimary():
-        firstNpr._ordering_service.requestQueues[DOMAIN_LEDGER_ID].add(propagated1.key)
-        ppReq = firstNpr._ordering_service.create_3pc_batch(DOMAIN_LEDGER_ID)
-        firstNpr._ordering_service.send_pre_prepare(ppReq)
+        firstNpr.requestQueues[DOMAIN_LEDGER_ID].add(propagated1.key)
+        ppReq = firstNpr.create_3pc_batch(DOMAIN_LEDGER_ID)
+        firstNpr.sendPrePrepare(ppReq)
         return ppReq
 
     ppr = sendPrePrepareFromNonPrimary()
 
     def chk():
         for r in remainingNpr:
             recvdPps = recvd_pre_prepares(r)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_pass_for_stashed.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_pass_for_stashed.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_fail_for_delayed.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_fail_for_delayed.py`

 * *Files 6% similar despite different names*

```diff
@@ -45,12 +45,12 @@
         looper.run(asyncio.sleep(delay))
 
     # Now delayed 3PC messages reach lagging node, so any delayed transactions
     # can be processed (PrePrepare would be discarded but requested after that),
     # ensure that all nodes will have same data after that
     ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
 
-    pp_count = get_count(lagging_node.master_replica._ordering_service,
-                         lagging_node.master_replica._ordering_service.process_preprepare)
+    pp_count = get_count(lagging_node.master_replica,
+        lagging_node.master_replica.processPrePrepare)
 
     assert pp_count > 0
     assert get_timestamp_suspicion_count(lagging_node) == 1
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_reply_from_ledger_for_request.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_reply_from_ledger_for_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_num_of_propagate_with_zero_faulty_node.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_num_of_propagate_with_zero_faulty_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_num_of_propagate_with_one_fault.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_num_of_propagate_with_one_fault.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_no_reauth.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_no_reauth.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_node_lacks_finalised_requests.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_node_lacks_finalised_requests.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_num_of_propagate_with_f_plus_one_faulty_nodes.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_num_of_propagate_with_f_plus_one_faulty_nodes.py`

 * *Files 13% similar despite different names*

```diff
@@ -20,14 +20,16 @@
 @pytest.fixture(scope="module")
 def setup(txnPoolNodeSet):
     E = txnPoolNodeSet[-3]
     G = txnPoolNodeSet[-2]
     Z = txnPoolNodeSet[-1]
     for node in E, G, Z:
         makeNodeFaulty(node, changesRequest)
+        # Delaying nomination to avoid becoming primary
+        # node.delaySelfNomination(10)
     return adict(faulties=(E, G, Z))
 
 
 @pytest.fixture(scope="module")
 def afterElection(setup):
     for n in setup.faulties:
         for r in n.replicas.values():
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_node_request_only_needed_propagates.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_node_request_only_needed_propagates.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_clean_verified_reqs.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_clean_verified_reqs.py`

 * *Files 18% similar despite different names*

```diff
@@ -23,15 +23,15 @@
                            txnPoolNodeSet,
                            sdk_wallet_steward,
                            sdk_pool_handle):
     """ As for now requests object is cleaned only after checkpoint stabilization,
     therefore need to forcing checkpoint sending"""
     def checkpoint_check(nodes):
         for node in nodes:
-            assert get_count(node.master_replica._checkpointer, node.master_replica._checkpointer._mark_checkpoint_stable) > 0
+            assert get_count(node.master_replica, node.master_replica.markCheckPointStable) > 0
 
     sdk_send_random_and_check(looper,
                               txnPoolNodeSet,
                               sdk_pool_handle,
                               sdk_wallet_steward,
                               REQ_COUNT)
     looper.run(eventually(checkpoint_check, txnPoolNodeSet))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_num_of_sufficient_propagate.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_num_of_sufficient_propagate.py`

 * *Files 5% similar despite different names*

```diff
@@ -42,14 +42,15 @@
 @pytest.fixture(scope="module")
 def setup(txnPoolNodeSet):
     # Making nodes faulty such that no primary is chosen
     G = txnPoolNodeSet[-2]
     Z = txnPoolNodeSet[-1]
     for node in G, Z:
         makeNodeFaulty(node, changesRequest)
+        # node.delaySelfNomination(10)
     return adict(faulties=(G, Z))
 
 
 @pytest.fixture(scope="module")
 def afterElection(setup):
     for n in setup.faulties:
         for r in n.replicas.values():
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_propagate/test_node_request_propagates_with_delay.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_propagate/test_node_request_propagates_with_delay.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_request_forwarding.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_request_forwarding.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from plenum.common.constants import DOMAIN_LEDGER_ID
 from plenum.test import waits
-from plenum.test.delayers import delay_3pc_messages
+from plenum.test.delayers import nom_delay, delay_3pc_messages
 from plenum.test.batching_3pc.conftest import tconf
 from plenum.test.test_node import ensureElectionsDone
 from plenum.test.view_change.helper import ensure_view_change
 from stp_core.loop.eventually import eventually
 from plenum.test.view_change.conftest import perf_chk_patched
 from plenum.test.helper import sdk_send_signed_requests, sdk_get_replies, \
     sdk_signed_random_requests, sdk_eval_timeout
@@ -27,20 +27,20 @@
 
     def chk(count):
         # All replicas have same amount of forwarded request keys and all keys
         # are finalised.
         for node in txnPoolNodeSet:
             for r in node.replicas.values():
                 if r.isPrimary is False:
-                    assert len(r._ordering_service.requestQueues[DOMAIN_LEDGER_ID]) == count
+                    assert len(r.requestQueues[DOMAIN_LEDGER_ID]) == count
                     for i in range(count):
-                        k = r._ordering_service.requestQueues[DOMAIN_LEDGER_ID][i]
+                        k = r.requestQueues[DOMAIN_LEDGER_ID][i]
                         assert r.requests[k].finalised
                 elif r.isPrimary is True:
-                    assert len(r._ordering_service.requestQueues[DOMAIN_LEDGER_ID]) == 0
+                    assert len(r.requestQueues[DOMAIN_LEDGER_ID]) == 0
 
     reqs = sdk_signed_random_requests(looper,
                                       sdk_wallet_client,
                                       tconf.Max3PCBatchSize - 1)
     req_resps = sdk_send_signed_requests(sdk_pool_handle, reqs)
     # Only non primary replicas should have all request keys with them
     looper.run(eventually(chk, tconf.Max3PCBatchSize - 1))
@@ -48,14 +48,17 @@
         tconf.Max3PCBatchSize - 1, len(txnPoolNodeSet),
         add_delay_to_timeout=delay_3pc))
     # Replicas should have no request keys with them since they are ordered
     looper.run(eventually(chk, 0))  # Need to wait since one node might not
     # have processed it.
 
     delay = 1
+    for node in txnPoolNodeSet:
+        node.nodeIbStasher.delay(nom_delay(delay))
+
     ensure_view_change(looper, txnPoolNodeSet)
     reqs = sdk_signed_random_requests(looper,
                                       sdk_wallet_client,
                                       2 * tconf.Max3PCBatchSize)
     req_resps = sdk_send_signed_requests(sdk_pool_handle, reqs)
     looper.run(eventually(chk, 2 * tconf.Max3PCBatchSize))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_quorum_f_plus_2_nodes_but_not_primary_off_and_on.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_quorum_f_plus_2_nodes_but_not_primary_off_and_on.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_different_ledger_request_interleave.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_different_ledger_request_interleave.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_quorum_faulty.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_quorum_faulty.py`

 * *Files 5% similar despite different names*

```diff
@@ -23,14 +23,15 @@
 def setup(txnPoolNodeSet):
     # A = startedNodes.Alpha
     # B = startedNodes.Beta
     A, B = nodes_by_rank(txnPoolNodeSet)[-2:]
     for node in A, B:
         makeNodeFaulty(node, changesRequest,
                        partial(delaysPrePrepareProcessing, delay=90))
+        # node.delaySelfNomination(10)
     return adict(faulties=(A, B))
 
 
 @pytest.fixture(scope="module")
 def afterElection(setup):
     for n in setup.faulties:
         for r in n.replicas.values():
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/conftest.py`

 * *Files 12% similar despite different names*

```diff
@@ -14,14 +14,14 @@
         # Give a little time to process any delayed messages
         looper.runFor(3)
 
         # Check each node has same data
         ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
 
         # Check each node has ordered all requests (no catchup)
-        assert check_if_all_equal_in_list([n.master_replica._ordering_service.ordered
+        assert check_if_all_equal_in_list([n.master_replica.ordered
                                            for n in txnPoolNodeSet])
 
         # Check the network is functional since all nodes reply
         sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, 5)
 
     request.addfinalizer(tear)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_node_requests_missing_preprepares_and_prepares.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_node_requests_missing_preprepares_and_prepares.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-from plenum.server.consensus.ordering_service import OrderingService
 from plenum.server.replica import Replica
 from plenum.test.node_request.message_request.helper import \
     check_pp_out_of_sync
 from plenum.test.waits import expectedPoolGetReadyTimeout
 from stp_core.common.log import getlogger
 from plenum.test.node_catchup.helper import waitNodeDataEquality
 from plenum.test.pool_transactions.helper import \
@@ -54,29 +53,29 @@
         current_node_set.add(node)
         reconnect_node_and_ensure_connected(looper, current_node_set, node)
 
     for node in txnPoolNodeSet:
         assert node.domainLedger.size == init_ledger_size
 
     for node in disconnected_nodes:
-        assert node.master_replica._ordering_service.spylog.count(OrderingService._request_pre_prepare) == 0
-        assert node.master_replica._ordering_service.spylog.count(OrderingService._request_prepare) == 0
+        assert node.master_replica.spylog.count(Replica._request_pre_prepare) == 0
+        assert node.master_replica.spylog.count(Replica._request_prepare) == 0
         assert node.master_replica.spylog.count(Replica.process_requested_pre_prepare) == 0
         assert node.master_replica.spylog.count(Replica.process_requested_prepare) == 0
 
     sdk_send_random_and_check(looper,
                               txnPoolNodeSet,
                               sdk_pool_handle,
                               sdk_wallet_client,
                               REQS_AFTER_RECONNECT_CNT)
     waitNodeDataEquality(looper, disconnected_nodes[0], *txnPoolNodeSet[:-1])
 
     for node in disconnected_nodes:
-        assert node.master_replica._ordering_service.spylog.count(OrderingService._request_pre_prepare) > 0
-        assert node.master_replica._ordering_service.spylog.count(OrderingService._request_prepare) > 0
+        assert node.master_replica.spylog.count(Replica._request_pre_prepare) > 0
+        assert node.master_replica.spylog.count(Replica._request_prepare) > 0
         assert node.master_replica.spylog.count(Replica.process_requested_pre_prepare) > 0
         assert node.master_replica.spylog.count(Replica.process_requested_prepare) > 0
 
     for node in txnPoolNodeSet:
         assert node.domainLedger.size == (init_ledger_size +
                                           MISSING_REQS_CNT +
                                           REQS_AFTER_RECONNECT_CNT)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_preprepare_request.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_preprepare_request.py`

 * *Files 7% similar despite different names*

```diff
@@ -9,23 +9,22 @@
 
 from plenum.test.helper import sdk_send_batches_of_random_and_check
 
 
 def count_requested_preprepare_resp(node):
     # Returns the number of times PRE-PREPARE was requested
     sr = node.master_replica
-    return len(getAllReturnVals(sr._ordering_service,
-                                sr._ordering_service._request_pre_prepare_for_prepare,
+    return len(getAllReturnVals(sr, sr._request_pre_prepare_for_prepare,
                                 compare_val_to=True))
 
 
 def count_requested_preprepare_req(node):
     # Returns the number of times an attempt was made to request PRE-PREPARE
     sr = node.master_replica
-    return get_count(sr._ordering_service, sr._ordering_service._request_pre_prepare_for_prepare)
+    return get_count(sr, sr._request_pre_prepare_for_prepare)
 
 
 def test_node_request_preprepare(looper, txnPoolNodeSet,
                                  sdk_wallet_client, sdk_pool_handle,
                                  teardown):
     """
     Node requests PRE-PREPARE only once after getting PREPAREs.
@@ -51,15 +50,15 @@
     def chk(increase=True):
         # Method is called
         assert count_requested_preprepare_req(slow_node) > old_count_req
         # Requesting Preprepare
         assert count_requested_preprepare_resp(
             slow_node) - old_count_resp == (1 if increase else 0)
 
-    for pp in primary_node.master_replica._ordering_service.sentPrePrepares.values():
+    for pp in primary_node.master_replica.sentPrePrepares.values():
         for rep in [n.master_replica for n in other_primary_nodes]:
             prepare = Prepare(rep.instId,
                               pp.viewNo,
                               pp.ppSeqNo,
                               pp.ppTime,
                               pp.digest,
                               pp.stateRootHash,
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_node_requests_missing_preprepares_prepares_and_commits.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_node_requests_missing_preprepares_and_prepares_after_long_disconnection.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,110 +1,114 @@
 import pytest
+import time
 
-from plenum.server.consensus.ordering_service import OrderingService
 from plenum.server.replica import Replica
-from plenum.test import waits
+from plenum.test.node_catchup.helper import waitNodeDataEquality
 from plenum.test.node_request.message_request.helper import \
     check_pp_out_of_sync
+from plenum.test.pool_transactions.helper import disconnect_node_and_ensure_disconnected, \
+    reconnect_node_and_ensure_connected
 from plenum.test.waits import expectedPoolGetReadyTimeout
+from stp_core.loop.eventually import eventually
 from stp_core.common.log import getlogger
-from plenum.test.node_catchup.helper import waitNodeDataEquality
-from plenum.test.pool_transactions.helper import \
-    disconnect_node_and_ensure_disconnected, reconnect_node_and_ensure_connected
 from plenum.test.helper import sdk_send_random_requests, sdk_send_random_and_check
-from stp_core.loop.eventually import eventually
 
 logger = getlogger()
 
 nodeCount = 4
 
 
-@pytest.fixture(scope="module")
-def tconf(tconf):
-    oldMax3PCBatchSize = tconf.Max3PCBatchSize
-    oldMax3PCBatchWait = tconf.Max3PCBatchWait
-    tconf.Max3PCBatchSize = 5
-    tconf.Max3PCBatchWait = 2
-    yield tconf
-
-    tconf.Max3PCBatchSize = oldMax3PCBatchSize
-    tconf.Max3PCBatchWait = oldMax3PCBatchWait
-
-
-def test_node_requests_missing_preprepares_prepares_and_commits(
-        looper, txnPoolNodeSet, sdk_wallet_client, sdk_pool_handle):
+def test_node_requests_missing_preprepares_and_prepares_after_long_disconnection(
+        looper, txnPoolNodeSet, sdk_wallet_client, sdk_pool_handle,
+        tconf, tdirWithPoolTxns, allPluginsPath):
     """
-    1 of 4 nodes goes down. A new request comes in and is ordered by
-    the 3 remaining nodes. After a while the previously disconnected node
-    comes back alive. Another request comes in. Check that the previously
-    disconnected node requests missing PREPREPARES, PREPARES and COMMITS,
-    orders the previous request and all the nodes successfully handles
-    the last request.
+    2 of 4 nodes go down, so pool can not process any more incoming requests.
+    A new request comes in.
+    Test than waits for some time to ensure that PrePrepare was created
+    long enough seconds to be dropped by time checker.
+    Two stopped nodes come back alive.
+    Another request comes in.
+    Check that previously disconnected two nodes request missing PREPREPARES
+    and PREPARES and the pool successfully handles both transactions.
     """
     INIT_REQS_CNT = 5
     MISSING_REQS_CNT = 4
     REQS_AFTER_RECONNECT_CNT = 1
-    disconnected_node = txnPoolNodeSet[3]
-    alive_nodes = txnPoolNodeSet[:3]
+    alive_nodes = []
+    disconnected_nodes = []
+
+    for node in txnPoolNodeSet:
+        if node.hasPrimary:
+            alive_nodes.append(node)
+        else:
+            disconnected_nodes.append(node)
 
     sdk_send_random_and_check(looper,
                               txnPoolNodeSet,
                               sdk_pool_handle,
                               sdk_wallet_client,
                               INIT_REQS_CNT)
+
+    waitNodeDataEquality(looper, disconnected_nodes[0], *txnPoolNodeSet)
     init_ledger_size = txnPoolNodeSet[0].domainLedger.size
 
-    disconnect_node_and_ensure_disconnected(looper, txnPoolNodeSet,
-                                            disconnected_node, stopNode=False)
-
-    sdk_send_random_and_check(looper,
-                              txnPoolNodeSet,
-                              sdk_pool_handle,
-                              sdk_wallet_client,
-                              MISSING_REQS_CNT)
+    current_node_set = set(txnPoolNodeSet)
+    for node in disconnected_nodes:
+        disconnect_node_and_ensure_disconnected(looper,
+                                                current_node_set,
+                                                node,
+                                                stopNode=False)
+        current_node_set.remove(node)
+
+    sdk_send_random_requests(looper,
+                             sdk_pool_handle,
+                             sdk_wallet_client,
+                             MISSING_REQS_CNT)
 
     looper.run(eventually(check_pp_out_of_sync,
                           alive_nodes,
-                          [disconnected_node],
+                          disconnected_nodes,
                           retryWait=1,
                           timeout=expectedPoolGetReadyTimeout(len(txnPoolNodeSet))))
 
-    reconnect_node_and_ensure_connected(looper, txnPoolNodeSet, disconnected_node)
-    # Give time for the reconnected node to catch up if it is going to do it
-    looper.runFor(waits.expectedPoolConsistencyProof(len(txnPoolNodeSet)) +
-                  waits.expectedPoolCatchupTime(len(txnPoolNodeSet)))
-
-    for node in alive_nodes:
-        assert node.domainLedger.size == init_ledger_size + MISSING_REQS_CNT
-    # Ensure that the reconnected node has not caught up though
-    assert disconnected_node.domainLedger.size == init_ledger_size
-
-    ordering_service = disconnected_node.master_replica._ordering_service
-    assert ordering_service.spylog.count(OrderingService._request_pre_prepare) == 0
-    assert ordering_service.spylog.count(OrderingService._request_prepare) == 0
-    assert ordering_service.spylog.count(OrderingService._request_commit) == 0
-    assert disconnected_node.master_replica.spylog.count(Replica.process_requested_pre_prepare) == 0
-    assert disconnected_node.master_replica.spylog.count(Replica.process_requested_prepare) == 0
-    assert disconnected_node.master_replica.spylog.count(Replica.process_requested_commit) == 0
-    doOrderTimesBefore = ordering_service.spylog.count(OrderingService._do_order)
+    preprepare_deviation = 4
+    tconf.ACCEPTABLE_DEVIATION_PREPREPARE_SECS = preprepare_deviation
+    time.sleep(preprepare_deviation * 2)
+
+    for node in disconnected_nodes:
+        current_node_set.add(node)
+        reconnect_node_and_ensure_connected(looper, current_node_set, node)
+
+    for node in txnPoolNodeSet:
+        assert node.domainLedger.size == init_ledger_size
+
+    for node in disconnected_nodes:
+        assert node.master_replica.spylog.count(Replica._request_pre_prepare) == 0
+        assert node.master_replica.spylog.count(Replica._request_prepare) == 0
+        assert node.master_replica.spylog.count(Replica.process_requested_pre_prepare) == 0
+        assert node.master_replica.spylog.count(Replica.process_requested_prepare) == 0
 
     sdk_send_random_and_check(looper,
                               txnPoolNodeSet,
                               sdk_pool_handle,
                               sdk_wallet_client,
                               REQS_AFTER_RECONNECT_CNT)
-    waitNodeDataEquality(looper, disconnected_node, *alive_nodes)
 
-    assert ordering_service.spylog.count(OrderingService._request_pre_prepare) > 0
-    assert ordering_service.spylog.count(OrderingService._request_prepare) > 0
-    assert ordering_service.spylog.count(OrderingService._request_commit) > 0
-    assert disconnected_node.master_replica.spylog.count(Replica.process_requested_pre_prepare) > 0
-    assert disconnected_node.master_replica.spylog.count(Replica.process_requested_prepare) > 0
-    assert disconnected_node.master_replica.spylog.count(Replica.process_requested_commit) > 0
-    doOrderTimesAfter = ordering_service.spylog.count(OrderingService._do_order)
-    # Ensure that the reconnected node has ordered both the missed 3PC-batch and the new 3PC-batch
-    assert doOrderTimesAfter - doOrderTimesBefore == 2
+    waitNodeDataEquality(looper, disconnected_nodes[0], *txnPoolNodeSet)
+
+    for node in disconnected_nodes:
+        assert node.master_replica.spylog.count(Replica._request_pre_prepare) > 0
+        assert node.master_replica.spylog.count(Replica._request_prepare) > 0
+        assert node.master_replica.spylog.count(Replica.process_requested_pre_prepare) > 0
+        assert node.master_replica.spylog.count(Replica.process_requested_prepare) > 0
 
     for node in txnPoolNodeSet:
         assert node.domainLedger.size == (init_ledger_size +
                                           MISSING_REQS_CNT +
                                           REQS_AFTER_RECONNECT_CNT)
+
+
+@pytest.yield_fixture(autouse=True)
+def teardown(tconf):
+    original_deviation = tconf.ACCEPTABLE_DEVIATION_PREPREPARE_SECS
+    yield
+    tconf.ACCEPTABLE_DEVIATION_PREPREPARE_SECS = original_deviation
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_valid_message_request.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_valid_message_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_node_requests_missing_preprepare.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_node_requests_missing_preprepare.py`

 * *Files 5% similar despite different names*

```diff
@@ -21,15 +21,15 @@
     """
     slow_node, other_nodes, primary_node, other_non_primary_nodes = split_nodes(
         txnPoolNodeSet)
 
     # Delay PRE-PREPAREs by large amount simulating loss
     slow_node.nodeIbStasher.delay(ppDelay(300, 0))
     old_count_pp = get_count(slow_node.master_replica,
-                             slow_node.master_replica._ordering_service.process_preprepare)
+                             slow_node.master_replica.processPrePrepare)
     old_count_mrq = {n.name: get_count(n, n.process_message_req)
                      for n in other_nodes}
     old_count_mrp = get_count(slow_node, slow_node.process_message_rep)
 
     sdk_send_batches_of_random_and_check(looper,
                                          txnPoolNodeSet,
                                          sdk_pool_handle,
@@ -38,16 +38,16 @@
                                          num_batches=5)
 
     waitNodeDataEquality(looper, slow_node, *other_nodes)
 
     assert not slow_node.master_replica.requested_pre_prepares
 
     # `slow_node` processed PRE-PREPARE
-    # assert get_count(slow_node.master_replica,
-    #                  slow_node.master_replica._ordering_service.process_preprepare) > old_count_pp
+    assert get_count(slow_node.master_replica,
+                     slow_node.master_replica.processPrePrepare) > old_count_pp
 
     # `slow_node` did receive `MessageRep`
     assert get_count(slow_node, slow_node.process_message_rep) > old_count_mrp
 
     # Primary node should received `MessageReq` and other nodes shouldn't
     recv_reqs = set()
     for n in other_non_primary_nodes:
@@ -55,9 +55,9 @@
             recv_reqs.add(n.name)
 
     assert get_count(primary_node, primary_node.process_message_req) > \
            old_count_mrq[primary_node.name]
     assert len(recv_reqs) == 0
 
     # All nodes including the `slow_node` ordered the same requests
-    assert check_if_all_equal_in_list([n.master_replica._ordering_service.ordered
-                                       for n in txnPoolNodeSet])
+    assert check_if_all_equal_in_list([n.master_replica.ordered
+                                       for n in txnPoolNodeSet])
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_node_requests_missing_preprepares_and_prepares_after_long_disconnection.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_view_change.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,115 +1,102 @@
-import pytest
-import time
+from plenum.test.delayers import icDelay
+from plenum.test.stasher import delay_rules
 
-from plenum.server.consensus.ordering_service import OrderingService
-from plenum.server.replica import Replica
-from plenum.test.node_catchup.helper import waitNodeDataEquality
-from plenum.test.node_request.message_request.helper import \
-    check_pp_out_of_sync
-from plenum.test.pool_transactions.helper import disconnect_node_and_ensure_disconnected, \
-    reconnect_node_and_ensure_connected
-from plenum.test.waits import expectedPoolGetReadyTimeout
+from plenum.common.constants import DOMAIN_LEDGER_ID, STEWARD_STRING
+from plenum.test.audit_ledger.helper import check_audit_ledger_updated, check_audit_txn
+from plenum.test.helper import sdk_send_random_and_check, assertExp
+from plenum.test.pool_transactions.helper import sdk_add_new_nym, sdk_add_new_node
+from plenum.test.test_node import checkNodesConnected, ensureElectionsDone
 from stp_core.loop.eventually import eventually
-from stp_core.common.log import getlogger
-from plenum.test.helper import sdk_send_random_requests, sdk_send_random_and_check
 
-logger = getlogger()
+nodeCount = 6
 
-nodeCount = 4
 
-
-def test_node_requests_missing_preprepares_and_prepares_after_long_disconnection(
-        looper, txnPoolNodeSet, sdk_wallet_client, sdk_pool_handle,
-        tconf, tdirWithPoolTxns, allPluginsPath):
-    """
-    2 of 4 nodes go down, so pool can not process any more incoming requests.
-    A new request comes in.
-    Test than waits for some time to ensure that PrePrepare was created
-    long enough seconds to be dropped by time checker.
-    Two stopped nodes come back alive.
-    Another request comes in.
-    Check that previously disconnected two nodes request missing PREPREPARES
-    and PREPARES and the pool successfully handles both transactions.
-    """
-    INIT_REQS_CNT = 5
-    MISSING_REQS_CNT = 4
-    REQS_AFTER_RECONNECT_CNT = 1
-    alive_nodes = []
-    disconnected_nodes = []
-
-    for node in txnPoolNodeSet:
-        if node.hasPrimary:
-            alive_nodes.append(node)
-        else:
-            disconnected_nodes.append(node)
-
-    sdk_send_random_and_check(looper,
-                              txnPoolNodeSet,
-                              sdk_pool_handle,
-                              sdk_wallet_client,
-                              INIT_REQS_CNT)
-
-    waitNodeDataEquality(looper, disconnected_nodes[0], *txnPoolNodeSet)
-    init_ledger_size = txnPoolNodeSet[0].domainLedger.size
-
-    current_node_set = set(txnPoolNodeSet)
-    for node in disconnected_nodes:
-        disconnect_node_and_ensure_disconnected(looper,
-                                                current_node_set,
-                                                node,
-                                                stopNode=False)
-        current_node_set.remove(node)
-
-    sdk_send_random_requests(looper,
-                             sdk_pool_handle,
-                             sdk_wallet_client,
-                             MISSING_REQS_CNT)
-
-    looper.run(eventually(check_pp_out_of_sync,
-                          alive_nodes,
-                          disconnected_nodes,
-                          retryWait=1,
-                          timeout=expectedPoolGetReadyTimeout(len(txnPoolNodeSet))))
-
-    preprepare_deviation = 4
-    tconf.ACCEPTABLE_DEVIATION_PREPREPARE_SECS = preprepare_deviation
-    time.sleep(preprepare_deviation * 2)
-
-    for node in disconnected_nodes:
-        current_node_set.add(node)
-        reconnect_node_and_ensure_connected(looper, current_node_set, node)
-
-    for node in txnPoolNodeSet:
-        assert node.domainLedger.size == init_ledger_size
-
-    for node in disconnected_nodes:
-        assert node.master_replica._ordering_service.spylog.count(OrderingService._request_pre_prepare) == 0
-        assert node.master_replica._ordering_service.spylog.count(OrderingService._request_prepare) == 0
-        assert node.master_replica.spylog.count(Replica.process_requested_pre_prepare) == 0
-        assert node.master_replica.spylog.count(Replica.process_requested_prepare) == 0
-
-    sdk_send_random_and_check(looper,
-                              txnPoolNodeSet,
-                              sdk_pool_handle,
-                              sdk_wallet_client,
-                              REQS_AFTER_RECONNECT_CNT)
-
-    waitNodeDataEquality(looper, disconnected_nodes[0], *txnPoolNodeSet)
-
-    for node in disconnected_nodes:
-        assert node.master_replica._ordering_service.spylog.count(OrderingService._request_pre_prepare) > 0
-        assert node.master_replica._ordering_service.spylog.count(OrderingService._request_prepare) > 0
-        assert node.master_replica.spylog.count(Replica.process_requested_pre_prepare) > 0
-        assert node.master_replica.spylog.count(Replica.process_requested_prepare) > 0
+def test_audit_ledger_view_change(looper, txnPoolNodeSet,
+                                  sdk_pool_handle, sdk_wallet_client, sdk_wallet_steward,
+                                  initial_domain_size, initial_pool_size, initial_config_size,
+                                  tdir,
+                                  tconf,
+                                  allPluginsPath,
+                                  view_no, pp_seq_no,
+                                  initial_seq_no,
+                                  monkeypatch):
+    '''
+    1. Send a NODE transaction and add a 7th Node for adding a new instance,
+    but delay Ordered messages.
+    2. Send a NYM txn.
+    3. Reset delays in executing force_process_ordered
+    4. Check that an audit txn for the NYM txn uses primary list from uncommitted
+    audit with a new list of primaries.
+    '''
+    other_nodes = txnPoolNodeSet[:-1]
+    slow_node = txnPoolNodeSet[-1]
+    # Add a new steward for creating a new node
+    new_steward_wallet_handle = sdk_add_new_nym(looper,
+                                                sdk_pool_handle,
+                                                sdk_wallet_steward,
+                                                alias="newSteward",
+                                                role=STEWARD_STRING)
+
+    audit_size_initial = [node.auditLedger.size for node in txnPoolNodeSet]
+
+    ordereds = []
+    monkeypatch.setattr(slow_node, 'try_processing_ordered', lambda msg: ordereds.append(msg))
+
+    with delay_rules([n.nodeIbStasher for n in txnPoolNodeSet], icDelay()):
+
+        # Send NODE txn fo 7th node
+        new_node = sdk_add_new_node(looper,
+                                    sdk_pool_handle,
+                                    new_steward_wallet_handle,
+                                    "Theta",
+                                    tdir,
+                                    tconf,
+                                    allPluginsPath)
+
+        txnPoolNodeSet.append(new_node)
+        looper.run(checkNodesConnected(other_nodes + [new_node]))
+
+        sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
+                                  sdk_wallet_client, 1)
+
+        check_audit_ledger_updated(audit_size_initial, [slow_node],
+                                   audit_txns_added=0)
+        looper.run(eventually(check_audit_ledger_uncommitted_updated,
+                              audit_size_initial, [slow_node], 2))
+
+        def patch_force_process_ordered():
+            for msg in list(ordereds):
+                slow_node.replicas[msg.instId].outBox.append(msg)
+                ordereds.remove(msg)
+            monkeypatch.undo()
+            slow_node.force_process_ordered()
+
+        assert ordereds
+        monkeypatch.setattr(slow_node, 'force_process_ordered', patch_force_process_ordered)
+
+    looper.run(eventually(lambda: assertExp(all(n.viewNo == 1 for n in txnPoolNodeSet))))
+    ensureElectionsDone(looper=looper, nodes=txnPoolNodeSet)
+    looper.run(eventually(lambda: assertExp(not ordereds)))
 
     for node in txnPoolNodeSet:
-        assert node.domainLedger.size == (init_ledger_size +
-                                          MISSING_REQS_CNT +
-                                          REQS_AFTER_RECONNECT_CNT)
-
-
-@pytest.yield_fixture(autouse=True)
-def teardown(tconf):
-    original_deviation = tconf.ACCEPTABLE_DEVIATION_PREPREPARE_SECS
-    yield
-    tconf.ACCEPTABLE_DEVIATION_PREPREPARE_SECS = original_deviation
+        last_txn = node.auditLedger.get_last_txn()
+        last_txn['txn']['data']['primaries'] = node.elector._get_last_audited_primaries()
+        check_audit_txn(txn=last_txn,
+                        view_no=view_no + 1, pp_seq_no=1,
+                        seq_no=initial_seq_no + 4,
+                        txn_time=node.master_replica.last_accepted_pre_prepare_time,
+                        txn_roots={DOMAIN_LEDGER_ID: node.getLedger(DOMAIN_LEDGER_ID).tree.root_hash},
+                        state_roots={DOMAIN_LEDGER_ID: node.getState(DOMAIN_LEDGER_ID).committedHeadHash},
+                        pool_size=initial_pool_size + 1, domain_size=initial_domain_size + 2,
+                        config_size=initial_config_size,
+                        last_pool_seqno=2,
+                        last_domain_seqno=1,
+                        last_config_seqno=None,
+                        primaries=node.write_manager.future_primary_handler.get_last_primaries() or node.primaries)
+
+
+def check_audit_ledger_uncommitted_updated(audit_size_initial, nodes, audit_txns_added):
+    audit_size_after = [node.auditLedger.uncommitted_size for node in nodes]
+    for i in range(len(nodes)):
+        assert audit_size_after[i] == audit_size_initial[i] + audit_txns_added, \
+            "{} != {}".format(audit_size_after[i], audit_size_initial[i] + audit_txns_added)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/helper.py`

 * *Files 6% similar despite different names*

```diff
@@ -19,15 +19,15 @@
                                (slow_node, primary_node)]
     return slow_node, other_nodes, primary_node, other_non_primary_nodes
 
 
 def check_pp_out_of_sync(alive_nodes, disconnected_nodes):
 
     def get_last_pp(node):
-        return node.master_replica._ordering_service.last_preprepare
+        return node.master_replica.lastPrePrepare
 
     last_3pc_key_alive = get_last_pp(alive_nodes[0])
     for node in alive_nodes[1:]:
         assert get_last_pp(node) == last_3pc_key_alive
 
     last_3pc_key_diconnected = get_last_pp(disconnected_nodes[0])
     assert last_3pc_key_diconnected != last_3pc_key_alive
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/message_request/test_requested_preprepare_handling.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/message_request/test_requested_preprepare_handling.py`

 * *Files 6% similar despite different names*

```diff
@@ -44,16 +44,16 @@
                                          num_batches=5)
     waitNodeDataEquality(looper, slow_node, *other_nodes)
 
     slow_master_replica = slow_node.master_replica
     count_pr_req = get_count(slow_master_replica,
                              slow_master_replica.process_requested_pre_prepare)
 
-    count_pr_tpc = get_count(slow_master_replica._ordering_service,
-                             slow_master_replica._ordering_service._validate)
+    count_pr_tpc = get_count(slow_master_replica,
+                             slow_master_replica.process_three_phase_msg)
 
     primary_node.sendToNodes(MessageRep(**{
         f.MSG_TYPE.nm: PREPREPARE,
         f.PARAMS.nm: {
             f.INST_ID.nm: last_pp.instId,
             f.VIEW_NO.nm: last_pp.viewNo,
             f.PP_SEQ_NO.nm: last_pp.ppSeqNo
@@ -64,11 +64,11 @@
     def chk():
         # `process_requested_pre_prepare` is called but
         # `processThreePhaseMsg` is not called
         assert get_count(
             slow_master_replica,
             slow_master_replica.process_requested_pre_prepare) > count_pr_req
         assert get_count(
-            slow_master_replica._ordering_service,
-            slow_master_replica._ordering_service._validate) == count_pr_tpc
+            slow_master_replica,
+            slow_master_replica.process_three_phase_msg) == count_pr_tpc
 
     looper.run(eventually(chk, retryWait=1))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_order/test_request_ordering_2.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_order/test_request_ordering_2.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_order/test_request_ordering_1.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_order/test_request_ordering_1.py`

 * *Files 5% similar despite different names*

```diff
@@ -25,14 +25,14 @@
     def doNotProcessReqDigest(self, _):
         pass
 
     patchedMethod = types.MethodType(doNotProcessReqDigest, replica)
     replica.processRequest = patchedMethod
 
     def chk(n):
-        assert replica._ordering_service.spylog.count(replica._ordering_service._do_order.__name__) == n
+        assert replica.spylog.count(replica.doOrder.__name__) == n
 
     sdk_send_random_request(looper, sdk_pool_handle, sdk_wallet_client)
     timeout = delay - 5
     looper.run(eventually(chk, 0, retryWait=1, timeout=timeout))
     timeout = delay + 5
     looper.run(eventually(chk, 1, retryWait=1, timeout=timeout))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_split_non_3pc_messages_on_batches.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_split_non_3pc_messages_on_batches.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_quorum_disconnected.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_quorum_disconnected.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/node_request_helper.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/node_request_helper.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 from functools import partial
 
 from plenum.common.messages.node_messages import PrePrepare
 from plenum.common.types import OPERATION, f
 from plenum.common.constants import DOMAIN_LEDGER_ID, POOL_LEDGER_ID, AUDIT_LEDGER_ID
 from plenum.common.util import getMaxFailures, get_utc_epoch
-from plenum.server.consensus.ordering_service import OrderingService
 from plenum.server.node import Node
 from plenum.server.quorums import Quorums
 from plenum.server.replica import Replica
 from plenum.test import waits
 from plenum.test.helper import chk_all_funcs, init_discarded
 from plenum.test.spy_helpers import getAllArgs
 from plenum.test.test_node import TestNode, getNonPrimaryReplicas, \
@@ -61,16 +60,15 @@
 
         def primarySeesCorrectNumberOfPREPREPAREs():
             """
             no of PRE-PREPARE as seen by processPrePrepare
             method for primary must be 0 with or without faults in system
             """
             l1 = len([param for param in
-                      getAllArgs(primary._ordering_service,
-                                 primary._ordering_service.process_preprepare)])
+                      getAllArgs(primary, primary.processPrePrepare)])
             assert l1 == 0, 'Primary {} sees no pre-prepare'.format(primary)
 
         def nonPrimarySeesCorrectNumberOfPREPREPAREs():
             """
             1. no of PRE-PREPARE as seen by processPrePrepare method for
             non-primaries must be 1; whn zero faulty nodes in system.
 
@@ -83,27 +81,26 @@
                 primary.viewNo,
                 primary.lastPrePrepareSeqNo,
                 get_utc_epoch(),
                 [propagated1.digest],
                 init_discarded(),
                 Replica.batchDigest([propagated1, ]),
                 DOMAIN_LEDGER_ID,
-                primary._ordering_service.get_state_root_hash(DOMAIN_LEDGER_ID),
-                primary._ordering_service.get_txn_root_hash(DOMAIN_LEDGER_ID),
+                primary.stateRootHash(DOMAIN_LEDGER_ID),
+                primary.txnRootHash(DOMAIN_LEDGER_ID),
                 0,
                 True,
-                primary._ordering_service.get_state_root_hash(POOL_LEDGER_ID),
-                primary._ordering_service.get_txn_root_hash(AUDIT_LEDGER_ID)
+                primary.stateRootHash(POOL_LEDGER_ID),
+                primary.txnRootHash(AUDIT_LEDGER_ID)
             )
 
             passes = 0
             for npr in nonPrimaryReplicas:
                 actualMsgs = len([param for param in
-                                  getAllArgs(npr._ordering_service,
-                                             npr._ordering_service.process_preprepare)
+                                  getAllArgs(npr, npr.processPrePrepare)
                                   if (param['pre_prepare'][0:3] +
                                       param['pre_prepare'][4:],
                                       param['sender']) == (
                                       expectedPrePrepareRequest[0:3] +
                                       expectedPrePrepareRequest[4:],
                                       primary.name)])
 
@@ -121,16 +118,15 @@
         def primarySentsCorrectNumberOfPREPREPAREs():
             """
             1. no of PRE-PREPARE sent by primary is 1 with or without
             fault in system but, when primary is faulty no of sent PRE_PREPARE
              will be zero and primary must be marked as malicious.
             """
             actualMsgs = len([param for param in
-                              getAllArgs(primary._ordering_service,
-                                         primary._ordering_service.send_pre_prepare)
+                              getAllArgs(primary, primary.sendPrePrepare)
                               if param['ppReq'].reqIdr[0] == propagated1.digest
                               and param['ppReq'].digest ==
                               primary.batchDigest([propagated1, ])])
 
             numOfMsgsWithZFN = 1
 
             # TODO: Considering, Primary is not faulty and will always send
@@ -146,16 +142,15 @@
             """
             1. no of PRE-PREPARE received by non-primaries must be 1
             with zero faults in system, and 0 faults in system.
             """
             passes = 0
             for npr in nonPrimaryReplicas:
                 l4 = len([param for param in
-                          getAllArgs(npr._ordering_service,
-                                     npr._ordering_service._add_to_pre_prepares)
+                          getAllArgs(npr, npr.addToPrePrepares)
                           if param['pp'].reqIdr[0] == propagated1.digest
                           and param['pp'].digest ==
                           primary.batchDigest([propagated1, ])])
 
                 numOfMsgsWithZFN = 1
                 numOfMsgsWithFaults = 0
 
@@ -190,16 +185,15 @@
         nonPrimaryReplicas = getNonPrimaryReplicas(txnPoolNodeSet, instId)
 
         def primaryDontSendAnyPREPAREs():
             """
             1. no of PREPARE sent by primary should be 0
             """
             for r in allReplicas:
-                for param in getAllArgs(r._ordering_service,
-                                        OrderingService.process_prepare):
+                for param in getAllArgs(r, Replica.processPrepare):
                     sender = param['sender']
                     assert sender != primary.name
 
         def allReplicasSeeCorrectNumberOfPREPAREs():
             """
             1. no of PREPARE received by replicas must be n - 1;
             n = num of nodes without fault, and greater than or equal to
@@ -207,16 +201,16 @@
             """
             passes = 0
             numOfMsgsWithZFN = nodeCount - 1
             numOfMsgsWithFaults = quorums.prepare.value
 
             for replica in allReplicas:
                 key = primary.viewNo, primary.lastPrePrepareSeqNo
-                if key in replica._ordering_service.prepares:
-                    actualMsgs = len(replica._ordering_service.prepares[key].voters)
+                if key in replica.prepares:
+                    actualMsgs = len(replica.prepares[key].voters)
 
                     passes += int(msgCountOK(nodeCount,
                                              faultyNodes,
                                              actualMsgs,
                                              numOfMsgsWithZFN,
                                              numOfMsgsWithFaults))
             assert passes >= len(allReplicas) - faultyNodes
@@ -224,16 +218,16 @@
         def primaryReceivesCorrectNumberOfPREPAREs():
             """
             num of PREPARE seen by primary replica is n - 1;
                 n = num of nodes without fault, and greater than or equal to
              n-f-1 with faults.
             """
             actualMsgs = len([param for param in
-                              getAllArgs(primary._ordering_service,
-                                         primary._ordering_service.process_prepare)
+                              getAllArgs(primary,
+                                         primary.processPrepare)
                               if (param['prepare'].instId,
                                   param['prepare'].viewNo,
                                   param['prepare'].ppSeqNo) == (
                                   primary.instId, primary.viewNo,
                                   primary.lastPrePrepareSeqNo) and
                               param['sender'] != primary.name])
 
@@ -256,21 +250,22 @@
             numOfMsgsWithZFN = nodeCount - 2
             numOfMsgsWithFaults = quorums.prepare.value - 1
 
             for npr in nonPrimaryReplicas:
                 actualMsgs = len(
                     [
                         param for param in getAllArgs(
-                        npr._ordering_service,
-                        npr._ordering_service.process_prepare) if (param['prepare'].instId,
-                                                                   param['prepare'].viewNo,
-                                                                   param['prepare'].ppSeqNo) == (
-                                                                      primary.instId,
-                                                                      primary.viewNo,
-                                                                      primary.lastPrePrepareSeqNo)])
+                        npr,
+                        npr.processPrepare) if (
+                                                   param['prepare'].instId,
+                                                   param['prepare'].viewNo,
+                                                   param['prepare'].ppSeqNo) == (
+                                                   primary.instId,
+                                                   primary.viewNo,
+                                                   primary.lastPrePrepareSeqNo)])
 
                 passes += int(msgCountOK(nodeCount,
                                          faultyNodes,
                                          actualMsgs,
                                          numOfMsgsWithZFN,
                                          numOfMsgsWithFaults))
 
@@ -306,16 +301,16 @@
             """
             passes = 0
             numOfMsgsWithZFN = quorums.commit.value
             numOfMsgsWithFault = quorums.commit.value
 
             key = (primaryReplica.viewNo, primaryReplica.lastPrePrepareSeqNo)
             for r in allReplicas:
-                if key in r._ordering_service.commits:
-                    rcvdCommitRqst = r._ordering_service.commits[key]
+                if key in r.commits:
+                    rcvdCommitRqst = r.commits[key]
                     actualMsgsReceived = len(rcvdCommitRqst.voters)
 
                     passes += int(msgCountOK(nodeCount,
                                              faultyNodes,
                                              actualMsgsReceived,
                                              numOfMsgsWithZFN,
                                              numOfMsgsWithFault))
@@ -351,13 +346,13 @@
     counts = {}
     sender_replica_names = {r.instId: r.name for r in sender.replicas.values()}
     for node in receivers:
         for replica in node.replicas.values():
             if replica.instId not in counts:
                 counts[replica.instId] = 0
             nm = sender_replica_names[replica.instId]
-            for commit in replica._ordering_service.commits.values():
+            for commit in replica.commits.values():
                 counts[replica.instId] += int(nm in commit.voters)
-            for prepare in replica._ordering_service.prepares.values():
+            for prepare in replica.prepares.values():
                 counts[replica.instId] += int(nm in prepare.voters)
     for c in counts.values():
         assert count == c, "expected {}, but have {}".format(count, c)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_req_idr_to_txn.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_req_idr_to_txn.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_prepare/test_num_of_prepare_with_one_fault.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_prepare/test_num_of_sufficient_prepare.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,29 +1,39 @@
 from functools import partial
 
 import pytest
-
 from plenum.test.malicious_behaviors_node import makeNodeFaulty, \
     delaysPrePrepareProcessing
 from stp_core.common.util import adict
+from stp_core.common.log import getlogger
 
-nodeCount = 4
-faultyNodes = 1
+nodeCount = 7
+faultyNodes = 2
 whitelist = ['cannot process incoming PREPARE']
 
+logger = getlogger()
+
 
 @pytest.fixture(scope="module")
 def setup(txnPoolNodeSet):
-    # Making nodes faulty such that no primary is chosen
-    G = txnPoolNodeSet[-1]
-    makeNodeFaulty(G,
-                   partial(delaysPrePrepareProcessing, delay=60))
-    return adict(faulty=G)
+    G = txnPoolNodeSet[-2]
+    Z = txnPoolNodeSet[-1]
+    for node in G, Z:
+        makeNodeFaulty(node,
+                       partial(delaysPrePrepareProcessing, delay=60))
+        # Delaying nomination to avoid becoming primary
+        # node.delaySelfNomination(10)
+    return adict(faulties=(G, Z))
 
 
 @pytest.fixture(scope="module")
 def afterElection(setup):
-    assert not setup.faulty.hasPrimary
+    for n in setup.faulties:
+        for r in n.replicas.values():
+            assert not r.isPrimary
 
 
-def testNumOfPrepareWithOneFault(afterElection, prepared1):
-    pass
+def testNumOfSufficientPrepare(afterElection, prepared1, txnPoolNodeSet):
+    for n in txnPoolNodeSet:
+        for r in n.replicas.values():
+            if r.isPrimary:
+                logger.info("{} is primary".format(r))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_prepare/test_num_prepare_with_2_of_6_faulty.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_prepare/test_num_prepare_with_2_of_6_faulty.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_prepare/test_num_of_prepare_with_f_plus_one_faults.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_num_of_commit_with_f_plus_one_faults.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,54 +1,50 @@
 from functools import partial
 
 import pytest
+
 from plenum.common.util import getNoInstances
 from stp_core.common.util import adict
-from plenum.test import waits
-
+from plenum.test.node_request.node_request_helper import checkCommitted
 from plenum.test.malicious_behaviors_node import makeNodeFaulty, \
     delaysPrePrepareProcessing, \
     changesRequest
-from plenum.test.node_request.node_request_helper import checkPrePrepared
 
 nodeCount = 7
 # f + 1 faults, i.e, num of faults greater than system can tolerate
 faultyNodes = 3
+
 whitelist = ['InvalidSignature',
              'cannot process incoming PREPARE']
-delayPrePrepareSec = 60
 
 
 @pytest.fixture(scope="module")
 def setup(txnPoolNodeSet):
     # Making nodes faulty such that no primary is chosen
-    E = txnPoolNodeSet[-3]
-    G = txnPoolNodeSet[-2]
-    Z = txnPoolNodeSet[-1]
-    for node in E, G, Z:
-        makeNodeFaulty(node,
-                       changesRequest, partial(delaysPrePrepareProcessing,
-                                               delay=delayPrePrepareSec))
-    return adict(faulties=(E, G, Z))
+    A = txnPoolNodeSet[-3]
+    B = txnPoolNodeSet[-2]
+    G = txnPoolNodeSet[-1]
+    for node in A, B, G:
+        makeNodeFaulty(
+            node, changesRequest, partial(
+                delaysPrePrepareProcessing, delay=90))
+        # Delaying nomination to avoid becoming primary
+        # node.delaySelfNomination(10)
+    return adict(faulties=(A, B, G))
 
 
 @pytest.fixture(scope="module")
 def afterElection(setup):
     for n in setup.faulties:
         for r in n.replicas.values():
             assert not r.isPrimary
 
 
-@pytest.fixture(scope="module")
-def preprepared1WithDelay(looper, txnPoolNodeSet, propagated1, faultyNodes):
-    timeouts = waits.expectedPrePrepareTime(len(txnPoolNodeSet)) + delayPrePrepareSec
-    checkPrePrepared(looper,
-                     txnPoolNodeSet,
-                     propagated1,
-                     range(getNoInstances(len(txnPoolNodeSet))),
-                     faultyNodes,
-                     timeout=timeouts)
-
-
-def testNumOfPrepareWithFPlusOneFaults(
-        afterElection, noRetryReq, preprepared1WithDelay):
-    pass
+def testNumOfCommitMsgsWithFPlusOneFaults(afterElection, looper,
+                                          txnPoolNodeSet, prepared1, noRetryReq):
+    with pytest.raises(AssertionError):
+        # To raise an error pass less than the actual number of faults
+        checkCommitted(looper,
+                       txnPoolNodeSet,
+                       prepared1,
+                       range(getNoInstances(len(txnPoolNodeSet))),
+                       faultyNodes - 1)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_num_commit_with_2_of_6_faulty.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_num_commit_with_2_of_6_faulty.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_num_of_sufficient_commit.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_prepare/test_num_of_prepare_with_one_fault.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,35 +1,31 @@
 from functools import partial
 
 import pytest
 
-from stp_core.common.util import adict
-
 from plenum.test.malicious_behaviors_node import makeNodeFaulty, \
     delaysPrePrepareProcessing
+from stp_core.common.util import adict
 
-nodeCount = 7
-faultyNodes = 2
+nodeCount = 4
+faultyNodes = 1
 whitelist = ['cannot process incoming PREPARE']
 
 
 @pytest.fixture(scope="module")
 def setup(txnPoolNodeSet):
     # Making nodes faulty such that no primary is chosen
-    A = txnPoolNodeSet[-2]
-    B = txnPoolNodeSet[-1]
-    # Delay processing of PRE-PREPARE messages by Alpha and Beta for 90
-    # seconds since the timeout for checking sufficient commits is 60 seconds
-    makeNodeFaulty(A, partial(delaysPrePrepareProcessing, delay=90))
-    makeNodeFaulty(B, partial(delaysPrePrepareProcessing, delay=90))
-    return adict(faulties=(A, B))
+    G = txnPoolNodeSet[-1]
+    # Delaying nomination to avoid becoming primary
+    # G.delaySelfNomination(10)
+    makeNodeFaulty(G,
+                   partial(delaysPrePrepareProcessing, delay=60))
+    return adict(faulty=G)
 
 
 @pytest.fixture(scope="module")
 def afterElection(setup):
-    for n in setup.faulties:
-        for r in n.replicas.values():
-            assert not r.isPrimary
+    assert not setup.faulty.hasPrimary
 
 
-def testNumOfSufficientCommitMsgs(afterElection, committed1):
+def testNumOfPrepareWithOneFault(afterElection, prepared1):
     pass
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_num_commit_with_one_fault.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_num_commit_with_one_fault.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_num_of_commit_with_f_plus_one_faults.py` & `indy-plenum-1.9.2rc1/plenum/test/watermarks/test_watermarks_after_view_change.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,48 +1,58 @@
-from functools import partial
-
 import pytest
 
-from plenum.common.util import getNoInstances
-from stp_core.common.util import adict
-from plenum.test.node_request.node_request_helper import checkCommitted
-from plenum.test.malicious_behaviors_node import makeNodeFaulty, \
-    delaysPrePrepareProcessing, \
-    changesRequest
-
-nodeCount = 7
-# f + 1 faults, i.e, num of faults greater than system can tolerate
-faultyNodes = 3
-
-whitelist = ['InvalidSignature',
-             'cannot process incoming PREPARE']
-
-
-@pytest.fixture(scope="module")
-def setup(txnPoolNodeSet):
-    # Making nodes faulty such that no primary is chosen
-    A = txnPoolNodeSet[-3]
-    B = txnPoolNodeSet[-2]
-    G = txnPoolNodeSet[-1]
-    for node in A, B, G:
-        makeNodeFaulty(
-            node, changesRequest, partial(
-                delaysPrePrepareProcessing, delay=90))
-    return adict(faulties=(A, B, G))
-
-
-@pytest.fixture(scope="module")
-def afterElection(setup):
-    for n in setup.faulties:
-        for r in n.replicas.values():
-            assert not r.isPrimary
-
-
-def testNumOfCommitMsgsWithFPlusOneFaults(afterElection, looper,
-                                          txnPoolNodeSet, prepared1, noRetryReq):
-    with pytest.raises(AssertionError):
-        # To raise an error pass less than the actual number of faults
-        checkCommitted(looper,
-                       txnPoolNodeSet,
-                       prepared1,
-                       range(getNoInstances(len(txnPoolNodeSet))),
-                       faultyNodes - 1)
+from plenum.test import waits
+from plenum.test.delayers import cDelay, chk_delay, icDelay, vcd_delay
+from plenum.test.helper import sdk_send_random_and_check, waitForViewChange
+from plenum.test.node_catchup.helper import ensure_all_nodes_have_same_data
+from plenum.test.stasher import delay_rules
+
+CHK_FREQ = 2
+LOG_SIZE = 2 * CHK_FREQ
+
+Max3PCBatchSize = 1
+
+
+@pytest.fixture(scope='module')
+def tconf(tconf):
+    old_max_3pc_batch_size = tconf.Max3PCBatchSize
+    old_log_size = tconf.LOG_SIZE
+    old_chk_freq = tconf.CHK_FREQ
+    tconf.Max3PCBatchSize = Max3PCBatchSize
+    tconf.LOG_SIZE = LOG_SIZE
+    tconf.CHK_FREQ = CHK_FREQ
+
+    yield tconf
+    tconf.Max3PCBatchSize = old_max_3pc_batch_size
+    tconf.LOG_SIZE = old_log_size
+    tconf.CHK_FREQ = old_chk_freq
+
+
+def test_watermarks_after_view_change(tdir, tconf,
+                                      looper,
+                                      txnPoolNodeSet,
+                                      sdk_pool_handle,
+                                      sdk_wallet_client):
+    """
+    Delay commit, checkpoint, InstanceChange and ViewChangeDone messages for lagging_node.
+    Start ViewChange.
+    Check that ViewChange finished.
+    Reset delays.
+    Check that lagging_node can order transactions and has same data with other nodes.
+    """
+    lagging_node = txnPoolNodeSet[-1]
+    lagging_node.master_replica.config.LOG_SIZE = LOG_SIZE
+    start_view_no = lagging_node.viewNo
+    with delay_rules(lagging_node.nodeIbStasher, cDelay(), chk_delay(), icDelay(), vcd_delay()):
+        for n in txnPoolNodeSet:
+            n.view_changer.on_master_degradation()
+        waitForViewChange(looper,
+                          txnPoolNodeSet[:-1],
+                          expectedViewNo=start_view_no + 1,
+                          customTimeout=waits.expectedPoolViewChangeStartedTimeout(len(txnPoolNodeSet)))
+        ensure_all_nodes_have_same_data(looper, txnPoolNodeSet[:-1])
+        sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
+                                  sdk_wallet_client, 6)
+    ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
+    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
+                              sdk_wallet_client, 1)
+    ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_commits_without_prepares.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_commits_without_prepares.py`

 * *Files 11% similar despite different names*

```diff
@@ -18,14 +18,14 @@
     sdk_send_random_and_check(looper,
                               txnPoolNodeSet,
                               sdk_pool_handle,
                               sdk_wallet_client,
                               count=10)
 
     for node in other_nodes:
-        assert node.master_replica._ordering_service.prePrepares
-        assert node.master_replica._ordering_service.prepares
-        assert node.master_replica._ordering_service.commits
+        assert node.master_replica.prePrepares
+        assert node.master_replica.prepares
+        assert node.master_replica.commits
 
-    assert primary_node.master_replica._ordering_service.sentPrePrepares
-    assert not primary_node.master_replica._ordering_service.prepares
-    assert primary_node.master_replica._ordering_service.commits
+    assert primary_node.master_replica.sentPrePrepares
+    assert not primary_node.master_replica.prepares
+    assert primary_node.master_replica.commits
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_commits_dequeue_commits.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_commits_recvd_first.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,42 +1,36 @@
-from plenum.common.constants import PREPREPARE, PREPARE
 from plenum.common.util import check_if_all_equal_in_list
-from plenum.test.delayers import ppDelay, pDelay, msg_rep_delay
+from plenum.test.delayers import ppDelay, pDelay
 from plenum.test.node_catchup.helper import waitNodeDataEquality
-from plenum.test.stasher import delay_rules
 from plenum.test.test_node import getNonPrimaryReplicas
 from plenum.test.helper import sdk_send_batches_of_random_and_check
 
 
-def test_dequeue_and_validate_commits(looper, txnPoolNodeSet,
-                                      sdk_wallet_client, sdk_pool_handle):
+def test_commits_recvd_first(looper, txnPoolNodeSet,
+                             sdk_wallet_client, sdk_pool_handle):
     slow_node = [r.node for r in getNonPrimaryReplicas(txnPoolNodeSet, 0)][-1]
     other_nodes = [n for n in txnPoolNodeSet if n != slow_node]
     delay = 50
-    with delay_rules(slow_node.nodeIbStasher,
-                     pDelay(delay),
-                     msg_rep_delay(delay, [PREPARE, PREPREPARE])):
-        with delay_rules(slow_node.nodeIbStasher, ppDelay(delay)):
-
-            sdk_send_batches_of_random_and_check(looper,
-                                                 txnPoolNodeSet,
-                                                 sdk_pool_handle,
-                                                 sdk_wallet_client,
-                                                 num_reqs=1,
-                                                 num_batches=1)
-
-            assert not slow_node.master_replica._ordering_service.prePrepares
-            assert not slow_node.master_replica._ordering_service.prepares
-            assert not slow_node.master_replica._ordering_service.commits
-            assert len(slow_node.master_replica._ordering_service.commitsWaitingForPrepare) > 0
-
-        waitNodeDataEquality(looper, slow_node, *other_nodes)
-        assert check_if_all_equal_in_list([n.master_replica._ordering_service.ordered
-                                           for n in txnPoolNodeSet])
-
-        assert slow_node.master_replica._ordering_service.prePrepares
-        assert slow_node.master_replica._ordering_service.prepares
-        assert slow_node.master_replica._ordering_service.commits
-        assert not slow_node.master_replica._ordering_service.commitsWaitingForPrepare
+    slow_node.nodeIbStasher.delay(ppDelay(delay, 0))
+    slow_node.nodeIbStasher.delay(pDelay(delay, 0))
 
-        assert all(slow_node.master_replica.last_ordered_3pc == n.master_replica.last_ordered_3pc
-                   for n in other_nodes)
+    sdk_send_batches_of_random_and_check(looper,
+                                         txnPoolNodeSet,
+                                         sdk_pool_handle,
+                                         sdk_wallet_client,
+                                         num_reqs=20,
+                                         num_batches=4)
+
+    assert not slow_node.master_replica.prePrepares
+    assert not slow_node.master_replica.prepares
+    assert not slow_node.master_replica.commits
+    assert len(slow_node.master_replica.commitsWaitingForPrepare) > 0
+
+    slow_node.reset_delays_and_process_delayeds()
+    waitNodeDataEquality(looper, slow_node, *other_nodes)
+    assert check_if_all_equal_in_list([n.master_replica.ordered
+                                       for n in txnPoolNodeSet])
+
+    assert slow_node.master_replica.prePrepares
+    assert slow_node.master_replica.prepares
+    assert slow_node.master_replica.commits
+    assert not slow_node.master_replica.commitsWaitingForPrepare
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_commit/test_commits_recvd_first.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_commit/test_commits_dequeue_commits.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,36 +1,42 @@
+from plenum.common.constants import PREPREPARE, PREPARE
 from plenum.common.util import check_if_all_equal_in_list
-from plenum.test.delayers import ppDelay, pDelay
+from plenum.test.delayers import ppDelay, pDelay, msg_rep_delay
 from plenum.test.node_catchup.helper import waitNodeDataEquality
+from plenum.test.stasher import delay_rules
 from plenum.test.test_node import getNonPrimaryReplicas
 from plenum.test.helper import sdk_send_batches_of_random_and_check
 
 
-def test_commits_recvd_first(looper, txnPoolNodeSet,
-                             sdk_wallet_client, sdk_pool_handle):
+def test_dequeue_and_validate_commits(looper, txnPoolNodeSet,
+                                      sdk_wallet_client, sdk_pool_handle):
     slow_node = [r.node for r in getNonPrimaryReplicas(txnPoolNodeSet, 0)][-1]
     other_nodes = [n for n in txnPoolNodeSet if n != slow_node]
     delay = 50
-    slow_node.nodeIbStasher.delay(ppDelay(delay, 0))
-    slow_node.nodeIbStasher.delay(pDelay(delay, 0))
+    with delay_rules(slow_node.nodeIbStasher,
+                     pDelay(delay),
+                     msg_rep_delay(delay, [PREPARE, PREPREPARE])):
+        with delay_rules(slow_node.nodeIbStasher, ppDelay(delay)):
+
+            sdk_send_batches_of_random_and_check(looper,
+                                                 txnPoolNodeSet,
+                                                 sdk_pool_handle,
+                                                 sdk_wallet_client,
+                                                 num_reqs=1,
+                                                 num_batches=1)
+
+            assert not slow_node.master_replica.prePrepares
+            assert not slow_node.master_replica.prepares
+            assert not slow_node.master_replica.commits
+            assert len(slow_node.master_replica.commitsWaitingForPrepare) > 0
+
+        waitNodeDataEquality(looper, slow_node, *other_nodes)
+        assert check_if_all_equal_in_list([n.master_replica.ordered
+                                           for n in txnPoolNodeSet])
+
+        assert slow_node.master_replica.prePrepares
+        assert slow_node.master_replica.prepares
+        assert slow_node.master_replica.commits
+        assert not slow_node.master_replica.commitsWaitingForPrepare
 
-    sdk_send_batches_of_random_and_check(looper,
-                                         txnPoolNodeSet,
-                                         sdk_pool_handle,
-                                         sdk_wallet_client,
-                                         num_reqs=20,
-                                         num_batches=4)
-
-    assert not slow_node.master_replica._ordering_service.prePrepares
-    assert not slow_node.master_replica._ordering_service.prepares
-    assert not slow_node.master_replica._ordering_service.commits
-    assert len(slow_node.master_replica._ordering_service.commitsWaitingForPrepare) > 0
-
-    slow_node.reset_delays_and_process_delayeds()
-    waitNodeDataEquality(looper, slow_node, *other_nodes)
-    assert check_if_all_equal_in_list([n.master_replica._ordering_service.ordered
-                                       for n in txnPoolNodeSet])
-
-    assert slow_node.master_replica._ordering_service.prePrepares
-    assert slow_node.master_replica._ordering_service.prepares
-    assert slow_node.master_replica._ordering_service.commits
-    assert not slow_node.master_replica._ordering_service.commitsWaitingForPrepare
+        assert all(slow_node.master_replica.last_ordered_3pc == n.master_replica.last_ordered_3pc
+                   for n in other_nodes)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_1_node_got_only_preprepare.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_1_node_got_only_preprepare.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_msg_len_limit_large_enough.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_msg_len_limit_large_enough.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_request/test_already_processed_request.py` & `indy-plenum-1.9.2rc1/plenum/test/node_request/test_already_processed_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/conftest.py`

 * *Files 3% similar despite different names*

```diff
@@ -54,15 +54,15 @@
 from plenum.common.types import PLUGIN_TYPE_STATS_CONSUMER, f
 from plenum.common.util import getNoInstances, randomString
 from plenum.server.notifier_plugin_manager import PluginManager
 from plenum.test.helper import checkLastClientReqForNode, \
     waitForViewChange, requestReturnedToNode, randomText, \
     mockGetInstalledDistributions, mockImportModule, chk_all_funcs, \
     create_new_test_node, sdk_json_to_request_object, sdk_send_random_requests, \
-    sdk_get_and_check_replies, sdk_set_protocol_version, sdk_send_random_and_check, MockTimer, create_pool_txn_data
+    sdk_get_and_check_replies, sdk_set_protocol_version, sdk_send_random_and_check, MockTimer
 from plenum.test.node_request.node_request_helper import checkPrePrepared, \
     checkPropagated, checkPrepared, checkCommitted
 from plenum.test.plugin.helper import getPluginPath
 from plenum.test.test_node import TestNode, Pool, \
     checkNodesConnected, ensureElectionsDone, genNodeReg, getPrimaryReplica, \
     getNonPrimaryReplicas, TestNodeBootstrap
 from plenum.common.config_helper import PConfigHelper, PNodeConfigHelper
@@ -580,21 +580,86 @@
 @pytest.fixture(scope="module")
 def dirName():
     return os.path.dirname
 
 
 @pytest.fixture(scope="module")
 def poolTxnData(request):
-    node_count = getValueFromModule(request, "nodeCount", 4)
-    nodes_with_bls = getValueFromModule(request, "nodes_wth_bls", node_count)
-    node_names = genNodeNames(node_count)
-    return create_pool_txn_data(node_names=node_names,
-                                crypto_factory=create_default_bls_crypto_factory(),
-                                get_free_port=lambda: genHa()[1],
-                                nodes_with_bls=nodes_with_bls)
+    nodeCount = getValueFromModule(request, "nodeCount", 4)
+    nodes_with_bls = getValueFromModule(request, "nodes_wth_bls", nodeCount)
+    data = {'txns': [], 'seeds': {}, 'nodesWithBls': {}}
+    for i, node_name in zip(range(1, nodeCount + 1), genNodeNames(nodeCount)):
+        data['seeds'][node_name] = node_name + '0' * (32 - len(node_name))
+        steward_name = 'Steward' + str(i)
+        data['seeds'][steward_name] = steward_name + \
+                                      '0' * (32 - len(steward_name))
+
+        n_idr = SimpleSigner(seed=data['seeds'][node_name].encode()).identifier
+        s_idr = DidSigner(seed=data['seeds'][steward_name].encode())
+
+        data['txns'].append(
+                Member.nym_txn(nym=s_idr.identifier,
+                               verkey=s_idr.verkey,
+                               role=STEWARD,
+                               name=steward_name,
+                               seq_no=i)
+        )
+
+        node_txn = Steward.node_txn(steward_nym=s_idr.identifier,
+                                    node_name=node_name,
+                                    nym=n_idr,
+                                    ip='127.0.0.1',
+                                    node_port=genHa()[1],
+                                    client_port=genHa()[1],
+                                    client_ip='127.0.0.1',
+                                    services=[VALIDATOR],
+                                    seq_no=i)
+
+        if i <= nodes_with_bls:
+            _, bls_key, bls_key_proof = create_default_bls_crypto_factory().generate_bls_keys(
+                seed=data['seeds'][node_name])
+            get_payload_data(node_txn)[DATA][BLS_KEY] = bls_key
+            get_payload_data(node_txn)[DATA][BLS_KEY_PROOF] = bls_key_proof
+            data['nodesWithBls'][node_name] = True
+
+        data['txns'].append(node_txn)
+
+    # Add 4 Trustees
+    for i in range(4):
+        trustee_name = 'Trs' + str(i)
+        data['seeds'][trustee_name] = trustee_name + '0' * (
+                32 - len(trustee_name))
+        t_sgnr = DidSigner(seed=data['seeds'][trustee_name].encode())
+        data['txns'].append(
+            Member.nym_txn(nym=t_sgnr.identifier,
+                           verkey=t_sgnr.verkey,
+                           role=TRUSTEE,
+                           name=trustee_name)
+        )
+
+    more_data_seeds = \
+        {
+            "Alice": "99999999999999999999999999999999",
+            "Jason": "bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb",
+            "John": "dddddddddddddddddddddddddddddddd",
+            "Les": "ffffffffffffffffffffffffffffffff"
+        }
+    more_data_users = []
+    for more_name, more_seed in more_data_seeds.items():
+        signer = DidSigner(seed=more_seed.encode())
+        more_data_users.append(
+            Member.nym_txn(nym=signer.identifier,
+                           verkey=signer.verkey,
+                           name=more_name,
+                           creator="5rArie7XKukPCaEwq5XGQJnM9Fc5aZE3M9HAPVfMU2xC")
+        )
+
+    data['txns'].extend(more_data_users)
+    data['seeds'].update(more_data_seeds)
+    return data
 
 
 @pytest.fixture(scope="module")
 def tdirWithPoolTxns(config_helper_class, poolTxnData, tdir, tconf):
     import getpass
     logging.debug("current user when creating new pool txn file: {}".
                   format(getpass.getuser()))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/malicious_behaviors_node.py` & `indy-plenum-1.9.2rc1/plenum/test/malicious_behaviors_node.py`

 * *Files 5% similar despite different names*

```diff
@@ -111,43 +111,43 @@
         node: TestNode,
         msgType: ThreePhaseMsg,
         count: int = 2,
         instId=None):
     def evilSendPrePrepareRequest(self, ppReq: PrePrepare):
         logger.debug("EVIL: Sending duplicate pre-prepare message: {}".
                      format(ppReq))
-        self._ordering_service.sentPrePrepares[self.viewNo, self.lastPrePrepareSeqNo] = ppReq
-        sendDup(self, ppReq, count)
+        self.sentPrePrepares[self.viewNo, self.lastPrePrepareSeqNo] = ppReq
+        sendDup(self, ppReq, TPCStat.PrePrepareSent, count)
 
     def evilSendPrepare(self, ppReq: PrePrepare):
         prepare = Prepare(self.instId,
                           ppReq.viewNo,
                           ppReq.ppSeqNo,
                           ppReq.ppTime,
                           ppReq.digest,
                           ppReq.stateRootHash,
                           ppReq.txnRootHash,
                           ppReq.auditTxnRootHash)
         logger.debug("EVIL: Creating prepare message for request {}: {}".
                      format(ppReq, prepare))
-        self._ordering_service._add_to_prepares(prepare, self.name)
-        sendDup(self, prepare, count)
+        self.addToPrepares(prepare, self.name)
+        sendDup(self, prepare, TPCStat.PrepareSent, count)
 
     def evilSendCommit(self, request):
         commit = Commit(self.instId,
                         request.viewNo,
                         request.ppSeqNo)
         logger.debug("EVIL: Creating commit message for request {}: {}".
                      format(request, commit))
-        self._ordering_service._add_to_commits(commit, self.name)
-        sendDup(self, commit, count)
+        self.addToCommits(commit, self.name)
+        sendDup(self, commit, TPCStat.CommitSent, count)
 
-    def sendDup(sender, msg, count: int):
+    def sendDup(sender, msg, stat, count: int):
         for i in range(count):
-            sender.send(msg)
+            sender.send(msg, stat)
 
     methodMap = {
         PrePrepare: evilSendPrePrepareRequest,
         Prepare: evilSendPrepare,
         Commit: evilSendCommit
     }
 
@@ -159,19 +159,19 @@
 
 
 def malign3PhaseSendingMethod(replica: TestReplica, msgType: ThreePhaseMsg,
                               evilMethod):
     evilMethod = types.MethodType(evilMethod, replica)
 
     if msgType == PrePrepare:
-        replica._ordering_service.send_pre_prepare = evilMethod
+        replica.sendPrePrepare = evilMethod
     elif msgType == Prepare:
-        replica._ordering_service._do_prepare = evilMethod
+        replica.doPrepare = evilMethod
     elif msgType == Commit:
-        replica._ordering_service._do_commit = evilMethod
+        replica.doCommit = evilMethod
     else:
         common.error.error("Not a 3 phase message")
 
 
 def malignInstancesOfNode(node: TestNode, malignMethod, instId: int = None):
     if instId is not None:
         malignMethod(replica=node.replicas[instId])
@@ -184,39 +184,39 @@
 
 def send3PhaseMsgWithIncorrectDigest(node: TestNode, msgType: ThreePhaseMsg,
                                      instId: int = None):
     def evilSendPrePrepareRequest(self, ppReq: PrePrepare):
         logger.debug("EVIL: Creating pre-prepare message for request : {}".
                      format(ppReq))
         ppReq = updateNamedTuple(ppReq, digest=ppReq.digest + 'random')
-        self._ordering_service.sentPrePrepares[self.viewNo, self.lastPrePrepareSeqNo] = ppReq
-        self.send(ppReq)
+        self.sentPrePrepares[self.viewNo, self.lastPrePrepareSeqNo] = ppReq
+        self.send(ppReq, TPCStat.PrePrepareSent)
 
     def evilSendPrepare(self, ppReq):
         digest = "random"
         prepare = Prepare(self.instId,
                           ppReq.viewNo,
                           ppReq.ppSeqNo,
                           ppReq.ppTime,
                           digest,
                           ppReq.stateRootHash,
                           ppReq.txnRootHash)
         logger.debug("EVIL: Creating prepare message for request {}: {}".
                      format(ppReq, prepare))
-        self._ordering_service._add_to_prepares(prepare, self.name)
-        self.send(prepare)
+        self.addToPrepares(prepare, self.name)
+        self.send(prepare, TPCStat.PrepareSent)
 
     def evilSendCommit(self, request):
         commit = Commit(self.instId,
                         request.viewNo,
                         request.ppSeqNo)
         logger.debug("EVIL: Creating commit message for request {}: {}".
                      format(request, commit))
-        self.send(commit)
-        self._ordering_service._add_to_commits(commit, self.name)
+        self.send(commit, TPCStat.CommitSent)
+        self.addToCommits(commit, self.name)
 
     methodMap = {
         PrePrepare: evilSendPrePrepareRequest,
         Prepare: evilSendPrepare,
         Commit: evilSendCommit
     }
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_taa_aml_module.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_taa_aml_module.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_taa_aml_integration.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_taa_aml_integration.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/conftest.py`

 * *Files 1% similar despite different names*

```diff
@@ -108,16 +108,16 @@
 
 @pytest.fixture
 def max_last_accepted_pre_prepare_time(looper, txnPoolNodeSet):
     ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
     pp_times = []
     for node in txnPoolNodeSet:
         for replica in node.replicas.values():
-            if replica._ordering_service.last_accepted_pre_prepare_time:
-                pp_times.append(replica._ordering_service.last_accepted_pre_prepare_time)
+            if replica.last_accepted_pre_prepare_time:
+                pp_times.append(replica.last_accepted_pre_prepare_time)
     return max(pp_times)
 
 
 @pytest.fixture(params=ValidationType, ids=lambda x: x.name)
 def validation_type(request):
     return request.param
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/test_taa_not_set.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/test_taa_not_set.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/test_taa_acceptance_integration_validation.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/test_taa_acceptance_integration_validation.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/acceptance/test_taa_acceptance_validation.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/acceptance/test_taa_acceptance_validation.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_config_req_handler_taa_utils.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_config_req_handler_taa_utils.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_txn_author_agreement.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_txn_author_agreement.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_get_txn_author_agreement.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_get_txn_author_agreement.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_get_empty_txn_author_agreement.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_get_empty_txn_author_agreement.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 
 
 @pytest.fixture(scope='module')
 def nodeSetWithoutTaaAlwaysResponding(txnPoolNodeSet, looper):
     global TIMESTAMP_NONE
 
     # Simulate freshness update
-    txnPoolNodeSet[0].master_replica._ordering_service._do_send_3pc_batch(ledger_id=CONFIG_LEDGER_ID)
+    txnPoolNodeSet[0].master_replica._do_send_3pc_batch(ledger_id=CONFIG_LEDGER_ID)
 
     looper.runFor(1)  # Make sure we have long enough gap between updates
     TIMESTAMP_NONE = get_utc_epoch()
 
     return txnPoolNodeSet
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/txn_author_agreement/test_get_taa_aml.py` & `indy-plenum-1.9.2rc1/plenum/test/txn_author_agreement/test_get_taa_aml.py`

 * *Files 0% similar despite different names*

```diff
@@ -42,15 +42,15 @@
 
 @pytest.fixture(scope='module')
 def nodeSetWithTaaAlwaysResponding(txnPoolNodeSet, looper, sdk_pool_handle,
                                    sdk_wallet_trustee):
     global TIMESTAMP_V1, TIMESTAMP_V2
 
     # Force signing empty config state
-    txnPoolNodeSet[0].master_replica._ordering_service._do_send_3pc_batch(ledger_id=CONFIG_LEDGER_ID)
+    txnPoolNodeSet[0].master_replica._do_send_3pc_batch(ledger_id=CONFIG_LEDGER_ID)
 
     looper.runFor(3)  # Make sure we have long enough gap between updates
     reply = send_aml_request(looper, sdk_wallet_trustee, sdk_pool_handle, version=V1, aml=json.dumps(AML1),
                              context=CONTEXT1)
     TIMESTAMP_V1 = reply[1]['result'][TXN_METADATA][TXN_METADATA_TIME]
 
     looper.runFor(3)  # Make sure we have long enough gap between updates
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/profiler.py` & `indy-plenum-1.9.2rc1/plenum/test/profiler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_round_trip_with_one_faulty_node.py` & `indy-plenum-1.9.2rc1/plenum/test/test_round_trip_with_one_faulty_node.py`

 * *Files 19% similar despite different names*

```diff
@@ -31,14 +31,17 @@
     node = txnPoolNodeSet[3]
     epp = types.MethodType(evilProcessPropagate, node)
     node.nodeMsgRouter.routes[Propagate] = epp
     node.processPropagate = epp
 
     node.propagate = types.MethodType(evilPropagateRequest, node)
 
+    # we don't want `node` being a primary (another test?)
+    # nodes.Alpha.delaySelfNomination(100)
+
     return node
 
 
 # noinspection PyIncorrectDocstring
 def testRequestFullRoundTrip(node_doesnt_propagate, replied1):
     """
     With an Alpha that doesn't send propagate requests, the request should
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_memory_consumpion.py` & `indy-plenum-1.9.2rc1/plenum/test/test_memory_consumpion.py`

 * *Files 10% similar despite different names*

```diff
@@ -23,12 +23,12 @@
         logger.debug("{} sent {} requests".format(nym, numRequests))
 
     for node in txnPoolNodeSet:
         logger.debug("{} has requests {} with size {}".
                      format(node, len(node.requests), get_size(node.requests)))
         for replica in node.replicas.values():
             logger.debug("{} has prepares {} with size {}".
-                         format(replica, len(replica._ordering_service.prepares),
-                                get_size(replica._ordering_service.prepares)))
+                         format(replica, len(replica.prepares),
+                                get_size(replica.prepares)))
             logger.debug("{} has commits {} with size {}".
-                         format(replica, len(replica._ordering_service.commits),
-                                get_size(replica._ordering_service.commits)))
+                         format(replica, len(replica.commits),
+                                get_size(replica.commits)))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/blacklist/test_blacklist_client.py` & `indy-plenum-1.9.2rc1/plenum/test/blacklist/test_blacklist_client.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_req_authenticator.py` & `indy-plenum-1.9.2rc1/plenum/test/test_req_authenticator.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/tracker/test_ledger_uncommitted_tracker.py` & `indy-plenum-1.9.2rc1/plenum/test/tracker/test_ledger_uncommitted_tracker.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_delay.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_no_view_change_while_catchup.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,44 +1,59 @@
 import pytest
 
-from stp_core.loop.eventually import eventually, slowFactor
-from stp_core.common.log import getlogger
-from stp_core.loop.looper import Looper
-from plenum.server.node import Node
-from plenum.test import waits
-from plenum.test.delayers import delayerMsgTuple
-from plenum.test.helper import sendMessageAndCheckDelivery, addNodeBack, assertExp
-from plenum.test.msgs import TestMsg
-from plenum.test.test_node import TestNodeSet, checkNodesConnected, \
-    ensureElectionsDone, prepareNodeSet
-
-logger = getlogger()
-
-nodeCount = 2
-
-
-@pytest.mark.skipif('sys.platform == "win32"', reason='SOV-457')
-def testTestNodeDelay(looper, txnPoolNodeSet):
-    looper.run(checkNodesConnected(txnPoolNodeSet))
-    nodeA = txnPoolNodeSet[0]
-    nodeB = txnPoolNodeSet[1]
-    # send one message, without delay
-    looper.run(sendMessageAndCheckDelivery(nodeA, nodeB))
-
-    # set delay, then send another message
-    # and find that it doesn't arrive
-    delay = 5 * waits.expectedNodeToNodeMessageDeliveryTime()
-    nodeB.nodeIbStasher.delay(
-        delayerMsgTuple(delay, TestMsg, nodeA.name)
-    )
-    with pytest.raises(AssertionError):
-        looper.run(sendMessageAndCheckDelivery(nodeA, nodeB))
-
-    # but then find that it arrives after the delay
-    # duration has passed
-    timeout = waits.expectedNodeToNodeMessageDeliveryTime() + delay
-    looper.run(sendMessageAndCheckDelivery(nodeA, nodeB,
-                                           customTimeout=timeout))
-
-    # reset the delay, and find another message comes quickly
-    nodeB.nodeIbStasher.reset_delays_and_process_delayeds()
-    looper.run(sendMessageAndCheckDelivery(nodeA, nodeB))
+from plenum.common.messages.node_messages import InstanceChange
+from plenum.common.startable import Mode
+from plenum.test.helper import checkViewNoForNodes, waitForViewChange
+from plenum.test.test_node import ensureElectionsDone
+from plenum.test.view_change.helper import do_view_change, revert_do_view_change
+from plenum.test.waits import expectedPoolViewChangeStartedTimeout
+from stp_core.loop.eventually import eventually
+
+
+@pytest.fixture(params=[Mode.starting, Mode.discovering, Mode.discovered, Mode.syncing])
+def mode(request):
+    return request.param
+
+
+def check_instance_change_count(nodes, expected_count):
+    for node in nodes:
+        ic_count = sum(1 for msg in node.view_changer.inBox if isinstance(msg[0], InstanceChange))
+        assert expected_count == ic_count
+
+
+def try_view_change(looper, nodes):
+    for node in nodes:
+        looper.run(eventually(node.view_changer.serviceQueues))
+
+
+def check_no_view_change(looper, nodes):
+    looper.run(eventually(check_instance_change_count, nodes, 3,
+                          timeout=expectedPoolViewChangeStartedTimeout(len(nodes))))
+    try_view_change(looper, nodes)
+    check_instance_change_count(nodes, 3)
+
+
+def test_no_view_change_until_synced(txnPoolNodeSet, looper, mode):
+    # emulate catchup by setting non-synced status
+    for node in txnPoolNodeSet:
+        node.mode = mode
+
+    check_instance_change_count(txnPoolNodeSet, 0)
+
+    # start View Change
+    old_view_no = checkViewNoForNodes(txnPoolNodeSet)
+    old_meths = do_view_change(txnPoolNodeSet)
+    for node in txnPoolNodeSet:
+        node.view_changer.sendInstanceChange(old_view_no + 1)
+
+    # make sure View Change is not started
+    check_no_view_change(looper, txnPoolNodeSet)
+    assert old_view_no == checkViewNoForNodes(txnPoolNodeSet)
+
+    # emulate finishing of catchup by setting Participating status
+    revert_do_view_change(txnPoolNodeSet, old_meths)
+    for node in txnPoolNodeSet:
+        node.mode = Mode.participating
+
+    # make sure that View Change happened
+    waitForViewChange(looper, txnPoolNodeSet, expectedViewNo=old_view_no + 1)
+    ensureElectionsDone(looper=looper, nodes=txnPoolNodeSet)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_stashing_queue.py` & `indy-plenum-1.9.2rc1/plenum/test/test_stashing_queue.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/deep_eq.py` & `indy-plenum-1.9.2rc1/plenum/test/deep_eq.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_request_executed_once_and_without_failing_behind.py` & `indy-plenum-1.9.2rc1/plenum/test/test_request_executed_once_and_without_failing_behind.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_get_txn_state_proof.py` & `indy-plenum-1.9.2rc1/plenum/test/test_get_txn_state_proof.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/recorder/test_node_msgs_recording.py` & `indy-plenum-1.9.2rc1/plenum/test/recorder/test_node_msgs_recording.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/recorder/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/recorder/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/recorder/test_replayer.py` & `indy-plenum-1.9.2rc1/plenum/test/recorder/test_replayer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/recorder/test_recorder.py` & `indy-plenum-1.9.2rc1/plenum/test/recorder/test_recorder.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/recorder/test_combined_recorder.py` & `indy-plenum-1.9.2rc1/plenum/test/recorder/test_combined_recorder.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/recorder/test_replay_on_new_node.py` & `indy-plenum-1.9.2rc1/plenum/test/recorder/test_replay_on_new_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/recorder/test_replay_node_bouncing.py` & `indy-plenum-1.9.2rc1/plenum/test/recorder/test_replay_node_bouncing.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/recorder/test_replay_with_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/recorder/test_replay_with_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/recorder/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/recorder/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/recorder/test_replay.py` & `indy-plenum-1.9.2rc1/plenum/test/recorder/test_replay.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/instances/test_multiple_pre_prepare.py` & `indy-plenum-1.9.2rc1/plenum/test/instances/test_multiple_pre_prepare.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/instances/test_msgs_from_slow_instances.py` & `indy-plenum-1.9.2rc1/plenum/test/instances/test_msgs_from_slow_instances.py`

 * *Files 5% similar despite different names*

```diff
@@ -29,15 +29,15 @@
                              sdk_pool_handle, sdk_wallet_client):
     A, B, C, D = configNodeSet
 
     sdk_send_random_request(looper, sdk_pool_handle, sdk_wallet_client)
 
     def getCommits(node: TestNode, instId: int):
         replica = node.replicas[instId]  # type: Replica
-        return list(replica._ordering_service.commits.values())
+        return list(replica.commits.values())
 
     def checkPresence():
         for node in [C, D]:
             commReqs = getCommits(node, 0)
             assert len(commReqs) > 0
             assert Replica.generateName(A.name, 0) not in commReqs[0][0]
             commReqs = getCommits(node, 1)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/instances/test_multiple_instance_change_msgs.py` & `indy-plenum-1.9.2rc1/plenum/test/instances/test_multiple_instance_change_msgs.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/instances/test_multiple_commit.py` & `indy-plenum-1.9.2rc1/plenum/test/instances/test_multiple_commit.py`

 * *Files 3% similar despite different names*

```diff
@@ -33,15 +33,15 @@
                   Suspicions.DUPLICATE_CM_SENT.code)
 
     # If the request is ordered then COMMIT will be rejected much earlier
     for r in [primaryRep, *nonPrimaryReps]:
         def do_nothing(self, commit):
             pass
 
-        r._ordering_service._do_order = types.MethodType(do_nothing, r)
+        r.doOrder = types.MethodType(do_nothing, r)
 
     return adict(primaryRep=primaryRep, nonPrimaryReps=nonPrimaryReps,
                  faultyRep=faultyRep)
 
 
 # noinspection PyIncorrectDocstring,PyUnusedLocal,PyShadowingNames
 def testMultipleCommit(setup, looper, sent1):
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/instances/test_multiple_prepare.py` & `indy-plenum-1.9.2rc1/plenum/test/instances/test_multiple_prepare.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/instances/test_prepare_digest.py` & `indy-plenum-1.9.2rc1/plenum/test/instances/test_prepare_digest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/instances/test_pre_prepare_digest.py` & `indy-plenum-1.9.2rc1/plenum/test/instances/test_pre_prepare_digest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/instances/test_instance_cannot_become_active_with_less_than_four_servers.py` & `indy-plenum-1.9.2rc1/plenum/test/instances/test_instance_cannot_become_active_with_less_than_four_servers.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/server/test_instance_change_provider.py` & `indy-plenum-1.9.2rc1/plenum/test/server/test_instance_change_provider.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_config_helper.py` & `indy-plenum-1.9.2rc1/plenum/test/test_config_helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/grouped_load_scheduling.py` & `indy-plenum-1.9.2rc1/plenum/test/grouped_load_scheduling.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/client/test_protocol_version.py` & `indy-plenum-1.9.2rc1/plenum/test/client/test_protocol_version.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/client/test_client.py` & `indy-plenum-1.9.2rc1/plenum/test/client/test_client.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_not_depend_on_node_reg.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_not_depend_on_node_reg.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 def test_bls_not_depend_on_node_reg(looper, txnPoolNodeSet,
                                     sdk_pool_handle, sdk_wallet_client):
     sdk_send_batches_of_random_and_check(looper, txnPoolNodeSet,
                                          sdk_pool_handle, sdk_wallet_client, 3, 3)
 
     node = txnPoolNodeSet[2]
     last_pre_prepare = \
-        node.master_replica._ordering_service.prePrepares[node.master_replica.last_ordered_3pc]
+        node.master_replica.prePrepares[node.master_replica.last_ordered_3pc]
 
     bls = getattr(last_pre_prepare, f.BLS_MULTI_SIG.nm)
 
     # Get random participant
     node_name = next(iter(bls[1]))
 
     # We've removed one of the nodes from another node's log
@@ -76,11 +76,11 @@
         return node.master_replica._bls_bft_replica._bls_bft.bls_key_register._current_bls_keys
 
     assert get_current_bls_keys(restarted_node) == get_current_bls_keys(primary_node)
 
 
 def get_last_ordered_state_root_hash(node):
     last_pre_prepare = \
-        node.master_replica._ordering_service.prePrepares[node.master_replica.last_ordered_3pc]
+        node.master_replica.prePrepares[node.master_replica.last_ordered_3pc]
     multi_sig = MultiSignature.from_list(*last_pre_prepare.blsMultiSig)
     state_root_hash = serializer.deserialize(multi_sig.value.pool_state_root_hash)
     return state_root_hash
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_key_registry_pool_manager.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_key_registry_pool_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_no_state_proof.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_no_state_proof.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_send_txns_bls_consensus.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_send_txns_bls_consensus.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_commit_signature_validation_integration.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_commit_signature_validation_integration.py`

 * *Files 3% similar despite different names*

```diff
@@ -77,15 +77,15 @@
         request1 = sdk_sign_and_send_prepared_request(looper, new_steward_wallet_handle,
                                                       sdk_pool_handle, node_request)
 
         key1 = get_key_from_req(request1[0])
 
         def check_nodes_receive_pp(view_no, seq_no):
             for node in txnPoolNodeSet:
-                assert node.master_replica._ordering_service.get_preprepare(view_no, seq_no)
+                assert node.master_replica.getPrePrepare(view_no, seq_no)
 
         looper.run(eventually(check_nodes_receive_pp, first_ordered[0], first_ordered[1] + 1))
 
         def check_fast_nodes_ordered_request():
             for n in fast_nodes:
                 assert key1 not in n.requests or n.requests[key1].executed
             for n in slow_nodes:
@@ -94,12 +94,12 @@
         looper.run(eventually(check_fast_nodes_ordered_request))
 
         request2 = sdk_send_random_request(looper, sdk_pool_handle, sdk_wallet_client)
         looper.run(eventually(check_nodes_receive_pp, first_ordered[0], first_ordered[1] + 2))
 
         def check_nodes_receive_commits(view_no, seq_no):
             for node in txnPoolNodeSet:
-                assert len(node.master_replica._ordering_service.commits[view_no, seq_no].voters) >= node.f + 1
+                assert len(node.master_replica.commits[view_no, seq_no].voters) >= node.f + 1
         looper.run(eventually(check_nodes_receive_commits, first_ordered[0], first_ordered[1] + 2))
 
     sdk_get_and_check_replies(looper, [request1])
     sdk_get_and_check_replies(looper, [request2])
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_sign_validation_for_key_proof_exist_ordering.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_sign_validation_for_key_proof_exist_ordering.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_send_txns_full_bls.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_send_txns_full_bls.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_send_txns_bls_less_than_consensus.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_send_txns_bls_less_than_consensus.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_get_state_proof.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_get_state_proof.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_add_incorrect_bls_key.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_add_incorrect_bls_key.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_update_bls_key.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_update_bls_key.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_crypto_factory.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_crypto_factory.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_sign_validation_for_key_proof_exist.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_sign_validation_for_key_proof_exist.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_store.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_bft_replica.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_bft_replica.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_send_txns_no_bls.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_send_txns_no_bls.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_bft_factory.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_bft_factory.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_bls_key_manager_file.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_bls_key_manager_file.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_add_bls_key.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_update_incorrect_bls_key.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,90 +1,76 @@
-import functools
-
-from plenum.server.consensus.consensus_shared_data import ConsensusSharedData
 from plenum.server.quorums import Quorum
 from plenum.test.bls.helper import check_update_bls_key
 
 nodeCount = 4
-nodes_wth_bls = 0
+nodes_wth_bls = 4
 
 
 # As we use tests with Module scope, results from previous tests are accumulated, so
 # rotating BLS keys one by one, eventually we will have all keys changed
 
-def test_add_bls_one_node(looper,
-                          txnPoolNodeSet,
-                          sdk_pool_handle,
-                          sdk_wallet_stewards, sdk_wallet_client):
+def test_update_incorrect_bls_one_node(looper, txnPoolNodeSet,
+                                       sdk_wallet_stewards,
+                                       sdk_wallet_client,
+                                       sdk_pool_handle):
     '''
-    Added BLS key for 1st Node;
-    do not expect that BLS multi-sigs are applied since no consensus (n-f)
+    Updated with wrong BLS key for 1st Node;
+    Expect that BLS multi-sigs are applied since we have 3 correct signatures
     '''
-    check_update_bls_key(node_num=0,
-                         saved_multi_sigs_count=0,
+    # make sure that we have commits from all nodes, and have 3 of 4 (n-f) BLS sigs there is enough
+    # otherwise we may have 3 commits, but 1 of them may be without BLS, so we will Order this txn, but without multi-sig
+    for node in txnPoolNodeSet:
+        node.quorums.commit = Quorum(nodeCount)
+    check_update_bls_key(node_num=0, saved_multi_sigs_count=4,
                          looper=looper, txnPoolNodeSet=txnPoolNodeSet,
                          sdk_wallet_stewards=sdk_wallet_stewards,
                          sdk_wallet_client=sdk_wallet_client,
-                         sdk_pool_handle=sdk_pool_handle)
+                         sdk_pool_handle=sdk_pool_handle,
+                         add_wrong=True)
 
 
-def test_add_bls_two_nodes(looper,
-                           txnPoolNodeSet,
-                           sdk_pool_handle,
-                           sdk_wallet_stewards,
-                           sdk_wallet_client):
+def test_update_incorrect_bls_two_nodes(looper, txnPoolNodeSet,
+                                        sdk_wallet_stewards,
+                                        sdk_wallet_client,
+                                        sdk_pool_handle):
     '''
-    Added BLS key for 1st and 2d Nodes;
-    do not expect that BLS multi-sigs are applied since no consensus (n-f)
+    Updated with wrong BLS key for 1st and 2d Nodes;
+    do not expect that BLS multi-sigs are applied (we have less than n-f correct BLS sigs)
     '''
-    check_update_bls_key(node_num=1,
-                         saved_multi_sigs_count=0,
+    check_update_bls_key(node_num=1, saved_multi_sigs_count=0,
                          looper=looper, txnPoolNodeSet=txnPoolNodeSet,
                          sdk_wallet_stewards=sdk_wallet_stewards,
                          sdk_wallet_client=sdk_wallet_client,
-                         sdk_pool_handle=sdk_pool_handle)
+                         sdk_pool_handle=sdk_pool_handle,
+                         add_wrong=True)
 
 
-def test_add_bls_three_nodes(looper,
-                             txnPoolNodeSet,
-                             sdk_pool_handle,
-                             sdk_wallet_stewards,
-                             sdk_wallet_client):
+def test_update_incorrect_bls_three_nodes(looper, txnPoolNodeSet,
+                                          sdk_wallet_stewards,
+                                          sdk_wallet_client,
+                                          sdk_pool_handle):
     '''
-    Added BLS key for 1st, 2d and 3d Nodes;
-    expect that BLS multi-sigs are applied since we have consensus now (3=n-f)
+    Updated with wrong BLS keys 1-3 Nodes;
+    do not expect that BLS multi-sigs are applied (we have less than n-f correct BLS sigs)
     '''
-    # make sure that we have commits from all nodes, and have 3 of 4 (n-f) BLS sigs there is enough
-    # otherwise we may have 3 commits, but 1 of them may be without BLS, so we will Order this txn, but without multi-sig
-
-    def patched_set_validators(self, validators):
-        ConsensusSharedData.set_validators(self, validators)
-        self.quorums.commit = Quorum(nodeCount)
-
-    for node in txnPoolNodeSet:
-        for r in node.replicas.values():
-            r._consensus_data.quorums.commit = Quorum(nodeCount)
-            r._consensus_data.set_validators = functools.partial(patched_set_validators, r._consensus_data)
-        node.quorums.commit = Quorum(nodeCount)
-    check_update_bls_key(node_num=2,
-                         saved_multi_sigs_count=4,
+    check_update_bls_key(node_num=2, saved_multi_sigs_count=0,
                          looper=looper, txnPoolNodeSet=txnPoolNodeSet,
                          sdk_wallet_stewards=sdk_wallet_stewards,
                          sdk_wallet_client=sdk_wallet_client,
-                         sdk_pool_handle=sdk_pool_handle)
+                         sdk_pool_handle=sdk_pool_handle,
+                         add_wrong=True)
 
 
-def test_add_bls_all_nodes(looper,
-                           txnPoolNodeSet,
-                           sdk_pool_handle,
-                           sdk_wallet_stewards,
-                           sdk_wallet_client):
+def test_update_incorrect_bls_all_nodes(looper, txnPoolNodeSet,
+                                        sdk_wallet_stewards,
+                                        sdk_wallet_client,
+                                        sdk_pool_handle):
     '''
-    Eventually added BLS key for all Nodes;
-    expect that BLS multi-sigs are applied since we have consensus now (4 > n-f)
+    Updated with wrong BLS keys all Nodes;
+    do not expect that BLS multi-sigs are applied (we have less than n-f correct BLS sigs)
     '''
-    check_update_bls_key(node_num=3,
-                         saved_multi_sigs_count=4,
+    check_update_bls_key(node_num=3, saved_multi_sigs_count=0,
                          looper=looper, txnPoolNodeSet=txnPoolNodeSet,
                          sdk_wallet_stewards=sdk_wallet_stewards,
                          sdk_wallet_client=sdk_wallet_client,
-                         sdk_pool_handle=sdk_pool_handle)
+                         sdk_pool_handle=sdk_pool_handle,
+                         add_wrong=True)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_state_proof.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_state_proof.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_update_incorrect_bls_key.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_add_bls_key.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,86 +1,79 @@
-import functools
-
-from plenum.server.consensus.consensus_shared_data import ConsensusSharedData
 from plenum.server.quorums import Quorum
 from plenum.test.bls.helper import check_update_bls_key
 
 nodeCount = 4
-nodes_wth_bls = 4
+nodes_wth_bls = 0
 
 
 # As we use tests with Module scope, results from previous tests are accumulated, so
 # rotating BLS keys one by one, eventually we will have all keys changed
 
-def test_update_incorrect_bls_one_node(looper, txnPoolNodeSet,
-                                       sdk_wallet_stewards,
-                                       sdk_wallet_client,
-                                       sdk_pool_handle):
+def test_add_bls_one_node(looper,
+                          txnPoolNodeSet,
+                          sdk_pool_handle,
+                          sdk_wallet_stewards, sdk_wallet_client):
     '''
-    Updated with wrong BLS key for 1st Node;
-    Expect that BLS multi-sigs are applied since we have 3 correct signatures
+    Added BLS key for 1st Node;
+    do not expect that BLS multi-sigs are applied since no consensus (n-f)
     '''
-    # make sure that we have commits from all nodes, and have 3 of 4 (n-f) BLS sigs there is enough
-    # otherwise we may have 3 commits, but 1 of them may be without BLS, so we will Order this txn, but without multi-sig
-    def patched_set_validators(self, validators):
-        ConsensusSharedData.set_validators(self, validators)
-        self.quorums.commit = Quorum(nodeCount)
-
-    for node in txnPoolNodeSet:
-        for r in node.replicas.values():
-            r._consensus_data.quorums.commit = Quorum(nodeCount)
-            r._consensus_data.set_validators = functools.partial(patched_set_validators, r._consensus_data)
-        node.quorums.commit = Quorum(nodeCount)
-    check_update_bls_key(node_num=0, saved_multi_sigs_count=4,
+    check_update_bls_key(node_num=0,
+                         saved_multi_sigs_count=0,
                          looper=looper, txnPoolNodeSet=txnPoolNodeSet,
                          sdk_wallet_stewards=sdk_wallet_stewards,
                          sdk_wallet_client=sdk_wallet_client,
-                         sdk_pool_handle=sdk_pool_handle,
-                         add_wrong=True)
+                         sdk_pool_handle=sdk_pool_handle)
 
 
-def test_update_incorrect_bls_two_nodes(looper, txnPoolNodeSet,
-                                        sdk_wallet_stewards,
-                                        sdk_wallet_client,
-                                        sdk_pool_handle):
+def test_add_bls_two_nodes(looper,
+                           txnPoolNodeSet,
+                           sdk_pool_handle,
+                           sdk_wallet_stewards,
+                           sdk_wallet_client):
     '''
-    Updated with wrong BLS key for 1st and 2d Nodes;
-    do not expect that BLS multi-sigs are applied (we have less than n-f correct BLS sigs)
+    Added BLS key for 1st and 2d Nodes;
+    do not expect that BLS multi-sigs are applied since no consensus (n-f)
     '''
-    check_update_bls_key(node_num=1, saved_multi_sigs_count=0,
+    check_update_bls_key(node_num=1,
+                         saved_multi_sigs_count=0,
                          looper=looper, txnPoolNodeSet=txnPoolNodeSet,
                          sdk_wallet_stewards=sdk_wallet_stewards,
                          sdk_wallet_client=sdk_wallet_client,
-                         sdk_pool_handle=sdk_pool_handle,
-                         add_wrong=True)
+                         sdk_pool_handle=sdk_pool_handle)
 
 
-def test_update_incorrect_bls_three_nodes(looper, txnPoolNodeSet,
-                                          sdk_wallet_stewards,
-                                          sdk_wallet_client,
-                                          sdk_pool_handle):
+def test_add_bls_three_nodes(looper,
+                             txnPoolNodeSet,
+                             sdk_pool_handle,
+                             sdk_wallet_stewards,
+                             sdk_wallet_client):
     '''
-    Updated with wrong BLS keys 1-3 Nodes;
-    do not expect that BLS multi-sigs are applied (we have less than n-f correct BLS sigs)
+    Added BLS key for 1st, 2d and 3d Nodes;
+    expect that BLS multi-sigs are applied since we have consensus now (3=n-f)
     '''
-    check_update_bls_key(node_num=2, saved_multi_sigs_count=0,
+    # make sure that we have commits from all nodes, and have 3 of 4 (n-f) BLS sigs there is enough
+    # otherwise we may have 3 commits, but 1 of them may be without BLS, so we will Order this txn, but without multi-sig
+    for node in txnPoolNodeSet:
+        node.quorums.commit = Quorum(nodeCount)
+    check_update_bls_key(node_num=2,
+                         saved_multi_sigs_count=4,
                          looper=looper, txnPoolNodeSet=txnPoolNodeSet,
                          sdk_wallet_stewards=sdk_wallet_stewards,
                          sdk_wallet_client=sdk_wallet_client,
-                         sdk_pool_handle=sdk_pool_handle,
-                         add_wrong=True)
+                         sdk_pool_handle=sdk_pool_handle)
 
 
-def test_update_incorrect_bls_all_nodes(looper, txnPoolNodeSet,
-                                        sdk_wallet_stewards,
-                                        sdk_wallet_client,
-                                        sdk_pool_handle):
+def test_add_bls_all_nodes(looper,
+                           txnPoolNodeSet,
+                           sdk_pool_handle,
+                           sdk_wallet_stewards,
+                           sdk_wallet_client):
     '''
-    Updated with wrong BLS keys all Nodes;
-    do not expect that BLS multi-sigs are applied (we have less than n-f correct BLS sigs)
+    Eventually added BLS key for all Nodes;
+    expect that BLS multi-sigs are applied since we have consensus now (4 > n-f)
     '''
-    check_update_bls_key(node_num=3, saved_multi_sigs_count=0,
+    check_update_bls_key(node_num=3,
+                         saved_multi_sigs_count=4,
                          looper=looper, txnPoolNodeSet=txnPoolNodeSet,
                          sdk_wallet_stewards=sdk_wallet_stewards,
                          sdk_wallet_client=sdk_wallet_client,
-                         sdk_pool_handle=sdk_pool_handle,
-                         add_wrong=True)
+                         sdk_pool_handle=sdk_pool_handle)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/bls/test_multi_signature_verifier.py` & `indy-plenum-1.9.2rc1/plenum/test/bls/test_multi_signature_verifier.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/test_basic_batching.py` & `indy-plenum-1.9.2rc1/plenum/test/batching_3pc/test_basic_batching.py`

 * *Files 7% similar despite different names*

```diff
@@ -60,29 +60,33 @@
 def testRequestDynamicValidation(tconf, looper, txnPoolNodeSet,
                                  sdk_pool_handle, sdk_wallet_client):
     """
     Check that for requests which fail dynamic (state based) validation,
     REJECT is sent to the client
     :return:
     """
-    names = {node.master_replica.name: 0 for node in txnPoolNodeSet}
+    origMethods = []
+    names = {node.name: 0 for node in txnPoolNodeSet}
 
     def rejectingMethod(self, req, pp_time):
         names[self.name] += 1
         # Raise rejection for last request of batch
         if tconf.Max3PCBatchSize - names[self.name] == 0:
             raise UnauthorizedClientRequest(req.identifier,
                                             req.reqId,
                                             'Simulated rejection')
 
     for node in txnPoolNodeSet:
-        for replica in node.replicas._replicas.values():
-            replica._ordering_service._do_dynamic_validation = types.MethodType(rejectingMethod, replica._ordering_service)
+        origMethods.append(node.doDynamicValidation)
+        node.doDynamicValidation = types.MethodType(rejectingMethod, node)
 
     reqs = sdk_send_random_requests(looper, sdk_pool_handle,
                                     sdk_wallet_client,
                                     tconf.Max3PCBatchSize)
     sdk_get_and_check_replies(looper, reqs[:-1])
     with pytest.raises(RequestRejectedException) as e:
         sdk_get_and_check_replies(looper, reqs[-1:])
 
     assert 'Simulated rejection' in e._excinfo[1].args[0]
+
+    for i, node in enumerate(txnPoolNodeSet):
+        node.doDynamicValidation = origMethods[i]
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/test_batch_rejection.py` & `indy-plenum-1.9.2rc1/plenum/test/batching_3pc/test_batch_rejection.py`

 * *Files 12% similar despite different names*

```diff
@@ -15,40 +15,40 @@
 def setup(tconf, looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client):
     # Patch the 3phase request sending method to send incorrect digest and
     pr, otherR = getPrimaryReplica(txnPoolNodeSet, instId=0), \
                  getNonPrimaryReplicas(txnPoolNodeSet, instId=0)
 
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
                               sdk_wallet_client, tconf.Max3PCBatchSize)
-    stateRoot = pr._ordering_service.get_state_root_hash(DOMAIN_LEDGER_ID, to_str=False)
+    stateRoot = pr.stateRootHash(DOMAIN_LEDGER_ID, to_str=False)
 
-    origMethod = pr._ordering_service.create_3pc_batch
+    origMethod = pr.create_3pc_batch
     malignedOnce = None
 
     def badMethod(self, ledgerId):
         nonlocal malignedOnce
         pp = origMethod(ledgerId)
         if not malignedOnce:
             pp = updateNamedTuple(pp, digest=pp.digest + '123')
             malignedOnce = True
         return pp
 
-    pr._ordering_service.create_3pc_batch = types.MethodType(badMethod, pr._ordering_service)
+    pr.create_3pc_batch = types.MethodType(badMethod, pr)
     sdk_send_random_requests(looper, sdk_pool_handle, sdk_wallet_client,
                              tconf.Max3PCBatchSize)
     return pr, otherR, stateRoot
 
 
 @pytest.fixture(scope="module")
 def reverted(setup, looper):
     pr, otherR, oldStateRoot = setup
 
     def chkStateRoot(root):
         for r in [pr] + otherR:
-            r._ordering_service.get_state_root_hash(DOMAIN_LEDGER_ID, to_str=False) == root
+            r.stateRootHash(DOMAIN_LEDGER_ID, to_str=False) == root
 
     looper.run(eventually(chkStateRoot, oldStateRoot))
 
 
 @pytest.fixture(scope="module")
 def viewChanged(reverted, looper, txnPoolNodeSet):
     def chk():
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_freeing_forwarded_not_preprepared_request.py` & `indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_freeing_forwarded_not_preprepared_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_freeing_forwarded_preprepared_request.py` & `indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_freeing_forwarded_preprepared_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_catchup_during_3pc.py` & `indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_catchup_during_3pc.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_state_reverted_before_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_state_reverted_before_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_3pc_paused_during_catch_up.py` & `indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_3pc_paused_during_catch_up.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/catch-up/test_clearing_requests_after_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/batching_3pc/catch-up/test_clearing_requests_after_catchup.py`

 * *Files 3% similar despite different names*

```diff
@@ -50,15 +50,15 @@
 
         sdk_send_batches_of_random(looper, txnPoolNodeSet, sdk_pool_handle,
                                    sdk_wallet_steward, req_num, req_num)
 
         looper.run(eventually(node_caughtup, behind_node, count, retryWait=1))
 
     assert len(behind_node.requests) == 0
-    assert all([len(q) == 0 for r in behind_node.replicas.values() for q in r._ordering_service.requestQueues.values()])
+    assert all([len(q) == 0 for r in behind_node.replicas.values() for q in r.requestQueues.values()])
     assert len(behind_node.clientAuthNr._verified_reqs) == 0
     assert len(behind_node.requestSender) == 0
 
 
 def test_deletion_non_forwarded_request(
         looper, chkFreqPatched, reqs_for_checkpoint, txnPoolNodeSet,
         sdk_pool_handle, sdk_wallet_steward, tconf, tdir, allPluginsPath):
@@ -77,10 +77,10 @@
         count = behind_node.spylog.count(behind_node.allLedgersCaughtUp)
         sdk_send_batches_of_random(looper, txnPoolNodeSet, sdk_pool_handle,
                                    sdk_wallet_steward, req_num, req_num)
         looper.run(eventually(node_caughtup, behind_node, count, retryWait=1))
 
     # We clear caughtup requests
     looper.run(eventually(lambda: assertExp(len(behind_node.requests) == 0)))
-    assert all([len(q) == 0 for r in behind_node.replicas.values() for q in r._ordering_service.requestQueues.values()])
+    assert all([len(q) == 0 for r in behind_node.replicas.values() for q in r.requestQueues.values()])
     assert len(behind_node.clientAuthNr._verified_reqs) == 0
     assert len(behind_node.requestSender) == 0
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/batching_3pc/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/batching_3pc/test_batching_scenarios.py` & `indy-plenum-1.9.2rc1/plenum/test/batching_3pc/test_batching_scenarios.py`

 * *Files 10% similar despite different names*

```diff
@@ -45,10 +45,10 @@
 
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client,
                               (ppsToDelay + 1) * tconf.Max3PCBatchSize)
 
     checkNodesHaveSameRoots(txnPoolNodeSet)
 
     for r in otherR:
-        seqNos = [a['pp'].ppSeqNo for a in getAllArgs(r, r._ordering_service._add_to_pre_prepares)]
+        seqNos = [a['pp'].ppSeqNo for a in getAllArgs(r, r.addToPrePrepares)]
         seqNos.reverse()
         assert sorted(seqNos) == seqNos
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/msgs.py` & `indy-plenum-1.9.2rc1/plenum/test/msgs.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_unaligned_prepare_certificates_on_half_nodes.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_unaligned_prepare_certificates_on_half_nodes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_delay_on_one_node.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_delay_on_one_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_advancing_node.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_advancing_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_unaligned_prepare_certificates_on_one_node.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_unaligned_prepare_certificates_on_one_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_delayed_instance_changes.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_delayed_instance_changes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_during_unstash.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_during_unstash.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_two_view_changes_with_delay_on_one_node.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_two_view_changes_with_delay_on_one_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_different_prepare_certificate.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_different_prepare_certificate.py`

 * *Files 8% similar despite different names*

```diff
@@ -29,10 +29,10 @@
             sdk_send_random_request(looper, sdk_pool_handle, sdk_wallet_client)
             looper.run(eventually(check_prepare_certificate,
                                   txnPoolNodeSet[0:-1],
                                   last_ordered[1] + 1))
 
             for n in txnPoolNodeSet:
                 n.view_changer.on_master_degradation()
-            assert slow_node.master_replica._ordering_service.l_last_prepared_certificate_in_view() == \
+            assert slow_node.master_replica.last_prepared_certificate_in_view() == \
                    (0, last_ordered[1])
-            ensureElectionsDone(looper, txnPoolNodeSet)
+            ensureElectionsDone(looper, txnPoolNodeSet)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_delayed_commits.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_delayed_commits.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_two_view_changes_with_delayed_commits.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_two_view_changes_with_delayed_commits.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,19 +35,19 @@
     f = getMaxFailures(n)
     assert sum(1 for n in nodes if n.master_replica.last_prepared_certificate_in_view() == num) >= n - f
 
 
 def check_last_prepared_certificate(nodes, num):
     # Check that last_prepared_certificate reaches some 3PC key on all nodes
     for n in nodes:
-        assert n.master_replica._ordering_service.l_last_prepared_certificate_in_view() == num
+        assert n.master_replica.last_prepared_certificate_in_view() == num
 
 
 def check_last_prepared_certificate_after_view_change_start(nodes, num):
-    # Check that last_prepared_certificate reaches some 3PC key on all nodestest_slow_node_reverts_unordered_state_during_catchup
+    # Check that last_prepared_certificate reaches some 3PC key on all nodes
     for n in nodes:
         assert n.master_replica.last_prepared_before_view_change == num
 
 
 def check_view_change_done(nodes, view_no):
     # Check that view change is done and view_no is not less than target
     for n in nodes:
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_view_change_with_propagate_primary_on_one_delayed_node.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_view_change_with_propagate_primary_on_one_delayed_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_with_delays/test_two_view_changes_with_propagate_primary_on_one_delayed_node.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_with_delays/test_two_view_changes_with_propagate_primary_on_one_delayed_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/stasher.py` & `indy-plenum-1.9.2rc1/plenum/test/stasher.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/sdk/test_sdk_bindings.py` & `indy-plenum-1.9.2rc1/plenum/test/sdk/test_sdk_bindings.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/sdk/test_sdk_many_stewards.py` & `indy-plenum-1.9.2rc1/plenum/test/sdk/test_sdk_many_stewards.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 from logging import getLogger
 
 from plenum.common.constants import DOMAIN_LEDGER_ID
 from plenum.common.messages.node_messages import CatchupRep
 from plenum.common.startable import Mode
 from plenum.server.node import Node
-from plenum.server.replica_validator_enums import STASH_CATCH_UP
 from plenum.test import waits
 from plenum.test.delayers import cr_delay
 from plenum.test.pool_transactions.helper import \
     disconnect_node_and_ensure_disconnected
 from plenum.test.helper import sdk_send_random_and_check, assertExp
 from plenum.test.node_catchup.helper import waitNodeDataEquality
 from plenum.test.stasher import delay_rules
@@ -58,15 +57,15 @@
                                       tconf,
                                       tdir,
                                       allPluginsPath,
                                       start=False,
                                       )
 
     initial_all_ledgers_caught_up = lagging_node.spylog.count(Node.allLedgersCaughtUp)
-    assert all(replica.stasher.stash_size(STASH_CATCH_UP) == 0 for inst_id, replica in lagging_node.replicas.items())
+    assert all(replica.stasher.num_stashed_catchup == 0 for inst_id, replica in lagging_node.replicas.items())
 
     with delay_rules(lagging_node.nodeIbStasher, cr_delay(ledger_filter=DOMAIN_LEDGER_ID)):
         looper.add(lagging_node)
         txnPoolNodeSet[-1] = lagging_node
         looper.run(checkNodesConnected(txnPoolNodeSet))
 
         # wait till we got catchup replies for messages missed while the node was offline,
@@ -76,15 +75,15 @@
                        timeout=60))
 
         # make sure that more requests are being ordered while catch-up is in progress
         sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
                                   sdk_wallet_client, 10)
 
         assert lagging_node.mode == Mode.syncing
-        assert all(replica.stasher.stash_size(STASH_CATCH_UP) > 0 for inst_id, replica in lagging_node.replicas.items())
+        assert all(replica.stasher.num_stashed_catchup > 0 for inst_id, replica in lagging_node.replicas.items())
 
     # check that the catch-up is finished
     looper.run(
         eventually(
             lambda: assertExp(lagging_node.mode == Mode.participating), retryWait=1,
             timeout=waits.expectedPoolCatchupTime(len(txnPoolNodeSet))
         )
@@ -93,8 +92,8 @@
         eventually(
             lambda: assertExp(
                 lagging_node.spylog.count(Node.allLedgersCaughtUp) == initial_all_ledgers_caught_up + 1)
         )
     )
 
     waitNodeDataEquality(looper, *txnPoolNodeSet, customTimeout=5)
-    assert all(replica.stasher.stash_size(STASH_CATCH_UP) == 0 for inst_id, replica in lagging_node.replicas.items())
+    assert all(replica.stasher.num_stashed_catchup == 0 for inst_id, replica in lagging_node.replicas.items())
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_stashing_checkpoints_after_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_stashing_checkpoints_after_view_change.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,17 +2,16 @@
 
 import pytest
 
 from plenum.common.messages.node_messages import Checkpoint
 from plenum.common.startable import Mode
 from plenum.server.node import Node
 from plenum.server.replica import Replica
-from plenum.server.replica_validator_enums import STASH_VIEW
 from plenum.test import waits
-from plenum.test.checkpoints.helper import check_for_nodes, check_stable_checkpoint, check_for_instance
+from plenum.test.checkpoints.helper import chkChkpoints, chk_chkpoints_for_instance
 from plenum.test.delayers import lsDelay, vcd_delay
 from plenum.test.helper import sdk_send_random_and_check, assertExp, max_3pc_batch_limits, \
     check_last_ordered_3pc_on_all_replicas, check_last_ordered_3pc_on_master, check_last_ordered_3pc_on_backup
 from plenum.test.node_catchup.helper import waitNodeDataEquality
 from plenum.test.stasher import delay_rules
 from plenum.test.test_node import ensureElectionsDone
 from plenum.test.view_change.helper import ensure_view_change
@@ -77,15 +76,15 @@
             )
             looper.run(
                 eventually(check_last_ordered_3pc_on_backup, rest_nodes,
                            (1, num_reqs + 1))
             )
 
             # all good nodes stabilized checkpoint
-            looper.run(eventually(check_for_nodes, rest_nodes, check_stable_checkpoint, 10))
+            looper.run(eventually(chkChkpoints, rest_nodes, 2, 0))
 
             assert get_stashed_checkpoints(lagging_node) == num_checkpoints * len(rest_nodes)
             # lagging node is doing the view change and stashing all checkpoints
             assert lagging_node.view_change_in_progress is True
             looper.run(
                 eventually(
                     lambda: assertExp(get_stashed_checkpoints(lagging_node) == 2 * len(rest_nodes)),
@@ -104,20 +103,20 @@
     )
     looper.run(
         eventually(check_last_ordered_3pc_on_backup, [lagging_node],
                    (1, num_reqs + 1))
     )
 
     # check that checkpoint is stabilized for master
-    looper.run(eventually(check_for_instance, [lagging_node], 0, check_stable_checkpoint, 10))
+    looper.run(eventually(chk_chkpoints_for_instance, [lagging_node], 0, 2, 0))
 
     # check that the catch-up is finished
     assert lagging_node.mode == Mode.participating
     assert lagging_node.spylog.count(Node.allLedgersCaughtUp) == initial_all_ledgers_caught_up + 1
     assert lagging_node.spylog.count(Node.start_catchup) == initial_start_catchup + 1
 
     waitNodeDataEquality(looper, *txnPoolNodeSet, customTimeout=5)
 
 
 def get_stashed_checkpoints(node):
     return sum(
-        1 for (stashed, sender) in node.master_replica.stasher._queues[STASH_VIEW] if isinstance(stashed, Checkpoint))
+        1 for (stashed, sender) in node.master_replica.stasher._stashed_future_view if isinstance(stashed, Checkpoint))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits_received_before_catchup_pool.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits_received_before_catchup_pool.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup_only_checkpoints.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup_only_checkpoints.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,31 +1,34 @@
+from logging import getLogger
+
 import pytest
 from plenum.common.constants import LEDGER_STATUS
 
 from plenum.common.messages.node_messages import Checkpoint, LedgerStatus
 from plenum.common.startable import Mode
 from plenum.server.node import Node
 from plenum.server.replica import Replica
-from plenum.server.replica_validator_enums import STASH_CATCH_UP
 from plenum.test import waits
-from plenum.test.checkpoints.helper import check_for_nodes, check_stable_checkpoint
+from plenum.test.checkpoints.helper import chkChkpoints
 from plenum.test.delayers import cs_delay, lsDelay, \
     ppDelay, pDelay, cDelay, msg_rep_delay, cr_delay
 from plenum.test.pool_transactions.helper import \
     disconnect_node_and_ensure_disconnected
 from plenum.test.helper import sdk_send_random_and_check, assertExp, max_3pc_batch_limits, \
     check_last_ordered_3pc_on_all_replicas, check_last_ordered_3pc_on_master
 from plenum.test.node_catchup.helper import waitNodeDataEquality
 from plenum.test.stasher import delay_rules
 from plenum.test.test_node import checkNodesConnected
 from plenum.test.view_change.helper import start_stopped_node
 from stp_core.loop.eventually import eventually
 
 from plenum.test.checkpoints.conftest import chkFreqPatched, reqs_for_checkpoint
 
+logger = getLogger()
+
 CHK_FREQ = 5
 
 
 @pytest.fixture(scope="module")
 def tconf(tconf):
     with max_3pc_batch_limits(tconf, size=1) as tconf:
         yield tconf
@@ -107,15 +110,15 @@
                                   num_reqs)
         looper.run(
             eventually(check_last_ordered_3pc_on_all_replicas, rest_nodes,
                        (0, num_reqs + 2))
         )
 
         # all good nodes stabilized checkpoint
-        looper.run(eventually(check_for_nodes, rest_nodes, check_stable_checkpoint, 10))
+        looper.run(eventually(chkChkpoints, rest_nodes, 2, 0))
 
         assert lagging_node.mode != Mode.participating
         # lagging node is catching up and stashing all checkpoints
         looper.run(
             eventually(
                 lambda: assertExp(get_stashed_checkpoints(lagging_node) == num_checkpoints * len(rest_nodes)),
                 timeout=waits.expectedPoolCatchupTime(len(txnPoolNodeSet))
@@ -142,8 +145,8 @@
     assert lagging_node.spylog.count(Node.start_catchup) == 1
 
     waitNodeDataEquality(looper, *txnPoolNodeSet, customTimeout=5)
 
 
 def get_stashed_checkpoints(node):
     return sum(
-        1 for (stashed, sender) in node.master_replica.stasher._queues[STASH_CATCH_UP] if isinstance(stashed, Checkpoint))
+        1 for (stashed, sender) in node.master_replica.stasher._stashed_catch_up if isinstance(stashed, Checkpoint))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_slow_catchup_while_ordering.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_slow_catchup_while_ordering.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits_received_before_catchup_audit.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_catchup_with_skipped_commits_received_before_catchup_audit.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_limited_stashing_3pc_while_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_limited_stashing_3pc_while_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup_checkpoints.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/test_stashing_3pc_while_catchup_checkpoints.py`

 * *Files 11% similar despite different names*

```diff
@@ -3,17 +3,16 @@
 import pytest
 
 from plenum.common.constants import DOMAIN_LEDGER_ID
 from plenum.common.messages.node_messages import Checkpoint, CatchupRep
 from plenum.common.startable import Mode
 from plenum.server.node import Node
 from plenum.server.replica import Replica
-from plenum.server.replica_validator_enums import STASH_CATCH_UP
 from plenum.test import waits
-from plenum.test.checkpoints.helper import check_for_nodes, check_stable_checkpoint, check_for_instance
+from plenum.test.checkpoints.helper import chkChkpoints, chk_chkpoints_for_instance
 from plenum.test.delayers import cr_delay
 from plenum.test.pool_transactions.helper import \
     disconnect_node_and_ensure_disconnected
 from plenum.test.helper import sdk_send_random_and_check, assertExp, max_3pc_batch_limits, \
     check_last_ordered_3pc_on_all_replicas
 from plenum.test.node_catchup.helper import waitNodeDataEquality
 from plenum.test.stasher import delay_rules
@@ -104,15 +103,15 @@
                                   num_reqs)
         looper.run(
             eventually(check_last_ordered_3pc_on_all_replicas, rest_nodes,
                        (0, num_reqs + 2))
         )
 
         # all good nodes stabilized checkpoint
-        looper.run(eventually(check_for_nodes, rest_nodes, check_stable_checkpoint, 10))
+        looper.run(eventually(chkChkpoints, rest_nodes, 2, 0))
 
         # lagging node is catching up and stashing all checkpoints
         assert lagging_node.mode == Mode.syncing
         looper.run(
             eventually(
                 lambda: assertExp(get_stashed_checkpoints(lagging_node) == num_checkpoints * len(rest_nodes)),
                 timeout=waits.expectedPoolCatchupTime(len(txnPoolNodeSet))
@@ -122,15 +121,15 @@
     # check that last_ordered is set
     looper.run(
         eventually(check_last_ordered_3pc_on_all_replicas, [lagging_node],
                    (0, num_reqs + 2))
     )
 
     # check that checkpoint is stabilized for master
-    looper.run(eventually(check_for_instance, [lagging_node], 0, check_stable_checkpoint, 10))
+    looper.run(eventually(chk_chkpoints_for_instance, [lagging_node], 0, 2, 0))
 
     # check that the catch-up is finished
     looper.run(
         eventually(
             lambda: assertExp(lagging_node.mode == Mode.participating), retryWait=1,
             timeout=waits.expectedPoolCatchupTime(len(txnPoolNodeSet))
         )
@@ -151,8 +150,8 @@
     )
 
     waitNodeDataEquality(looper, *txnPoolNodeSet, customTimeout=5)
 
 
 def get_stashed_checkpoints(node):
     return sum(
-        1 for (stashed, sender) in node.master_replica.stasher._queues[STASH_CATCH_UP] if isinstance(stashed, Checkpoint))
+        1 for (stashed, sender) in node.master_replica.stasher._stashed_catch_up if isinstance(stashed, Checkpoint))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup_with_3pc/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup_with_3pc/helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -65,14 +65,14 @@
         # Start catchup
         lagging_node.start_catchup()
 
         # Wait until catchup reaches desired state
         looper.run(eventually(check_lagging_node_catchup_state, catchup_state))
 
         # Emulate scheduled action
-        lagging_node.master_replica._ordering_service._process_stashed_out_of_order_commits()
+        lagging_node.master_replica.process_stashed_out_of_order_commits()
 
     # Ensure that audit ledger is caught up by lagging node
     looper.run(eventually(check_lagging_node_catchup_state, NodeLeecherService.State.Idle))
 
     # Ensure that all nodes will eventually have same data
     ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_event_bus.py` & `indy-plenum-1.9.2rc1/plenum/test/test_event_bus.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,15 +3,14 @@
 from unittest.mock import Mock, call
 
 from plenum.common.event_bus import InternalBus, ExternalBus
 from plenum.common.util import randomString
 
 SomeMessage = NamedTuple('SomeMessage', [('int_field', int), ('str_field', str)])
 OtherMessage = NamedTuple('OtherMessage', [('float_field', float)])
-NoParamsMessage = NamedTuple('NoParamsMessage', [])
 
 
 def create_some_message() -> SomeMessage:
     return SomeMessage(int_field=randint(0, 1000), str_field=randomString(16))
 
 
 def create_other_message() -> OtherMessage:
@@ -25,25 +24,14 @@
     bus = InternalBus()
     bus.subscribe(SomeMessage, handler)
     bus.send(message)
 
     handler.assert_called_once_with(message)
 
 
-def test_event_bus_routes_no_params_message():
-    message = NoParamsMessage()
-    handler = Mock()
-
-    bus = InternalBus()
-    bus.subscribe(NoParamsMessage, handler)
-    bus.send(message)
-
-    handler.assert_called_once_with(message)
-
-
 def test_internal_bus_doesnt_route_unregistered_message():
     handler = Mock()
 
     bus = InternalBus()
     bus.subscribe(SomeMessage, handler)
     bus.send(create_other_message())
 
@@ -111,15 +99,15 @@
     bus.send(message, 'some_node')
 
     send_handler.assert_called_once_with(message, 'some_node')
 
 
 def test_external_bus_queues_sent_messages_sequentially():
     messages = [(create_some_message(), choice(['some_node', 'other_node', None]))
-                for _ in range(100)]
+                 for _ in range(100)]
     send_handler = Mock()
 
     bus = ExternalBus(send_handler)
     for message, dst in messages:
         bus.send(message, dst)
 
     assert send_handler.mock_calls == [call(msg, dst) for msg, dst in messages]
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/buy_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/buy_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,10 +35,7 @@
         identifier = get_from(txn)
         req_id = get_req_id(txn)
         return self.prepare_buy_key(identifier, req_id)
 
     @staticmethod
     def prepare_buy_key(identifier, req_id):
         return sha256('{}{}:buy'.format(identifier, req_id).encode()).digest()
-
-    def __repr__(self):
-        return "TestHandler"
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_connections_with_converted_key.py` & `indy-plenum-1.9.2rc1/plenum/test/test_connections_with_converted_key.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/watermarks/test_watermarks_after_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_max_catchup_rounds.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,58 +1,66 @@
-import pytest
-
-from plenum.test import waits
-from plenum.test.delayers import cDelay, chk_delay, icDelay, vcd_delay
-from plenum.test.helper import sdk_send_random_and_check, waitForViewChange
+from plenum.common.util import check_if_all_equal_in_list
+from plenum.test.delayers import pDelay, cDelay
+from plenum.test.helper import sdk_send_batches_of_random_and_check, sdk_send_random_requests
 from plenum.test.node_catchup.helper import ensure_all_nodes_have_same_data
-from plenum.test.stasher import delay_rules
-
-CHK_FREQ = 2
-LOG_SIZE = 2 * CHK_FREQ
+from plenum.test.test_node import getNonPrimaryReplicas, ensureElectionsDone
+from plenum.test.view_change.helper import ensure_view_change
 
-Max3PCBatchSize = 1
 
-
-@pytest.fixture(scope='module')
-def tconf(tconf):
-    old_max_3pc_batch_size = tconf.Max3PCBatchSize
-    old_log_size = tconf.LOG_SIZE
-    old_chk_freq = tconf.CHK_FREQ
-    tconf.Max3PCBatchSize = Max3PCBatchSize
-    tconf.LOG_SIZE = LOG_SIZE
-    tconf.CHK_FREQ = CHK_FREQ
-
-    yield tconf
-    tconf.Max3PCBatchSize = old_max_3pc_batch_size
-    tconf.LOG_SIZE = old_log_size
-    tconf.CHK_FREQ = old_chk_freq
-
-
-def test_watermarks_after_view_change(tdir, tconf,
-                                      looper,
-                                      txnPoolNodeSet,
-                                      sdk_pool_handle,
-                                      sdk_wallet_client):
+def test_view_change_after_max_catchup_rounds(txnPoolNodeSet, looper, sdk_pool_handle, sdk_wallet_client):
     """
-    Delay commit, checkpoint, InstanceChange and ViewChangeDone messages for lagging_node.
-    Start ViewChange.
-    Check that ViewChange finished.
-    Reset delays.
-    Check that lagging_node can order transactions and has same data with other nodes.
+    The node should do only a fixed rounds of catchup. For this delay Prepares
+    and Commits for 2 non-primary nodes by a large amount which is equivalent
+    to loss of Prepares and Commits. Make sure 2 nodes have a different last
+    prepared certificate from other two. Then do a view change, make sure view
+    change completes and the pool does not process the request that were
+    prepared by only a subset of the nodes
     """
-    lagging_node = txnPoolNodeSet[-1]
-    lagging_node.master_replica.config.LOG_SIZE = LOG_SIZE
-    start_view_no = lagging_node.viewNo
-    with delay_rules(lagging_node.nodeIbStasher, cDelay(), chk_delay(), icDelay(), vcd_delay()):
-        for n in txnPoolNodeSet:
-            n.view_changer.on_master_degradation()
-        waitForViewChange(looper,
-                          txnPoolNodeSet[:-1],
-                          expectedViewNo=start_view_no + 1,
-                          customTimeout=waits.expectedPoolViewChangeStartedTimeout(len(txnPoolNodeSet)))
-        ensure_all_nodes_have_same_data(looper, txnPoolNodeSet[:-1])
-        sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
-                                  sdk_wallet_client, 6)
+    sdk_send_batches_of_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
+                                         sdk_wallet_client, 2 * 3, 3)
+    ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
+    ledger_summary = txnPoolNodeSet[0].ledger_summary
+
+    slow_nodes = [r.node for r in getNonPrimaryReplicas(
+        txnPoolNodeSet, 0)[-2:]]
+    fast_nodes = [n for n in txnPoolNodeSet if n not in slow_nodes]
+
+    # Make node slow to process Prepares and Commits
+    for node in slow_nodes:
+        node.nodeIbStasher.delay(pDelay(120, 0))
+        node.nodeIbStasher.delay(cDelay(120, 0))
+
+    sdk_send_random_requests(looper, sdk_pool_handle, sdk_wallet_client, 5)
+    looper.runFor(3)
+
+    ensure_view_change(looper, nodes=txnPoolNodeSet)
+
+    def last_prepared(nodes):
+        lst = [n.master_replica.last_prepared_certificate_in_view()
+               for n in nodes]
+        # All nodes have same last prepared
+        assert check_if_all_equal_in_list(lst)
+        return lst[0]
+
+    last_prepared_slow = last_prepared(slow_nodes)
+    last_prepared_fast = last_prepared(fast_nodes)
+
+    # Check `slow_nodes` and `fast_nodes` set different last_prepared
+    assert last_prepared_fast != last_prepared_slow
+
+    # View change complete
+    ensureElectionsDone(looper, txnPoolNodeSet)
+    ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
+
+    # The requests which were prepared by only a subset of the nodes were
+    # not ordered
+    assert txnPoolNodeSet[0].ledger_summary == ledger_summary
+
+    for node in slow_nodes:
+        node.nodeIbStasher.reset_delays_and_process_delayeds()
+
+    # Make sure pool is functional
+    sdk_send_batches_of_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
+                                         sdk_wallet_client, 10, 2)
     ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
-    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
-                              sdk_wallet_client, 1)
     ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
+    last_prepared(txnPoolNodeSet)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/delayers.py` & `indy-plenum-1.9.2rc1/plenum/test/delayers.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 import random
 from typing import Iterable, List, Optional
 
 from plenum.common.messages.message_base import MessageBase
 from plenum.common.request import Request
 
-from plenum.common.messages.node_messages import Propagate, PrePrepare, Prepare, Commit, Checkpoint, InstanceChange, LedgerStatus, \
+from plenum.common.messages.node_messages import Nomination, Reelection, Primary, \
+    Propagate, PrePrepare, Prepare, Commit, Checkpoint, InstanceChange, LedgerStatus, \
     ConsistencyProof, CatchupReq, CatchupRep, ViewChangeDone, MessageReq, MessageRep, CurrentState
 from plenum.common.constants import OP_FIELD_NAME, MESSAGE_REQUEST, MESSAGE_RESPONSE
 from plenum.common.types import f
 from plenum.common.util import getCallableName
 
 DEFAULT_DELAY = 600
 
@@ -77,14 +78,32 @@
         actionName = getCallableName(action)
         if actionName == method.__name__:
             return delay
 
     return inner
 
 
+def nom_delay(delay: float = DEFAULT_DELAY, inst_id=None, sender_filter: str = None):
+    # Delayer of NOMINATE requests
+    return delayerMsgTuple(
+        delay, Nomination, instFilter=inst_id, senderFilter=sender_filter)
+
+
+def prim_delay(delay: float = DEFAULT_DELAY, inst_id=None, sender_filter: str = None):
+    # Delayer of PRIMARY requests
+    return delayerMsgTuple(
+        delay, Primary, instFilter=inst_id, senderFilter=sender_filter)
+
+
+def rel_delay(delay: float = DEFAULT_DELAY, inst_id=None, sender_filter: str = None):
+    # Delayer of REELECTION requests
+    return delayerMsgTuple(
+        delay, Reelection, instFilter=inst_id, senderFilter=sender_filter)
+
+
 def ppgDelay(delay: float = DEFAULT_DELAY, sender_filter: str = None):
     # Delayer of PROPAGATE requests
     return delayerMsgTuple(delay, Propagate, senderFilter=sender_filter)
 
 
 def ppDelay(delay: float = DEFAULT_DELAY, instId: int = None, sender_filter: str = None):
     # Delayer of PRE-PREPARE requests from a particular instance
@@ -207,15 +226,17 @@
     for r in nonPrimReps:
         r.node.nodeIbStasher.delay(ppDelay(delay, instId))
     return nonPrimReps
 
 
 def delay_messages(typ, nodes, inst_id, delay=None,
                    min_delay=None, max_delay=None):
-    if typ == '3pc':
+    if typ == 'election':
+        delay_meths = (nom_delay, prim_delay, rel_delay)
+    elif typ == '3pc':
         delay_meths = (ppDelay, pDelay, cDelay)
     else:
         RuntimeError('Unknown type')
     assert delay is not None or (
             min_delay is not None and max_delay is not None)
     for node in nodes:
         if delay:
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_config_util.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_config_util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_digest_validation.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_digest_validation.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,29 +1,27 @@
 import json
 import types
 
 import pytest
 from indy.did import create_and_store_my_did
-
-from plenum.server.consensus.ordering_service import OrderingService
 from plenum.server.replica import Replica
 
 from plenum.common.messages.node_messages import InstanceChange
 
 from plenum.server.suspicion_codes import Suspicions
 from plenum.server.node import Node
 from plenum.test.node_catchup.helper import ensure_all_nodes_have_same_data
 from plenum.test.test_node import getPrimaryReplica
 from plenum.common.exceptions import RequestNackedException, SuspiciousPrePrepare
 from plenum.common.request import Request, ReqKey
 from plenum.test.helper import sdk_gen_request, sdk_multisign_request_object, sdk_send_signed_requests, \
     sdk_get_and_check_replies, sdk_random_request_objects, waitForViewChange, max_3pc_batch_limits, \
     sdk_send_random_and_check
 
-from plenum.common.constants import CURRENT_PROTOCOL_VERSION, DOMAIN_LEDGER_ID, TXN_TYPE
+from plenum.common.constants import CURRENT_PROTOCOL_VERSION, DOMAIN_LEDGER_ID, NodeHooks, TXN_TYPE
 
 from plenum.common.util import randomString
 from plenum.test.testing_utils import FakeSomething
 from stp_core.loop.eventually import eventually
 
 
 @pytest.fixture(scope='function')
@@ -116,15 +114,15 @@
 
         # We need to check for ordering this way, cause sdk do not allow
         # track two requests with same reqId at the same time
         looper.run(eventually(wait_one_batch, txnPoolNodeSet[0], lo_before))
 
     assert len(txnPoolNodeSet[0].requests) == old_reqs + 2
 
-    pps = txnPoolNodeSet[0].master_replica._ordering_service.sentPrePrepares
+    pps = txnPoolNodeSet[0].master_replica.sentPrePrepares
     pp = pps[pps.keys()[-1]]
     idrs = pp.reqIdr
     assert len(idrs) == 1
     assert Request(**json.loads(req1)).digest in idrs
 
 
 def test_parts_of_nodes_have_same_request_with_different_signatures(
@@ -171,48 +169,44 @@
 
     ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
 
 
 def test_suspicious_primary_send_same_request_with_different_signatures(
         looper, txnPoolNodeSet, sdk_pool_handle, two_requests):
     assert txnPoolNodeSet[0].master_replica.isPrimary
-    txnPoolNodeSet[0].master_replica._ordering_service._do_dynamic_validation = \
-        types.MethodType(malicious_dynamic_validation,
-                         txnPoolNodeSet[0])
+    txnPoolNodeSet[0].doDynamicValidation = types.MethodType(malicious_dynamic_validation, txnPoolNodeSet[0])
 
     req1, req2 = two_requests
 
     old_view = txnPoolNodeSet[0].viewNo
     sdk_send_signed_requests(sdk_pool_handle, [req1])
     sdk_send_signed_requests(sdk_pool_handle, [req2])
 
     waitForViewChange(looper, txnPoolNodeSet, expectedViewNo=old_view + 1)
     all(cll.params['msg'][1] == Suspicions.PPR_WITH_ORDERED_REQUEST.code for cll in
         txnPoolNodeSet[0].spylog.getAll('sendToViewChanger') if isinstance(cll.params['msg'], InstanceChange))
-    txnPoolNodeSet[0].master_replica._ordering_service._do_dynamic_validation = \
-        types.MethodType(OrderingService._do_dynamic_validation,
-                         txnPoolNodeSet[0].master_replica._ordering_service)
+    txnPoolNodeSet[0].doDynamicValidation = types.MethodType(Node.doDynamicValidation, txnPoolNodeSet[0])
 
 
 def test_suspicious_primary_send_same_request_with_same_signatures(
         looper, txnPoolNodeSet, sdk_pool_handle, two_requests, sdk_wallet_stewards, tconf):
     couple = sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_stewards[0], 1)[0]
     req = Request(**couple[0])
     replica = getPrimaryReplica(txnPoolNodeSet)
-    replica._ordering_service._do_dynamic_validation = types.MethodType(malicious_dynamic_validation, replica.node)
+    replica.node.doDynamicValidation = types.MethodType(malicious_dynamic_validation, replica.node)
 
     txnPoolNodeSet.remove(replica.node)
     old_reverts = {}
     for i, node in enumerate(txnPoolNodeSet):
-        old_reverts[i] = node.master_replica._ordering_service.spylog.count(OrderingService._revert)
+        old_reverts[i] = node.master_replica.spylog.count(Replica.revert)
         node.seqNoDB._keyValueStorage.remove(req.digest)
         node.seqNoDB._keyValueStorage.remove(req.payload_digest)
 
-    ppReq = replica._ordering_service.create_3pc_batch(DOMAIN_LEDGER_ID)
+    ppReq = replica.create_3pc_batch(DOMAIN_LEDGER_ID)
     ppReq._fields['reqIdr'] = [req.digest, req.digest]
-    replica._ordering_service.send_pre_prepare(ppReq)
+    replica.sendPrePrepare(ppReq)
 
     def reverts():
         for i, node in enumerate(txnPoolNodeSet):
-            assert old_reverts[i] + 1 == node.master_replica._ordering_service.spylog.count(OrderingService._revert)
+            assert old_reverts[i] + 1 == node.master_replica.spylog.count(Replica.revert)
 
     looper.run(eventually(reverts))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_random_string.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_random_string.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_pool_file_raises_descriptive_error.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_pool_file_raises_descriptive_error.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_parse_ledger.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_parse_ledger.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_splitting_large_messages.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_splitting_large_messages.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_throttler.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_throttler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_verifier.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_verifier.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_replicas_suspicious.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_replicas_suspicious.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,32 +6,32 @@
 
 
 def test_nodes_make_view_change_only_on_master_suspicious(
         looper, txnPoolNodeSet):
     old_view = txnPoolNodeSet[0].viewNo
 
     master_primary = txnPoolNodeSet[0].replicas[0]
-    backup_primary = txnPoolNodeSet[1].replicas[1]
+    bacup_primary = txnPoolNodeSet[1].replicas[1]
     assert master_primary.isPrimary is True
-    assert backup_primary.isPrimary is True
+    assert bacup_primary.isPrimary is True
 
-    master_primary._ordering_service.replica_batch_digest = lambda reqs: 'asd'
-    backup_primary._ordering_service.replica_batch_digest = lambda reqs: 'asd'
+    master_primary.batchDigest = lambda reqs: 'asd'
+    bacup_primary.batchDigest = lambda reqs: 'asd'
 
     non_primary_backup = txnPoolNodeSet[0].replicas[1]
-    old_pp = non_primary_backup.spylog.count(non_primary_backup._ordering_service.process_preprepare)
+    old_pp = non_primary_backup.spylog.count(non_primary_backup.processPrePrepare)
 
     def pp_processed(replica, old_pp):
-        assert replica._ordering_service.spylog.count(replica._ordering_service.process_preprepare) == old_pp + 1
+        assert replica.spylog.count(replica.processPrePrepare) == old_pp + 1
 
-    backup_primary._ordering_service._do_send_3pc_batch(DOMAIN_LEDGER_ID)
+    bacup_primary._do_send_3pc_batch(DOMAIN_LEDGER_ID)
     looper.run(eventually(pp_processed, non_primary_backup, old_pp))
     assert all(node.view_changer.spylog.count(ViewChanger.sendInstanceChange) == 0
                for node in txnPoolNodeSet)
     waitForViewChange(looper, txnPoolNodeSet, old_view)
 
     non_primary_master = txnPoolNodeSet[1].replicas[0]
-    old_pp = non_primary_master.spylog.count(non_primary_master._ordering_service.process_preprepare)
+    old_pp = non_primary_master.spylog.count(non_primary_master.processPrePrepare)
 
-    master_primary._ordering_service._do_send_3pc_batch(DOMAIN_LEDGER_ID)
+    master_primary._do_send_3pc_batch(DOMAIN_LEDGER_ID)
     looper.run(eventually(pp_processed, non_primary_master, old_pp))
     waitForViewChange(looper, txnPoolNodeSet, old_view + 1)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_database_manager.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_database_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_signers.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_signers.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/common/test_prepare_batch.py` & `indy-plenum-1.9.2rc1/plenum/test/common/test_prepare_batch.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_node_reg.py` & `indy-plenum-1.9.2rc1/plenum/test/test_node_reg.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_to_inconsistent_state.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_to_inconsistent_state.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_4_all_wp.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_4_all_wp.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_6_wp.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_6_wp.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_4_np.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_4_np.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_6_all_wp.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_6_all_wp.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_6_np.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_6_np.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_node_4_all.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_node_4_all.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_7.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_7.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_7_all.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_7_all.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_network_inconsistency_watcher.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_network_inconsistency_watcher.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_node_with_view_changes.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_node_with_view_changes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_nodes_6_all_np.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_nodes_6_all_np.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/restart/test_restart_to_same_view_with_killed_primary.py` & `indy-plenum-1.9.2rc1/plenum/test/restart/test_restart_to_same_view_with_killed_primary.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_nym_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/req_handler/test_nym_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_txn_author_agreement_aml_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/req_handler/test_txn_author_agreement_aml_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_get_txn_author_agreement_aml_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/req_handler/test_get_txn_author_agreement_aml_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_get_value_from_state.py` & `indy-plenum-1.9.2rc1/plenum/test/req_handler/test_get_value_from_state.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_get_txn_author_agreement_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/req_handler/test_get_txn_author_agreement_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_txn_author_agreement_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/req_handler/test_txn_author_agreement_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_node_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/req_handler/test_node_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_request_manager.py` & `indy-plenum-1.9.2rc1/plenum/test/req_handler/test_request_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/req_handler/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/req_handler/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/req_handler/test_node_req_handler_static_validation.py` & `indy-plenum-1.9.2rc1/plenum/test/req_handler/test_node_req_handler_static_validation.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_stasher.py` & `indy-plenum-1.9.2rc1/plenum/test/test_stasher.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/conftest.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,17 +1,14 @@
 import pytest
 from orderedset._orderedset import OrderedSet
 
-from plenum.common.event_bus import InternalBus
 from plenum.common.messages.node_messages import PrePrepare
 from plenum.common.startable import Mode
 from plenum.common.constants import POOL_LEDGER_ID, DOMAIN_LEDGER_ID, CURRENT_PROTOCOL_VERSION, AUDIT_LEDGER_ID
-from plenum.common.timer import QueueTimer
 from plenum.common.util import get_utc_epoch
-from plenum.server.database_manager import DatabaseManager
 from plenum.server.propagator import Requests
 from plenum.server.quorums import Quorums
 from plenum.server.replica import Replica
 from plenum.test.conftest import getValueFromModule
 from plenum.test.helper import MockTimestamp, sdk_random_request_objects, create_pre_prepare_params, \
     create_prepare_from_pre_prepare
 from plenum.test.testing_utils import FakeSomething
@@ -36,20 +33,17 @@
             utc_epoch=lambda *args: get_utc_epoch(),
             mode=Mode.participating,
             view_change_in_progress=False,
             pre_view_change_in_progress=False,
             requests=Requests(),
             onBatchCreated=lambda self, *args, **kwargs: True,
             applyReq=lambda self, *args, **kwargs: True,
+            primaries_batch_needed=False,
             primaries=[],
-            get_validators=lambda: [],
-            db_manager=None,
-            write_manager=FakeSomething(database_manager=DatabaseManager(),
-                                        apply_request=lambda req, cons_time: None),
-            timer=QueueTimer()
+            get_validators=lambda: []
         )
 
     @property
     def viewNo(self):
         return self._viewNo
 
     @viewNo.setter
@@ -134,26 +128,25 @@
         config=tconf, bls_bft_replica=bls_bft_replica,
         get_current_time=mock_timestamp,
         get_time_for_3pc_batch=mock_timestamp
     )
     node.add_replica(replica)
     ReplicaFakeNode.master_last_ordered_3PC = replica.last_ordered_3pc
 
-    replica._ordering_service.last_accepted_pre_prepare_time = replica.get_time_for_3pc_batch()
+    replica.last_accepted_pre_prepare_time = replica.get_time_for_3pc_batch()
+    replica.revert = lambda ledgerId, stateRootHash, reqCount: None
     replica.primaryName = "Alpha:{}".format(replica.instId)
     replica.primaryNames[replica.viewNo] = replica.primaryName
 
-    replica._ordering_service.get_txn_root_hash = lambda ledger, to_str=False: txn_roots[ledger]
-    replica._ordering_service.get_state_root_hash = lambda ledger, to_str=False: state_roots[ledger]
-    replica._ordering_service._revert = lambda ledgerId, stateRootHash, reqCount: None
-    replica._ordering_service.post_batch_creation = lambda three_pc_batch: None
+    replica.txnRootHash = lambda ledger, to_str=False: txn_roots[ledger]
+    replica.stateRootHash = lambda ledger, to_str=False: state_roots[ledger]
 
-    replica._ordering_service.requestQueues[DOMAIN_LEDGER_ID] = OrderedSet()
+    replica.requestQueues[DOMAIN_LEDGER_ID] = OrderedSet()
 
-    replica._ordering_service._get_primaries_for_ordered = lambda pp: [replica.primaryName]
+    replica._get_primaries_for_ordered = lambda pp: [replica.primaryName]
 
     def reportSuspiciousNodeEx(ex):
         assert False, ex
 
     replica.node.reportSuspiciousNodeEx = reportSuspiciousNodeEx
 
     return replica
@@ -163,17 +156,17 @@
 def primary_replica(replica):
     replica.primaryName = replica.name
     return replica
 
 
 @pytest.fixture(scope='function')
 def replica_with_requests(replica, fake_requests):
-    replica._ordering_service._apply_pre_prepare = lambda a: (fake_requests, [], [], False)
+    replica._apply_pre_prepare = lambda a: (fake_requests, [], [], False)
     for req in fake_requests:
-        replica._ordering_service.requestQueues[DOMAIN_LEDGER_ID].add(req.key)
+        replica.requestQueues[DOMAIN_LEDGER_ID].add(req.key)
         replica.requests.add(req)
         replica.requests.set_finalised(req)
 
     return replica
 
 
 @pytest.fixture(scope="function",
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_consensus_dp_batches.py` & `indy-plenum-1.9.2rc1/plenum/test/consensus/test_consensus_dp_batches.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,30 +1,38 @@
-from plenum.test.helper import sdk_send_random_request
+from plenum.test.pool_transactions.helper import sdk_add_new_nym_without_waiting
 from stp_core.loop.eventually import eventually
-from plenum.test.delayers import ppDelay, pDelay
-from plenum.test.stasher import delay_rules
 
+from plenum.test.delayers import ppDelay, cDelay, pDelay
 
-def test_check_cdp_pp_storages(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client):
-    def check_all_empty(replica):
-        assert not bool(replica._consensus_data.preprepared)
-        assert not bool(replica._consensus_data.prepared)
+from plenum.test.stasher import delay_rules
 
-    def check_preprepared_not_empty(replica):
-        assert bool(replica._consensus_data.preprepared)
 
-    def check_prepared_not_empty(replica):
-        assert bool(replica._consensus_data.prepared)
+def test_check_cdp_pp_storages(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_steward):
+    def check_all_empty(replica, reverse=False):
+        check_preprepared_empty(replica, reverse)
+        check_prepared_empty(replica, reverse)
+
+    def check_preprepared_empty(replica, reverse=False):
+        statement_pp = bool(replica._consensus_data.preprepared)
+        statement_pp ^= reverse
+        assert not statement_pp
+
+    def check_prepared_empty(replica, reverse=False):
+        statement_p = bool(replica._consensus_data.prepared)
+        statement_p ^= reverse
+        assert not statement_p
 
-    def operation_for_replicas(operation, node_set=txnPoolNodeSet):
+    def operation_for_replicas(operation, node_set=txnPoolNodeSet, reverse=False):
         for node in node_set:
-            operation(node.master_replica)
+            operation(node.master_replica, reverse)
 
     node_stashers = [n.nodeIbStasher for n in txnPoolNodeSet]
 
-    with delay_rules(node_stashers, pDelay()):
-        with delay_rules(node_stashers, ppDelay()):
-            sdk_send_random_request(looper, sdk_pool_handle, sdk_wallet_client)
-            looper.run(eventually(operation_for_replicas, check_all_empty, txnPoolNodeSet[1:]))
-            looper.run(eventually(operation_for_replicas, check_preprepared_not_empty, txnPoolNodeSet[0:1]))
-        looper.run(eventually(operation_for_replicas, check_preprepared_not_empty, txnPoolNodeSet))
-    looper.run(eventually(operation_for_replicas, check_prepared_not_empty, txnPoolNodeSet))
+    with delay_rules(node_stashers, cDelay()):
+        with delay_rules(node_stashers, pDelay()):
+            with delay_rules(node_stashers, ppDelay()):
+                sdk_add_new_nym_without_waiting(looper, sdk_pool_handle, sdk_wallet_steward)
+                looper.run(eventually(operation_for_replicas, check_all_empty, txnPoolNodeSet[1:]))
+                looper.run(eventually(operation_for_replicas, check_preprepared_empty, txnPoolNodeSet[0:1], True))
+            looper.run(eventually(operation_for_replicas, check_preprepared_empty, txnPoolNodeSet, True))
+        looper.run(eventually(operation_for_replicas, check_prepared_empty, txnPoolNodeSet, True))
+    looper.run(eventually(operation_for_replicas, check_all_empty, txnPoolNodeSet, True))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_catchup_after_replica_removing.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_catchup_after_replica_removing.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_buffers_cleaning.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_buffers_cleaning.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,69 +1,52 @@
-from plenum.common.event_bus import InternalBus
-from plenum.common.startable import Mode
-from plenum.common.timer import QueueTimer
 from plenum.common.util import get_utc_epoch
-from plenum.server.database_manager import DatabaseManager
-from plenum.server.quorums import Quorums
 from plenum.server.replica import Replica
 from plenum.test.testing_utils import FakeSomething
 
 
 def test_ordered_cleaning(tconf):
     global_view_no = 2
 
     node = FakeSomething(
         name="fake node",
         ledger_ids=[0],
         viewNo=global_view_no,
         utc_epoch=get_utc_epoch,
         get_validators=lambda: [],
-        db_manager=DatabaseManager(),
-        requests=[],
-        mode=Mode.participating,
-        timer=QueueTimer(),
-        quorums=Quorums(4),
-        write_manager=None
     )
     bls_bft_replica = FakeSomething(
         gc=lambda *args: None,
     )
 
     replica = Replica(node, instId=0, config=tconf, bls_bft_replica=bls_bft_replica)
     replica._consensus_data.view_no = global_view_no
     total = []
 
     num_requests_per_view = 3
     for viewNo in range(global_view_no + 1):
         for seqNo in range(num_requests_per_view):
             reqId = viewNo, seqNo
-            replica._ordering_service._add_to_ordered(*reqId)
+            replica.addToOrdered(*reqId)
             total.append(reqId)
 
     # gc is called after stable checkpoint, since no request executed
     # in this test starting it manually
-    replica._ordering_service.gc(100)
+    replica._gc(100)
     # Requests with view lower then previous view
     # should not be in ordered
-    assert len(replica._ordering_service.ordered) == len(total[num_requests_per_view:])
+    assert len(replica.ordered) == len(total[num_requests_per_view:])
 
 
 def test_primary_names_cleaning(tconf):
     node = FakeSomething(
         name="fake node",
         ledger_ids=[0],
         viewNo=0,
         utc_epoch=get_utc_epoch,
         get_validators=lambda: [],
-        db_manager=DatabaseManager(),
-        requests=[],
-        mode=Mode.participating,
-        timer=QueueTimer(),
-        quorums=Quorums(4),
-        write_manager=None
     )
     bls_bft_replica = FakeSomething(
         gc=lambda *args: None,
     )
 
     replica = Replica(node, instId=0, config=tconf, bls_bft_replica=bls_bft_replica)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_primary_marked_suspicious_for_sending_prepare.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_primary_marked_suspicious_for_sending_prepare.py`

 * *Files 20% similar despite different names*

```diff
@@ -19,21 +19,21 @@
         n.nodeIbStasher.delay(cDelay(5))
 
 
 def testPrimarySendsAPrepareAndMarkedSuspicious(looper, txnPoolNodeSet, delay_commits,
                                                 preprepared1):
     def sendPrepareFromPrimary(instId):
         primary = getPrimaryReplica(txnPoolNodeSet, instId)
-        viewNo, ppSeqNo = next(iter(primary._ordering_service.sentPrePrepares.keys()))
-        ppReq = primary._ordering_service.sentPrePrepares[viewNo, ppSeqNo]
-        primary._ordering_service._do_prepare(ppReq)
+        viewNo, ppSeqNo = next(iter(primary.sentPrePrepares.keys()))
+        ppReq = primary.sentPrePrepares[viewNo, ppSeqNo]
+        primary.doPrepare(ppReq)
 
         def chk():
             for r in getNonPrimaryReplicas(txnPoolNodeSet, instId):
-                l = len([param for param in getAllArgs(r._ordering_service, r._ordering_service.process_prepare)
+                l = len([param for param in getAllArgs(r, r.processPrepare)
                          if param['sender'] == primary.name])
                 assert l == 1
 
         looper.run(eventually(chk))
 
     sendPrepareFromPrimary(0)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_ordered_tracker.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_ordered_tracker.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from random import randint
 
 import pytest
-from plenum.server.replica_helper import IntervalList, OrderedTracker
+from plenum.server.replica import IntervalList, OrderedTracker
 
 
 @pytest.fixture
 def intervals():
     return IntervalList()
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_create_3pc_batch.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_create_3pc_batch.py`

 * *Files 18% similar despite different names*

```diff
@@ -4,29 +4,29 @@
 from plenum.common.constants import POOL_LEDGER_ID, CONFIG_LEDGER_ID, DOMAIN_LEDGER_ID
 from plenum.common.exceptions import InvalidClientMessageException
 
 nodeCount = 4
 
 
 def test_create_3pc_batch_with_empty_requests(replica):
-    pp = replica._ordering_service.create_3pc_batch(0)
+    pp = replica.create_3pc_batch(0)
     assert pp is not None
     assert pp.reqIdr == tuple()
 
 
 def test_create_3pc_batch(replica_with_requests, fake_requests, txn_roots, state_roots):
-    replica_with_requests._ordering_service._consume_req_queue_for_pre_prepare = \
+    replica_with_requests.consume_req_queue_for_pre_prepare = \
         lambda ledger, tm, view_no, pp_seq_no: (fake_requests, [], [])
 
     for (ledger_id, pp_seq_no, has_bls) in [
         (POOL_LEDGER_ID, 1, False),
         (DOMAIN_LEDGER_ID, 2, True),
         (CONFIG_LEDGER_ID, 3, True)
     ]:
-        pre_prepare_msg = replica_with_requests._ordering_service.create_3pc_batch(ledger_id)
+        pre_prepare_msg = replica_with_requests.create_3pc_batch(ledger_id)
 
         assert pre_prepare_msg.poolStateRootHash == state_roots[POOL_LEDGER_ID]
         assert pre_prepare_msg.stateRootHash == state_roots[ledger_id]
         assert pre_prepare_msg.txnRootHash == txn_roots[ledger_id]
         assert pre_prepare_msg.ppSeqNo == pp_seq_no
         assert pre_prepare_msg.ledgerId == ledger_id
         assert pre_prepare_msg.viewNo == replica_with_requests.viewNo
@@ -38,14 +38,13 @@
     def randomDynamicValidation(self, req, pp_time):
         req_keys = [fake_req.key for fake_req in fake_requests]
         if req_keys.index(req.key) % 2:
             raise InvalidClientMessageException('aaaaaaaa',
                                                 req.reqId,
                                                 "not valid req")
 
-    replica_with_requests._ordering_service._do_dynamic_validation = \
-        functools.partial(randomDynamicValidation,
-                          replica_with_requests._ordering_service)
-    pre_prepare_msg = replica_with_requests._ordering_service.create_3pc_batch(DOMAIN_LEDGER_ID)
+    replica_with_requests.node.doDynamicValidation = functools.partial(randomDynamicValidation,
+                                                                       replica_with_requests.node)
+    pre_prepare_msg = replica_with_requests.create_3pc_batch(DOMAIN_LEDGER_ID)
     invalid_from_pp = invalid_index_serializer.deserialize(pre_prepare_msg.discarded)
     assert len(invalid_from_pp) == len(fake_requests) / 2
     assert pre_prepare_msg.reqIdr == tuple(res.key for res in fake_requests)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_3pc_messages_validation.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_3pc_messages_validation.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import pytest
 
 from plenum.common.config_helper import PNodeConfigHelper
 from plenum.common.startable import Mode
-from plenum.server.replica_validator_enums import INCORRECT_PP_SEQ_NO, OLD_VIEW, ALREADY_ORDERED, STASH_WATERMARKS, \
-    STASH_CATCH_UP, STASH_VIEW
+from plenum.server.replica_validator_enums import INCORRECT_PP_SEQ_NO, OLD_VIEW, ALREADY_ORDERED
 from plenum.test.helper import checkDiscardMsg, generate_state_root, create_prepare
 from plenum.test.test_node import TestNode
 from plenum.test.testing_utils import FakeSomething
 
 
 @pytest.fixture(scope='function')
 def test_node(
@@ -39,70 +38,70 @@
 def test_discard_process_three_phase_msg(test_node, looper):
     sender = "NodeSender"
     inst_id = 0
     replica = test_node.replicas[inst_id]
     view_no = test_node.viewNo
     pp_seq_no = 0  # should start with 1
     msg = create_prepare((view_no, pp_seq_no), generate_state_root(), inst_id)
-    replica._external_bus.process_incoming(msg, sender)
-    checkDiscardMsg([replica.stasher, ], msg, INCORRECT_PP_SEQ_NO)
+    replica.process_three_phase_msg(msg, sender)
+    checkDiscardMsg([replica, ], msg, INCORRECT_PP_SEQ_NO)
 
 
 def test_discard_process_three_phase_msg_for_old_view(test_node, looper):
     sender = "NodeSender"
     inst_id = 0
     replica = test_node.replicas[inst_id]
     view_no = test_node.viewNo - 1
     pp_seq_no = replica.last_ordered_3pc[1] + 1
     msg = create_prepare((view_no, pp_seq_no), generate_state_root(), inst_id)
-    replica._external_bus.process_incoming(msg, sender)
-    checkDiscardMsg([replica.stasher, ], msg, OLD_VIEW)
+    replica.process_three_phase_msg(msg, sender)
+    checkDiscardMsg([replica, ], msg, OLD_VIEW)
 
 
 def test_discard_process_three_phase_already_ordered_msg(test_node, looper):
     sender = "NodeSender"
     inst_id = 0
     replica = test_node.replicas[inst_id]
     replica.last_ordered_3pc = (test_node.viewNo, 100)
-    replica._checkpointer.update_watermark_from_3pc()
+    replica.update_watermark_from_3pc()
     view_no = test_node.viewNo
     pp_seq_no = replica.h
     msg = create_prepare((view_no, pp_seq_no), generate_state_root(), inst_id)
-    replica._external_bus.process_incoming(msg, sender)
-    checkDiscardMsg([replica.stasher, ], msg, ALREADY_ORDERED)
+    replica.process_three_phase_msg(msg, sender)
+    checkDiscardMsg([replica, ], msg, ALREADY_ORDERED)
 
 
 def test_process_three_phase_msg_with_catchup_stash(test_node, looper):
     sender = "NodeSender"
     inst_id = 0
     replica = test_node.replicas[inst_id]
-    old_catchup_stashed_msgs = replica.stasher.stash_size(STASH_CATCH_UP)
+    old_catchup_stashed_msgs = replica.stasher.num_stashed_catchup
     test_node.mode = Mode.syncing  # catchup in process
     view_no = test_node.viewNo
     pp_seq_no = replica.last_ordered_3pc[1] + 1
     msg = create_prepare((view_no, pp_seq_no), generate_state_root(), inst_id)
-    replica._external_bus.process_incoming(msg, sender)
-    assert old_catchup_stashed_msgs + 1 == replica.stasher.stash_size(STASH_CATCH_UP)
+    replica.process_three_phase_msg(msg, sender)
+    assert old_catchup_stashed_msgs + 1 == replica.stasher.num_stashed_catchup
 
 
 def test_process_three_phase_msg_and_stashed_future_view(test_node, looper):
     sender = "NodeSender"
     inst_id = 0
     replica = test_node.replicas[inst_id]
-    old_stashed_future_view_msgs = replica.stasher.stash_size(STASH_VIEW)
+    old_stashed_future_view_msgs = replica.stasher.num_stashed_future_view
     view_no = test_node.viewNo + 1
     pp_seq_no = replica.last_ordered_3pc[1] + 1
     msg = create_prepare((view_no, pp_seq_no), generate_state_root(), inst_id)
-    replica._external_bus.process_incoming(msg, sender)
-    assert old_stashed_future_view_msgs + 1 == replica.stasher.stash_size(STASH_VIEW)
+    replica.process_three_phase_msg(msg, sender)
+    assert old_stashed_future_view_msgs + 1 == replica.stasher.num_stashed_future_view
 
 
 def test_process_three_phase_msg_and_stashed_for_next_checkpoint(test_node, looper):
     sender = "NodeSender"
     inst_id = 0
     replica = test_node.replicas[inst_id]
-    old_stashed_watermarks_msgs = replica.stasher.stash_size(STASH_WATERMARKS)
+    old_stashed_watermarks_msgs = replica.stasher.num_stashed_watermarks
     view_no = test_node.viewNo
     pp_seq_no = replica.H + 1
     msg = create_prepare((view_no, pp_seq_no), generate_state_root(), inst_id)
-    replica._external_bus.process_incoming(msg, sender)
-    assert old_stashed_watermarks_msgs + 1 == replica.stasher.stash_size(STASH_WATERMARKS)
+    replica.process_three_phase_msg(msg, sender)
+    assert old_stashed_watermarks_msgs + 1 == replica.stasher.num_stashed_watermarks
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_get_last_timestamp_from_state.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_get_last_timestamp_from_state.py`

 * *Files 5% similar despite different names*

```diff
@@ -53,14 +53,14 @@
                               sdk_wallet_steward,
                               1)
     primary_node = get_master_primary_node(txnPoolNodeSet)
     excpected_ts = get_utc_epoch() + 30
     req_handler = primary_node.write_manager.request_handlers[NYM][0]
     req_handler.database_manager.ts_store.set(excpected_ts,
                                               req_handler.state.headHash)
-    primary_node.master_replica._ordering_service.last_accepted_pre_prepare_time = None
+    primary_node.master_replica.last_accepted_pre_prepare_time = None
     reply = sdk_send_random_and_check(looper,
                                       txnPoolNodeSet,
                                       sdk_pool_handle,
                                       sdk_wallet_steward,
                                       1)[0][1]
     assert abs(excpected_ts - int(get_txn_time(reply['result']))) < 3
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_replicas_primary_names.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_replicas_primary_names.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_update_watermarks_api.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_update_watermarks_api.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,99 +2,99 @@
 
 from plenum.test.replica.helper import emulate_catchup, emulate_select_primaries
 
 
 def test_propagate_primary_is_Master_update_watermarks(replica):
     # expected behaviour is that h must be set as last ordered ppSeqNo
     replica.isMaster = True
-    replica._checkpointer.set_watermarks(low_watermark=0)
+    replica.h = 0
     replica.last_ordered_3pc = (replica.viewNo, 500)
     assert replica.h == 0
     replica.on_propagate_primary_done()
     assert replica.h == 500
 
 
 def test_propagate_primary_is_Master_watermarks_not_changed_if_last_ordered_not_changed(replica):
     replica.isMaster = True
-    replica._checkpointer.set_watermarks(low_watermark=0)
+    replica.h = 0
     assert replica.h == 0
     replica.on_propagate_primary_done()
     assert replica.h == 0
 
 
 def test_propagate_primary_not_Master_update_watermarks_to_maxsize(replica):
     # expected behaviour is that h must be set as last ordered ppSeqNo
     replica.isMaster = False
-    replica._checkpointer.set_watermarks(low_watermark=0)
+    replica.h = 0
     replica.last_ordered_3pc = (0, 500)
     assert replica.h == 0
     replica.on_propagate_primary_done()
     assert replica.h == 0
     assert replica.H == sys.maxsize
 
 
 def test_propagate_primary_not_Master_watermarks_not_changed_if_last_ordered_not_changed(replica):
     replica.isMaster = False
-    replica._checkpointer.set_watermarks(low_watermark=0)
+    replica.h = 0
     assert replica.h == 0
     replica.on_propagate_primary_done()
     assert replica.h == 0
 
 
 def test_propagate_primary_non_Master_watermarks_not_maxsize_if_is_primary(replica, tconf):
     replica.isMaster = False
-    replica._consensus_data.primary_name = replica.name
-    replica._checkpointer.set_watermarks(low_watermark=100)
+    replica._primaryName = replica.name
+    replica.h = 100
     replica.on_propagate_primary_done()
     assert replica.H == 100 + tconf.LOG_SIZE
 
 
 def test_catchup_clear_for_backup(replica):
-    replica._consensus_data.primary_name = None
+    replica._primaryName = None
     replica.isMaster = False
     replica._catchup_clear_for_backup()
     assert replica.last_ordered_3pc == (replica.viewNo, 0)
     assert replica.h == 0
     assert replica.H == sys.maxsize
 
 def test_reset_watermarks_before_new_view_on_master(replica, tconf):
     replica.isMaster = True
-    replica._checkpointer.set_watermarks(low_watermark=100)
-    replica._checkpointer.reset_watermarks_before_new_view()
+    replica.h = 100
+    replica._reset_watermarks_before_new_view()
     assert replica.h == 0
     assert replica.H == tconf.LOG_SIZE
-    assert replica._ordering_service._lastPrePrepareSeqNo == replica.h
+    assert replica._lastPrePrepareSeqNo == replica.h
 
 
 def test_reset_watermarks_before_new_view_non_master(replica, tconf):
     replica.isMaster = False
-    replica._checkpointer.set_watermarks(low_watermark=100)
-    replica._checkpointer.reset_watermarks_before_new_view()
+    replica.h = 100
+    replica._reset_watermarks_before_new_view()
     assert replica.h == 0
     assert replica.H == tconf.LOG_SIZE
-    assert replica._ordering_service._lastPrePrepareSeqNo == replica.h
+    assert replica._lastPrePrepareSeqNo == replica.h
 
 
 def test_catchup_without_vc_and_no_primary_on_master(replica, tconf):
     ppSeqNo = 100
     replica.isMaster = True
-    replica._consensus_data.primary_name = None
+    replica._primaryName = None
     # next calls emulate a catchup without vc when there is no primary selected
     # like it will be called for 'update watermark' procedure
     emulate_catchup(replica, ppSeqNo)
     # select_primaries after allLedgersCaughtUp
     emulate_select_primaries(replica)
     assert replica.h == ppSeqNo
     assert replica.H == ppSeqNo + tconf.LOG_SIZE
 
 
 def test_catchup_without_vc_and_no_primary_on_backup(replica, tconf):
     ppSeqNo = 100
     replica.isMaster = False
-    replica._consensus_data.primary_name = None
+    replica._primaryName = None
     # next calls emulate a catchup without vc when there is no primary selected
     # like it will be called for 'update watermark' procedure
     emulate_catchup(replica, ppSeqNo)
     # select_primaries after allLedgersCaughtUp
     emulate_select_primaries(replica)
     if replica.viewNo > 0:
         assert replica.h == 0
@@ -104,78 +104,78 @@
         assert replica.H == sys.maxsize
 
 
 def test_catchup_without_during_vc_with_primary_on_master(replica, tconf):
     # this test emulate situation of simple catchup procedure without view_change
     # (by checkpoints or ledger_statuses)
     ppSeqNo = 100
-    replica._consensus_data.primary_name = 'SomeNode'
+    replica._primaryName = 'SomeNode'
     replica.isMaster = True
     emulate_catchup(replica, ppSeqNo)
     assert replica.last_ordered_3pc == (replica.viewNo, 100)
     assert replica.h == ppSeqNo
     assert replica.H == ppSeqNo + tconf.LOG_SIZE
 
 
 
 def test_catchup_without_during_vc_with_primary_on_backup(replica):
     # this test emulate situation of simple catchup procedure without view_change
     # (by checkpoints or ledger_statuses)
     ppSeqNo = 100
-    replica._consensus_data.primary_name = 'SomeNode'
+    replica._primaryName = 'SomeNode'
     replica.isMaster = False
     emulate_catchup(replica, ppSeqNo)
     assert replica.last_ordered_3pc == (replica.viewNo, 0)
     assert replica.h == 0
     assert replica.H == sys.maxsize
 
 
 def test_view_change_no_propagate_primary_on_master(replica, tconf):
     ppSeqNo = 100
     replica.isMaster = True
-    replica._consensus_data.primary_name = 'SomeNode'
+    replica._primaryName = 'SomeNode'
     # next calls emulate simple view_change procedure (replica's watermark related steps)
     emulate_catchup(replica, ppSeqNo)
     emulate_select_primaries(replica)
     assert replica.h == ppSeqNo
     assert replica.H == ppSeqNo + tconf.LOG_SIZE
 
 
 def test_view_change_no_propagate_primary_on_backup(replica, tconf):
     ppSeqNo = 100
     replica.isMaster = False
-    replica._consensus_data.primary_name = 'SomeNode'
+    replica._primaryName = 'SomeNode'
     # next calls emulate simple view_change procedure (replica's watermark related steps)
     emulate_catchup(replica, ppSeqNo)
     emulate_select_primaries(replica)
     if replica.viewNo > 0:
         assert replica.h == 0
         assert replica.H == tconf.LOG_SIZE
     else:
         assert replica.h == 0
         assert replica.H == sys.maxsize
 
 
 def test_view_change_propagate_primary_on_master(replica, tconf):
     ppSeqNo = 100
     replica.isMaster = True
-    replica._consensus_data.primary_name = 'SomeNode'
+    replica._primaryName = 'SomeNode'
     # next calls emulate view_change for propagate primary situation
     # (when the new node join to the pool)
     emulate_catchup(replica, ppSeqNo)
     emulate_select_primaries(replica)
     replica.on_propagate_primary_done()
     assert replica.h == ppSeqNo
     assert replica.H == ppSeqNo + tconf.LOG_SIZE
 
 
 def test_view_change_propagate_primary_on_backup(replica):
     ppSeqNo = 100
     replica.isMaster = False
-    replica._consensus_data.primary_name = 'SomeNode'
+    replica._primaryName = 'SomeNode'
     # next calls emulate view_change for propagate primary situation
     # (when the new node join to the pool)
     emulate_catchup(replica, ppSeqNo)
     emulate_select_primaries(replica)
     replica.on_propagate_primary_done()
     assert replica.h == 0
     assert replica.H == sys.maxsize
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_catchup_after_replica_addition.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_catchup_after_replica_addition.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/stashing/test_stash_future_view.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/stashing/test_stash_future_view.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 import pytest as pytest
 
 from plenum.common.constants import COMMIT, PREPREPARE, PREPARE
 from plenum.common.startable import Mode
-from plenum.server.replica_validator_enums import STASH_VIEW
 from plenum.test.delayers import vcd_delay, msg_rep_delay
 from plenum.test.helper import waitForViewChange, sdk_send_random_and_check, assertExp
 from plenum.test.node_catchup.helper import waitNodeDataEquality
 from plenum.test.stasher import delay_rules
 from plenum.test.test_node import ensureElectionsDone
 from stp_core.loop.eventually import eventually
 
@@ -28,15 +27,15 @@
     and other nodes ordered.
     6. Reset delays.
     7. Check that the last request is ordered on the slow_node and stashed messages were removed.
     """
     slow_node = txnPoolNodeSet[-1]
     fast_nodes = txnPoolNodeSet[:-1]
     view_no = slow_node.viewNo
-    old_stashed = {inst_id: r.stasher.stash_size(STASH_VIEW)
+    old_stashed = {inst_id: r.stasher.num_stashed_future_view
                    for inst_id, r in slow_node.replicas.items()}
     last_ordered = {inst_id: r.last_ordered_3pc
                     for inst_id, r in slow_node.replicas.items()}
     with delay_rules([slow_node.nodeIbStasher, ],
                      msg_rep_delay(types_to_delay=[PREPREPARE, PREPARE, COMMIT])):
         with delay_rules([slow_node.nodeIbStasher, ], vcd_delay()):
             for n in txnPoolNodeSet:
@@ -54,23 +53,23 @@
                                       1)
             assert slow_node.view_change_in_progress
             # 1 - pre-prepare msg
             # (len(txnPoolNodeSet) - 2) - prepare msgs
             # (len(txnPoolNodeSet) - 1) - commit msgs
             stashed_master_messages = 2 * (1 + (len(txnPoolNodeSet) - 2) + (len(txnPoolNodeSet) - 1))
             stashed_backup_messages = 2 * (1 + (len(txnPoolNodeSet) - 2) + (len(txnPoolNodeSet) - 1))
-            assert slow_node.master_replica.stasher.stash_size(STASH_VIEW) == old_stashed[0] + stashed_master_messages
-            assert all(r.stasher.stash_size(STASH_VIEW) == old_stashed[inst_id] + stashed_backup_messages
+            assert slow_node.master_replica.stasher.num_stashed_future_view == old_stashed[0] + stashed_master_messages
+            assert all(r.stasher.num_stashed_future_view == old_stashed[inst_id] + stashed_backup_messages
                        for inst_id, r in slow_node.replicas.items() if inst_id != 0)
             assert all(r.last_ordered_3pc == last_ordered[inst_id]
                        for inst_id, r in slow_node.replicas.items())
 
         def chk():
             for inst_id, r in slow_node.replicas.items():
                 if inst_id == 0:
                     assert r.last_ordered_3pc == (view_no + 1, 2)
                 else:
                     assert r.last_ordered_3pc == (view_no + 1, 2)
-                assert r.stasher.stash_size(STASH_VIEW) == old_stashed[inst_id]
+                assert r.stasher.num_stashed_future_view == old_stashed[inst_id]
 
         looper.run(eventually(chk))
         waitNodeDataEquality(looper, slow_node, *fast_nodes)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/stashing/test_stash_out_of_watermarks.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/stashing/test_stash_out_of_watermarks.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 from plenum.common.constants import COMMIT, PREPREPARE, PREPARE
-from plenum.server.replica_validator_enums import STASH_WATERMARKS
 from plenum.test.delayers import chk_delay, msg_rep_delay
 from plenum.test.helper import sdk_send_random_and_check, sdk_send_batches_of_random_and_check, incoming_3pc_msgs_count
 from plenum.test.node_catchup.helper import ensure_all_nodes_have_same_data
 from plenum.test.stasher import delay_rules
 from stp_core.loop.eventually import eventually
 from plenum.test.checkpoints.conftest import chkFreqPatched
 
@@ -26,20 +25,20 @@
     6. Reset delays.
     7. Check that the last request is ordered on the slow_node, checkpoint is finalized
     and stashed messages were removed.
     """
 
     for n in txnPoolNodeSet:
         for r in n.replicas.values():
-            r._checkpointer.update_watermark_from_3pc()
+            r.update_watermark_from_3pc()
 
     slow_node = txnPoolNodeSet[-1]
     fast_nodes = txnPoolNodeSet[:-1]
 
-    old_stashed = {inst_id: r.stasher.stash_size(STASH_WATERMARKS)
+    old_stashed = {inst_id: r.stasher.num_stashed_watermarks
                    for inst_id, r in slow_node.replicas.items()}
     last_ordered = {inst_id: r.last_ordered_3pc
                     for inst_id, r in slow_node.replicas.items()}
     with delay_rules([slow_node.nodeIbStasher, ],
                      msg_rep_delay(types_to_delay=[PREPREPARE, PREPARE, COMMIT])):
         with delay_rules([slow_node.nodeIbStasher, ], chk_delay()):
             sdk_send_batches_of_random_and_check(looper,
@@ -47,40 +46,41 @@
                                                  sdk_pool_handle,
                                                  sdk_wallet_client,
                                                  num_reqs=1 * CHK_FREQ,
                                                  num_batches=CHK_FREQ)
             ensure_all_nodes_have_same_data(looper, nodes=txnPoolNodeSet)
             looper.run(eventually(_check_checkpoint_finalize,
                                   fast_nodes,
-                                  CHK_FREQ))
+                                  1, CHK_FREQ))
             sdk_send_random_and_check(looper,
                                       txnPoolNodeSet,
                                       sdk_pool_handle,
                                       sdk_wallet_client,
                                       1)
 
             stashed_messages = incoming_3pc_msgs_count(len(txnPoolNodeSet))
-            assert all(r.stasher.stash_size(STASH_WATERMARKS) == old_stashed[inst_id] + stashed_messages
+            assert all(r.stasher.num_stashed_watermarks == old_stashed[inst_id] + stashed_messages
                        for inst_id, r in slow_node.replicas.items())
 
             _check_batches_ordered(slow_node, last_ordered, CHK_FREQ)
             for n in fast_nodes:
                 _check_batches_ordered(n, last_ordered, CHK_FREQ + 1)
 
         looper.run(eventually(_check_checkpoint_finalize,
                               [slow_node, ],
-                              CHK_FREQ))
+                              1, CHK_FREQ))
         looper.run(eventually(_check_batches_ordered,
                               slow_node, last_ordered, CHK_FREQ + 1))
-        assert all(r.stasher.stash_size(STASH_WATERMARKS) == old_stashed[inst_id]
+        assert all(r.stasher.num_stashed_watermarks == old_stashed[inst_id]
                    for inst_id, r in slow_node.replicas.items())
 
 
-def _check_checkpoint_finalize(nodes, end_pp_seq_no):
+def _check_checkpoint_finalize(nodes, start_pp_seq_no, end_pp_seq_no):
     for n in nodes:
-        assert n.master_replica._consensus_data.stable_checkpoint == end_pp_seq_no
+        checkpoint = n.master_replica.checkpoints[(start_pp_seq_no, end_pp_seq_no)]
+        assert checkpoint.isStable
 
 
 def _check_batches_ordered(node, last_ordered, num_batches_ordered):
     assert all(r.last_ordered_3pc == (last_ordered[inst_id][0],
                                       last_ordered[inst_id][1] + num_batches_ordered)
                for inst_id, r in node.replicas.items())
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/stashing/test_unstash_after_catchup_in_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/stashing/test_unstash_after_catchup_in_view_change.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 import sys
 
 import pytest as pytest
 
 from plenum.common.constants import COMMIT, PREPREPARE, PREPARE, LEDGER_STATUS
 from plenum.common.startable import Mode
-from plenum.server.replica_validator_enums import STASH_CATCH_UP, STASH_VIEW
 from plenum.test.delayers import vcd_delay, msg_rep_delay, cDelay, cr_delay
 from plenum.test.helper import waitForViewChange, sdk_send_random_and_check, assertExp, sdk_send_random_request, \
     sdk_get_and_check_replies
 from plenum.test.node_catchup.helper import ensure_all_nodes_have_same_data
 from plenum.test.node_request.helper import sdk_ensure_pool_functional
 from plenum.test.stasher import delay_rules
 from plenum.test.test_node import ensureElectionsDone
@@ -37,15 +36,15 @@
     11. Check that 3 nodes finished VC while Node4 is syncing and not finished
     12. Reset CatchupRep on Node4
     13. Check that Node4 finished VC, and there was just 1 round of catch-up
     """
     slow_node = txnPoolNodeSet[-1]
     fast_nodes = txnPoolNodeSet[:-1]
     view_no = txnPoolNodeSet[0].viewNo
-    old_stashed = slow_node.master_replica.stasher.stash_size(STASH_VIEW)
+    old_stashed = slow_node.master_replica.stasher.num_stashed_future_view
     last_ordered = txnPoolNodeSet[0].master_replica.last_ordered_3pc
 
     with delay_rules([n.nodeIbStasher for n in txnPoolNodeSet],
                      msg_rep_delay(types_to_delay=[PREPREPARE, PREPARE, COMMIT])):
 
         # Delay Commit messages for slow_node.
         slow_node.nodeIbStasher.delay(cDelay(sys.maxsize))
@@ -57,16 +56,16 @@
             n.nodeIbStasher.delay(cDelay(sys.maxsize))
 
         request2 = sdk_send_random_request(looper, sdk_pool_handle, sdk_wallet_steward)
 
         def check_commits(commit_key):
             for n in fast_nodes:
                 for r in n.replicas.values():
-                    assert commit_key in r._ordering_service.commits
-                    assert len(r._ordering_service.commits[commit_key].voters) == 1
+                    assert commit_key in r.commits
+                    assert len(r.commits[commit_key].voters) == 1
 
         looper.run(eventually(check_commits,
                               (view_no, last_ordered[1] + 2)))
 
         # Delay CatchupRep messages for the slow_node.
         with delay_rules([slow_node.nodeIbStasher], cr_delay()):
             with delay_rules([n.nodeIbStasher for n in fast_nodes], vcd_delay()):
@@ -111,8 +110,8 @@
 
     ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
     sdk_ensure_pool_functional(looper, txnPoolNodeSet, sdk_wallet_steward, sdk_pool_handle)
 
 
 def _check_nodes_stashed(nodes, old_stashed, new_stashed):
     for n in nodes:
-        assert n.master_replica.stasher.stash_size(STASH_CATCH_UP) == old_stashed + new_stashed
+        assert n.master_replica.stasher.num_stashed_catchup == old_stashed + new_stashed
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/stashing/test_replica_stasher.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/stashing/test_replica_stasher.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_max_3pc_batches_in_flight.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_max_3pc_batches_in_flight.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_replica_reject_same_pre_prepare.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_replica_reject_same_pre_prepare.py`

 * *Files 2% similar despite different names*

```diff
@@ -47,34 +47,34 @@
     request1 = sdk_json_to_request_object(req1[0])
     for npr in nonPrimaryReplicas:
         looper.run(eventually(checkPrepareReqSent,
                               npr,
                               request1.key,
                               primaryRepl.viewNo,
                               retryWait=1))
-    prePrepareReq = primaryRepl._ordering_service.sentPrePrepares[primaryRepl.viewNo,
+    prePrepareReq = primaryRepl.sentPrePrepares[primaryRepl.viewNo,
                                                 primaryRepl.lastPrePrepareSeqNo]
     looper.run(eventually(checkPrePrepareReqRecvd,
                           nonPrimaryReplicas,
                           prePrepareReq,
                           retryWait=1))
 
     # logger.debug("Patching the primary replica's pre-prepare sending method ")
     # orig_method = primaryRepl.sendPrePrepare
 
     # def patched(self, ppReq):
-    #     self._ordering_service.sentPrePrepares[ppReq.viewNo, ppReq.ppSeqNo] = ppReq
+    #     self.sentPrePrepares[ppReq.viewNo, ppReq.ppSeqNo] = ppReq
     #     ppReq = updateNamedTuple(ppReq, **{f.PP_SEQ_NO.nm: 1})
     #     self.send(ppReq, TPCStat.PrePrepareSent)
     #
     # primaryRepl.sendPrePrepare = types.MethodType(patched, primaryRepl)
     logger.debug(
         "Decrementing the primary replica's pre-prepare sequence number by "
         "one...")
-    primaryRepl._ordering_service._lastPrePrepareSeqNo -= 1
+    primaryRepl._lastPrePrepareSeqNo -= 1
     view_no = primaryRepl.viewNo
     request2 = sdk_json_to_request_object(
         sdk_send_random_requests(looper,
                                  sdk_pool_handle,
                                  sdk_wallet_client,
                                  1)[0][0])
     timeout = waits.expectedPrePrepareTime(len(txnPoolNodeSet))
@@ -92,16 +92,16 @@
         view_no,
         primaryRepl.lastPrePrepareSeqNo,
         get_utc_epoch(),
         reqIdr,
         init_discarded(),
         primaryRepl.batchDigest([request2]),
         DOMAIN_LEDGER_ID,
-        primaryRepl._ordering_service.get_state_root_hash(DOMAIN_LEDGER_ID),
-        primaryRepl._ordering_service.get_txn_root_hash(DOMAIN_LEDGER_ID),
+        primaryRepl.stateRootHash(DOMAIN_LEDGER_ID),
+        primaryRepl.txnRootHash(DOMAIN_LEDGER_ID),
         0,
         True
     )
 
     logger.debug("""Checking whether all the non primary replicas have received
                 the pre-prepare request with same sequence number""")
     timeout = waits.expectedPrePrepareTime(len(txnPoolNodeSet))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_replica_clear_collections_after_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_replica_clear_collections_after_view_change.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,10 +54,10 @@
                                          sdk_pool_handle,
                                          sdk_wallet_client,
                                          num_reqs=reqs_for_checkpoint)
 
     def check_request_queues():
         assert len(txnPoolNodeSet[0].requests) == 1
         for n in txnPoolNodeSet:
-            assert len(n.replicas[1]._ordering_service.requestQueues[DOMAIN_LEDGER_ID]) == 0
+            assert len(n.replicas[1].requestQueues[DOMAIN_LEDGER_ID]) == 0
 
     looper.run(eventually(check_request_queues))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_bitmask_apply.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_bitmask_apply.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_replica_3pc_validation.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_replica_3pc_validation.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import pytest
 
 from plenum.common.startable import Mode
-from plenum.common.stashing_router import PROCESS, DISCARD
 from plenum.server.replica_validator import ReplicaValidator
-from plenum.server.replica_validator_enums import INCORRECT_INSTANCE, ALREADY_ORDERED, FUTURE_VIEW, \
+from plenum.server.replica_validator_enums import DISCARD, INCORRECT_INSTANCE, PROCESS, ALREADY_ORDERED, FUTURE_VIEW, \
     GREATER_PREP_CERT, OLD_VIEW, CATCHING_UP, OUTSIDE_WATERMARKS, INCORRECT_PP_SEQ_NO, STASH_VIEW, STASH_WATERMARKS, \
     STASH_CATCH_UP
 from plenum.test.helper import create_pre_prepare_no_bls, generate_state_root, create_commit_no_bls_sig, create_prepare
 
 
 @pytest.fixture(scope='function', params=[0, 1])
 def inst_id(request):
@@ -256,15 +255,15 @@
     (101, (PROCESS, None)),
     (400, (PROCESS, None)),
     (401, (STASH_WATERMARKS, OUTSIDE_WATERMARKS)),
     (402, (STASH_WATERMARKS, OUTSIDE_WATERMARKS)),
     (100000, (STASH_WATERMARKS, OUTSIDE_WATERMARKS)),
 ])
 def test_check_watermarks_changed(validator, pp_seq_no, result):
-    validator.replica._checkpointer.set_watermarks(low_watermark=100)
+    validator.replica.h = 100
     for msg in create_3pc_msgs(view_no=validator.view_no,
                                pp_seq_no=pp_seq_no,
                                inst_id=validator.inst_id):
         assert validator.validate_3pc_msg(msg) == result
 
 
 def test_check_zero_pp_seq_no(validator):
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_monitor_reset_after_replica_addition.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_monitor_reset_after_replica_addition.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_revert_from_malicious.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_revert_from_malicious.py`

 * *Files 22% similar despite different names*

```diff
@@ -10,14 +10,14 @@
                                   sdk_pool_handle,
                                   sdk_wallet_client):
     def raise_invalid_ex():
         raise InvalidClientMessageException(1,2,3)
     malicious_primary = getPrimaryReplica(txnPoolNodeSet).node
     not_malicious_nodes = set(txnPoolNodeSet) - {malicious_primary}
     for n in not_malicious_nodes:
-        n.master_replica._ordering_service._do_dynamic_validation = lambda *args, **kwargs: raise_invalid_ex()
+        n.doDynamicValidation = lambda *args, **kwargs: raise_invalid_ex()
     with pytest.raises(RequestRejectedException, match="client request invalid"):
         sdk_send_random_and_check(looper,
                                   txnPoolNodeSet,
                                   sdk_pool_handle,
                                   sdk_wallet_client,
                                   1)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/helper.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,8 +14,8 @@
         raise ex
 
     replica.node.reportSuspiciousNodeEx = reportSuspiciousNodeEx
 
 def register_pp_ts(replica, pp, sender):
     tpcKey = (pp.viewNo, pp.ppSeqNo)
     ppKey = (pp, sender)
-    replica._ordering_service.pre_prepare_tss[tpcKey][ppKey] = replica.get_time_for_3pc_batch()
+    replica.pre_prepare_tss[tpcKey][ppKey] = replica.get_time_for_3pc_batch()
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_replica_received_preprepare_with_unknown_request.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_replica_received_preprepare_with_unknown_request.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-from plenum.server.consensus.ordering_service import OrderingService
 from plenum.server.node import Node
 from plenum.server.replica import Replica
 from plenum.server.suspicion_codes import Suspicions
 from plenum.test.delayers import ppgDelay
 from plenum.test.node_request.test_propagate.helper import sum_of_request_propagates
 from plenum.test.stasher import delay_rules
 from stp_core.common.log import getlogger
@@ -19,30 +18,30 @@
                                                           sdk_pool_handle,
                                                           sdk_wallet_steward,
                                                           chkFreqPatched):
     sdk_send_random_and_check(looper, txnPoolNodeSet,
                               sdk_pool_handle, sdk_wallet_steward, 1)
 
     replica = txnPoolNodeSet[1].master_replica
-    params = replica._ordering_service.spylog.getLastParams(OrderingService.process_preprepare)
+    params = replica.spylog.getLastParams(Replica.processPrePrepare)
     pp = params["pre_prepare"]
     sender = params["sender"]
     start_request_propagate_count = replica.node.spylog.count(Node.request_propagates)
 
     def discard(offendingMsg, reason, logger, cliOutput=False):
         assert offendingMsg == pp
         assert Suspicions.PPR_WITH_ORDERED_REQUEST.reason == reason
 
     replica.node.discard = discard
 
     register_pp_ts(replica, pp, sender)
-    replica._ordering_service.process_preprepare(pp, sender)
+    replica.processPrePrepare(pp, sender)
 
     assert 0 == replica.node.spylog.count(Node.request_propagates) - start_request_propagate_count
-    assert (pp, sender, set(pp.reqIdr)) not in replica._ordering_service.prePreparesPendingFinReqs
+    assert (pp, sender, set(pp.reqIdr)) not in replica.prePreparesPendingFinReqs
 
 
 def test_replica_received_preprepare_with_unknown_request(looper,
                                                           txnPoolNodeSet,
                                                           sdk_pool_handle,
                                                           sdk_wallet_steward,
                                                           chkFreqPatched,
@@ -51,13 +50,13 @@
                               sdk_pool_handle, sdk_wallet_steward, 1)
     replica = txnPoolNodeSet[1].master_replica
     start_request_propagate_count = sum_of_request_propagates(txnPoolNodeSet[1])
     with delay_rules(txnPoolNodeSet[1].nodeIbStasher, ppgDelay(delay=10)):
         sdk_send_random_and_check(looper, txnPoolNodeSet,
                                   sdk_pool_handle, sdk_wallet_steward, 1)
 
-    params = replica._ordering_service.spylog.getLastParams(OrderingService.process_preprepare)
+    params = replica.spylog.getLastParams(Replica.processPrePrepare)
     pp = params["pre_prepare"]
     sender = params["sender"]
     looper.runFor(tconf.PROPAGATE_REQUEST_DELAY)
-    assert (pp, sender, set(pp.reqIdr)) not in replica._ordering_service.prePreparesPendingFinReqs
+    assert (pp, sender, set(pp.reqIdr)) not in replica.prePreparesPendingFinReqs
     assert 1 == sum_of_request_propagates(txnPoolNodeSet[1]) - start_request_propagate_count
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica/test_instance_faulty_processor.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_instance_faulty_processor.py`

 * *Files 0% similar despite different names*

```diff
@@ -2,15 +2,16 @@
 
 from plenum.common.messages.node_messages import BackupInstanceFaulty
 from plenum.common.types import f
 from plenum.server.backup_instance_faulty_processor import BackupInstanceFaultyProcessor
 from plenum.server.quorums import Quorums
 from plenum.server.replica import Replica
 from plenum.server.suspicion_codes import Suspicions
-from plenum.test.primary_selection.test_view_changer_primary_selection import FakeNode
+from plenum.test.primary_selection.test_primary_selector import FakeNode
+from plenum.test.testing_utils import FakeSomething
 
 
 class FakeReplicas:
     def __init__(self, node, replicas):
         self._replicas = replicas
         self._node = node
         self.remove_replica_calls = []
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_log_rotation.py` & `indy-plenum-1.9.2rc1/plenum/test/test_log_rotation.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_restarted_node_not_complete_vc_before_others.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_restarted_node_not_complete_vc_before_others.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_changes_if_master_primary_disconnected.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_changes_if_master_primary_disconnected.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_by_current_state.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_by_current_state.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/conftest.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,19 @@
 import functools
 from collections import deque
 
 import pytest
 
-from plenum.common.event_bus import InternalBus
 from plenum.common.timer import QueueTimer
 from plenum.common.util import get_utc_epoch
 from plenum.server.node import Node
 from plenum.server.quorums import Quorums
 from plenum.server.view_change.node_view_changer import create_view_changer
 from plenum.test.conftest import getValueFromModule
-from plenum.test.primary_selection.test_view_changer_primary_selection import FakeNode
+from plenum.test.primary_selection.test_primary_selector import FakeNode
 from plenum.test.test_node import getRequiredInstances
 from plenum.test.testing_utils import FakeSomething
 
 
 @pytest.fixture()
 def viewNo(txnPoolNodeSet):
     viewNos = set()
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_pp_seq_no_starts_from_1.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_pp_seq_no_starts_from_1.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_if_primary_disconnected.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_if_primary_disconnected.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_vc_started_in_different_time.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_vc_started_in_different_time.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_n_minus_f_quorum.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_n_minus_f_quorum.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_pre_vc_strategy_3PC_msgs.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_pre_vc_strategy_3PC_msgs.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_resend_inst_ch_in_progress_v_ch.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_resend_inst_ch_in_progress_v_ch.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_discard_inst_chng_msg_from_past_view.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_discard_inst_chng_msg_from_past_view.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_no_instance_change_before_node_is_ready.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_no_instance_change_before_node_is_ready.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_that_domain_ledger_the_same_after_restart_for_all_nodes.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_that_domain_ledger_the_same_after_restart_for_all_nodes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_add_vc_start_msg_during_start_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_add_vc_start_msg_during_start_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_timeout_reset.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_timeout_reset.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_last_completed_view_no_set_after_vc_complete.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_last_completed_view_no_set_after_vc_complete.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_api.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_api.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_timeout.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_timeout.py`

 * *Files 3% similar despite different names*

```diff
@@ -56,18 +56,17 @@
     if not ok:
         actual = ""
         for node in nodes:
             actual += "{}: called {}, returned true {}\n".format(node.name, call_count(node), true_count(node))
         raise AssertionError("Watchdog expected to be called {} times, actual counts:\n{}".format(times, actual))
 
 
-def stop_master_primary(nodes, view_no):
-    m_next_primary_name = nodes[0].primaries_selector.select_primaries(view_no,
-                                                                       nodes[0].requiredNumberOfInstances,
-                                                                       nodes[0].poolManager.node_names_ordered_by_rank())[0]
+def stop_next_primary(nodes):
+    m_next_primary_name = nodes[0]._elector._next_primary_node_name_for_master(
+        nodes[0].nodeReg, nodes[0].nodeIds)
     next(node for node in nodes if node.name == m_next_primary_name).stop()
     alive_nodes = list(filter(lambda x: x.name != m_next_primary_name, nodes))
     return alive_nodes
 
 
 def start_view_change(nodes, next_view_no):
     for n in nodes:
@@ -159,15 +158,16 @@
     """
     Verifies that a view change is restarted by timeout
     if the next primary has been disconnected
     """
     _, initial_view_no, timeout_callback_stats = setup
 
     start_view_change(txnPoolNodeSet, initial_view_no + 1)
-    alive_nodes = stop_master_primary(txnPoolNodeSet, initial_view_no + 1)
+
+    alive_nodes = stop_next_primary(txnPoolNodeSet)
 
     ensureElectionsDone(looper=looper, nodes=alive_nodes, instances_list=range(3))
 
     # There were 2 view changes
     for node in alive_nodes:
         assert (node.viewNo - initial_view_no) == 2
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_by_catchup_and_order.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_by_catchup_and_order.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_client_req_during_view_change_integration.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_client_req_during_view_change_integration.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_can_finish_despite_perpetual_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_can_finish_despite_perpetual_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_no_propagate_request_on_different_last_ordered_before_vc.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_no_propagate_request_on_different_last_ordered_before_vc.py`

 * *Files 1% similar despite different names*

```diff
@@ -169,16 +169,16 @@
     assert all(0 == node.spylog.count(node.request_propagates)
                for node in txnPoolNodeSet)
 
 
 def is_prepared(nodes: [Node], ppSeqNo, instId):
     for node in nodes:
         replica = node.replicas[instId]
-        assert (node.viewNo, ppSeqNo) in replica._ordering_service.prepares or \
-               (node.viewNo, ppSeqNo) in replica._ordering_service.sentPrePrepares
+        assert (node.viewNo, ppSeqNo) in replica.prepares or \
+               (node.viewNo, ppSeqNo) in replica.sentPrePrepares
 
 
 def check_last_ordered(nodes: [Node],
                        instId,
                        last_ordered=None):
     if last_ordered is None:
         last_ordered = nodes[0].replicas[instId].last_ordered_3pc
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_after_back_to_quorum_with_disconnected_primary_and_slow_node.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_after_back_to_quorum_with_disconnected_primary_and_slow_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_on_master_degraded.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_on_master_degraded.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_not_changed_when_primary_disconnected_from_less_than_quorum.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_not_changed_when_primary_disconnected_from_less_than_quorum.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_no_view_change_while_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_no_future_view_change_while_catchup.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,59 +1,65 @@
 import pytest
 
-from plenum.common.messages.node_messages import InstanceChange
+from plenum.common.messages.node_messages import FutureViewChangeDone
 from plenum.common.startable import Mode
+from plenum.test.delayers import icDelay
 from plenum.test.helper import checkViewNoForNodes, waitForViewChange
-from plenum.test.test_node import ensureElectionsDone
-from plenum.test.view_change.helper import do_view_change, revert_do_view_change
+from plenum.test.node_catchup.helper import ensure_all_nodes_have_same_data
+from plenum.test.stasher import delay_rules
+from plenum.test.test_node import ensureElectionsDone, checkProtocolInstanceSetup
+from plenum.test.view_change.helper import ensure_view_change
 from plenum.test.waits import expectedPoolViewChangeStartedTimeout
 from stp_core.loop.eventually import eventually
 
 
 @pytest.fixture(params=[Mode.starting, Mode.discovering, Mode.discovered, Mode.syncing])
 def mode(request):
     return request.param
 
 
-def check_instance_change_count(nodes, expected_count):
-    for node in nodes:
-        ic_count = sum(1 for msg in node.view_changer.inBox if isinstance(msg[0], InstanceChange))
-        assert expected_count == ic_count
+def check_future_vcd_count(node, expected_count):
+    vcd_count = sum(1 for msg in node.view_changer.inBox if isinstance(msg[0], FutureViewChangeDone))
+    assert expected_count == vcd_count
 
 
-def try_view_change(looper, nodes):
-    for node in nodes:
-        looper.run(eventually(node.view_changer.serviceQueues))
+def try_view_change(looper, node):
+    looper.run(eventually(node.view_changer.serviceQueues))
 
 
-def check_no_view_change(looper, nodes):
-    looper.run(eventually(check_instance_change_count, nodes, 3,
-                          timeout=expectedPoolViewChangeStartedTimeout(len(nodes))))
-    try_view_change(looper, nodes)
-    check_instance_change_count(nodes, 3)
+def check_no_view_change(looper, node):
+    looper.run(eventually(check_future_vcd_count, node, 3,
+                          timeout=expectedPoolViewChangeStartedTimeout(4)))
+    try_view_change(looper, node)
+    check_future_vcd_count(node, 3)
 
 
-def test_no_view_change_until_synced(txnPoolNodeSet, looper, mode):
+def test_no_propagated_future_view_change_until_synced(txnPoolNodeSet, looper, mode):
+    # the last node is a lagging one, which will receive ViewChangeDone messages for future view
+    viewNo = checkViewNoForNodes(txnPoolNodeSet)
+    lagged_node_index = (viewNo + 3) % len(txnPoolNodeSet)
+    lagged_node = txnPoolNodeSet[lagged_node_index]
+    other_nodes = list(set(txnPoolNodeSet) - {lagged_node})
+
     # emulate catchup by setting non-synced status
-    for node in txnPoolNodeSet:
-        node.mode = mode
+    lagged_node.mode = mode
+    old_view_no = checkViewNoForNodes([lagged_node])
 
-    check_instance_change_count(txnPoolNodeSet, 0)
+    check_future_vcd_count(lagged_node, 0)
 
-    # start View Change
-    old_view_no = checkViewNoForNodes(txnPoolNodeSet)
-    old_meths = do_view_change(txnPoolNodeSet)
-    for node in txnPoolNodeSet:
-        node.view_changer.sendInstanceChange(old_view_no + 1)
-
-    # make sure View Change is not started
-    check_no_view_change(looper, txnPoolNodeSet)
-    assert old_view_no == checkViewNoForNodes(txnPoolNodeSet)
-
-    # emulate finishing of catchup by setting Participating status
-    revert_do_view_change(txnPoolNodeSet, old_meths)
-    for node in txnPoolNodeSet:
-        node.mode = Mode.participating
-
-    # make sure that View Change happened
-    waitForViewChange(looper, txnPoolNodeSet, expectedViewNo=old_view_no + 1)
-    ensureElectionsDone(looper=looper, nodes=txnPoolNodeSet)
+    # delay INSTANCE CHANGE on lagged nodes, so all nodes except the lagging one finish View Change
+    with delay_rules(lagged_node.nodeIbStasher, icDelay()):
+        # make sure that View Change happened on all nodes but the lagging one
+        ensure_view_change(looper, other_nodes)
+        checkProtocolInstanceSetup(looper=looper, nodes=other_nodes, instances=range(2))
+        ensure_all_nodes_have_same_data(looper, nodes=other_nodes)
+
+        check_no_view_change(looper, lagged_node)
+        assert old_view_no == checkViewNoForNodes([lagged_node])
+
+        # emulate finishing of catchup by setting Participating status
+        lagged_node.mode = Mode.participating
+
+        # make sure that View Change happened on lagging node
+        waitForViewChange(looper, [lagged_node], expectedViewNo=old_view_no + 1,
+                          customTimeout=10)
+        ensureElectionsDone(looper=looper, nodes=txnPoolNodeSet)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_missing_pp_before_starting_vc.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_missing_pp_before_starting_vc.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_done_delayed.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_done_delayed.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_start_without_primary.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_start_without_primary.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_instance_change_from_unknown.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_instance_change_from_unknown.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_max_catchup_rounds.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_queueing_req_from_future_view.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,66 +1,71 @@
-from plenum.common.util import check_if_all_equal_in_list
-from plenum.test.delayers import pDelay, cDelay
-from plenum.test.helper import sdk_send_batches_of_random_and_check, sdk_send_random_requests
-from plenum.test.node_catchup.helper import ensure_all_nodes_have_same_data
-from plenum.test.test_node import getNonPrimaryReplicas, ensureElectionsDone
 from plenum.test.view_change.helper import ensure_view_change
+from stp_core.loop.eventually import eventually
+from stp_core.common.log import getlogger
+from plenum.test.delayers import icDelay, vcd_delay
+from plenum.test.helper import sdk_send_random_requests, \
+    sdk_get_replies, sdk_send_random_and_check
+from plenum.test.test_node import get_last_master_non_primary_node
 
+nodeCount = 7
 
-def test_view_change_after_max_catchup_rounds(txnPoolNodeSet, looper, sdk_pool_handle, sdk_wallet_client):
+logger = getlogger()
+
+
+# noinspection PyIncorrectDocstring
+def testQueueingReqFromFutureView(delayed_perf_chk, looper, txnPoolNodeSet,
+                                  sdk_pool_handle, sdk_wallet_client):
     """
-    The node should do only a fixed rounds of catchup. For this delay Prepares
-    and Commits for 2 non-primary nodes by a large amount which is equivalent
-    to loss of Prepares and Commits. Make sure 2 nodes have a different last
-    prepared certificate from other two. Then do a view change, make sure view
-    change completes and the pool does not process the request that were
-    prepared by only a subset of the nodes
+    Test if every node queues 3 Phase requests(PRE-PREPARE, PREPARE and COMMIT)
+    that come from a view which is greater than the current view.
+    - Delay reception and processing of view change messages by a non primary for master instance
+       => it starts receiving 3 phase commit messages for next view
     """
-    sdk_send_batches_of_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
-                                         sdk_wallet_client, 2 * 3, 3)
-    ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
-    ledger_summary = txnPoolNodeSet[0].ledger_summary
-
-    slow_nodes = [r.node for r in getNonPrimaryReplicas(
-        txnPoolNodeSet, 0)[-2:]]
-    fast_nodes = [n for n in txnPoolNodeSet if n not in slow_nodes]
-
-    # Make node slow to process Prepares and Commits
-    for node in slow_nodes:
-        node.nodeIbStasher.delay(pDelay(120, 0))
-        node.nodeIbStasher.delay(cDelay(120, 0))
-
-    sdk_send_random_requests(looper, sdk_pool_handle, sdk_wallet_client, 5)
-    looper.runFor(3)
-
-    ensure_view_change(looper, nodes=txnPoolNodeSet)
-
-    def last_prepared(nodes):
-        lst = [n.master_replica._ordering_service.l_last_prepared_certificate_in_view()
-               for n in nodes]
-        # All nodes have same last prepared
-        assert check_if_all_equal_in_list(lst)
-        return lst[0]
-
-    last_prepared_slow = last_prepared(slow_nodes)
-    last_prepared_fast = last_prepared(fast_nodes)
-
-    # Check `slow_nodes` and `fast_nodes` set different last_prepared
-    assert last_prepared_fast != last_prepared_slow
-
-    # View change complete
-    ensureElectionsDone(looper, txnPoolNodeSet)
-    ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
-
-    # The requests which were prepared by only a subset of the nodes were
-    # not ordered
-    assert txnPoolNodeSet[0].ledger_summary == ledger_summary
-
-    for node in slow_nodes:
-        node.nodeIbStasher.reset_delays_and_process_delayeds()
-
-    # Make sure pool is functional
-    sdk_send_batches_of_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
-                                         sdk_wallet_client, 10, 2)
-    ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
-    ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
-    last_prepared(txnPoolNodeSet)
+
+    lagging_node = get_last_master_non_primary_node(txnPoolNodeSet)
+    old_view_no = lagging_node.viewNo
+
+    # Delay processing of InstanceChange and ViewChangeDone so node stashes
+    # 3PC messages
+    delay_ic = 60
+    lagging_node.nodeIbStasher.delay(icDelay(delay_ic))
+    lagging_node.nodeIbStasher.delay(vcd_delay(delay_ic))
+    logger.debug('{} will delay its view change'.format(lagging_node))
+
+    def chk_fut_view(is_empty):
+        num_stashed = lagging_node.master_replica.stasher.num_stashed_future_view
+        if is_empty:
+            assert num_stashed == 0
+        else:
+            assert num_stashed > 0
+        return num_stashed
+
+    # No messages queued for future view
+    chk_fut_view(is_empty=True)
+    logger.debug('{} does not have any messages for future views'
+                 .format(lagging_node))
+
+    # Every node except Node A should do a view change
+    ensure_view_change(looper,
+                       [n for n in txnPoolNodeSet if n != lagging_node],
+                       [lagging_node])
+
+    # send more requests that will be queued for the lagged node
+    reqs = sdk_send_random_requests(looper, sdk_pool_handle,
+                                    sdk_wallet_client, 5)
+    l = looper.run(eventually(chk_fut_view, False,
+                              retryWait=1))
+    logger.debug('{} has {} messages for future views'
+                 .format(lagging_node, l))
+    sdk_get_replies(looper, reqs)
+    # reset delays for the lagging_node node so that it finally makes view
+    # change
+    lagging_node.reset_delays_and_process_delayeds()
+
+    # Eventually no messages queued for future view
+    looper.run(eventually(chk_fut_view, True,
+                          retryWait=1, timeout=delay_ic + 10))
+    logger.debug('{} exhausted pending messages for future views'
+                 .format(lagging_node))
+
+    sdk_send_random_and_check(looper, txnPoolNodeSet,
+                              sdk_pool_handle, sdk_wallet_client, 2)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_catchup_to_next_view_during_view_change_by_primary.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_catchup_to_next_view_during_view_change_by_primary.py`

 * *Files 1% similar despite different names*

```diff
@@ -49,15 +49,15 @@
 
             # order some txns
             sdk_send_random_and_check(looper, txnPoolNodeSet,
                                       sdk_pool_handle, sdk_wallet_steward, 5)
 
             assert initial_view_no == lagging_node.viewNo
             assert initial_last_ordered == lagging_node.master_last_ordered_3PC
-            assert len(lagging_node.master_replica._ordering_service.requestQueues[DOMAIN_LEDGER_ID]) > 0
+            assert len(lagging_node.master_replica.requestQueues[DOMAIN_LEDGER_ID]) > 0
 
         # make sure that the first View Change happened on lagging node
         waitForViewChange(looper, [lagging_node], expectedViewNo=initial_view_no + 1,
                           customTimeout=20)
         assert initial_view_no + 1 == lagging_node.viewNo
 
     # make sure that the second View Change happened on lagging node
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_complete_with_delayed_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_complete_with_delayed_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_prepare_in_queue_before_vc.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_prepare_in_queue_before_vc.py`

 * *Files 4% similar despite different names*

```diff
@@ -85,22 +85,22 @@
         while stashed_msgs:
             self.node.nodestack.rxMsgs.append(stashed_msgs.popleft())
 
     """Send REQ_COUNT txns"""
     slow_node = txnPoolNodeSet[-1]
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_steward, REQ_COUNT)
     """Check that there is REQ_COUNT prepares with quorum in queue"""
-    chk_quorumed_prepares_count(slow_node.master_replica._ordering_service.prepares, REQ_COUNT)
+    chk_quorumed_prepares_count(slow_node.master_replica.prepares, REQ_COUNT)
     """Patch processNodeInBox method for saving Prepares in nodeInBox queue"""
     not_processing_prepare(slow_node)
 
     """Send 1 txn"""
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_steward, REQ_COUNT_AFTER_SLOW)
 
-    chk_quorumed_prepares_count(slow_node.master_replica._ordering_service.prepares, REQ_COUNT)
+    chk_quorumed_prepares_count(slow_node.master_replica.prepares, REQ_COUNT)
 
     """Get last ordered 3pc key (should be (0, REQ_COUNT))"""
     ordered_lpc = slow_node.master_replica.last_ordered_3pc
     """Delay view_change_done messages"""
     slow_node.nodeIbStasher.delay(vcd_delay(100))
     """Patch on_view_change_start method for reverting processNodeInBox method"""
     slow_node.view_changer.start_view_change = functools.partial(patched_start_view_change, slow_node.view_changer)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_with_instance_change_lost_due_to_restarts.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_with_instance_change_lost_due_to_restarts.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_not_changed_when_short_disconnection.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_not_changed_when_short_disconnection.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_instance_change_if_needed.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_instance_change_if_needed.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_not_changed_if_backup_primary_disconnected.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_not_changed_if_backup_primary_disconnected.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_last_ordered_reset_for_new_view.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_last_ordered_reset_for_new_view.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_msgHasAcceptableViewNo.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_msgHasAcceptableViewNo.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_after_back_to_quorum_with_disconnected_primary.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_after_back_to_quorum_with_disconnected_primary.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_start_view_change_ts_set.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_start_view_change_ts_set.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_reset_monitor_after_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_reset_monitor_after_view_change.py`

 * *Files 15% similar despite different names*

```diff
@@ -3,16 +3,16 @@
 
 def test_reset_monitor_after_view_change(create_node_and_not_start):
     node = create_node_and_not_start
     node.monitor.throughputs[0].throughput = 1
     node.monitor.throughputs[0].first_ts = 0
     node.monitor.throughputs[1].throughput = 100
     node.monitor.throughputs[1].first_ts = 0
-    node.replicas._replicas[0].primaryName = "Alpha:0"
-    node.replicas._replicas[1].primaryName = "Beta:1"
+    node.replicas._replicas[0]._primaryName = "Alpha:0"
+    node.replicas._replicas[1]._primaryName = "Beta:1"
     node.view_changer = FakeSomething(propagate_primary=False,
                                       last_completed_view_no=0,
                                       view_no=1)
     node.on_view_change_complete()
     # After reset throughput must be 0 or None
     # depending on the throughput measurement strategy
     assert not node.monitor.getThroughput(0)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_instance_change_msg_checking.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_instance_change_msg_checking.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_checkPerformance.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_checkPerformance.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_catchup_to_next_view_during_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_catchup_to_next_view_during_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_new_node_joins_after_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_new_node_joins_after_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_queueing_req_from_future_view.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_vc_finished_when_less_than_quorum_started.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,72 +1,53 @@
-from plenum.server.replica_validator_enums import STASH_VIEW
-from plenum.test.view_change.helper import ensure_view_change
+import pytest
+
+from plenum.test.helper import sdk_send_random_and_check, assertExp, waitForViewChange
+from plenum.test.node_catchup.helper import ensure_all_nodes_have_same_data
+from plenum.test.test_node import ensureElectionsDone
+from plenum.test.view_change.helper import restart_node, nodes_received_ic
 from stp_core.loop.eventually import eventually
-from stp_core.common.log import getlogger
-from plenum.test.delayers import icDelay, vcd_delay
-from plenum.test.helper import sdk_send_random_requests, \
-    sdk_get_replies, sdk_send_random_and_check
-from plenum.test.test_node import get_last_master_non_primary_node
-
-nodeCount = 7
-
-logger = getlogger()
-
-
-# noinspection PyIncorrectDocstring
-def testQueueingReqFromFutureView(delayed_perf_chk, looper, txnPoolNodeSet,
-                                  sdk_pool_handle, sdk_wallet_client):
-    """
-    Test if every node queues 3 Phase requests(PRE-PREPARE, PREPARE and COMMIT)
-    that come from a view which is greater than the current view.
-    - Delay reception and processing of view change messages by a non primary for master instance
-       => it starts receiving 3 phase commit messages for next view
-    """
-
-    lagging_node = get_last_master_non_primary_node(txnPoolNodeSet)
-    old_view_no = lagging_node.viewNo
-
-    # Delay processing of InstanceChange and ViewChangeDone so node stashes
-    # 3PC messages
-    delay_ic = 60
-    lagging_node.nodeIbStasher.delay(icDelay(delay_ic))
-    lagging_node.nodeIbStasher.delay(vcd_delay(delay_ic))
-    logger.debug('{} will delay its view change'.format(lagging_node))
-
-    def chk_fut_view(is_empty):
-        num_stashed = lagging_node.master_replica.stasher.stash_size(STASH_VIEW)
-        if is_empty:
-            assert num_stashed == 0
-        else:
-            assert num_stashed > 0
-        return num_stashed
-
-    # No messages queued for future view
-    chk_fut_view(is_empty=True)
-    logger.debug('{} does not have any messages for future views'
-                 .format(lagging_node))
-
-    # Every node except Node A should do a view change
-    ensure_view_change(looper,
-                       [n for n in txnPoolNodeSet if n != lagging_node],
-                       [lagging_node])
-
-    # send more requests that will be queued for the lagged node
-    reqs = sdk_send_random_requests(looper, sdk_pool_handle,
-                                    sdk_wallet_client, 5)
-    l = looper.run(eventually(chk_fut_view, False,
-                              retryWait=1))
-    logger.debug('{} has {} messages for future views'
-                 .format(lagging_node, l))
-    sdk_get_replies(looper, reqs)
-    # reset delays for the lagging_node node so that it finally makes view
-    # change
-    lagging_node.reset_delays_and_process_delayeds()
-
-    # Eventually no messages queued for future view
-    looper.run(eventually(chk_fut_view, True,
-                          retryWait=1, timeout=delay_ic + 10))
-    logger.debug('{} exhausted pending messages for future views'
-                 .format(lagging_node))
 
+
+@pytest.fixture(scope="module")
+def tconf(tconf):
+    old_val = tconf.ToleratePrimaryDisconnection
+    tconf.ToleratePrimaryDisconnection = 1000
+    yield tconf
+    tconf.ToleratePrimaryDisconnection = old_val
+
+
+def test_vc_finished_when_less_than_quorum_started(looper, txnPoolNodeSet,
+                                                   sdk_wallet_client, sdk_pool_handle,
+                                                   tconf, tdir, allPluginsPath):
+
+    alpha, beta, gamma, delta = txnPoolNodeSet
+
+    # Delta and Gamma send InstanceChange for all nodes.
+    for node in [gamma, delta]:
+        node.view_changer.on_master_degradation()
+        looper.run(
+            eventually(nodes_received_ic, txnPoolNodeSet, node, 1))
+
+    # Restart Alpha, Beta, Gamma
+    for node in [alpha, beta, gamma]:
+        restart_node(looper, txnPoolNodeSet, node, tconf, tdir, allPluginsPath)
+    alpha, beta, gamma, delta = txnPoolNodeSet
+
+    # Send InstanceChange from Beta for all nodes
+    beta.view_changer.on_master_degradation()
+
+    # Ensure that pool is still functional
+    sdk_send_random_and_check(looper, txnPoolNodeSet,
+                              sdk_pool_handle, sdk_wallet_client, 1)
+
+    # Alpha and Gamma send InstanceChange for all nodes.
+    for node in [gamma, alpha]:
+        node.view_changer.on_master_degradation()
+
+    ensureElectionsDone(looper, txnPoolNodeSet)
+    waitForViewChange(looper, txnPoolNodeSet, expectedViewNo=1,
+                      customTimeout=tconf.VIEW_CHANGE_TIMEOUT)
+
+    # Ensure that pool is still functional
     sdk_send_random_and_check(looper, txnPoolNodeSet,
-                              sdk_pool_handle, sdk_wallet_client, 2)
+                              sdk_pool_handle, sdk_wallet_client, 1)
+    ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_select_primary_after_removed_backup.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_select_primary_after_removed_backup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_no_future_view_change_while_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_no_future_view_change_while_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_diconnected_node_reconnects_after_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_diconnected_node_reconnects_after_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_not_changed.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_not_changed.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_disable_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_disable_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -253,22 +253,22 @@
             node.nodestack.getRemote(pr_node.nodestack.name).disconnect()
     return pr_node
 
 
 def check_replica_queue_empty(node):
     replica = node.replicas[0]
 
-    assert len(replica._ordering_service.prePrepares) == 0
-    assert len(replica._ordering_service.prePreparesPendingFinReqs) == 0
-    assert len(replica._ordering_service.prepares) == 0
-    assert len(replica._ordering_service.sentPrePrepares) == 0
-    assert len(replica._ordering_service.batches) == 0
-    assert len(replica._ordering_service.commits) == 0
-    assert len(replica._ordering_service.commitsWaitingForPrepare) == 0
-    assert len(replica._ordering_service.ordered) == 0
+    assert len(replica.prePrepares) == 0
+    assert len(replica.prePreparesPendingFinReqs) == 0
+    assert len(replica.prepares) == 0
+    assert len(replica.sentPrePrepares) == 0
+    assert len(replica.batches) == 0
+    assert len(replica.commits) == 0
+    assert len(replica.commitsWaitingForPrepare) == 0
+    assert len(replica.ordered) == 0
 
 
 def check_all_replica_queue_empty(nodes):
     for node in nodes:
         check_replica_queue_empty(node)
 
 
@@ -380,9 +380,9 @@
         assert n.view_changer.instance_changes.has_inst_chng_from(view_no,
                                                                  frm.name)
 
 def check_prepare_certificate(nodes, ppSeqNo):
     for node in nodes:
         key = (node.viewNo, ppSeqNo)
         quorum = node.master_replica.quorums.prepare.value
-        assert node.master_replica._ordering_service.prepares.hasQuorum(ThreePhaseKey(*key),
+        assert node.master_replica.prepares.hasQuorum(ThreePhaseKey(*key),
                                                        quorum)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_reverted_unordered.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_reverted_unordered.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-from plenum.server.replica_validator_enums import STASH_CATCH_UP
 from plenum.test.spy_helpers import getAllReturnVals
 from stp_core.loop.eventually import eventually
 
 from plenum.common.constants import COMMIT, LEDGER_STATUS, MESSAGE_RESPONSE, CATCHUP_REP
 from plenum.common.messages.node_messages import Commit
 from plenum.common.util import check_if_all_equal_in_list
 from plenum.test.delayers import cDelay, msg_rep_delay, lsDelay, cr_delay
@@ -79,15 +78,15 @@
     assert last_ordered[0] == slow_node.master_replica.last_prepared_before_view_change
 
     # Deliver COMMITs
     slow_node.nodeIbStasher.reset_delays_and_process_delayeds(COMMIT)
 
     def chk2():
         # slow_node stashed commits
-        assert slow_node.master_replica.stasher.stash_size(STASH_CATCH_UP) == \
+        assert slow_node.master_replica.stasher.num_stashed_catchup == \
                sent_batches * (len(txnPoolNodeSet) - 1)
 
     looper.run(eventually(chk2, retryWait=1))
 
     # Deliver LEDGER_STATUS so catchup can complete
     slow_node.nodeIbStasher.reset_delays_and_process_delayeds(LEDGER_STATUS)
     slow_node.nodeIbStasher.reset_delays_and_process_delayeds(MESSAGE_RESPONSE)
@@ -95,15 +94,15 @@
 
     # Ensure all nodes have same data
     ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
     ensureElectionsDone(looper, txnPoolNodeSet)
 
     def chk3():
         # slow_node processed stashed messages successfully
-        assert slow_node.master_replica.stasher.stash_size(STASH_CATCH_UP) == 0
+        assert slow_node.master_replica.stasher.num_stashed_catchup == 0
 
     looper.run(eventually(chk3, retryWait=1))
 
     # Ensure pool is functional
     sdk_send_batches_of_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
                                          sdk_wallet_client, 10, 2)
     ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_3pc_msgs_during_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_3pc_msgs_during_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_old_instance_change_discarding.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_old_instance_change_discarding.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_client_req_during_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_client_req_during_view_change.py`

 * *Files 11% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 
 @pytest.fixture(scope='function')
 def test_node(test_node):
     test_node.view_changer = FakeSomething(view_change_in_progress=True,
                                            view_no=1,
                                            instance_changes=None)
     bs = ConfigTestBootstrapClass(test_node)
-    bs._register_config_req_handlers()
+    bs.register_config_req_handlers()
     return test_node
 
 
 def test_client_write_request_discard_in_view_change_with_dict(test_node):
     test_node.send_nack_to_client = check_nack_msg
 
     msg = sdk_gen_request({TXN_TYPE: NODE}).as_dict
@@ -66,8 +66,8 @@
     msg = sdk_gen_request({TXN_TYPE: NODE})
     test_node.unpackClientMsg(msg, "frm")
     checkDiscardMsg([test_node, ], msg.as_dict, "view change in progress")
 
 
 def check_nack_msg(req_key, reason, to_client):
     assert "Client request is discarded since view " \
-           "change is in progress" == reason
+"change is in progress" == reason
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_6th_node_join_after_view_change_by_primary_restart.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_6th_node_join_after_view_change_by_primary_restart.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_future_vc_done.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_future_vc_done.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_resend_instance_change_messages.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_resend_instance_change_messages.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_node_detecting_lag_from_view_change_messages.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_node_detecting_lag_from_view_change_messages.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_wont_happen_if_ic_is_discarded.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_wont_happen_if_ic_is_discarded.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_happens_post_timeout.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_happens_post_timeout.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_master_primary_different_from_previous.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_stable_checkpoint1.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,39 +1,21 @@
-import types
+from stp_core.loop.eventually import eventually
 
-import pytest
-
-from plenum.test.helper import checkViewNoForNodes, \
-    sdk_send_random_and_check, countDiscarded
-from plenum.test.malicious_behaviors_node import slow_primary
-from plenum.test.test_node import getPrimaryReplica, ensureElectionsDone
-from plenum.test.view_change.helper import provoke_and_wait_for_view_change, ensure_view_change
-
-from stp_core.common.log import getlogger
-
-logger = getlogger()
-
-
-def test_master_primary_different_from_previous(txnPoolNodeSet, looper,
-                                                sdk_pool_handle, sdk_wallet_client):
-    """
-    After a view change, primary must be different from previous primary for
-    master instance, it does not matter for other instance. The primary is
-    benign and does not vote for itself.
-    """
-    pr = slow_primary(txnPoolNodeSet, 0, delay=10)
-    old_pr_node_name = pr.node.name
-
-    # View change happens
-    ensure_view_change(looper, txnPoolNodeSet)
-    logger.debug("VIEW HAS BEEN CHANGED!")
-
-    # Elections done
-    ensureElectionsDone(looper=looper, nodes=txnPoolNodeSet)
-
-    # New primary is not same as old primary
-    assert getPrimaryReplica(txnPoolNodeSet, 0).node.name != old_pr_node_name
-
-    pr.outBoxTestStasher.resetDelays()
-
-    # The new primary can still process requests
-    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, 5)
+from plenum.test import waits
+from plenum.test.checkpoints.helper import chkChkpoints
+from plenum.test.delayers import ppDelay
+from plenum.test.test_node import getPrimaryReplica
+from plenum.test.helper import sdk_send_random_and_check
+
+
+def test_stable_checkpoint_when_one_instance_slow(chkFreqPatched, looper, txnPoolNodeSet, sdk_pool_handle,
+                                                  sdk_wallet_client, reqs_for_checkpoint):
+    delay = 5
+    pr = getPrimaryReplica(txnPoolNodeSet, 1)
+    slowNode = pr.node
+    otherNodes = [n for n in txnPoolNodeSet if n != slowNode]
+    for n in otherNodes:
+        n.nodeIbStasher.delay(ppDelay(delay, 1))
+
+    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, reqs_for_checkpoint)
+    timeout = waits.expectedTransactionExecutionTime(len(txnPoolNodeSet)) + delay
+    looper.run(eventually(chkChkpoints, txnPoolNodeSet, 1, 0, retryWait=1, timeout=timeout))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_min_cathup_timeout.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_min_cathup_timeout.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_vc_start_msg_strategy.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_vc_start_msg_strategy.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,40 +1,34 @@
 import json
 from collections import deque
 
 import functools
-from unittest.mock import Mock
-
 import pytest
 
-from plenum.common.event_bus import InternalBus
 from plenum.common.messages.node_messages import ViewChangeStartMessage, ViewChangeContinueMessage, Prepare, \
     InstanceChange
 from plenum.common.timer import QueueTimer
 from plenum.common.util import get_utc_epoch
 from plenum.server.node import Node
 from plenum.server.router import Router
 from plenum.server.view_change.node_view_changer import create_view_changer
 from plenum.server.view_change.pre_view_change_strategies import VCStartMsgStrategy
-from plenum.test.helper import MockNetwork
 from plenum.test.testing_utils import FakeSomething
 from stp_core.loop.eventually import eventually
 from stp_core.network.port_dispenser import genHa
 from stp_zmq.zstack import Quota
 
 
 @pytest.fixture(scope="function")
 def fake_node(tconf):
     node = FakeSomething(config=tconf,
                          timer=QueueTimer(),
                          nodeStatusDB=None,
                          master_replica=FakeSomething(inBox=deque(),
                                                       inBoxRouter=Router(),
-                                                      _external_bus=MockNetwork(),
-                                                      internal_bus=InternalBus(),
                                                       logger=FakeSomething(
                                                           info=lambda *args, **kwargs: True
                                                       )),
                          name="Alpha",
                          master_primary_name="Alpha",
                          on_view_change_start=lambda *args, **kwargs: True,
                          start_catchup=lambda *args, **kwargs: True,
@@ -43,16 +37,15 @@
                          metrics=None,
                          process_one_node_message=None,
                          quota_control=FakeSomething(
                              node_quota=Quota(count=100,
                                               size=100)),
                          nodestack=FakeSomething(
                              service=lambda *args, **kwargs: eventually(lambda: True)),
-                         set_view_for_replicas= lambda view_no: None,
-                         set_view_change_status=lambda view_no: None
+                         set_view_for_replicas= lambda view_no: None
                          )
     node.metrics = functools.partial(Node._createMetricsCollector, node)()
     node.process_one_node_message = functools.partial(Node.process_one_node_message, node)
     return node
 
 
 @pytest.fixture(scope="function")
@@ -84,17 +77,18 @@
     """Check, that req handler is set for ViewChangeStartMessage on node's nodeMsgRouter"""
     assert len(pre_vc_strategy.node.nodeMsgRouter.routes) == 1
     req_handler = pre_vc_strategy.node.nodeMsgRouter.routes.get(ViewChangeStartMessage, None)
     assert req_handler
     assert req_handler.func.__name__ == VCStartMsgStrategy.on_view_change_started.__name__
 
     """Check, that req handler is set for ViewChangeContinuedMessage on replica's inBoxRouter"""
-    req_handler = pre_vc_strategy.replica._external_bus.handlers(ViewChangeContinueMessage)[0]
+    assert len(pre_vc_strategy.node.master_replica.inBoxRouter.routes) == 1
+    req_handler = pre_vc_strategy.node.master_replica.inBoxRouter.routes.get(ViewChangeContinueMessage, None)
     assert req_handler
-    assert VCStartMsgStrategy.on_view_change_continued.__name__ in str(req_handler)
+    assert req_handler.func.__name__ == VCStartMsgStrategy.on_view_change_continued.__name__
 
 
 def test_add_vc_start_msg(pre_vc_strategy):
     pre_vc_strategy.is_preparing = False
     pre_vc_strategy.prepare_view_change(1)
 
     """Check, that ViewChangeStartMessage is added to nodeInBox queue"""
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_view_change_without_any_reqs.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change/test_view_change_without_any_reqs.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change/test_vc_finished_when_less_than_quorum_started.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_recover_primary_no_view_change.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,53 +1,69 @@
 import pytest
 
-from plenum.test.helper import sdk_send_random_and_check, assertExp, waitForViewChange
+from stp_core.common.log import getlogger
+
+from plenum.test.conftest import getValueFromModule
+from plenum.test.helper import waitForViewChange, \
+    sdk_send_random_and_check
 from plenum.test.node_catchup.helper import ensure_all_nodes_have_same_data
 from plenum.test.test_node import ensureElectionsDone
-from plenum.test.view_change.helper import restart_node, nodes_received_ic
-from stp_core.loop.eventually import eventually
+from plenum.test.view_change.helper import start_stopped_node
+
+from plenum.test.primary_selection.test_recover_more_than_f_failure import \
+    stop_primary, nodes_have_checkpoints, nodes_do_not_have_checkpoints
+
+logger = getlogger()
 
 
 @pytest.fixture(scope="module")
 def tconf(tconf):
     old_val = tconf.ToleratePrimaryDisconnection
     tconf.ToleratePrimaryDisconnection = 1000
     yield tconf
     tconf.ToleratePrimaryDisconnection = old_val
 
 
-def test_vc_finished_when_less_than_quorum_started(looper, txnPoolNodeSet,
-                                                   sdk_wallet_client, sdk_pool_handle,
-                                                   tconf, tdir, allPluginsPath):
-
-    alpha, beta, gamma, delta = txnPoolNodeSet
-
-    # Delta and Gamma send InstanceChange for all nodes.
-    for node in [gamma, delta]:
-        node.view_changer.on_master_degradation()
-        looper.run(
-            eventually(nodes_received_ic, txnPoolNodeSet, node, 1))
-
-    # Restart Alpha, Beta, Gamma
-    for node in [alpha, beta, gamma]:
-        restart_node(looper, txnPoolNodeSet, node, tconf, tdir, allPluginsPath)
-    alpha, beta, gamma, delta = txnPoolNodeSet
-
-    # Send InstanceChange from Beta for all nodes
-    beta.view_changer.on_master_degradation()
-
-    # Ensure that pool is still functional
-    sdk_send_random_and_check(looper, txnPoolNodeSet,
-                              sdk_pool_handle, sdk_wallet_client, 1)
-
-    # Alpha and Gamma send InstanceChange for all nodes.
-    for node in [gamma, alpha]:
-        node.view_changer.on_master_degradation()
-
-    ensureElectionsDone(looper, txnPoolNodeSet)
-    waitForViewChange(looper, txnPoolNodeSet, expectedViewNo=1,
-                      customTimeout=tconf.VIEW_CHANGE_TIMEOUT)
-
-    # Ensure that pool is still functional
-    sdk_send_random_and_check(looper, txnPoolNodeSet,
-                              sdk_pool_handle, sdk_wallet_client, 1)
-    ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
+def test_recover_stop_primaries_no_view_change(looper, checkpoint_size, txnPoolNodeSet,
+                                               allPluginsPath, tdir, tconf, sdk_pool_handle,
+                                               sdk_wallet_steward):
+    """
+    Test that we can recover after having more than f nodes disconnected:
+    - send txns
+    - stop current master primary
+    - restart current master primary
+    - send txns
+    """
+
+    active_nodes = list(txnPoolNodeSet)
+    assert 4 == len(active_nodes)
+    initial_view_no = active_nodes[0].viewNo
+
+    logger.info("send at least one checkpoint")
+    assert nodes_do_not_have_checkpoints(*active_nodes)
+    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
+                              sdk_wallet_steward, 2 * checkpoint_size)
+    assert nodes_have_checkpoints(*active_nodes)
+    ensure_all_nodes_have_same_data(looper, nodes=active_nodes)
+
+    logger.info("Stop first node (current Primary)")
+    stopped_node, active_nodes = stop_primary(looper, active_nodes)
+
+    logger.info("Restart the primary node")
+    restarted_node = start_stopped_node(stopped_node, looper, tconf, tdir, allPluginsPath)
+    assert nodes_do_not_have_checkpoints(restarted_node)
+    assert nodes_have_checkpoints(*active_nodes)
+    active_nodes = active_nodes + [restarted_node]
+
+    logger.info("Check that primary selected")
+    ensureElectionsDone(looper=looper, nodes=active_nodes,
+                        instances_list=range(2), customTimeout=30)
+    waitForViewChange(looper, active_nodes, expectedViewNo=0)
+    ensure_all_nodes_have_same_data(looper, nodes=active_nodes,
+                                    exclude_from_check=['check_last_ordered_3pc_backup'])
+
+    logger.info("Check if the pool is able to process requests")
+    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
+                              sdk_wallet_steward, 10 * checkpoint_size)
+    ensure_all_nodes_have_same_data(looper, nodes=active_nodes,
+                                    exclude_from_check=['check_last_ordered_3pc_backup'])
+    assert nodes_have_checkpoints(*active_nodes)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/test_last_sent_pp_store_helper.py` & `indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/test_last_sent_pp_store_helper.py`

 * *Files 1% similar despite different names*

```diff
@@ -30,28 +30,28 @@
 
 @pytest.fixture(scope="function")
 def setup(txnPoolNodeSet):
     for node in txnPoolNodeSet:
         if LAST_SENT_PRE_PREPARE in node.nodeStatusDB:
             node.nodeStatusDB.remove(LAST_SENT_PRE_PREPARE)
         for replica in node.replicas.values():
-            replica._checkpointer.set_watermarks(low_watermark=0)
-            replica._ordering_service._lastPrePrepareSeqNo = 0
+            replica.h = 0
+            replica._lastPrePrepareSeqNo = 0
             replica.last_ordered_3pc = (replica.viewNo, 0)
 
 
 @pytest.fixture(scope="function")
 def replica_with_unknown_primary_status(txnPoolNodeSet, setup):
     replica = txnPoolNodeSet[0].replicas[1]
-    old_primary_name = replica.primaryName
-    replica.primaryName = None
+    old_primary_name = replica._primaryName
+    replica._primaryName = None
 
     yield replica
 
-    replica.primaryName = old_primary_name
+    replica._primaryName = old_primary_name
 
 
 def test_store_last_sent_pp_seq_no_if_some_stored(
         txnPoolNodeSet, view_no_set, setup):
     node = txnPoolNodeSet[0]
     node.nodeStatusDB.put(LAST_SENT_PRE_PREPARE,
                           pack_pp_key({1: [2, 5]}))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/test_backup_primary_restores_pp_seq_no_if_view_is_same.py` & `indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/test_backup_primary_restores_pp_seq_no_if_view_is_same.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/test_node_erases_last_sent_pp_key_on_pool_restart.py` & `indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/test_node_erases_last_sent_pp_key_on_pool_restart.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pp_seq_no_restoration/test_node_erases_last_sent_pp_key_on_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/pp_seq_no_restoration/test_node_erases_last_sent_pp_key_on_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_batch_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/test_batch_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_large_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_large_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_build_ledger_status.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_build_ledger_status.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_revert_during_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_revert_during_catchup.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 from itertools import combinations
 
 import pytest
 
 from plenum.common.constants import DOMAIN_LEDGER_ID, COMMIT
-from plenum.server.replica_validator_enums import STASH_CATCH_UP
 from plenum.test import waits
 from plenum.test.delayers import cDelay, cr_delay, lsDelay
 from plenum.test.helper import check_last_ordered_3pc, \
     assertEquality, sdk_send_random_and_check
 from plenum.test.node_catchup.helper import waitNodeDataInequality, \
     ensure_all_nodes_have_same_data, make_a_node_catchup_less, \
     repair_node_catchup_less
@@ -90,43 +89,43 @@
 
     # Check last ordered of `other_nodes` is same
     for n1, n2 in combinations(other_nodes, 2):
         check_last_ordered_3pc(n1, n2)
 
     assert slow_master_replica.last_prepared_before_view_change == old_last_ordered
 
-    old_pc_count = slow_master_replica._ordering_service.spylog.count(
-        slow_master_replica._ordering_service._validate)
+    old_pc_count = slow_master_replica.spylog.count(
+        slow_master_replica.process_three_phase_msg)
 
-    assert slow_node.master_replica.stasher.stash_size(STASH_CATCH_UP) == 0
+    assert slow_node.master_replica.stasher.num_stashed_catchup == 0
 
     # Repair the network so COMMITs are received, processed and stashed
     slow_node.reset_delays_and_process_delayeds(COMMIT)
 
     def chk2():
         # COMMITs are processed for prepared messages
-        assert slow_master_replica._ordering_service.spylog.count(
-            slow_master_replica._ordering_service._validate) > old_pc_count
+        assert slow_master_replica.spylog.count(
+            slow_master_replica.process_three_phase_msg) > old_pc_count
 
     looper.run(eventually(chk2, retryWait=1, timeout=5))
 
     def chk3():
         # (delay_batches * Max3PCBatchSize * commits_count_in_phase) COMMITs are stashed
-        assert slow_node.master_replica.stasher.stash_size(STASH_CATCH_UP) == \
+        assert slow_node.master_replica.stasher.num_stashed_catchup == \
                delay_batches * Max3PCBatchSize * (len(txnPoolNodeSet) - 1)
 
     looper.run(eventually(chk3, retryWait=1, timeout=15))
 
     # fix catchup, so the node gets a chance to be caught-up
     repair_node_catchup_less(other_nodes)
 
     def chk4():
         # Some COMMITs were received but stashed and
         # they will processed after catchup
-        assert slow_node.master_replica.stasher.stash_size(STASH_CATCH_UP) == 0
+        assert slow_node.master_replica.stasher.num_stashed_catchup == 0
 
     looper.run(eventually(chk4, retryWait=1, timeout=catchup_rep_delay + 50))
 
     def chk5():
         # Catchup was done once
         assert slow_node.spylog.count(
             slow_node.allLedgersCaughtUp) > old_lcu_count
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/conftest.py`

 * *Files 2% similar despite different names*

```diff
@@ -58,15 +58,15 @@
 @pytest.fixture
 def broken_node_and_others(txnPoolNodeSet):
     node = getNonPrimaryReplicas(txnPoolNodeSet, 0)[-1].node
     other = [n for n in txnPoolNodeSet if n != node]
 
     def brokenSendToReplica(msg, frm):
         logger.warning(
-            "{} is broken. 'sendToReplica' does nothing. {} from {}".format(node.name, msg, frm))
+            "{} is broken. 'sendToReplica' does nothing".format(node.name))
 
     node.nodeMsgRouter.extend(
         (
             (PrePrepare, brokenSendToReplica),
             (Prepare, brokenSendToReplica),
             (Commit, brokenSendToReplica),
             (Checkpoint, brokenSendToReplica),
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_ledger_manager.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_ledger_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/catchup_req/test_catchup_with_one_slow_node.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/catchup_req/test_catchup_with_one_slow_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/catchup_req/test_catchup_with_disconnected_node.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/catchup_req/test_catchup_with_disconnected_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_after_checkpoints.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_after_checkpoints.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_ledger_info.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_ledger_info.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_new_node_catchup2.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_new_node_catchup2.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_uses_only_nodes_with_cons_proofs.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_uses_only_nodes_with_cons_proofs.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_after_restart_no_txns.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_after_restart_no_txns.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_req_distribution.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_req_distribution.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_from_unequal_nodes_without_reasking.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_from_unequal_nodes_without_reasking.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_discard_view_no.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_discard_view_no.py`

 * *Files 3% similar despite different names*

```diff
@@ -40,17 +40,17 @@
     rid_x_node = sender.nodestack.getRemote(new_node.name).uid
     messageTimeout = waits.expectedNodeToNodeMessageDeliveryTime()
 
     # 3 pc msg (PrePrepare) needs to be discarded
     _, did = sdk_wallet_client
     primaryRepl = getPrimaryReplica(txnPoolNodeSet)
     inst_id = 0
-    three_pc = create_pre_prepare_no_bls(primaryRepl.node.db_manager.get_state_root_hash(DOMAIN_LEDGER_ID),
+    three_pc = create_pre_prepare_no_bls(primaryRepl.stateRootHash(DOMAIN_LEDGER_ID),
                                          viewNo,
                                          pp_seq_no=10,
                                          inst_id=inst_id)
     sender.send(three_pc, rid_x_node)
-    looper.run(eventually(checkDiscardMsg, [new_node.replicas[inst_id].stasher, ], three_pc,
+    looper.run(eventually(checkDiscardMsg, [new_node.replicas[inst_id], ], three_pc,
                           ALREADY_ORDERED,
                           retryWait=1, timeout=messageTimeout))
 
     # TODO: the same check for ViewChangeDone
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_request_missing_transactions.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_request_missing_transactions.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_incorrect_catchup_request.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_incorrect_catchup_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_reject_invalid_txn_during_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_reject_invalid_txn_during_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_ts_store_after_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_ts_store_after_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_with_all_nodes_sending_cons_proofs_dead.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_with_all_nodes_sending_cons_proofs_dead.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_set_H_to_maxsize_and_not_stash_on_backup.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_set_H_to_maxsize_and_not_stash_on_backup.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 import pytest
 import sys
 
-from plenum.server.replica_validator_enums import STASH_WATERMARKS
 from plenum.test.helper import sdk_send_random_and_check
 from plenum.test.node_catchup.helper import ensure_all_nodes_have_same_data
 from plenum.test.test_node import ensureElectionsDone
 from plenum.test.view_change.helper import add_new_node, ensure_view_change
 
 Max3PCBatchSize = 1
 CHK_FREQ = 5
@@ -36,17 +35,14 @@
                                          allPluginsPath):
     # send LOG_SIZE requests and check, that all watermarks on all replicas is not changed
     # and now is (0, LOG_SIZE)
     """Send random requests for moving watermarks"""
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_steward, LOG_SIZE)
     # check, that all of node set up watermark greater, then default and
     # ppSeqNo with number LOG_SIZE + 1 will be out from default watermark
-    assert txnPoolNodeSet[0].replicas[1].last_ordered_3pc[1] > 0
-    looper.runFor(30)
-    assert txnPoolNodeSet[0].replicas[1].last_ordered_3pc[1] == LOG_SIZE
     for n in txnPoolNodeSet:
         for r in n.replicas._replicas.values():
             assert r.h >= LOG_SIZE
             assert r.H >= LOG_SIZE + LOG_SIZE
     """Adding new node, for scheduling propagate primary procedure"""
     new_node = add_new_node(looper, txnPoolNodeSet, sdk_pool_handle,
                             sdk_wallet_steward, tdir, tconf, allPluginsPath)
@@ -60,15 +56,15 @@
             assert r.H == r.last_ordered_3pc[1] + LOG_SIZE
         else:
             assert r.H == sys.maxsize
     """Send requests and check. that backup replicas does not stashing it by outside watermarks reason"""
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_steward, 1)
     # check, that there is no any stashed "outside watermark" messages.
     for r in new_node.replicas.values():
-        assert r.stasher.stash_size(STASH_WATERMARKS) == 0
+        assert r.stasher.num_stashed_watermarks == 0
 
     """Force view change and check, that all backup replicas setup H as a default<
     (not propagate primary logic)"""
     """This need to ensure, that next view_change does not break watermark setting logic"""
 
     ensure_view_change(looper, txnPoolNodeSet)
     ensureElectionsDone(looper, txnPoolNodeSet)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_request_consistency_proof.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_request_consistency_proof.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_config_ledger.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_config_ledger.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_not_set_H_as_maxsize_for_backup_if_is_primary.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_not_set_H_as_maxsize_for_backup_if_is_primary.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_process_catchup_replies.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_process_catchup_replies.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_inlcuding_3PC.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_inlcuding_3PC.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_get_last_txn_3PC_key.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_get_last_txn_3PC_key.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_f_plus_one.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_f_plus_one.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_after_disconnect.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_after_disconnect.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_remove_request_keys_post_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_remove_request_keys_post_catchup.py`

 * *Files 4% similar despite different names*

```diff
@@ -44,15 +44,15 @@
     ensure_all_nodes_have_same_data(looper, fast_nodes)
     assert slow_node.master_replica.last_ordered_3pc != \
            fast_nodes[0].master_replica.last_ordered_3pc
 
     def chk(key, nodes, present):
         for node in nodes:
             assert (
-                           key in node.master_replica._ordering_service.requestQueues[DOMAIN_LEDGER_ID]) == present
+                           key in node.master_replica.requestQueues[DOMAIN_LEDGER_ID]) == present
 
     for req in reqs:
         chk(req.digest, fast_nodes, False)
         chk(req.digest, [slow_node], True)
 
     # Reset catchup reply delay so that  catchup can complete
     slow_node.nodeIbStasher.reset_delays_and_process_delayeds(CatchupRep.typename)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_not_triggered_if_another_in_progress.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_not_triggered_if_another_in_progress.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 from logging import getLogger
 
 from plenum.common.startable import Mode
-from plenum.server.consensus.checkpoint_service import CheckpointService
 from plenum.server.node import Node
 from plenum.server.replica import Replica
 from plenum.test import waits
 
 from plenum.test.checkpoints.conftest import tconf, chkFreqPatched, \
     reqs_for_checkpoint
 from plenum.test.delayers import cr_delay
@@ -50,14 +49,15 @@
 
     logger.info(
         "Step 2: The node receives 3PC-messages but cannot process them because of "
         "missed ones. But the node eventually stashes some amount of checkpoints "
         "and after that starts catchup")
 
     repaired_node = repair_broken_node(broken_node)
+
     initial_do_start_catchup_times = repaired_node.spylog.count(Node._do_start_catchup)
     initial_all_ledgers_caught_up = repaired_node.spylog.count(Node.allLedgersCaughtUp)
 
     with delay_rules(repaired_node.nodeIbStasher, cr_delay()):
         send_reqs_batches_and_get_suff_replies(looper, txnPoolNodeSet,
                                                sdk_pool_handle,
                                                sdk_wallet_client,
@@ -73,30 +73,28 @@
         assert repaired_node.spylog.count(Node._do_start_catchup) - initial_do_start_catchup_times == 1
 
         logger.info(
             "Step 3: While doing the catchup, the node receives new checkpoints "
             "enough to start a new catchup but the node does not start it because "
             "the former is in progress")
 
-        process_checkpoint_times_before = \
-            repaired_node.master_replica._checkpointer.spylog.count(CheckpointService.process_checkpoint)
+        process_checkpoint_times_before = repaired_node.master_replica.spylog.count(Replica.process_checkpoint)
 
         send_reqs_batches_and_get_suff_replies(looper, txnPoolNodeSet,
                                                sdk_pool_handle,
                                                sdk_wallet_client,
                                                (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) *
                                                reqs_for_checkpoint)
 
         # Wait until the node receives the new checkpoints from all the other nodes
         looper.run(
-            eventually(lambda: assertExp(
-                repaired_node.master_replica._checkpointer.spylog.count(CheckpointService.process_checkpoint) -
-                process_checkpoint_times_before ==
-                (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) *
-                (len(txnPoolNodeSet) - 1)),
+            eventually(lambda: assertExp(repaired_node.master_replica.spylog.count(Replica.process_checkpoint) -
+                                         process_checkpoint_times_before ==
+                                         (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) *
+                                         (len(txnPoolNodeSet) - 1)),
                        timeout=waits.expectedPoolInterconnectionTime(len(txnPoolNodeSet))))
 
         # New catchup is not started when another one is in progress
         assert repaired_node.spylog.count(Node._do_start_catchup) - initial_do_start_catchup_times == 1
         assert repaired_node.mode == Mode.discovering
 
     logger.info("Step 4: The node completes the catchup. The ledger has been "
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_with_only_one_available_node.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_with_only_one_available_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_demoted.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_demoted.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_start.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_start.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_with_ledger_statuses_in_old_format.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_with_ledger_statuses_in_old_format.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_causes_no_desync.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_causes_no_desync.py`

 * *Files 2% similar despite different names*

```diff
@@ -56,15 +56,15 @@
 
     max_batch_size = chkFreqPatched.Max3PCBatchSize
     lagging_node = get_any_non_primary_node(txnPoolNodeSet)
     rest_nodes = set(txnPoolNodeSet).difference({lagging_node})
 
     # Make master replica lagging by hiding all messages sent to it
     make_master_replica_lag(lagging_node)
-    monkeypatch.setattr(lagging_node.master_replica._ordering_service,
+    monkeypatch.setattr(lagging_node.master_replica,
                         '_request_missing_three_phase_messages',
                         lambda *x, **y: None)
 
     # Send some requests and check that all replicas except master executed it
     sdk_send_random_and_check(looper, txnPoolNodeSet,
                               sdk_pool_handle, sdk_wallet_client,
                               reqs_for_checkpoint - max_batch_size)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_delayed_nodes.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_delayed_nodes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_when_3_not_primary_node_restarted.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_when_3_not_primary_node_restarted.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_reply.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_reply.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_with_old_txn_metadata_digest_format.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_with_old_txn_metadata_digest_format.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_post_genesis_txn_from_catchup_added_to_ledger.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_post_genesis_txn_from_catchup_added_to_ledger.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_new_node_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_new_node_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_from_unequal_nodes_without_waiting.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_from_unequal_nodes_without_waiting.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_catchup_reasking.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_catchup_reasking.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_with_connection_problem.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_with_connection_problem.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node_catchup/test_node_catchup_after_restart_after_txns.py` & `indy-plenum-1.9.2rc1/plenum/test/node_catchup/test_node_catchup_after_restart_after_txns.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/logging/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/logging/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/logging/test_logging_txn_state.py` & `indy-plenum-1.9.2rc1/plenum/test/logging/test_logging_txn_state.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,15 +30,15 @@
 
 
 def testLoggingTxnStateForValidRequest(
         looper, logsearch, txnPoolNodeSet,
         sdk_pool_handle, sdk_wallet_client):
     logsPropagate, _ = logsearch(files=['propagator.py'], funcs=['propagate'],
                                  msgs=['propagating.*request.*from client'])
-    logsOrdered, _ = logsearch(files=['ordering_service.py'], funcs=['_order_3pc_key'], msgs=['ordered batch request'])
+    logsOrdered, _ = logsearch(files=['replica.py'], funcs=['order_3pc_key'], msgs=['ordered batch request'])
     logsCommited, _ = logsearch(files=['node.py'], funcs=['executeBatch'], msgs=['committed batch request'])
 
     reqs = sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
                                      sdk_wallet_client, 1)
     req, _ = reqs[0]
 
     key = get_key_from_req(req)
@@ -47,15 +47,15 @@
     assert any(key in record.getMessage() for record in logsCommited)
 
 
 def testLoggingTxnStateForInvalidRequest(
         looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, logsearch):
     logsPropagate, _ = logsearch(files=['propagator.py'], funcs=['propagate'],
                                  msgs=['propagating.*request.*from client'])
-    logsReject, _ = logsearch(files=['ordering_service.py'], funcs=['_consume_req_queue_for_pre_prepare'],
+    logsReject, _ = logsearch(files=['replica.py'], funcs=['consume_req_queue_for_pre_prepare'],
                               msgs=['encountered exception.*while processing.*will reject'])
 
     seed = randomString(32)
     wh, _ = sdk_wallet_client
 
     nym_request, _ = looper.loop.run_until_complete(
         prepare_nym_request(sdk_wallet_client, seed,
@@ -75,15 +75,15 @@
     assert any(req_id in record.getMessage() for record in logsReject)
 
 
 def testLoggingTxnStateWhenCommitFails(
         looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_steward, logsearch):
     logsPropagate, _ = logsearch(files=['propagator.py'], funcs=['propagate'],
                                  msgs=['propagating.*request.*from client'])
-    logsOrdered, _ = logsearch(files=['ordering_service.py'], funcs=['_order_3pc_key'], msgs=['ordered batch request'])
+    logsOrdered, _ = logsearch(files=['replica.py'], funcs=['order_3pc_key'], msgs=['ordered batch request'])
     logsCommitFail, _ = logsearch(files=['node.py'], funcs=['executeBatch'],
                                   msgs=['commit failed for batch request'])
 
     seed = randomString(32)
     wh, _ = sdk_wallet_steward
 
     nym_request, _ = looper.loop.run_until_complete(
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_performance.py` & `indy-plenum-1.9.2rc1/plenum/test/test_performance.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/observer/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/observer/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/observer/test_observable_each_batch.py` & `indy-plenum-1.9.2rc1/plenum/test/observer/test_observable_each_batch.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/observer/test_observable_each_batch_node_integration.py` & `indy-plenum-1.9.2rc1/plenum/test/observer/test_observable_each_batch_node_integration.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/observer/test_observable_node_integration.py` & `indy-plenum-1.9.2rc1/plenum/test/observer/test_observable_node_integration.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/observer/test_observer_node_each_batch.py` & `indy-plenum-1.9.2rc1/plenum/test/observer/test_observer_node_each_batch.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/observer/test_observable.py` & `indy-plenum-1.9.2rc1/plenum/test/observer/test_observable.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/observer/test_observer_policy_each_batch.py` & `indy-plenum-1.9.2rc1/plenum/test/observer/test_observer_policy_each_batch.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/observer/test_observer_node_integration.py` & `indy-plenum-1.9.2rc1/plenum/test/observer/test_observer_node_integration.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_demote_backup_primary.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_demote_backup_primary.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_first_audit_catchup_during_ordering.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_first_audit_catchup_during_ordering.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_reconnect_primary_and_not_primary.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,102 +1,101 @@
-from plenum.test.delayers import icDelay
-from plenum.test.stasher import delay_rules
-
-from plenum.common.constants import DOMAIN_LEDGER_ID, STEWARD_STRING
-from plenum.test.audit_ledger.helper import check_audit_ledger_updated, check_audit_txn
-from plenum.test.helper import sdk_send_random_and_check, assertExp
-from plenum.test.pool_transactions.helper import sdk_add_new_nym, sdk_add_new_node
-from plenum.test.test_node import checkNodesConnected, ensureElectionsDone
+import pytest
+from plenum.test.pool_transactions.helper import disconnect_node_and_ensure_disconnected
+from plenum.test.helper import checkViewNoForNodes, sdk_send_random_and_check
+from plenum.test.test_node import checkNodesConnected
 from stp_core.loop.eventually import eventually
-
-nodeCount = 6
+from functools import partial
+from plenum.test.pool_transactions.helper import reconnect_node_and_ensure_connected
+from stp_core.common.log import getlogger
+
+
+logger = getlogger()
+
+nodeCount = 7
+
+@pytest.fixture(scope="module")
+def tconf(tconf):
+    old_timeout_restricted = tconf.RETRY_TIMEOUT_RESTRICTED
+    old_timeout_not_restricted = tconf.RETRY_TIMEOUT_NOT_RESTRICTED
+    tconf.RETRY_TIMEOUT_RESTRICTED = 2
+    tconf.RETRY_TIMEOUT_NOT_RESTRICTED = 2
+    yield tconf
+
+    tconf.RETRY_TIMEOUT_RESTRICTED = old_timeout_restricted
+    tconf.RETRY_TIMEOUT_NOT_RESTRICTED = old_timeout_not_restricted
+
+
+def check_count_connected_node(nodes, expected_count):
+    assert set([n.connectedNodeCount for n in nodes]) == {expected_count}
+
+
+def test_reconnect_primary_and_not_primary(looper,
+                                        txnPoolNodeSet,
+                                        sdk_wallet_steward,
+                                        sdk_pool_handle,
+                                        tconf):
+    """
+    Test steps:
+    Pool of 7 nodes.
+    count of instances must be 3
+    1. Choose node, that is not primary on all replicas (3 index)
+    2. Disconnect them
+    3. Ensure, that number of replicas was decreased
+    4. Choose current primary node (must be 0)
+    5. Disconnect primary
+    6. Ensure, that view change complete and primary was selected
+    7. Add node back from 1 step
+    8. Add node back from 4 step
+    9. Check, that count of instance (f+1 = 3)
+    10. Send some requests and check, that pool works.
+    """
+    restNodes = set(txnPoolNodeSet)
+    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_steward, 5)
+    assert txnPoolNodeSet[0].master_replica.isPrimary
+    node_after_all_primary = txnPoolNodeSet[3]
+    # Disconnect node after all primaries (after all backup primaries)
+    disconnect_node_and_ensure_disconnected(looper,
+                                            restNodes,
+                                            node_after_all_primary,
+                                            stopNode=False)
+    # -------------------------------------------------------
+    restNodes.remove(node_after_all_primary)
+    looper.run(eventually(partial(check_count_connected_node, restNodes, 6),
+                          timeout=5,
+                          acceptableExceptions=[AssertionError]))
+    sdk_send_random_and_check(looper, restNodes, sdk_pool_handle, sdk_wallet_steward, 5)
+    # Get primary node for backup replica
+    primary_node = txnPoolNodeSet[0]
+    assert primary_node.master_replica.isPrimary
+    old_view_no = checkViewNoForNodes(restNodes, 0)
+    # disconnect primary node
+    disconnect_node_and_ensure_disconnected(looper,
+                                            restNodes,
+                                            primary_node,
+                                            stopNode=False)
+    # -------------------------------------------------------
+    restNodes.remove(primary_node)
+    looper.run(eventually(partial(check_count_connected_node, restNodes, 5),
+                          timeout=5,
+                          acceptableExceptions=[AssertionError]))
+    looper.run(eventually(partial(checkViewNoForNodes, restNodes, expectedViewNo=old_view_no + 1),
+                          timeout=tconf.VIEW_CHANGE_TIMEOUT))
+    sdk_send_random_and_check(looper, restNodes, sdk_pool_handle, sdk_wallet_steward, 5)
+    logger.debug("restNodes: {}".format(restNodes))
+    restNodes.add(node_after_all_primary)
+    # Return back node after all primary
+    reconnect_node_and_ensure_connected(looper, restNodes, node_after_all_primary)
+    looper.run(checkNodesConnected(restNodes,
+                                   customTimeout=5*tconf.RETRY_TIMEOUT_RESTRICTED))
+    looper.run(eventually(partial(check_count_connected_node, restNodes, 6),
+                          timeout=5,
+                          acceptableExceptions=[AssertionError]))
+    assert len(set([len(n.replicas) for n in restNodes])) == 1
+    sdk_send_random_and_check(looper, restNodes, sdk_pool_handle, sdk_wallet_steward, 5)
+    # Return back primary node
+    restNodes.add(primary_node)
+    reconnect_node_and_ensure_connected(looper, restNodes, primary_node)
+    looper.run(checkNodesConnected(restNodes,
+                                   customTimeout=5*tconf.RETRY_TIMEOUT_RESTRICTED))
+    sdk_send_random_and_check(looper, restNodes, sdk_pool_handle, sdk_wallet_steward, 5)
 
 
-def test_audit_ledger_view_change(looper, txnPoolNodeSet,
-                                  sdk_pool_handle, sdk_wallet_client, sdk_wallet_steward,
-                                  initial_domain_size, initial_pool_size, initial_config_size,
-                                  tdir,
-                                  tconf,
-                                  allPluginsPath,
-                                  view_no, pp_seq_no,
-                                  initial_seq_no,
-                                  monkeypatch):
-    '''
-    1. Send a NODE transaction and add a 7th Node for adding a new instance,
-    but delay Ordered messages.
-    2. Send a NYM txn.
-    3. Reset delays in executing force_process_ordered
-    4. Check that an audit txn for the NYM txn uses primary list from uncommitted
-    audit with a new list of primaries.
-    '''
-    other_nodes = txnPoolNodeSet[:-1]
-    slow_node = txnPoolNodeSet[-1]
-    # Add a new steward for creating a new node
-    new_steward_wallet_handle = sdk_add_new_nym(looper,
-                                                sdk_pool_handle,
-                                                sdk_wallet_steward,
-                                                alias="newSteward",
-                                                role=STEWARD_STRING)
-
-    audit_size_initial = [node.auditLedger.size for node in txnPoolNodeSet]
-
-    ordereds = []
-    monkeypatch.setattr(slow_node, 'try_processing_ordered', lambda msg: ordereds.append(msg))
-
-    with delay_rules([n.nodeIbStasher for n in txnPoolNodeSet], icDelay()):
-
-        # Send NODE txn fo 7th node
-        new_node = sdk_add_new_node(looper,
-                                    sdk_pool_handle,
-                                    new_steward_wallet_handle,
-                                    "Theta",
-                                    tdir,
-                                    tconf,
-                                    allPluginsPath)
-
-        txnPoolNodeSet.append(new_node)
-        looper.run(checkNodesConnected(other_nodes + [new_node]))
-
-        sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
-                                  sdk_wallet_client, 1)
-
-        check_audit_ledger_updated(audit_size_initial, [slow_node],
-                                   audit_txns_added=0)
-        looper.run(eventually(check_audit_ledger_uncommitted_updated,
-                              audit_size_initial, [slow_node], 2))
-
-        def patch_force_process_ordered():
-            for msg in list(ordereds):
-                slow_node.replicas[msg.instId].outBox.append(msg)
-                ordereds.remove(msg)
-            monkeypatch.undo()
-            slow_node.force_process_ordered()
-
-        assert ordereds
-        monkeypatch.setattr(slow_node, 'force_process_ordered', patch_force_process_ordered)
-
-    looper.run(eventually(lambda: assertExp(all(n.viewNo == 1 for n in txnPoolNodeSet))))
-    ensureElectionsDone(looper=looper, nodes=txnPoolNodeSet)
-    looper.run(eventually(lambda: assertExp(not ordereds)))
-
-    for node in txnPoolNodeSet:
-        last_txn = node.auditLedger.get_last_txn()
-        last_txn['txn']['data']['primaries'] = node._get_last_audited_primaries()
-        check_audit_txn(txn=last_txn,
-                        view_no=view_no + 1, pp_seq_no=1,
-                        seq_no=initial_seq_no + 4,
-                        txn_time=node.master_replica._ordering_service.last_accepted_pre_prepare_time,
-                        txn_roots={DOMAIN_LEDGER_ID: node.getLedger(DOMAIN_LEDGER_ID).tree.root_hash},
-                        state_roots={DOMAIN_LEDGER_ID: node.getState(DOMAIN_LEDGER_ID).committedHeadHash},
-                        pool_size=initial_pool_size + 1, domain_size=initial_domain_size + 2,
-                        config_size=initial_config_size,
-                        last_pool_seqno=2,
-                        last_domain_seqno=1,
-                        last_config_seqno=None,
-                        primaries=node.write_manager.future_primary_handler.get_last_primaries() or node.primaries)
-
-
-def check_audit_ledger_uncommitted_updated(audit_size_initial, nodes, audit_txns_added):
-    audit_size_after = [node.auditLedger.uncommitted_size for node in nodes]
-    for i in range(len(nodes)):
-        assert audit_size_after[i] == audit_size_initial[i] + audit_txns_added, \
-            "{} != {}".format(audit_size_after[i], audit_size_initial[i] + audit_txns_added)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_future_primaries_addition.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_future_primaries_addition.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_freshness.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_freshness.py`

 * *Files 3% similar despite different names*

```diff
@@ -38,40 +38,39 @@
 
     # 3. check that there is audit ledger txn created for each ledger updated as a freshness check
     check_audit_ledger_updated(audit_size_initial, txnPoolNodeSet,
                                audit_txns_added=3)
     for node in txnPoolNodeSet:
         check_audit_txn(txn=node.auditLedger.getBySeqNo(node.auditLedger.size - 2),
                         view_no=view_no, pp_seq_no=pp_seq_no + 1,
-                        seq_no=initial_seq_no + 1, txn_time=node.master_replica._ordering_service.last_accepted_pre_prepare_time,
+                        seq_no=initial_seq_no + 1, txn_time=node.master_replica.last_accepted_pre_prepare_time,
                         txn_roots={POOL_LEDGER_ID: node.getLedger(POOL_LEDGER_ID).tree.root_hash},
                         state_roots={POOL_LEDGER_ID: node.getState(POOL_LEDGER_ID).committedHeadHash},
                         pool_size=initial_pool_size, domain_size=initial_domain_size,
                         config_size=initial_config_size,
                         last_pool_seqno=None,
                         last_domain_seqno=2,
                         last_config_seqno=3,
                         primaries=pp_seq_no + 1 - 1)
 
         check_audit_txn(txn=node.auditLedger.getBySeqNo(node.auditLedger.size - 1),
                         view_no=view_no, pp_seq_no=pp_seq_no + 2,
-                        seq_no=initial_seq_no + 2, txn_time=node.master_replica._ordering_service.last_accepted_pre_prepare_time,
+                        seq_no=initial_seq_no + 2, txn_time=node.master_replica.last_accepted_pre_prepare_time,
                         txn_roots={DOMAIN_LEDGER_ID: node.getLedger(DOMAIN_LEDGER_ID).tree.root_hash},
                         state_roots={DOMAIN_LEDGER_ID: node.getState(DOMAIN_LEDGER_ID).committedHeadHash},
                         pool_size=initial_pool_size, domain_size=initial_domain_size,
                         config_size=initial_config_size,
                         last_pool_seqno=4,
                         last_domain_seqno=None,
                         last_config_seqno=3,
                         primaries=pp_seq_no + 2 - 1)
 
         check_audit_txn(txn=node.auditLedger.getBySeqNo(node.auditLedger.size),
                         view_no=view_no, pp_seq_no=pp_seq_no + 3,
-                        seq_no=initial_seq_no + 3,
-                        txn_time=node.master_replica._ordering_service.last_accepted_pre_prepare_time,
+                        seq_no=initial_seq_no + 3, txn_time=node.master_replica.last_accepted_pre_prepare_time,
                         txn_roots={CONFIG_LEDGER_ID: node.getLedger(CONFIG_LEDGER_ID).tree.root_hash},
                         state_roots={CONFIG_LEDGER_ID: node.getState(CONFIG_LEDGER_ID).committedHeadHash},
                         pool_size=initial_pool_size, domain_size=initial_domain_size,
                         config_size=initial_config_size,
                         last_pool_seqno=4,
                         last_domain_seqno=5,
                         last_config_seqno=None,
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_demote_backup_primary_without_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_demote_backup_primary_without_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_calc_catchup_till.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_calc_catchup_till.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_handler_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_handler_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_handler_multiple_commits.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_handler_multiple_commits.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_multiple_ledgers_in_one_batch.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_multiple_ledgers_in_one_batch.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/audit_ledger/test_audit_ledger_ordering.py` & `indy-plenum-1.9.2rc1/plenum/test/audit_ledger/test_audit_ledger_ordering.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,16 +18,15 @@
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
                               sdk_wallet_client, 1)
     check_audit_ledger_updated(audit_size_initial, txnPoolNodeSet,
                                audit_txns_added=1)
     for node in txnPoolNodeSet:
         check_audit_txn(txn=node.auditLedger.get_last_txn(),
                         view_no=view_no, pp_seq_no=pp_seq_no + 1,
-                        seq_no=initial_seq_no + 1,
-                        txn_time=node.master_replica._ordering_service.last_accepted_pre_prepare_time,
+                        seq_no=initial_seq_no + 1, txn_time=node.master_replica.last_accepted_pre_prepare_time,
                         txn_roots={
                             POOL_LEDGER_ID: node.getLedger(POOL_LEDGER_ID).tree.root_hash,
                             DOMAIN_LEDGER_ID: node.getLedger(DOMAIN_LEDGER_ID).tree.root_hash
                         },
                         state_roots={
                             POOL_LEDGER_ID: node.getState(POOL_LEDGER_ID).committedHeadHash,
                             DOMAIN_LEDGER_ID: node.getState(DOMAIN_LEDGER_ID).committedHeadHash
@@ -43,16 +42,15 @@
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
                               sdk_wallet_client, 1)
     check_audit_ledger_updated(audit_size_initial, txnPoolNodeSet,
                                audit_txns_added=2)
     for node in txnPoolNodeSet:
         check_audit_txn(txn=node.auditLedger.get_last_txn(),
                         view_no=view_no, pp_seq_no=pp_seq_no + 2,
-                        seq_no=initial_seq_no + 2,
-                        txn_time=node.master_replica._ordering_service.last_accepted_pre_prepare_time,
+                        seq_no=initial_seq_no + 2, txn_time=node.master_replica.last_accepted_pre_prepare_time,
                         txn_roots={
                             DOMAIN_LEDGER_ID: node.getLedger(DOMAIN_LEDGER_ID).tree.root_hash
                         },
                         state_roots={
                             DOMAIN_LEDGER_ID: node.getState(DOMAIN_LEDGER_ID).committedHeadHash
                         },
                         pool_size=initial_pool_size, domain_size=initial_domain_size + 2,
@@ -69,16 +67,15 @@
                        sdk_wallet_stewards[3],
                        check_functional=False)
     check_audit_ledger_updated(audit_size_initial, txnPoolNodeSet,
                                audit_txns_added=3)
     for node in txnPoolNodeSet:
         check_audit_txn(txn=node.auditLedger.get_last_txn(),
                         view_no=view_no, pp_seq_no=pp_seq_no + 3,
-                        seq_no=initial_seq_no + 3,
-                        txn_time=node.master_replica._ordering_service.last_accepted_pre_prepare_time,
+                        seq_no=initial_seq_no + 3, txn_time=node.master_replica.last_accepted_pre_prepare_time,
                         txn_roots={
                             POOL_LEDGER_ID: node.getLedger(POOL_LEDGER_ID).tree.root_hash
                         },
                         state_roots={
                             POOL_LEDGER_ID: node.getState(POOL_LEDGER_ID).committedHeadHash
                         },
                         pool_size=initial_pool_size + 1, domain_size=initial_domain_size + 2,
@@ -95,16 +92,15 @@
                        sdk_wallet_stewards[3],
                        check_functional=False)
     check_audit_ledger_updated(audit_size_initial, txnPoolNodeSet,
                                audit_txns_added=4)
     for node in txnPoolNodeSet:
         check_audit_txn(txn=node.auditLedger.get_last_txn(),
                         view_no=view_no, pp_seq_no=pp_seq_no + 4,
-                        seq_no=initial_seq_no + 4,
-                        txn_time=node.master_replica._ordering_service.last_accepted_pre_prepare_time,
+                        seq_no=initial_seq_no + 4, txn_time=node.master_replica.last_accepted_pre_prepare_time,
                         txn_roots={
                             POOL_LEDGER_ID: node.getLedger(POOL_LEDGER_ID).tree.root_hash
                         },
                         state_roots={
                             POOL_LEDGER_ID: node.getState(POOL_LEDGER_ID).committedHeadHash
                         },
                         pool_size=initial_pool_size + 2, domain_size=initial_domain_size + 2,
@@ -118,16 +114,15 @@
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
                               sdk_wallet_client, 1)
     check_audit_ledger_updated(audit_size_initial, txnPoolNodeSet,
                                audit_txns_added=5)
     for node in txnPoolNodeSet:
         check_audit_txn(txn=node.auditLedger.get_last_txn(),
                         view_no=view_no, pp_seq_no=pp_seq_no + 5,
-                        seq_no=initial_seq_no + 5,
-                        txn_time=node.master_replica._ordering_service.last_accepted_pre_prepare_time,
+                        seq_no=initial_seq_no + 5, txn_time=node.master_replica.last_accepted_pre_prepare_time,
                         txn_roots={
                             DOMAIN_LEDGER_ID: node.getLedger(DOMAIN_LEDGER_ID).tree.root_hash
                         },
                         state_roots={
                             DOMAIN_LEDGER_ID: node.getState(DOMAIN_LEDGER_ID).committedHeadHash
                         },
                         pool_size=initial_pool_size + 2, domain_size=initial_domain_size + 3,
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/metrics/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/metrics/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/metrics/test_value_accumulator.py` & `indy-plenum-1.9.2rc1/plenum/test/metrics/test_value_accumulator.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/metrics/test_metrics_config.py` & `indy-plenum-1.9.2rc1/plenum/test/metrics/test_metrics_config.py`

 * *Files 12% similar despite different names*

```diff
@@ -90,13 +90,21 @@
             unexpected_events.add(MetricsName.BACKUP_PROCESS_PREPREPARE_TIME)
             unexpected_events.add(MetricsName.BACKUP_SEND_PREPARE_TIME)
         else:
             unexpected_events.add(MetricsName.BACKUP_SEND_PREPREPARE_TIME)
             unexpected_events.add(MetricsName.BACKUP_CREATE_3PC_BATCH_TIME)
             unexpected_events.add(MetricsName.BLS_UPDATE_PREPREPARE_TIME)
 
+        if not node.primaryDecider:
+            unexpected_events.add(MetricsName.PRIMARY_DECIDER_ACTION_QUEUE)
+            unexpected_events.add(MetricsName.PRIMARY_DECIDER_AQ_STASH)
+            unexpected_events.add(MetricsName.PRIMARY_DECIDER_REPEATING_ACTIONS)
+            unexpected_events.add(MetricsName.PRIMARY_DECIDER_SCHEDULED)
+            unexpected_events.add(MetricsName.PRIMARY_DECIDER_INBOX)
+            unexpected_events.add(MetricsName.PRIMARY_DECIDER_OUTBOX)
+
         # Check that all event types happened during test
         metric_names = {ev.name for ev in events}
         for t in MetricsName:
             if t in unexpected_events or t > TMP_METRIC:
                 continue
             assert t in metric_names
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/metrics/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/metrics/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/metrics/test_metrics_collector.py` & `indy-plenum-1.9.2rc1/plenum/test/metrics/test_metrics_collector.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/metrics/test_metrics_stats.py` & `indy-plenum-1.9.2rc1/plenum/test/metrics/test_metrics_stats.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_clientstack_restart_trigger.py` & `indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_clientstack_restart_trigger.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_send_too_many_reqs.py` & `indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_send_too_many_reqs.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_send_client_msgs_with_delay_reqs.py` & `indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_send_client_msgs_with_delay_reqs.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_zstack_reconnection.py` & `indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_zstack_reconnection.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_4_of_4_nodes.py` & `indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_4_of_4_nodes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_2_of_4_nodes.py` & `indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_2_of_4_nodes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_3_of_4_nodes.py` & `indy-plenum-1.9.2rc1/plenum/test/zstack_tests/test_restart_clientstack_before_reply_on_3_of_4_nodes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/simulation/test_sim_network.py` & `indy-plenum-1.9.2rc1/plenum/test/simulation/test_sim_network.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/simulation/test_sim_random.py` & `indy-plenum-1.9.2rc1/plenum/test/simulation/test_sim_random.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/simulation/sim_network.py` & `indy-plenum-1.9.2rc1/plenum/test/simulation/sim_network.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/simulation/sim_random.py` & `indy-plenum-1.9.2rc1/plenum/test/simulation/sim_random.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/get_buy_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/get_buy_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/consensus/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/conftest.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,160 +1,152 @@
-from functools import partial
-
 import pytest
+from unittest.mock import Mock
+from orderedset._orderedset import OrderedSet
 
-from plenum.common.constants import DOMAIN_LEDGER_ID
+from plenum.common.constants import DOMAIN_LEDGER_ID, CURRENT_PROTOCOL_VERSION, AUDIT_LEDGER_ID, POOL_LEDGER_ID
+from plenum.common.event_bus import InternalBus, ExternalBus
 from plenum.common.messages.internal_messages import RequestPropagates
+from plenum.common.messages.node_messages import PrePrepare
 from plenum.common.startable import Mode
-from plenum.common.event_bus import InternalBus
-from plenum.common.messages.node_messages import PrePrepare, ViewChange
-from plenum.common.stashing_router import StashingRouter
-from plenum.common.util import get_utc_epoch
-from plenum.server.consensus.consensus_shared_data import ConsensusSharedData
-from plenum.common.messages.node_messages import Checkpoint
-from plenum.server.consensus.view_change_service import ViewChangeService
+from plenum.common.timer import QueueTimer
+from plenum.server.consensus.ordering_service import OrderingService, ThreePCMsgValidator
 from plenum.server.database_manager import DatabaseManager
 from plenum.server.request_managers.write_request_manager import WriteRequestManager
-from plenum.test.checkpoints.helper import cp_digest
-from plenum.test.consensus.helper import primary_in_view
-from plenum.test.greek import genNodeNames
-from plenum.test.helper import MockTimer, MockNetwork
+from plenum.test.bls.conftest import fake_state_root_hash, fake_multi_sig, fake_multi_sig_value
+from plenum.test.consensus.order_service.helper import _register_pp_ts
+from plenum.test.helper import sdk_random_request_objects, create_pre_prepare_params
 from plenum.test.testing_utils import FakeSomething
 
 
-@pytest.fixture(params=[4, 6, 7])
-def validators(request):
-    return genNodeNames(request.param)
-
-
-@pytest.fixture(params=[0, 2])
-def initial_view_no(request):
-    return request.param
-
-
-@pytest.fixture(params=[False, True])
-def already_in_view_change(request):
-    return request.param
-
-
-@pytest.fixture
-def primary(validators):
-    return partial(primary_in_view, validators)
-
-
-@pytest.fixture
-def initial_checkpoints(initial_view_no):
-    return [Checkpoint(instId=0, viewNo=initial_view_no, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))]
-
-
-@pytest.fixture
-def consensus_data(validators, primary, initial_view_no, initial_checkpoints, is_master):
-    def _data(name):
-        data = ConsensusSharedData(name, validators, 0, is_master)
-        data.view_no = initial_view_no
-        data.checkpoints.update(initial_checkpoints)
-        return data
-
-    return _data
-
-
-@pytest.fixture
-def view_change_service(internal_bus, external_bus, stasher):
-    data = ConsensusSharedData("some_name", genNodeNames(4), 0)
-    return ViewChangeService(data, MockTimer(0), internal_bus, external_bus, stasher)
-
-
-@pytest.fixture
-def pre_prepare():
-    return PrePrepare(
-        0,
-        0,
-        1,
-        get_utc_epoch(),
-        ['f99937241d4c891c08e92a3cc25966607315ca66b51827b170d492962d58a9be'],
-        '[]',
-        'f99937241d4c891c08e92a3cc25966607315ca66b51827b170d492962d58a9be',
-        DOMAIN_LEDGER_ID,
-        'CZecK1m7VYjSNCC7pGHj938DSW2tfbqoJp1bMJEtFqvG',
-        '7WrAMboPTcMaQCU1raoj28vnhu2bPMMd2Lr9tEcsXeCJ',
-        0,
-        True
-    )
-
-
-@pytest.fixture(scope='function',
-                params=[Mode.starting, Mode.discovering, Mode.discovered,
-                        Mode.syncing, Mode.synced])
-def mode_not_participating(request):
-    return request.param
-
-
-@pytest.fixture(scope='function',
-                params=[Mode.starting, Mode.discovering, Mode.discovered,
-                        Mode.syncing, Mode.synced, Mode.participating])
-def mode(request):
-    return request.param
-
-
-@pytest.fixture
-def view_change_message():
-    def _view_change(view_no: int):
-        vc = ViewChange(
-            viewNo=view_no,
-            stableCheckpoint=4,
-            prepared=[],
-            preprepared=[],
-            checkpoints=[Checkpoint(instId=0, viewNo=view_no, seqNoStart=0, seqNoEnd=4, digest=cp_digest(4))]
-        )
-        return vc
-
-    return _view_change
-
-
 @pytest.fixture(params=[True, False])
 def is_master(request):
     return request.param
 
-
 @pytest.fixture()
 def internal_bus():
     def rp_handler(ib, msg):
         ib.msgs.setdefault(type(msg), []).append(msg)
 
     ib = InternalBus()
     ib.msgs = {}
     ib.subscribe(RequestPropagates, rp_handler)
-    return ib
-
+    return InternalBus()
 
 @pytest.fixture()
 def external_bus():
-    return MockNetwork()
-
+    send_handler = Mock()
+    return ExternalBus(send_handler=send_handler)
 
 @pytest.fixture()
 def bls_bft_replica():
     return FakeSomething(gc=lambda *args, **kwargs: True,
                          validate_pre_prepare=lambda *args, **kwargs: None,
                          update_prepare=lambda params, lid: params,
                          process_prepare=lambda *args, **kwargs: None,
                          process_pre_prepare=lambda *args, **kwargs: None,
                          validate_prepare=lambda *args, **kwargs: None,
-                         validate_commit=lambda *args, **kwargs: None,
                          update_commit=lambda params, pre_prepare: params,
                          process_commit=lambda *args, **kwargs: None)
 
-
 @pytest.fixture()
 def db_manager():
     dbm = DatabaseManager()
     return dbm
 
-
 @pytest.fixture()
 def write_manager(db_manager):
     return WriteRequestManager(database_manager=db_manager)
 
+@pytest.fixture()
+def name():
+    return "OrderingService"
+
+@pytest.fixture()
+def orderer(consensus_data, internal_bus, external_bus, name, write_manager, txn_roots, state_roots, bls_bft_replica):
+    orderer = OrderingService(data=consensus_data(name),
+                              timer=QueueTimer(),
+                              bus=internal_bus,
+                              network=external_bus,
+                              write_manager=write_manager,
+                              bls_bft_replica=bls_bft_replica,
+                              is_master=is_master)
+    orderer._data.node_mode = Mode.participating
+    orderer.primary_name = "Alpha:0"
+    orderer.l_txnRootHash = lambda ledger, to_str=False: txn_roots[ledger]
+    orderer.l_stateRootHash = lambda ledger, to_str=False: state_roots[ledger]
+    orderer.requestQueues[DOMAIN_LEDGER_ID] = OrderedSet()
+    orderer.l_revert = lambda *args, **kwargs: None
+    return orderer
+
+
+@pytest.fixture()
+def txn_roots():
+    return ["AAAgqga9DNr4bjH57Rdq6BRtvCN1PV9UX5Mpnm9gbMAZ",
+            "BBBJmfG5DYAE8ZcdTTFMiwcZaDN6CRVdSdkhBXnkYPio",
+            "CCCJmfG5DYAE8ZcdTTFMiwcZaDN6CRVdSdkhBXnkYPio",
+            "DDDJmfG5DYAE8ZcdTTFMiwcZaDN6CRVdSdkhBXnkYPio"]
+
+
+@pytest.fixture()
+def state_roots(fake_state_root_hash):
+    return ["EuDgqga9DNr4bjH57Rdq6BRtvCN1PV9UX5Mpnm9gbMAZ",
+            fake_state_root_hash,
+            "D95JmfG5DYAE8ZcdTTFMiwcZaDN6CRVdSdkhBXnkYPio",
+            None]
+
+
+@pytest.fixture(scope="function",
+                params=['BLS_not_None', 'BLS_None'])
+def multi_sig(fake_multi_sig, request):
+    if request.param == 'BLS_None':
+        return None
+    return fake_multi_sig
+
+
+@pytest.fixture(scope="function")
+def _pre_prepare(orderer, state_roots, txn_roots, multi_sig, fake_requests):
+    params = create_pre_prepare_params(state_root=state_roots[DOMAIN_LEDGER_ID],
+                                       ledger_id=DOMAIN_LEDGER_ID,
+                                       txn_root=txn_roots[DOMAIN_LEDGER_ID],
+                                       bls_multi_sig=multi_sig,
+                                       view_no=orderer.view_no,
+                                       inst_id=0,
+                                       pool_state_root=state_roots[POOL_LEDGER_ID],
+                                       audit_txn_root=txn_roots[AUDIT_LEDGER_ID],
+                                       reqs=fake_requests,
+                                       pp_seq_no=1)
+    pp = PrePrepare(*params)
+    return pp
+
+
+@pytest.fixture(scope="function")
+def pre_prepare(orderer, _pre_prepare):
+    _register_pp_ts(orderer, _pre_prepare, orderer.primary_name)
+    return _pre_prepare
+
+
+@pytest.fixture()
+def fake_requests():
+    return sdk_random_request_objects(10, identifier="fake_did",
+                                      protocol_version=CURRENT_PROTOCOL_VERSION)
+
+
+@pytest.fixture(scope='function')
+def orderer_with_requests(orderer, fake_requests):
+    orderer.l_apply_pre_prepare = lambda a: (fake_requests, [], [], False)
+    for req in fake_requests:
+        orderer.requestQueues[DOMAIN_LEDGER_ID].add(req.key)
+        orderer._requests.add(req)
+        orderer._requests.set_finalised(req)
+
+    return orderer
+
+
+@pytest.fixture()
+def validator(consensus_data):
+    return ThreePCMsgValidator(consensus_data)
+
 
 @pytest.fixture()
-def stasher(internal_bus, external_bus):
-    return StashingRouter(limit=100000, buses=[internal_bus, external_bus])
+def primary_orderer(orderer):
+    orderer.name = orderer.primary_name
+    return orderer
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/consensus/view_change/test_sim_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/consensus/view_change/test_sim_view_change.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,26 +1,23 @@
-from functools import partial
-
 import pytest
 
-from plenum.common.messages.internal_messages import NeedViewChange
 from plenum.server.consensus.view_change_service import BatchID
 from plenum.test.consensus.view_change.helper import some_pool
 from plenum.test.helper import MockNetwork
 from plenum.test.simulation.sim_random import SimRandom, DefaultSimRandom
 
 
 def check_view_change_completes_under_normal_conditions(random: SimRandom):
     # Create random pool with random initial state
     pool, committed = some_pool(random)
 
     # Schedule view change at different time on all nodes
     for node in pool.nodes:
         pool.timer.schedule(random.integer(0, 10000),
-                            partial(node._view_changer.process_need_view_change, NeedViewChange()))
+                            node._view_changer.start_view_change)
 
     # Make sure all nodes complete view change
     pool.timer.wait_for(lambda: all(not node._data.waiting_for_new_view
                                     and node._data.view_no > 0
                                     for node in pool.nodes))
 
     # Make sure all nodes end up in same state
@@ -53,15 +50,15 @@
             # pp_seq_no must be present in all Prepares
             if batch_id not in vc.prepared:
                 return committed
         committed.append(BatchID(*batch_id))
     return committed
 
 
-@pytest.mark.parametrize("seed", range(200))
+@pytest.mark.parametrize("seed", range(1000))
 def test_view_change_completes_under_normal_conditions(seed):
     random = DefaultSimRandom(seed)
     check_view_change_completes_under_normal_conditions(random)
 
 
 def test_new_view_combinations(random):
     # Create pool in some random initial state
@@ -69,15 +66,15 @@
     quorums = pool.nodes[0]._data.quorums
 
     # Get view change votes from all nodes
     view_change_messages = []
     for node in pool.nodes:
         network = MockNetwork()
         node._view_changer._network = network
-        node._view_changer._bus.send(NeedViewChange())
+        node._view_changer.start_view_change()
         view_change_messages.append(network.sent_messages[0][0])
 
     # Check that all committed requests are present in final batches
     for _ in range(10):
         num_votes = quorums.strong.value
         votes = random.sample(view_change_messages, num_votes)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/consensus/view_change/test_new_view_builder.py` & `indy-plenum-1.9.2rc1/plenum/test/consensus/view_change/test_new_view_builder.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import pytest
 
 from plenum.common.messages.node_messages import Checkpoint, ViewChange
 from plenum.server.consensus.consensus_shared_data import ConsensusSharedData
 from plenum.server.consensus.view_change_service import NewViewBuilder, BatchID
-from plenum.test.checkpoints.helper import cp_digest
 from plenum.test.consensus.view_change.helper import calc_committed
 from plenum.test.greek import genNodeNames
 from plenum.test.simulation.sim_random import DefaultSimRandom
 
 N = 4
 F = 1
 
@@ -22,26 +21,26 @@
 
 @pytest.fixture
 def builder(consensus_data_provider):
     return NewViewBuilder(consensus_data_provider)
 
 
 def test_calc_batches_empty(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vcs = [
         ViewChange(viewNo=0, stableCheckpoint=0, prepared=[], preprepared=[], checkpoints=[cp]),
         ViewChange(viewNo=0, stableCheckpoint=0, prepared=[], preprepared=[], checkpoints=[cp]),
         ViewChange(viewNo=0, stableCheckpoint=0, prepared=[], preprepared=[], checkpoints=[cp]),
         ViewChange(viewNo=0, stableCheckpoint=0, prepared=[], preprepared=[], checkpoints=[cp]),
     ]
     assert [] == builder.calc_batches(cp, vcs)
 
 
 def test_calc_batches_quorum(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc = ViewChange(viewNo=0, stableCheckpoint=0,
                     prepared=[(0, 1, "digest1"), (0, 2, "digest2")],
                     preprepared=[(0, 1, "digest1"), (0, 2, "digest2"), (0, 3, "digest3")],
                     checkpoints=[cp])
 
     vcs = [vc]
     assert builder.calc_batches(cp, vcs) is None
@@ -50,26 +49,26 @@
     assert builder.calc_batches(cp, vcs) is None
 
     vcs.append(vc)
     assert builder.calc_batches(cp, vcs)
 
 
 def test_calc_batches_same_data(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc = ViewChange(viewNo=0, stableCheckpoint=0,
                     prepared=[(0, 1, "digest1"), (0, 2, "digest2")],
                     preprepared=[(0, 1, "digest1"), (0, 2, "digest2")],
                     checkpoints=[cp])
 
     vcs = [vc, vc, vc, vc]
     assert builder.calc_batches(cp, vcs) == [BatchID(0, 1, "digest1"), BatchID(0, 2, "digest2")]
 
 
 def test_calc_batches_must_be_in_pre_prepare(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc = ViewChange(viewNo=0, stableCheckpoint=0,
                     prepared=[(0, 1, "digest1"), (0, 2, "digest2")],
                     preprepared=[(0, 1, "digest1")],
                     checkpoints=[cp])
 
     vcs = [vc, vc, vc, vc]
     # all nodes are malicious here since all added (0, 2) into prepared without adding to pre-prepared
@@ -86,66 +85,66 @@
                      checkpoints=[cp])
 
     vcs = [vc1, vc2, vc2, vc2]
     assert builder.calc_batches(cp, vcs) == [BatchID(0, 1, "digest1")]
 
 
 def test_calc_batches_takes_prepared_only(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc = ViewChange(viewNo=0, stableCheckpoint=0,
                     prepared=[],
                     preprepared=[(0, 1, "digest1"), (0, 2, "digest2"), (0, 3, "digest3"), (0, 4, "digest4")],
                     checkpoints=[cp])
 
     vcs = [vc, vc, vc, vc]
     assert builder.calc_batches(cp, vcs) == []
 
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc = ViewChange(viewNo=0, stableCheckpoint=0,
                     prepared=[(0, 1, "digest1"), (0, 2, "digest2")],
                     preprepared=[(0, 1, "digest1"), (0, 2, "digest2"), (0, 3, "digest3"), (0, 4, "digest4")],
                     checkpoints=[cp])
 
     vcs = [vc, vc, vc, vc]
     assert builder.calc_batches(cp, vcs) == [BatchID(0, 1, "digest1"), BatchID(0, 2, "digest2")]
 
 
 def test_calc_batches_takes_max_view(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc = ViewChange(viewNo=0, stableCheckpoint=0,
                     prepared=[(0, 1, "digest1"), (0, 2, "digest2"), (1, 1, "digest1"), (1, 2, "digest2")],
                     preprepared=[(0, 1, "digest1"), (0, 2, "digest2"), (1, 1, "digest1"), (1, 2, "digest2")],
                     checkpoints=[cp])
 
     vcs = [vc, vc, vc, vc]
     assert builder.calc_batches(cp, vcs) == [BatchID(1, 1, "digest1"), BatchID(1, 2, "digest2")]
 
 
 def test_calc_batches_respects_checkpoint(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest=cp_digest(10))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest='empty')
     vc = ViewChange(viewNo=0, stableCheckpoint=0,
                     prepared=[(0, 1, "digest1"), (0, 2, "digest2")],
                     preprepared=[(0, 1, "digest1"), (0, 2, "digest2")],
                     checkpoints=[cp])
 
     vcs = [vc, vc, vc, vc]
     assert builder.calc_batches(cp, vcs) == []
 
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest=cp_digest(10))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest='empty')
     vc = ViewChange(viewNo=0, stableCheckpoint=0,
                     prepared=[(0, 10, "digest10"), (0, 11, "digest11"), (1, 12, "digest12")],
                     preprepared=[(0, 10, "digest10"), (0, 11, "digest11"), (1, 12, "digest12")],
                     checkpoints=[cp])
 
     vcs = [vc, vc, vc, vc]
     assert builder.calc_batches(cp, vcs) == [BatchID(0, 11, "digest11"), BatchID(1, 12, "digest12")]
 
 
 def test_calc_batches_takes_quorum_of_prepared(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc1 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(0, 1, "digest2")],
                      preprepared=[(0, 1, "digest2")],
                      checkpoints=[cp])
     vc2 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(0, 1, "digest1")],
                      preprepared=[(0, 1, "digest1")],
@@ -171,15 +170,15 @@
     vcs = [vc2, vc3, vc3, vc3]
     assert builder.calc_batches(cp, vcs) == [BatchID(0, 1, "digest1")]
     vcs = [vc2, vc2, vc3, vc3]
     assert builder.calc_batches(cp, vcs) == [BatchID(0, 1, "digest1")]
 
 
 def test_calc_batches_takes_one_prepared_if_weak_quorum_of_preprepared(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc1 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(0, 1, "digest1"), (0, 2, "digest2")],
                      preprepared=[(0, 1, "digest1"), (0, 2, "digest2")],
                      checkpoints=[cp])
     vc2 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(0, 1, "digest1")],
                      preprepared=[(0, 1, "digest1"), (0, 2, "digest2")],
@@ -194,15 +193,15 @@
                      checkpoints=[cp])
 
     vcs = [vc1, vc2, vc3, vc4]
     assert builder.calc_batches(cp, vcs) == [BatchID(0, 1, "digest1"), (0, 2, "digest2")]
 
 
 def test_calc_batches_takes_next_view_one_prepared_if_weak_quorum_of_preprepared(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc1 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(0, 1, "digest1"), (1, 2, "digest2")],
                      preprepared=[(0, 1, "digest1"), (1, 2, "digest2")],
                      checkpoints=[cp])
     vc2 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(0, 1, "digest1")],
                      preprepared=[(0, 1, "digest1"), (1, 2, "digest2")],
@@ -217,15 +216,15 @@
                      checkpoints=[cp])
 
     vcs = [vc1, vc2, vc3, vc4]
     assert builder.calc_batches(cp, vcs) == [BatchID(0, 1, "digest1"), (1, 2, "digest2")]
 
 
 def test_calc_batches_takes_next_view_prepared_if_old_view_prepared(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc1 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(1, 1, "digest2")],
                      preprepared=[(0, 1, "digest1"), (1, 1, "digest2")],
                      checkpoints=[cp])
     vc2 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(0, 1, "digest1")],
                      preprepared=[(0, 1, "digest1"), (1, 1, "digest2")],
@@ -240,15 +239,15 @@
                      checkpoints=[cp])
 
     vcs = [vc1, vc2, vc3, vc4]
     assert builder.calc_batches(cp, vcs) == [BatchID(1, 1, "digest2")]
 
 
 def test_calc_batches_takes_prepared_if_preprepared_in_next_view(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc1 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(1, 1, "digest2")],
                      preprepared=[(0, 1, "digest1"), (2, 1, "digest2")],
                      checkpoints=[cp])
     vc2 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(0, 1, "digest1")],
                      preprepared=[(0, 1, "digest1"), (2, 1, "digest2")],
@@ -263,15 +262,15 @@
                      checkpoints=[cp])
 
     vcs = [vc1, vc2, vc3, vc4]
     assert builder.calc_batches(cp, vcs) == [BatchID(1, 1, "digest2")]
 
 
 def test_calc_batches_takes_prepared_with_same_batchid_only(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc1 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(1, 1, "digest1")],
                      preprepared=[(1, 1, "digest1")],
                      checkpoints=[cp])
     vc2 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(1, 1, "digest1")],
                      preprepared=[(1, 1, "digest1")],
@@ -295,46 +294,46 @@
                     preprepared=[(1, 1, "digest1")],
                     checkpoints=[])
     vcs = [vc, vc, vc, vc]
     assert builder.calc_checkpoint(vcs) is None
 
 
 def test_calc_checkpoints_equal_initial(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='empty')
     vc = ViewChange(viewNo=0, stableCheckpoint=0,
                     prepared=[(1, 1, "digest1")],
                     preprepared=[(1, 1, "digest1")],
                     checkpoints=[cp])
     vcs = [vc, vc, vc, vc]
     assert builder.calc_checkpoint(vcs) == cp
 
 
 def test_calc_checkpoints_equal_no_stable(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest='empty')
     vc = ViewChange(viewNo=0, stableCheckpoint=0,
                     prepared=[(1, 1, "digest1")],
                     preprepared=[(1, 1, "digest1")],
                     checkpoints=[cp])
     vcs = [vc, vc, vc, vc]
     assert builder.calc_checkpoint(vcs) == cp
 
 
 def test_calc_checkpoints_equal_stable(builder):
-    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest=cp_digest(0))
+    cp = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest='empty')
     vc = ViewChange(viewNo=0, stableCheckpoint=10,
                     prepared=[(1, 1, "digest1")],
                     preprepared=[(1, 1, "digest1")],
                     checkpoints=[cp])
     vcs = [vc, vc, vc, vc]
     assert builder.calc_checkpoint(vcs) == cp
 
 
 def test_calc_checkpoints_quorum(builder):
-    cp1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
-    cp2 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest=cp_digest(10))
+    cp1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='d1')
+    cp2 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest='d2')
 
     vc1 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(1, 1, "digest1")],
                      preprepared=[(1, 1, "digest1")],
                      checkpoints=[cp1])
     vc2 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(1, 1, "digest1")],
@@ -379,17 +378,17 @@
     assert builder.calc_checkpoint(vcs) == cp2
 
     vcs = [vc2_stable, vc2_stable, vc2_stable, vc2]
     assert builder.calc_checkpoint(vcs) == cp2
 
 
 def test_calc_checkpoints_selects_max(builder):
-    cp1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
-    cp2 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest=cp_digest(10))
-    cp3 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=20, digest=cp_digest(20))
+    cp1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='d1')
+    cp2 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest='d2')
+    cp3 = Checkpoint(instId=0, viewNo=0, seqNoStart=10, seqNoEnd=20, digest='d3')
 
     vc1 = ViewChange(viewNo=0, stableCheckpoint=0,
                      prepared=[(1, 1, "digest1")],
                      preprepared=[(1, 1, "digest1")],
                      checkpoints=[cp1])
 
     vc2_not_stable = ViewChange(viewNo=0, stableCheckpoint=0,
@@ -436,20 +435,17 @@
             assert builder.calc_checkpoint(vcs) == cp2
 
             vcs = [vc2, vc2, vc2, vc3]
             assert builder.calc_checkpoint(vcs) == cp2
 
 
 def test_calc_checkpoints_digest(builder):
-    d1 = cp_digest(0)
-    d2 = cp_digest(10)
-
-    cp1_d1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest=d1)
-    cp2_d2 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest=d2)
-    cp2_d1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest=d1)
+    cp1_d1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=0, digest='d1')
+    cp2_d2 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest='d2')
+    cp2_d1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest='d1')
 
     vc1_d1 = ViewChange(viewNo=0, stableCheckpoint=0,
                         prepared=[(1, 1, "digest1")],
                         preprepared=[(1, 1, "digest1")],
                         checkpoints=[cp1_d1])
     vc2_d2 = ViewChange(viewNo=0, stableCheckpoint=0,
                         prepared=[(1, 1, "digest1")],
@@ -484,16 +480,16 @@
 
 def test_calc_batches_combinations(builder, random):
     MAX_PP_SEQ_NO = 20
     CHEQ_FREQ = 10
     MAX_VIEW_NO = 3
     MAX_DIGEST_ID = 20
 
-    cp1 = Checkpoint(instId=0, viewNo=1, seqNoStart=0, seqNoEnd=0, digest=cp_digest(0))
-    cp2 = Checkpoint(instId=0, viewNo=1, seqNoStart=0, seqNoEnd=CHEQ_FREQ, digest=cp_digest(CHEQ_FREQ))
+    cp1 = Checkpoint(instId=0, viewNo=1, seqNoStart=0, seqNoEnd=0, digest='d1')
+    cp2 = Checkpoint(instId=0, viewNo=1, seqNoStart=0, seqNoEnd=CHEQ_FREQ, digest='d2')
 
     for i in range(100):
         for vc_count in range(N - F, N + 1):
             view_changes = []
 
             # 1. INIT
             for i in range(vc_count):
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/consensus/view_change/test_view_change_msg_creation.py` & `indy-plenum-1.9.2rc1/plenum/test/consensus/view_change/test_view_change_msg_creation.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,17 +1,14 @@
 import string
 
 import pytest
 
-from plenum.common.messages.internal_messages import NeedViewChange
 from plenum.common.messages.node_messages import ViewChange, Checkpoint
-from plenum.server.consensus.consensus_shared_data import BatchID
 from plenum.server.consensus.view_change_service import view_change_digest
-from plenum.test.checkpoints.helper import cp_digest
-from plenum.test.consensus.helper import create_view_change, create_batches
+from plenum.test.consensus.view_change.helper import some_preprepare
 
 
 @pytest.fixture
 def data(view_change_service):
     return view_change_service._data
 
 
@@ -20,68 +17,74 @@
     msg, _ = view_change_service._network.sent_messages[-1]
     assert isinstance(msg, ViewChange)
     return msg
 
 
 def test_view_change_data(view_change_service, data):
     data.view_no = 1
-    cp = Checkpoint(instId=0, viewNo=1, seqNoStart=0, seqNoEnd=10, digest=cp_digest(10))
+    cp = Checkpoint(instId=0, viewNo=1, seqNoStart=0, seqNoEnd=10, digest='empty')
     data.checkpoints.add(cp)
     data.stable_checkpoint = 10
-    data.prepared = [BatchID(0, 1, "digest1"),
-                     BatchID(0, 2, "digest2")]
-    data.preprepared = [BatchID(0, 1, "digest1"),
-                        BatchID(0, 2, "digest2"),
-                        BatchID(0, 3, "digest3")]
-
-    view_change_service._bus.send(NeedViewChange())
+    data.prepared = [some_preprepare(0, 1, "digest1"),
+                     some_preprepare(0, 2, "digest2")]
+    data.preprepared = [some_preprepare(0, 1, "digest1"),
+                        some_preprepare(0, 2, "digest2"),
+                        some_preprepare(0, 3, "digest3")]
+    view_change_service.start_view_change()
 
+    assert data.prepared == []
+    assert data.preprepared == []
     assert data.view_no == 2
+
     msg = get_view_change(view_change_service)
     assert msg.viewNo == 2
     assert msg.prepared == [(0, 1, "digest1"), (0, 2, "digest2")]
     assert msg.preprepared == [(0, 1, "digest1"), (0, 2, "digest2"), (0, 3, "digest3")]
     assert msg.stableCheckpoint == 10
     assert msg.checkpoints == [cp]
 
 
 def test_view_change_data_multiple(view_change_service, data):
     # view 0 -> 1
     data.view_no = 0
-    cp1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest=cp_digest(10))
+    cp1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest='empty')
     data.checkpoints.add(cp1)
     data.stable_checkpoint = 0
-    data.prepared = [BatchID(0, 1, "digest1"),
-                     BatchID(0, 2, "digest2")]
-    data.preprepared = [BatchID(0, 1, "digest1"),
-                        BatchID(0, 2, "digest2"),
-                        BatchID(0, 3, "digest3")]
+    data.prepared = [some_preprepare(0, 1, "digest1"),
+                     some_preprepare(0, 2, "digest2")]
+    data.preprepared = [some_preprepare(0, 1, "digest1"),
+                        some_preprepare(0, 2, "digest2"),
+                        some_preprepare(0, 3, "digest3")]
+    view_change_service.start_view_change()
 
-    view_change_service._bus.send(NeedViewChange())
+    assert data.prepared == []
+    assert data.preprepared == []
     assert data.view_no == 1
 
     msg = get_view_change(view_change_service)
     assert msg.viewNo == 1
     assert msg.prepared == [(0, 1, "digest1"), (0, 2, "digest2")]
     assert msg.preprepared == [(0, 1, "digest1"), (0, 2, "digest2"), (0, 3, "digest3")]
     assert msg.stableCheckpoint == 0
     assert msg.checkpoints == [cp1]
 
     # view 1 -> 2
     data.view_no = 1
-    cp2 = Checkpoint(instId=0, viewNo=1, seqNoStart=0, seqNoEnd=20, digest=cp_digest(20))
+    cp2 = Checkpoint(instId=0, viewNo=1, seqNoStart=10, seqNoEnd=20, digest='empty')
     data.checkpoints.add(cp2)
     data.stable_checkpoint = 0
-    data.prepared = [BatchID(1, 11, "digest11"),
-                     BatchID(1, 12, "digest12")]
-    data.preprepared = [BatchID(1, 11, "digest11"),
-                        BatchID(1, 12, "digest12"),
-                        BatchID(1, 13, "digest13")]
+    data.prepared = [some_preprepare(1, 11, "digest11"),
+                     some_preprepare(1, 12, "digest12")]
+    data.preprepared = [some_preprepare(1, 11, "digest11"),
+                        some_preprepare(1, 12, "digest12"),
+                        some_preprepare(1, 13, "digest13")]
+    view_change_service.start_view_change()
 
-    view_change_service._bus.send(NeedViewChange())
+    assert data.prepared == []
+    assert data.preprepared == []
     assert data.view_no == 2
 
     msg = get_view_change(view_change_service)
     assert msg.viewNo == 2
     assert msg.prepared == [(0, 1, "digest1"), (0, 2, "digest2"),
                             (1, 11, "digest11"), (1, 12, "digest12")]
     assert msg.preprepared == [(0, 1, "digest1"), (0, 2, "digest2"), (0, 3, "digest3"),
@@ -89,156 +92,165 @@
     assert msg.stableCheckpoint == 0
     assert msg.checkpoints == [cp1, cp2]
 
 
 def test_view_change_data_multiple_respects_checkpoint(view_change_service, data):
     # view 0 -> 1
     data.view_no = 0
-    cp1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest=cp_digest(10))
+    cp1 = Checkpoint(instId=0, viewNo=0, seqNoStart=0, seqNoEnd=10, digest='empty')
     data.checkpoints.add(cp1)
     data.stable_checkpoint = 0
-    data.prepared = [BatchID(0, 1, "digest1"),
-                     BatchID(0, 2, "digest2")]
-    data.preprepared = [BatchID(0, 1, "digest1"),
-                        BatchID(0, 2, "digest2"),
-                        BatchID(0, 3, "digest3")]
+    data.prepared = [some_preprepare(0, 1, "digest1"),
+                     some_preprepare(0, 2, "digest2")]
+    data.preprepared = [some_preprepare(0, 1, "digest1"),
+                        some_preprepare(0, 2, "digest2"),
+                        some_preprepare(0, 3, "digest3")]
+    view_change_service.start_view_change()
 
-    view_change_service._bus.send(NeedViewChange())
+    assert data.prepared == []
+    assert data.preprepared == []
     assert data.view_no == 1
 
     msg = get_view_change(view_change_service)
     assert msg.viewNo == 1
     assert msg.prepared == [(0, 1, "digest1"), (0, 2, "digest2")]
     assert msg.preprepared == [(0, 1, "digest1"), (0, 2, "digest2"), (0, 3, "digest3")]
     assert msg.stableCheckpoint == 0
     assert msg.checkpoints == [cp1]
 
     # view 1 -> 2
     data.view_no = 1
-    cp2 = Checkpoint(instId=0, viewNo=1, seqNoStart=0, seqNoEnd=20, digest=cp_digest(20))
+    cp2 = Checkpoint(instId=0, viewNo=1, seqNoStart=10, seqNoEnd=20, digest='empty')
     data.checkpoints.add(cp2)
     data.stable_checkpoint = 10
-    data.prepared = [BatchID(1, 11, "digest11"),
-                     BatchID(1, 12, "digest12")]
-    data.preprepared = [BatchID(1, 11, "digest11"),
-                        BatchID(1, 12, "digest12"),
-                        BatchID(1, 13, "digest13")]
+    data.prepared = [some_preprepare(1, 11, "digest11"),
+                     some_preprepare(1, 12, "digest12")]
+    data.preprepared = [some_preprepare(1, 11, "digest11"),
+                        some_preprepare(1, 12, "digest12"),
+                        some_preprepare(1, 13, "digest13")]
+    view_change_service.start_view_change()
 
-    view_change_service._bus.send(NeedViewChange())
+    assert data.prepared == []
+    assert data.preprepared == []
     assert data.view_no == 2
 
     msg = get_view_change(view_change_service)
     assert msg.viewNo == 2
     assert msg.prepared == [(1, 11, "digest11"), (1, 12, "digest12")]
     assert msg.preprepared == [(1, 11, "digest11"), (1, 12, "digest12"), (1, 13, "digest13")]
     assert msg.stableCheckpoint == 10
     assert msg.checkpoints == [cp1, cp2]
 
 
 def test_view_change_empty_prepares(view_change_service, data):
     data.prepared = []
     data.preprepared = []
 
-    view_change_service._bus.send(NeedViewChange())
+    view_change_service.start_view_change()
+
+    assert data.prepared == []
+    assert data.preprepared == []
 
     msg = get_view_change(view_change_service)
     assert msg.prepared == []
     assert msg.preprepared == []
 
 
 def test_view_change_replaces_prepare(view_change_service, data):
     data.view_no = 0
-    data.prepared = [BatchID(0, 1, "digest1"),
-                     BatchID(0, 2, "digest2")]
+    data.prepared = [some_preprepare(0, 1, "digest1"),
+                     some_preprepare(0, 2, "digest2")]
 
     # view no 0->1
-    view_change_service._bus.send(NeedViewChange())
+    view_change_service.start_view_change()
+
     assert data.view_no == 1
+    assert data.prepared == []
 
     msg = get_view_change(view_change_service)
     assert msg.viewNo == 1
     assert msg.prepared == [(0, 1, "digest1"), (0, 2, "digest2")]
 
     # view no 1->2
     # replace by different viewNo and digest
-    data.prepared = [BatchID(1, 1, "digest11"),
-                     BatchID(1, 2, "digest22")]
-    view_change_service._bus.send(NeedViewChange())
+    data.prepared = [some_preprepare(1, 1, "digest11"),
+                     some_preprepare(1, 2, "digest22")]
+    view_change_service.start_view_change()
+
     assert data.view_no == 2
+    assert data.prepared == []
 
     msg = get_view_change(view_change_service)
     assert msg.viewNo == 2
     assert msg.prepared == [(1, 1, "digest11"), (1, 2, "digest22")]
 
     # view no 2->3
     # replace by different viewNo only
-    data.prepared = [BatchID(2, 2, "digest22"),
-                     BatchID(2, 3, "digest3")]
-    view_change_service._bus.send(NeedViewChange())
+    data.prepared = [some_preprepare(2, 2, "digest22"),
+                     some_preprepare(2, 3, "digest3")]
+    view_change_service.start_view_change()
+
     assert data.view_no == 3
+    assert data.prepared == []
 
     msg = get_view_change(view_change_service)
     assert msg.viewNo == 3
     assert msg.prepared == [(1, 1, "digest11"), (2, 2, "digest22"), (2, 3, "digest3")]
 
 
 def test_view_change_keeps_preprepare(view_change_service, data):
     data.view_no = 0
-    data.preprepared = [BatchID(0, 1, "digest1"),
-                        BatchID(0, 2, "digest2")]
+    data.preprepared = [some_preprepare(0, 1, "digest1"),
+                        some_preprepare(0, 2, "digest2")]
 
     # view no 0->1
-    view_change_service._bus.send(NeedViewChange())
+    view_change_service.start_view_change()
 
     assert data.view_no == 1
+    assert data.preprepared == []
 
     msg = get_view_change(view_change_service)
     assert msg.viewNo == 1
     assert msg.preprepared == [(0, 1, "digest1"), (0, 2, "digest2")]
 
     # view no 1->2
     # do not replace since different viewNo and digest
-    data.preprepared = [BatchID(1, 1, "digest11"),
-                        BatchID(1, 2, "digest22")]
-    view_change_service._bus.send(NeedViewChange())
+    data.preprepared = [some_preprepare(1, 1, "digest11"),
+                        some_preprepare(1, 2, "digest22")]
+    view_change_service.start_view_change()
 
     assert data.view_no == 2
+    assert data.preprepared == []
 
     msg = get_view_change(view_change_service)
     assert msg.viewNo == 2
     assert msg.preprepared == [(0, 1, "digest1"), (0, 2, "digest2"),
                                (1, 1, "digest11"), (1, 2, "digest22")]
 
     # view no 2->3
     #  replace by different viewNo only
-    data.preprepared = [BatchID(2, 2, "digest22"),
-                        BatchID(2, 3, "digest3")]
-    view_change_service._bus.send(NeedViewChange())
+    data.preprepared = [some_preprepare(2, 2, "digest22"),
+                        some_preprepare(2, 3, "digest3")]
+    view_change_service.start_view_change()
 
     assert data.view_no == 3
+    assert data.preprepared == []
 
     msg = get_view_change(view_change_service)
     assert msg.viewNo == 3
     assert msg.preprepared == [(0, 1, "digest1"), (0, 2, "digest2"),
                                (1, 1, "digest11"),
                                (2, 2, "digest22"), (2, 3, "digest3")]
 
 
-def test_different_view_change_messages_have_different_digests(random):
-    batches = create_batches(view_no=0)
-    assert view_change_digest(create_view_change(initial_view_no=0, stable_cp=100, batches=batches)) != \
-           view_change_digest(create_view_change(initial_view_no=1, stable_cp=100, batches=batches))
-    assert view_change_digest(create_view_change(initial_view_no=1, stable_cp=100, batches=batches)) != \
-           view_change_digest(create_view_change(initial_view_no=1, stable_cp=101, batches=batches))
-    assert view_change_digest(
-        create_view_change(initial_view_no=1, stable_cp=100, batches=create_batches(view_no=0))) != \
-           view_change_digest(create_view_change(initial_view_no=1, stable_cp=100, batches=create_batches(view_no=1)))
-    assert view_change_digest(create_view_change(initial_view_no=0, stable_cp=100, batches=batches)) == \
-           view_change_digest(create_view_change(initial_view_no=0, stable_cp=100, batches=batches))
-
-
-def test_view_change_digest_is_256_bit_hexdigest(random):
-    digest = view_change_digest(
-        create_view_change(initial_view_no=0, stable_cp=random.integer(0, 10000), batches=create_batches(view_no=0)))
+def test_different_view_change_messages_have_different_digests(view_change_message, random):
+    vc = view_change_message(random.integer(0, 10000))
+    other_vc = view_change_message(random.integer(0, 10000))
+    assert view_change_digest(vc) != view_change_digest(other_vc)
+
+
+def test_view_change_digest_is_256_bit_hexdigest(view_change_message, random):
+    vc = view_change_message(random.integer(0, 10000))
+    digest = view_change_digest(vc)
     assert isinstance(digest, str)
     assert len(digest) == 64
     assert all(v in string.hexdigits for v in digest)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_pp_obsolescence.py` & `indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/test_pp_obsolescence.py`

 * *Files 2% similar despite different names*

```diff
@@ -84,28 +84,28 @@
 def test_pp_obsolete_if_older_than_last_accepted(primary_orderer, ts_now, sender, pp, sender_orderer):
     primary_orderer.last_accepted_pre_prepare_time = ts_now
     pp = FakeSomethingHashable(viewNo=0, ppSeqNo=1, ppTime=OBSOLETE_PP_TS)
 
     primary_orderer.pre_prepare_tss[pp.viewNo, pp.ppSeqNo][pp, sender_orderer] = \
         primary_orderer.last_accepted_pre_prepare_time
 
-    assert not primary_orderer._is_pre_prepare_time_correct(pp, sender)
+    assert not primary_orderer.l_is_pre_prepare_time_correct(pp, sender)
 
 
 def test_pp_obsolete_if_unknown(primary_orderer, pp):
     pp = FakeSomethingHashable(viewNo=0, ppSeqNo=1, ppTime=OBSOLETE_PP_TS)
-    assert not primary_orderer._is_pre_prepare_time_correct(pp, '')
+    assert not primary_orderer.l_is_pre_prepare_time_correct(pp, '')
 
 
 def test_pp_obsolete_if_older_than_threshold(primary_orderer, ts_now, pp, sender_orderer):
     pp = FakeSomethingHashable(viewNo=0, ppSeqNo=1, ppTime=OBSOLETE_PP_TS)
 
     primary_orderer.pre_prepare_tss[pp.viewNo, pp.ppSeqNo][pp, sender_orderer] = ts_now
 
-    assert not primary_orderer._is_pre_prepare_time_correct(pp, sender_orderer)
+    assert not primary_orderer.l_is_pre_prepare_time_correct(pp, sender_orderer)
 
 
 def test_ts_is_set_for_obsolete_pp(primary_orderer, ts_now, sender, pp, sender_orderer):
     pp.ppTime = OBSOLETE_PP_TS
     primary_orderer.process_preprepare(pp, sender_orderer)
     assert primary_orderer.pre_prepare_tss[pp.viewNo, pp.ppSeqNo][pp, sender_orderer] == ts_now
 
@@ -118,15 +118,15 @@
 def test_ts_is_set_for_discarded_pp(primary_orderer, ts_now, sender, pp, sender_orderer):
     pp.instId += 1
     primary_orderer.process_preprepare(pp, sender_orderer)
     assert primary_orderer.pre_prepare_tss[pp.viewNo, pp.ppSeqNo][pp, sender_orderer] == ts_now
 
 
 def test_ts_is_set_for_stahed_pp(primary_orderer, ts_now, sender, pp, sender_orderer):
-    pp.viewNo += 1
+    pp.viewNo +=1
     primary_orderer.process_preprepare(pp, sender_orderer)
     assert primary_orderer.pre_prepare_tss[pp.viewNo, pp.ppSeqNo][pp, sender_orderer] == ts_now
 
 
 def test_ts_is_not_set_for_non_pp(primary_orderer, ts_now, sender, pp, sender_orderer):
     pp = FakeSomethingHashable(**pp.__dict__)
     primary_orderer.process_prepare(pp, sender_orderer)
@@ -134,13 +134,13 @@
     assert len(primary_orderer.pre_prepare_tss) == 0
 
 
 def test_pre_prepare_tss_is_cleaned_in_gc(primary_orderer, pp, sender_orderer):
     primary_orderer.process_preprepare(pp, sender_orderer)
 
     # threshold is lower
-    primary_orderer.gc((pp.viewNo, pp.ppSeqNo - 1))
+    primary_orderer.l_gc((pp.viewNo, pp.ppSeqNo - 1))
     assert (pp.viewNo, pp.ppSeqNo) in primary_orderer.pre_prepare_tss
 
     # threshold is not lower
-    primary_orderer.gc((pp.viewNo, pp.ppSeqNo))
+    primary_orderer.l_gc((pp.viewNo, pp.ppSeqNo))
     assert (pp.viewNo, pp.ppSeqNo) not in primary_orderer.pre_prepare_tss
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_can_send_3pc.py` & `indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/test_can_send_3pc.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import pytest
 
 from plenum.common.startable import Mode
 
 
 def test_can_send_3pc_batch_by_primary_only(primary_orderer):
     assert primary_orderer.can_send_3pc_batch()
-    primary_orderer._data.primary_name = "SomeNode:0"
+    primary_orderer.primary_name = "SomeNode:0"
     assert not primary_orderer.can_send_3pc_batch()
 
 
 def test_can_send_3pc_batch_not_participating(primary_orderer, mode):
     primary_orderer._data.node_mode = mode
     result = primary_orderer.can_send_3pc_batch()
     assert result == (mode == Mode.participating)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_orderer_api.py` & `indy-plenum-1.9.2rc1/plenum/test/consensus/order_service/test_orderer_api.py`

 * *Files 18% similar despite different names*

```diff
@@ -12,58 +12,43 @@
 
 def test_is_next_pre_prepare(orderer):
     pp_view_no = 2
     pp_seq_no = 1
     orderer.last_ordered_3pc = (1, 2)
 
     assert orderer.view_no != pp_view_no
-    assert not orderer._is_next_pre_prepare(pp_view_no, pp_seq_no)
+    assert not orderer.l__is_next_pre_prepare(pp_view_no, pp_seq_no)
 
 
 def test_order_3pc_key(orderer):
     with pytest.raises(ValueError) as excinfo:
-        orderer._order_3pc_key((1, 1))
+        orderer.l_order_3pc_key((1, 1))
     assert ("no PrePrepare with a 'key' {} found"
             .format((1, 1))) in str(excinfo.value)
 
 
 @pytest.mark.skip(reason="dequeue_prepares not implemented yet")
 def test_can_pp_seq_no_be_in_view(orderer):
     view_no = orderer.view_no + 1
     assert orderer.view_no < view_no
     with pytest.raises(PlenumValueError) as excinfo:
-        orderer._can_pp_seq_no_be_in_view(view_no, 1)
+        orderer.l_can_pp_seq_no_be_in_view(view_no, 1)
     assert ("expected: <= current view_no {}"
             .format(orderer.view_no)) in str(excinfo.value)
 
 
 def test_is_msg_from_primary_doesnt_crash_on_msg_with_view_greater_than_current(orderer):
     class FakeMsg:
         def __init__(self, viewNo):
             self.viewNo = viewNo
 
     invalid_view_no = 1 if orderer.view_no is None else orderer.view_no + 1
 
     # This shouldn't crash
-    orderer._is_msg_from_primary(FakeMsg(invalid_view_no), "some_sender")
+    orderer.l_isMsgFromPrimary(FakeMsg(invalid_view_no), "some_sender")
 
 
 def test_request_prepare_doesnt_crash_when_primary_is_not_connected(orderer):
-    orderer._data.primary_name = 'Omega:0'
+    orderer.primary_name = 'Omega:0'
     orderer._request_msg = lambda *args, **kwargs: None
     # This shouldn't crash
     orderer._request_prepare((0, 1))
-
-
-def test_pp_storages_ordering(pre_prepare, orderer):
-    orderer._preprepare_batch(pre_prepare)
-    assert orderer._data.preprepared
-    assert not orderer._data.prepared
-
-    orderer._prepare_batch(pre_prepare)
-    assert orderer._data.preprepared
-    assert orderer._data.prepared
-
-    orderer._clear_batch(pre_prepare)
-    assert not orderer._data.preprepared
-    assert not orderer._data.prepared
-
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/consensus/order_service/test_ordering_process_preprepare.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_process_preprepare.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,137 +1,153 @@
-from unittest.mock import Mock
+from copy import copy
 
 import pytest
-from plenum.common.constants import DOMAIN_LEDGER_ID, CURRENT_PROTOCOL_VERSION, AUDIT_LEDGER_ID, POOL_LEDGER_ID, \
-    SEQ_NO_DB_LABEL, LAST_SENT_PP_STORE_LABEL
+
+from plenum.common.constants import DOMAIN_LEDGER_ID, POOL_LEDGER_ID, AUDIT_LEDGER_ID
 from plenum.common.exceptions import SuspiciousNode
-from plenum.common.messages.internal_messages import RequestPropagates, RaisedSuspicion
 from plenum.common.messages.node_messages import PrePrepare
-from plenum.server.replica_helper import PP_SUB_SEQ_NO_WRONG, PP_NOT_FINAL
+from plenum.common.types import f
+from plenum.server.replica import PP_SUB_SEQ_NO_WRONG, PP_NOT_FINAL
 from plenum.server.suspicion_codes import Suspicions
-from plenum.test.consensus.order_service.helper import _register_pp_ts, check_suspicious
-from plenum.test.helper import sdk_random_request_objects, create_pre_prepare_params
+from plenum.test.helper import create_pre_prepare_params
+from plenum.test.replica.conftest import pre_prepare as _pre_prepare
+from plenum.test.replica.helper import expect_suspicious, register_pp_ts
 from plenum.test.testing_utils import FakeSomething
+from stp_zmq.zstack import ZStack
+
+nodeCount = 4
+
+
+@pytest.fixture(scope='function')
+def mock_schema_pool_state_root():
+    old_schema = copy(PrePrepare.schema)
+    PrePrepare.schema = tuple(y for y in PrePrepare.schema if y[0] != f.POOL_STATE_ROOT_HASH.nm)
+    yield PrePrepare.schema
+    PrePrepare.schema = old_schema
+
+
+@pytest.fixture(scope='function')
+def mock_schema_audit_txn_root():
+    old_schema = copy(PrePrepare.schema)
+    PrePrepare.schema = tuple(y for y in PrePrepare.schema if y[0] != f.AUDIT_TXN_ROOT_HASH.nm)
+    yield PrePrepare.schema
+    PrePrepare.schema = old_schema
 
 
 @pytest.fixture(scope="function")
-def pre_prepare(orderer, _pre_prepare):
-    _register_pp_ts(orderer, _pre_prepare, orderer.primary_name)
+def pre_prepare(replica, _pre_prepare):
+    register_pp_ts(replica, _pre_prepare, replica.primaryName)
     return _pre_prepare
 
 
-@pytest.fixture()
-def fake_requests():
-    return sdk_random_request_objects(10, identifier="fake_did",
-                                      protocol_version=CURRENT_PROTOCOL_VERSION)
+def test_process_pre_prepare_validation(replica_with_requests,
+                                        pre_prepare):
+    replica_with_requests.processPrePrepare(pre_prepare, replica_with_requests.primaryName)
 
 
-@pytest.fixture(scope='function')
-def orderer_with_requests(orderer, fake_requests):
-    orderer._apply_pre_prepare = lambda a: (fake_requests, [], [], False)
-    for req in fake_requests:
-        orderer.requestQueues[DOMAIN_LEDGER_ID].add(req.key)
-        orderer._requests.add(req)
-        orderer._requests.set_finalised(req)
-    return orderer
+def test_process_pre_prepare_validation_old_schema_no_pool(replica_with_requests,
+                                                           pre_prepare,
+                                                           mock_schema_pool_state_root):
+    serialized_pp = ZStack.serializeMsg(pre_prepare)
+    deserialized_pp = ZStack.deserializeMsg(serialized_pp)
+    assert f.POOL_STATE_ROOT_HASH.nm not in PrePrepare.schema
 
+    pp = PrePrepare(**deserialized_pp)
+    register_pp_ts(replica_with_requests, pp, replica_with_requests.primaryName)
+    replica_with_requests.processPrePrepare(pp, replica_with_requests.primaryName)
 
-def test_process_pre_prepare_validation(orderer_with_requests,
-                                        pre_prepare):
-    orderer_with_requests.process_preprepare(pre_prepare, orderer_with_requests.primary_name)
 
+def test_process_pre_prepare_validation_old_schema_no_audit(replica_with_requests,
+                                                            pre_prepare,
+                                                            mock_schema_audit_txn_root):
+    serialized_pp = ZStack.serializeMsg(pre_prepare)
+    deserialized_pp = ZStack.deserializeMsg(serialized_pp)
+    assert f.AUDIT_TXN_ROOT_HASH.nm not in PrePrepare.schema
+
+    pp = PrePrepare(**deserialized_pp)
+    register_pp_ts(replica_with_requests, pp, replica_with_requests.primaryName)
+    replica_with_requests.processPrePrepare(pp, replica_with_requests.primaryName)
 
-def test_process_pre_prepare_with_incorrect_pool_state_root(orderer_with_requests,
+
+def test_process_pre_prepare_with_incorrect_pool_state_root(replica_with_requests,
                                                             state_roots, txn_roots, multi_sig, fake_requests):
-    handler = Mock()
-    orderer_with_requests._bus.subscribe(RaisedSuspicion, handler)
+    expect_suspicious(replica_with_requests, Suspicions.PPR_POOL_STATE_ROOT_HASH_WRONG.code)
+
     pre_prepare_params = create_pre_prepare_params(state_root=state_roots[DOMAIN_LEDGER_ID],
                                                    ledger_id=DOMAIN_LEDGER_ID,
                                                    txn_root=txn_roots[DOMAIN_LEDGER_ID],
                                                    bls_multi_sig=multi_sig,
-                                                   view_no=orderer_with_requests.view_no,
-                                                   inst_id=0,
+                                                   view_no=replica_with_requests.viewNo,
+                                                   inst_id=replica_with_requests.instId,
                                                    # INVALID!
                                                    pool_state_root="HSai3sMHKeAva4gWMabDrm1yNhezvPHfXnGyHf2ex1L4",
                                                    audit_txn_root=txn_roots[AUDIT_LEDGER_ID],
-                                                   reqs=fake_requests,
-                                                   pp_seq_no=1)
+                                                   reqs=fake_requests)
     pre_prepare = PrePrepare(*pre_prepare_params)
-    _register_pp_ts(orderer_with_requests, pre_prepare, orderer_with_requests.primary_name)
+    register_pp_ts(replica_with_requests, pre_prepare, replica_with_requests.primaryName)
 
-    orderer_with_requests.process_preprepare(pre_prepare, orderer_with_requests.primary_name)
-    check_suspicious(handler, RaisedSuspicion(inst_id=orderer_with_requests._data.inst_id,
-                                              ex=SuspiciousNode(orderer_with_requests.primary_name,
-                                                                Suspicions.PPR_POOL_STATE_ROOT_HASH_WRONG,
-                                                                pre_prepare)))
+    with pytest.raises(SuspiciousNode):
+        replica_with_requests.processPrePrepare(pre_prepare, replica_with_requests.primaryName)
 
 
-def test_process_pre_prepare_with_incorrect_audit_txn_root(orderer_with_requests,
+def test_process_pre_prepare_with_incorrect_audit_txn_root(replica_with_requests,
                                                            state_roots, txn_roots, multi_sig, fake_requests):
-    if not orderer_with_requests.is_master:
-        return
-    handler = Mock()
-    orderer_with_requests._bus.subscribe(RaisedSuspicion, handler)
+    expect_suspicious(replica_with_requests, Suspicions.PPR_AUDIT_TXN_ROOT_HASH_WRONG.code)
+
     pre_prepare_params = create_pre_prepare_params(state_root=state_roots[DOMAIN_LEDGER_ID],
                                                    ledger_id=DOMAIN_LEDGER_ID,
                                                    txn_root=txn_roots[DOMAIN_LEDGER_ID],
                                                    bls_multi_sig=multi_sig,
-                                                   view_no=orderer_with_requests.view_no,
-                                                   inst_id=0,
+                                                   view_no=replica_with_requests.viewNo,
+                                                   inst_id=replica_with_requests.instId,
                                                    pool_state_root=state_roots[POOL_LEDGER_ID],
                                                    # INVALID!
                                                    audit_txn_root="HSai3sMHKeAva4gWMabDrm1yNhezvPHfXnGyHf2ex1L4",
-                                                   reqs=fake_requests,
-                                                   pp_seq_no=1)
+                                                   reqs=fake_requests)
     pre_prepare = PrePrepare(*pre_prepare_params)
-    _register_pp_ts(orderer_with_requests, pre_prepare, orderer_with_requests.primary_name)
+    register_pp_ts(replica_with_requests, pre_prepare, replica_with_requests.primaryName)
 
-    orderer_with_requests.process_preprepare(pre_prepare, orderer_with_requests.primary_name)
-    check_suspicious(handler, RaisedSuspicion(inst_id=orderer_with_requests._data.inst_id,
-                                              ex=SuspiciousNode(orderer_with_requests.primary_name,
-                                                                Suspicions.PPR_AUDIT_TXN_ROOT_HASH_WRONG,
-                                                                pre_prepare)))
+    with pytest.raises(SuspiciousNode):
+        replica_with_requests.processPrePrepare(pre_prepare, replica_with_requests.primaryName)
 
 
-def test_process_pre_prepare_with_not_final_request(orderer, pre_prepare):
-    orderer.db_manager.stores[SEQ_NO_DB_LABEL] = FakeSomething(get_by_full_digest=lambda req: None,
-                                                               get_by_payload_digest=lambda req: (None, None))
-    orderer._non_finalised_reqs = lambda a: set(pre_prepare.reqIdr)
+def test_process_pre_prepare_with_not_final_request(replica, pre_prepare):
+    replica.node.seqNoDB = FakeSomething(get_by_full_digest=lambda req: None,
+                                         get_by_payload_digest=lambda req: (None, None))
+    replica.nonFinalisedReqs = lambda a: set(pre_prepare.reqIdr)
 
     def request_propagates(reqs):
         assert reqs == set(pre_prepare.reqIdr)
 
-    orderer._bus.subscribe(RequestPropagates, request_propagates)
+    replica.node.request_propagates = request_propagates
 
-    orderer.process_preprepare(pre_prepare, orderer.primary_name)
+    replica.processPrePrepare(pre_prepare, replica.primaryName)
+    assert (pre_prepare, replica.primaryName, set(pre_prepare.reqIdr)) in replica.prePreparesPendingFinReqs
 
 
-def test_process_pre_prepare_with_ordered_request(orderer, pre_prepare):
-    handler = Mock()
-    orderer._bus.subscribe(RaisedSuspicion, handler)
+def test_process_pre_prepare_with_ordered_request(replica, pre_prepare):
+    expect_suspicious(replica, Suspicions.PPR_WITH_ORDERED_REQUEST.code)
 
-    orderer.db_manager.stores[SEQ_NO_DB_LABEL] = FakeSomething(get_by_full_digest=lambda req: 'sample',
-                                                               get_by_payload_digest=lambda req: (1, 1))
-    orderer._non_finalised_reqs = lambda a: pre_prepare.reqIdr
+    replica.node.seqNoDB = FakeSomething(get_by_full_digest=lambda req: 'sample',
+                                         get_by_payload_digest=lambda req: (1, 1))
+    replica.nonFinalisedReqs = lambda a: pre_prepare.reqIdr
 
     def request_propagates(reqs):
         assert False, "Requested propagates for: {}".format(reqs)
 
-    orderer._bus.subscribe(RequestPropagates, request_propagates)
+    replica.node.request_propagates = request_propagates
 
-    orderer.process_preprepare(pre_prepare, orderer.primary_name)
-    check_suspicious(handler, RaisedSuspicion(inst_id=orderer._data.inst_id,
-                                              ex=SuspiciousNode(orderer.primary_name,
-                                                                Suspicions.PPR_WITH_ORDERED_REQUEST,
-                                                                pre_prepare)))
+    with pytest.raises(SuspiciousNode):
+        replica.processPrePrepare(pre_prepare, replica.primaryName)
+    assert (pre_prepare, replica.primaryName, set(pre_prepare.reqIdr)) not in replica.prePreparesPendingFinReqs
 
 
-def test_suspicious_on_wrong_sub_seq_no(orderer_with_requests, pre_prepare):
+def test_suspicious_on_wrong_sub_seq_no(replica_with_requests, pre_prepare):
     pre_prepare.sub_seq_no = 1
-    assert PP_SUB_SEQ_NO_WRONG == orderer_with_requests._process_valid_preprepare(pre_prepare,
-                                                                                   orderer_with_requests.primary_name)
+    assert PP_SUB_SEQ_NO_WRONG == replica_with_requests._process_valid_preprepare(pre_prepare,
+                                                                                  replica_with_requests.primaryName)
 
 
-def test_suspicious_on_not_final(orderer_with_requests, pre_prepare):
+def test_suspicious_on_not_final(replica_with_requests, pre_prepare):
     pre_prepare.final = False
-    assert PP_NOT_FINAL == orderer_with_requests._process_valid_preprepare(pre_prepare,
-                                                                            orderer_with_requests.primary_name)
+    assert PP_NOT_FINAL == replica_with_requests._process_valid_preprepare(pre_prepare,
+                                                                           replica_with_requests.primaryName)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/consensus/test_three_pc_validator.py` & `indy-plenum-1.9.2rc1/plenum/test/consensus/test_three_pc_validator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import pytest
 
 from plenum.common.startable import Mode
-from plenum.common.stashing_router import PROCESS, DISCARD
 from plenum.server.consensus.ordering_service import ThreePCMsgValidator
-from plenum.server.replica_validator_enums import INCORRECT_PP_SEQ_NO, ALREADY_ORDERED, FUTURE_VIEW, \
+from plenum.server.replica_validator_enums import PROCESS, DISCARD, INCORRECT_PP_SEQ_NO, ALREADY_ORDERED, FUTURE_VIEW, \
     STASH_VIEW, OLD_VIEW, STASH_CATCH_UP, CATCHING_UP, OUTSIDE_WATERMARKS, STASH_WATERMARKS, GREATER_PREP_CERT
 from plenum.test.bls.helper import generate_state_root
 from plenum.test.helper import create_pre_prepare_no_bls, create_prepare, create_commit_no_bls_sig
 
 
 @pytest.fixture(scope='function', params=[0, 1])
 def inst_id(request):
@@ -44,15 +43,15 @@
     commit = create_commit_no_bls_sig(req_key=(view_no, pp_seq_no),
                                       inst_id=inst_id)
     return [pre_prepare, prepare, commit]
 
 
 def test_check_all_correct(validator, inst_id):
     validator._data.node_mode = Mode.participating
-    for msg in create_3pc_msgs(view_no=validator._data.view_no,
+    for msg in create_3pc_msgs(view_no=validator.view_no,
                                pp_seq_no=1,
                                inst_id=inst_id):
         assert validator.validate(msg) == (PROCESS, None)
 
 
 @pytest.mark.parametrize('mode, result', [
     (Mode.starting, (STASH_CATCH_UP, CATCHING_UP)),
@@ -60,22 +59,22 @@
     (Mode.discovered, (STASH_CATCH_UP, CATCHING_UP)),
     (Mode.syncing, (STASH_CATCH_UP, CATCHING_UP)),
     (Mode.synced, (STASH_CATCH_UP, CATCHING_UP)),
     (Mode.participating, (PROCESS, None)),
 ])
 def test_check_participating(validator, mode, result, inst_id):
     validator._data.node_mode = mode
-    for msg in create_3pc_msgs(view_no=validator._data.view_no,
+    for msg in create_3pc_msgs(view_no=validator.view_no,
                                pp_seq_no=1,
                                inst_id=inst_id):
         assert validator.validate(msg) == result
 
 
 def test_check_current_view(validator, inst_id):
-    for msg in create_3pc_msgs(view_no=validator._data.view_no,
+    for msg in create_3pc_msgs(view_no=validator.view_no,
                                pp_seq_no=1,
                                inst_id=inst_id):
         assert validator.validate(msg) == (PROCESS, None)
 
 
 def test_check_old_view(validator, inst_id, view_no):
     pp_seq_no = 1
@@ -84,15 +83,15 @@
     for msg in create_3pc_msgs(view_no=view_no,
                                pp_seq_no=pp_seq_no + 1,
                                inst_id=inst_id):
         assert validator.validate(msg) == (DISCARD, OLD_VIEW)
 
 
 def test_check_future_view(validator, inst_id):
-    for msg in create_3pc_msgs(view_no=validator._data.view_no + 1,
+    for msg in create_3pc_msgs(view_no=validator.view_no + 1,
                                pp_seq_no=1,
                                inst_id=inst_id):
         assert validator.validate(msg) == (STASH_VIEW, FUTURE_VIEW)
 
 
 def test_check_previous_view_no_view_change(validator, inst_id, view_no):
     validator._data.view_no = view_no + 1
@@ -276,13 +275,13 @@
     (10, (DISCARD, ALREADY_ORDERED)),
     # assume last ordered is 10
     (11, (STASH_CATCH_UP, CATCHING_UP)),
     (12, (STASH_CATCH_UP, CATCHING_UP)),
     (100, (STASH_CATCH_UP, CATCHING_UP)),
 ])
 def test_check_ordered_not_participating(validator, pp_seq_no, result, inst_id):
-    validator._data.last_ordered_3pc = (validator._data.view_no, 10)
+    validator._data.last_ordered_3pc = (validator.view_no, 10)
     validator._data.node_mode = Mode.syncing
-    for msg in create_3pc_msgs(view_no=validator._data.view_no,
+    for msg in create_3pc_msgs(view_no=validator.view_no,
                                pp_seq_no=pp_seq_no,
                                inst_id=inst_id):
         assert validator.validate(msg) == result
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/consensus/checkpoint_service/test_checkpoint_validation.py` & `indy-plenum-1.9.2rc1/plenum/test/replica/test_replica_checkpoint_validation.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,128 +1,127 @@
 import pytest
 
 from plenum.common.messages.node_messages import Checkpoint
 from plenum.common.startable import Mode
-from plenum.common.stashing_router import PROCESS, DISCARD
-from plenum.common.util import SortedDict
-from plenum.server.consensus.msg_validator import CheckpointMsgValidator
 from plenum.server.replica_validator import ReplicaValidator
-from plenum.server.replica_validator_enums import INCORRECT_INSTANCE, CATCHING_UP, ALREADY_STABLE, \
+from plenum.server.replica_validator_enums import DISCARD, INCORRECT_INSTANCE, PROCESS, CATCHING_UP, ALREADY_STABLE, \
     STASH_CATCH_UP, OLD_VIEW, FUTURE_VIEW, STASH_VIEW
-from plenum.test.checkpoints.helper import cp_digest
 
 
 @pytest.fixture(scope='function', params=[0, 1])
 def inst_id(request):
     return request.param
 
 
 @pytest.fixture(scope='function', params=[2])
 def viewNo(tconf, request):
     return request.param
 
 
 @pytest.fixture(scope='function')
-def validator(consensus_data):
-    data = consensus_data("some_name")
-    data.node_mode = Mode.participating
-    return CheckpointMsgValidator(data)
+def validator(replica, inst_id):
+    return ReplicaValidator(replica=replica)
 
 
-def checkpoint(view_no, inst_id, pp_seq_no):
+def checkpoint(view_no, inst_id, seq_no_start, seq_no_end):
     return Checkpoint(instId=inst_id,
                       viewNo=view_no,
-                      seqNoStart=0,
-                      seqNoEnd=pp_seq_no,
-                      digest=cp_digest(pp_seq_no))
+                      seqNoStart=seq_no_start,
+                      seqNoEnd=seq_no_end,
+                      digest='digest-{}-{}'.format(str(seq_no_start), str(seq_no_end)))
 
 
 def test_check_all_correct(validator):
-    msg = checkpoint(view_no=validator._data.view_no,
-                     inst_id=validator._data.inst_id,
-                     pp_seq_no=10)
-    assert validator.validate(msg) == (PROCESS, None)
+    msg = checkpoint(view_no=validator.view_no,
+                     inst_id=validator.inst_id,
+                     seq_no_start=0,
+                     seq_no_end=10)
+    assert validator.validate_checkpoint_msg(msg) == (PROCESS, None)
 
 
 def test_check_inst_id_incorrect(validator):
-    msg = checkpoint(view_no=validator._data.view_no,
-                     inst_id=validator._data.inst_id + 1,
-                     pp_seq_no=10)
-    assert validator.validate(msg) == (DISCARD, INCORRECT_INSTANCE)
+    msg = checkpoint(view_no=validator.view_no,
+                     inst_id=validator.inst_id + 1,
+                     seq_no_start=0,
+                     seq_no_end=10)
+    assert validator.validate_checkpoint_msg(msg) == (DISCARD, INCORRECT_INSTANCE)
 
 
 @pytest.mark.parametrize('mode, result', [
     (Mode.starting, (STASH_CATCH_UP, CATCHING_UP)),
     (Mode.discovering, (STASH_CATCH_UP, CATCHING_UP)),
     (Mode.discovered, (STASH_CATCH_UP, CATCHING_UP)),
     (Mode.syncing, (STASH_CATCH_UP, CATCHING_UP)),
     (Mode.synced, (STASH_CATCH_UP, CATCHING_UP)),
     (Mode.participating, (PROCESS, None)),
 ])
-
-
 def test_check_participating(validator, mode, result):
-    validator._data.node_mode = mode
-    msg = checkpoint(view_no=validator._data.view_no,
-                     inst_id=validator._data.inst_id,
-                     pp_seq_no=10)
-    assert validator.validate(msg) == result
+    validator.replica.node.mode = mode
+    msg = checkpoint(view_no=validator.view_no,
+                     inst_id=validator.inst_id,
+                     seq_no_start=0,
+                     seq_no_end=10)
+    assert validator.validate_checkpoint_msg(msg) == result
 
 
 @pytest.mark.parametrize('seq_no_end, result', [
     (0, (DISCARD, ALREADY_STABLE)),
     (1, (DISCARD, ALREADY_STABLE)),
     (19, (DISCARD, ALREADY_STABLE)),
     (20, (DISCARD, ALREADY_STABLE)),
     # assume stable is 10
     (21, (PROCESS, None)),
     (22, (PROCESS, None)),
     (100, (PROCESS, None)),
 ])
 def test_check_stable(validator, seq_no_end, result):
-    validator._is_pp_seq_no_stable = lambda msg: msg.seqNoEnd <= 20
-    msg = checkpoint(view_no=validator._data.view_no,
-                     inst_id=validator._data.inst_id,
-                     pp_seq_no=seq_no_end)
-    assert validator.validate(msg) == result
+    validator.replica.is_pp_seq_no_stable = lambda msg: msg.seqNoEnd <= 20
+    msg = checkpoint(view_no=validator.view_no,
+                     inst_id=validator.inst_id,
+                     seq_no_start=0,
+                     seq_no_end=seq_no_end)
+    assert validator.validate_checkpoint_msg(msg) == result
 
 
 @pytest.mark.parametrize('seq_no_end, result', [
     (0, (DISCARD, ALREADY_STABLE)),
     (1, (DISCARD, ALREADY_STABLE)),
     (19, (DISCARD, ALREADY_STABLE)),
     (20, (DISCARD, ALREADY_STABLE)),
     # assume stable is 10
     (21, (STASH_CATCH_UP, CATCHING_UP)),
     (22, (STASH_CATCH_UP, CATCHING_UP)),
     (100, (STASH_CATCH_UP, CATCHING_UP)),
 ])
 def test_check_stable_not_participating(validator, seq_no_end, result):
-    validator._is_pp_seq_no_stable = lambda msg: msg.seqNoEnd <= 20
-    validator._data.node_mode = Mode.syncing
-    msg = checkpoint(view_no=validator._data.view_no,
-                     inst_id=validator._data.inst_id,
-                     pp_seq_no=seq_no_end)
-    assert validator.validate(msg) == result
+    validator.replica.is_pp_seq_no_stable = lambda msg: msg.seqNoEnd <= 20
+    validator.replica.node.mode = Mode.syncing
+    msg = checkpoint(view_no=validator.view_no,
+                     inst_id=validator.inst_id,
+                     seq_no_start=0,
+                     seq_no_end=seq_no_end)
+    assert validator.validate_checkpoint_msg(msg) == result
 
 
 def test_check_old_view(validator):
-    msg = checkpoint(view_no=validator._data.view_no,
-                     inst_id=validator._data.inst_id,
-                     pp_seq_no=10)
-    validator._data.view_no += 1
-    assert validator.validate(msg) == (DISCARD, OLD_VIEW)
+    msg = checkpoint(view_no=validator.view_no - 1,
+                     inst_id=validator.inst_id,
+                     seq_no_start=0,
+                     seq_no_end=10)
+    assert validator.validate_checkpoint_msg(msg) == (DISCARD, OLD_VIEW)
 
 
 def test_check_future_view(validator):
-    msg = checkpoint(view_no=validator._data.view_no + 1,
-                     inst_id=validator._data.inst_id,
-                     pp_seq_no=10)
-    assert validator.validate(msg) == (STASH_VIEW, FUTURE_VIEW)
+    msg = checkpoint(view_no=validator.view_no + 1,
+                     inst_id=validator.inst_id,
+                     seq_no_start=0,
+                     seq_no_end=10)
+    assert validator.validate_checkpoint_msg(msg) == (STASH_VIEW, FUTURE_VIEW)
 
 
 def test_check_view_chnange(validator):
-    validator._data.legacy_vc_in_progress = True
-    msg = checkpoint(view_no=validator._data.view_no,
-                     inst_id=validator._data.inst_id,
-                     pp_seq_no=10)
-    assert validator.validate(msg) == (STASH_VIEW, FUTURE_VIEW)
+    validator.replica.node.view_change_in_progress = True
+    msg = checkpoint(view_no=validator.view_no,
+                     inst_id=validator.inst_id,
+                     seq_no_start=0,
+                     seq_no_end=10)
+    assert validator.validate_checkpoint_msg(msg) == (STASH_VIEW, FUTURE_VIEW)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/consensus/test_consensus_data_provider.py` & `indy-plenum-1.9.2rc1/plenum/test/consensus/test_consensus_data_provider.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_stashing_router.py` & `indy-plenum-1.9.2rc1/plenum/test/test_stashing_router.py`

 * *Files 20% similar despite different names*

```diff
@@ -2,21 +2,22 @@
 
 from plenum.common.event_bus import InternalBus
 from plenum.common.stashing_router import StashingRouter, STASH, PROCESS, DISCARD
 from plenum.test.test_event_bus import SomeMessage, OtherMessage, create_some_message, create_other_message
 
 
 def test_stashing_router_correctly_routes_messages():
-    some_handler = Mock(return_value=(PROCESS, ""))
-    other_handler = Mock(return_value=(DISCARD, ""))
+    some_handler = Mock(return_value=PROCESS)
+    other_handler = Mock(return_value=DISCARD)
 
     bus = InternalBus()
-    router = StashingRouter(10, buses=[bus])
+    router = StashingRouter(10)
     router.subscribe(SomeMessage, some_handler)
     router.subscribe(OtherMessage, other_handler)
+    router.subscribe_to(bus)
 
     some_handler.assert_not_called()
     other_handler.assert_not_called()
 
     some_message = create_some_message()
     bus.send(some_message)
     some_handler.assert_called_once_with(some_message)
@@ -25,49 +26,52 @@
     other_message = create_other_message()
     bus.send(other_message)
     some_handler.assert_called_once_with(some_message)
     other_handler.assert_called_once_with(other_message)
 
 
 def test_stashing_router_correctly_handles_multiple_arguments():
-    handler = Mock(return_value=(PROCESS, ""))
+    handler = Mock(return_value=PROCESS)
 
     bus = InternalBus()
-    router = StashingRouter(10, buses=[bus])
+    router = StashingRouter(10)
     router.subscribe(SomeMessage, handler)
+    router.subscribe_to(bus)
 
     message = create_some_message()
     bus.send(message, 'hello')
     handler.assert_called_once_with(message, 'hello')
 
 
 def test_process_all_stashed_doesnt_do_anything_when_there_are_no_items_in_stash():
-    handler = Mock(return_value=(PROCESS, ""))
+    handler = Mock(return_value=PROCESS)
 
     bus = InternalBus()
-    router = StashingRouter(10, buses=[bus])
+    router = StashingRouter(10)
     router.subscribe(SomeMessage, handler)
+    router.subscribe_to(bus)
 
     router.process_all_stashed()
     handler.assert_not_called()
 
     message = create_some_message()
     bus.send(message, 'hello')
     handler.assert_called_once_with(message, 'hello')
 
     router.process_all_stashed()
     handler.assert_called_once_with(message, 'hello')
 
 
 def test_process_stashed_until_restash_doesnt_do_anything_when_there_are_no_items_in_stash():
-    handler = Mock(return_value=(PROCESS, ""))
+    handler = Mock(return_value=PROCESS)
 
     bus = InternalBus()
-    router = StashingRouter(10, buses=[bus])
+    router = StashingRouter(10)
     router.subscribe(SomeMessage, handler)
+    router.subscribe_to(bus)
 
     router.process_stashed_until_first_restash()
     handler.assert_not_called()
 
     message = create_some_message()
     bus.send(message, 'hello')
     handler.assert_called_once_with(message, 'hello')
@@ -81,21 +85,20 @@
     calls = []
 
     def handler(msg):
         nonlocal stash_count
         calls.append(msg)
         if stash_count > 0:
             stash_count -= 1
-            return STASH, "reason"
-        else:
-            return None, None
+            return STASH
 
     bus = InternalBus()
-    router = StashingRouter(10, buses=[bus])
+    router = StashingRouter(10)
     router.subscribe(SomeMessage, handler)
+    router.subscribe_to(bus)
 
     msg_a = create_some_message()
     msg_b = create_some_message()
     bus.send(msg_a)
     bus.send(msg_b)
     assert router.stash_size() == 2
     assert calls == [msg_a, msg_b]
@@ -118,21 +121,20 @@
     calls = []
 
     def handler(msg, frm):
         nonlocal stash_count
         calls.append((msg, frm))
         if stash_count > 0:
             stash_count -= 1
-            return STASH, "reason"
-        else:
-            return None, None
+            return STASH
 
     bus = InternalBus()
-    router = StashingRouter(10, buses=[bus])
+    router = StashingRouter(10)
     router.subscribe(SomeMessage, handler)
+    router.subscribe_to(bus)
 
     msg_a = create_some_message()
     msg_b = create_some_message()
     bus.send(msg_a, 'A')
     bus.send(msg_b, 'B')
     assert router.stash_size() == 2
     assert calls == [(msg_a, 'A'), (msg_b, 'B')]
@@ -152,21 +154,22 @@
 
 def test_stashing_router_can_stash_messages_with_different_reasons():
     calls = []
 
     def handler(message: SomeMessage):
         calls.append(message)
         if message.int_field % 2 == 0:
-            return STASH + 0, "reason"
+            return STASH + 0
         else:
-            return STASH + 1, "reason"
+            return STASH + 1
 
     bus = InternalBus()
-    router = StashingRouter(10, buses=[bus])
+    router = StashingRouter(10)
     router.subscribe(SomeMessage, handler)
+    router.subscribe_to(bus)
 
     messages = [create_some_message() for _ in range(10)]
     for msg in messages:
         bus.send(msg)
     assert router.stash_size() == len(messages)
     assert router.stash_size(STASH + 0) + router.stash_size(STASH + 1) == router.stash_size()
 
@@ -191,23 +194,24 @@
 
 
 def test_stashing_router_can_stash_and_sort_messages():
     calls = []
 
     def handler(message: SomeMessage):
         calls.append(message)
-        return STASH, "reason"
+        return STASH
 
     def sort_key(message: SomeMessage):
         return message.int_field
 
     bus = InternalBus()
-    router = StashingRouter(10, buses=[bus])
+    router = StashingRouter(10)
     router.set_sorted_stasher(STASH, key=sort_key)
     router.subscribe(SomeMessage, handler)
+    router.subscribe_to(bus)
 
     messages = [create_some_message() for _ in range(10)]
     for msg in messages:
         bus.send(msg)
 
     assert calls == messages
 
@@ -218,21 +222,20 @@
 
 def test_stashing_router_can_process_stashed_until_first_restash():
     calls = []
 
     def handler(msg):
         calls.append(msg)
         if len(calls) % 2 != 0:
-            return STASH, "reason"
-        else:
-            return None, None
+            return STASH
 
     bus = InternalBus()
-    router = StashingRouter(10, buses=[bus])
+    router = StashingRouter(10)
     router.subscribe(SomeMessage, handler)
+    router.subscribe_to(bus)
 
     msg_a = create_some_message()
     msg_b = create_some_message()
     msg_c = create_some_message()
     msg_d = create_some_message()
     msg_e = create_some_message()
     bus.send(msg_a)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/wallet/test_wallet_storage_helper.py` & `indy-plenum-1.9.2rc1/plenum/test/wallet/test_wallet_storage_helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/wallet/test_wallet.py` & `indy-plenum-1.9.2rc1/plenum/test/wallet/test_wallet.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_bootstrapping.py` & `indy-plenum-1.9.2rc1/plenum/test/test_bootstrapping.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/package/test_metadata.py` & `indy-plenum-1.9.2rc1/plenum/test/package/test_metadata.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_lazy_field.py` & `indy-plenum-1.9.2rc1/plenum/test/test_lazy_field.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/validator_info/test_start_vc_ts_in_node_info.py` & `indy-plenum-1.9.2rc1/plenum/test/validator_info/test_start_vc_ts_in_node_info.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/validator_info/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/validator_info/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/validator_info/test_validator_info_vc.py` & `indy-plenum-1.9.2rc1/plenum/test/validator_info/test_validator_info_vc.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/validator_info/test_validator_info_dump.py` & `indy-plenum-1.9.2rc1/plenum/test/validator_info/test_validator_info_dump.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/validator_info/test_validator_info.py` & `indy-plenum-1.9.2rc1/plenum/test/validator_info/test_validator_info.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/validator_info/test_upgrade_log.py` & `indy-plenum-1.9.2rc1/plenum/test/validator_info/test_upgrade_log.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_node_connection.py` & `indy-plenum-1.9.2rc1/plenum/test/test_node_connection.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_repeating_timer.py` & `indy-plenum-1.9.2rc1/plenum/test/test_repeating_timer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_not_removing_by_latency.py` & `indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_not_removing_by_latency.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_after_node_started.py` & `indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_after_node_started.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_acc_quorum.py` & `indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_acc_quorum.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_acc_local.py` & `indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_acc_local.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_quorum.py` & `indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_quorum.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_after_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_after_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_local.py` & `indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_with_backup_degraded_local.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing_with_primary_disconnected.py` & `indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing_with_primary_disconnected.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica_removing/test_replica_removing.py` & `indy-plenum-1.9.2rc1/plenum/test/replica_removing/test_replica_removing.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import sys
 
 import pytest
 
 from plenum.test import waits
-from plenum.test.checkpoints.helper import check_for_nodes, check_stable_checkpoint
+from plenum.test.checkpoints.helper import chkChkpoints
 from plenum.test.delayers import cDelay
 from plenum.test.node_catchup.test_config_ledger import start_stopped_node
 from plenum.test.pool_transactions.helper import disconnect_node_and_ensure_disconnected
 from plenum.test.replica_removing.helper import check_replica_removed
 from plenum.test.stasher import delay_rules
 from stp_core.loop.eventually import eventually
 from stp_core.common.log import getlogger
@@ -108,20 +108,20 @@
 
     forwardedToBefore = next(iter(node.requests.values())).forwardedTo
     node.replicas.remove_replica(node.replicas.num_replicas - 1)
 
     assert len(node.requests) == 1
     forwardedToAfter = next(iter(node.requests.values())).forwardedTo
     assert forwardedToAfter == forwardedToBefore - 1
-    check_for_nodes(txnPoolNodeSet, check_stable_checkpoint, 0)
+    chkChkpoints(txnPoolNodeSet, 1)
 
     # Send one more request to stabilize checkpoint
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, 1)
 
-    looper.run(eventually(check_for_nodes, txnPoolNodeSet, check_stable_checkpoint, 3))
+    looper.run(eventually(chkChkpoints, txnPoolNodeSet, 1, 0))
     assert len(node.requests) == 0
 
 
 def test_unordered_request_freed_on_replica_removal(looper,
                                                     txnPoolNodeSet,
                                                     sdk_pool_handle,
                                                     sdk_wallet_client,
@@ -144,23 +144,23 @@
 
         forwardedToBefore = next(iter(node.requests.values())).forwardedTo
         node.replicas.remove_replica(node.replicas.num_replicas - 1)
 
         assert len(node.requests) == 1
         forwardedToAfter = next(iter(node.requests.values())).forwardedTo
         assert forwardedToAfter == forwardedToBefore - 1
-        check_for_nodes(txnPoolNodeSet, check_stable_checkpoint, 0)
+        chkChkpoints(txnPoolNodeSet, 1)
 
     sdk_get_replies(looper, req)
-    check_for_nodes(txnPoolNodeSet, check_stable_checkpoint, 0)
+    chkChkpoints(txnPoolNodeSet, 1)
 
     # Send one more request to stabilize checkpoint
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, 1)
 
-    looper.run(eventually(check_for_nodes, txnPoolNodeSet, check_stable_checkpoint, 3))
+    looper.run(eventually(chkChkpoints, txnPoolNodeSet, 1, 0))
     assert len(node.requests) == 0
 
 
 def do_view_change(txnPoolNodeSet, looper):
     # trigger view change on all nodes
     for node in txnPoolNodeSet:
         node.view_changer.on_master_degradation()
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/replica_removing/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/replica_removing/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_queue_timer.py` & `indy-plenum-1.9.2rc1/plenum/test/test_queue_timer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_testable.py` & `indy-plenum-1.9.2rc1/plenum/test/test_testable.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/spy_helpers.py` & `indy-plenum-1.9.2rc1/plenum/test/spy_helpers.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_dirty_read.py` & `indy-plenum-1.9.2rc1/plenum/test/test_dirty_read.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/random_buy_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/random_buy_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_new_primary.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_new_primary.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_all_nodes.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_all_nodes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_old_primary.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_old_primary.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_gc_all_nodes_random_delay.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_gc_all_nodes_random_delay.py`

 * *Files 1% similar despite different names*

```diff
@@ -74,15 +74,15 @@
                        delay=propagationTimeout * 2)
     requests = sdk_send_random_request(looper, sdk_pool_handle, sdk_wallet_client)
 
     def checkPrePrepareSentAtLeastByPrimary():
         for node in txnPoolNodeSet:
             for replica in node.replicas.values():
                 if replica.isPrimary:
-                    assert len(replica._ordering_service.sentPrePrepares)
+                    assert len(replica.sentPrePrepares)
 
     looper.run(eventually(checkPrePrepareSentAtLeastByPrimary,
                           retryWait=0.1,
                           timeout=propagationTimeout))
     # 4 do view change
     #    -> GC shouldn't remove anything because
     #       last_ordered_3pc (+0, 1) < last message's 3pc key (+1, 1)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_non_primary.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_non_primary.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_complex.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_complex.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_old_and_new_primary.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_2_of_4_nodes_with_old_and_new_primary.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/view_change_slow_nodes/test_view_change_all_nodes_random_delay.py` & `indy-plenum-1.9.2rc1/plenum/test/view_change_slow_nodes/test_view_change_all_nodes_random_delay.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_watermarks_on_delayed_backup.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_watermarks_on_delayed_backup.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 from logging import getLogger
 
 import sys
 
 from plenum.common.messages.node_messages import PrePrepare
-from plenum.common.stashing_router import DISCARD
 from plenum.test.helper import sdk_send_batches_of_random_and_check
 from plenum.test.test_node import getNonPrimaryReplicas
 
 logger = getLogger()
 
 inst_id = 1
 
@@ -69,21 +68,28 @@
     assert non_broken_replica.H == 35
 
 
 def break_backup_replica(txnPoolNodeSet):
     node = getNonPrimaryReplicas(txnPoolNodeSet, inst_id)[-1].node
     broken_replica = node.replicas[inst_id]
     non_broken_replica = node.replicas[0]
-    broken_replica._ordering_service.old_validate = broken_replica._ordering_service._validate
 
-    def fakeProcessPrePrepare(pre_prepare):
+    def fakeProcessPrePrepare(pre_prepare, sender):
         logger.warning(
             "{} is broken. 'processPrePrepare' does nothing".format(broken_replica.name))
-        return DISCARD, None
 
-    broken_replica._ordering_service._validate = fakeProcessPrePrepare
+    broken_replica.threePhaseRouter.extend(
+        (
+            (PrePrepare, fakeProcessPrePrepare),
+        )
+    )
+
     return broken_replica, non_broken_replica
 
 
 def repair_broken_replica(replica):
-    replica._ordering_service._validate = replica._ordering_service.old_validate
+    replica.threePhaseRouter.extend(
+        (
+            (PrePrepare, replica.processPrePrepare),
+        )
+    )
     return replica
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_message_outside_watermark1.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_message_outside_watermark1.py`

 * *Files 6% similar despite different names*

```diff
@@ -31,22 +31,22 @@
     delay = 2
     instId = 0
     reqs_to_send = 2 * reqs_for_logsize + 1
     logger.debug('Will send {} requests'.format(reqs_to_send))
 
     npr = getNonPrimaryReplicas(txnPoolNodeSet, instId)
     pr = getPrimaryReplica(txnPoolNodeSet, instId)
-    orderedCount = pr._ordering_service.spylog.count(pr._ordering_service._order_3pc_key)
+    orderedCount = pr.stats.get(TPCStat.OrderSent)
 
     for r in npr:
         r.node.nodeIbStasher.delay(ppDelay(delay, instId))
         r.node.nodeIbStasher.delay(pDelay(delay, instId))
 
     tm_exec_1_batch = waits.expectedTransactionExecutionTime(len(txnPoolNodeSet))
     batch_count = math.ceil(reqs_to_send / tconf.Max3PCBatchSize)
     total_timeout = (tm_exec_1_batch + delay) * batch_count
 
     def chk():
-        assert orderedCount + batch_count == pr._ordering_service.spylog.count(pr._ordering_service._order_3pc_key)
+        assert orderedCount + batch_count == pr.stats.get(TPCStat.OrderSent)
 
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, reqs_to_send)
     looper.run(eventually(chk, retryWait=1, timeout=total_timeout))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_discard_old_checkpoint_messages.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_lagged_checkpoint_completion.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,34 +1,68 @@
-from plenum.common.constants import AUDIT_LEDGER_ID
-from plenum.common.messages.node_messages import Checkpoint
-from plenum.server.replica_validator_enums import ALREADY_STABLE
-from plenum.test.checkpoints.helper import check_for_instance, check_stable_checkpoint
-from stp_core.loop.eventually import eventually
-from plenum.test.helper import checkDiscardMsg
+from plenum.test import waits
+from plenum.test.delayers import cDelay
 from plenum.test.helper import sdk_send_random_and_check
+from stp_core.loop.eventually import eventually
+
+CHK_FREQ = 5
+
+
+def test_lagged_checkpoint_completion(chkFreqPatched, looper, txnPoolNodeSet,
+                                      sdk_wallet_client, sdk_pool_handle):
+    """
+    One node in a pool lags to order the last 3PC-batch in a checkpoint so that
+    when it eventually orders this 3PC-batch and thus completes the checkpoint
+    it has already received and stashed the corresponding checkpoint messages
+    from all the other nodes. The test verifies that the node successfully
+    processes the stashed checkpoint messages and stabilizes the checkpoint.
+    """
+    slow_node = txnPoolNodeSet[-1]
+
+    # All the nodes in the pool normally orders all the 3PC-batches in a
+    # checkpoint except the last 3PC-batch. The last 3PC-batch in the
+    # checkpoint is ordered by all the nodes except one slow node because this
+    # node lags to receive Commits.
+    sdk_send_random_and_check(looper, txnPoolNodeSet,
+                              sdk_pool_handle, sdk_wallet_client, 4)
+
+    slow_node.nodeIbStasher.delay(cDelay())
+
+    sdk_send_random_and_check(looper, txnPoolNodeSet,
+                              sdk_pool_handle, sdk_wallet_client, 1)
+
+    # All the other nodes complete the checkpoint and send Checkpoint messages
+    # to others. The slow node receives and stashes these messages because it
+    # has not completed the checkpoint.
+    def check():
+        for replica in slow_node.replicas.values():
+            assert len(replica.checkpoints) == 1
+            assert (1, 5) in replica.checkpoints
+            assert replica.checkpoints[(1, 5)].seqNo == 4
+            assert replica.checkpoints[(1, 5)].digest is None
+            assert replica.checkpoints[(1, 5)].isStable is False
+
+            assert len(replica.stashedRecvdCheckpoints) == 1
+            assert 0 in replica.stashedRecvdCheckpoints
+            assert len(replica.stashedRecvdCheckpoints[0]) == 1
+            assert (1, 5) in replica.stashedRecvdCheckpoints[0]
+            assert len(replica.stashedRecvdCheckpoints[0][(1, 5)]) == \
+                len(txnPoolNodeSet) - 1
+
+    stabilization_timeout = \
+        waits.expectedTransactionExecutionTime(len(txnPoolNodeSet))
+    looper.run(eventually(check, timeout=stabilization_timeout))
+
+    # Eventually the slow node receives Commits, orders the last 3PC-batch in
+    # the checkpoint and thus completes it, processes the stashed checkpoint
+    # messages and stabilizes the checkpoint.
+    slow_node.nodeIbStasher.reset_delays_and_process_delayeds()
+
+    looper.runFor(waits.expectedOrderingTime(len(txnPoolNodeSet)))
 
+    for replica in slow_node.replicas.values():
+        assert len(replica.checkpoints) == 1
+        assert (1, 5) in replica.checkpoints
+        assert replica.checkpoints[(1, 5)].seqNo == 5
+        assert replica.checkpoints[(1, 5)].digest is not None
+        assert replica.checkpoints[(1, 5)].isStable is True
 
-def test_discard_checkpoint_msg_for_stable_checkpoint(chkFreqPatched,
-                                                      tconf, looper,
-                                                      txnPoolNodeSet,
-                                                      sdk_pool_handle,
-                                                      sdk_wallet_client,
-                                                      reqs_for_checkpoint):
-    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
-                              sdk_wallet_client, reqs_for_checkpoint)
-    next_checkpoint = tconf.CHK_FREQ
-    for inst_id in txnPoolNodeSet[0].replicas.keys():
-        looper.run(eventually(check_for_instance, txnPoolNodeSet, inst_id,
-                              check_stable_checkpoint, next_checkpoint, retryWait=1))
-    node1 = txnPoolNodeSet[0]
-    rep1 = node1.replicas[0]
-    # TODO: Use old checkpoint message when we retain stabilized checkpoint
-    # oldChkpointMsg = rep1._consensus_data.checkpoints[0]
-    oldChkpointMsg = Checkpoint(instId=0,
-                                viewNo=0,
-                                seqNoStart=0,
-                                seqNoEnd=next_checkpoint,
-                                digest=node1.db_manager.get_txn_root_hash(AUDIT_LEDGER_ID))
-    rep1.send(oldChkpointMsg)
-    recvReplicas = [n.replicas[0].stasher for n in txnPoolNodeSet[1:]]
-    looper.run(eventually(checkDiscardMsg, recvReplicas, oldChkpointMsg,
-                          ALREADY_STABLE, retryWait=1))
+        assert len(replica.stashedRecvdCheckpoints) == 0
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_complete_short_checkpoint_not_included_in_lag_for_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_complete_short_checkpoint_not_included_in_lag_for_catchup.py`

 * *Files 12% similar despite different names*

```diff
@@ -10,16 +10,14 @@
 from plenum.test.node_catchup.helper import waitNodeDataEquality, \
     checkNodeDataForInequality, waitNodeDataInequality, get_number_of_completed_catchups
 from plenum.test.pool_transactions.helper import sdk_add_new_steward_and_node
 from plenum.test.test_node import checkNodesConnected, ensureElectionsDone
 
 logger = getLogger()
 
-whitelist = ['PREPARE request already received']
-
 TestRunningTimeLimitSec = 200
 
 
 CHK_FREQ = 5
 LOG_SIZE = 3 * CHK_FREQ
 
 
@@ -59,17 +57,21 @@
                                            sdk_pool_handle,
                                            sdk_wallet_client,
                                            reqs_for_checkpoint - 2 * max_batch_size)
 
     waitNodeDataEquality(looper, new_node, *txnPoolNodeSet[:-1], exclude_from_check=['check_last_ordered_3pc_backup'])
 
     # The master replica of the new node stops to receive 3PC-messages
-    new_node.master_replica.external_bus._handlers[PrePrepare] = [lambda *x, **y: (None, None)]
-    new_node.master_replica.external_bus._handlers[Prepare] = [lambda *x, **y: (None, None)]
-    new_node.master_replica.external_bus._handlers[Commit] = [lambda *x, **y: (None, None)]
+    new_node.master_replica.threePhaseRouter.extend(
+        (
+            (PrePrepare, lambda *x, **y: None),
+            (Prepare, lambda *x, **y: None),
+            (Commit, lambda *x, **y: None),
+        )
+    )
 
     completed_catchups_before_reqs = get_number_of_completed_catchups(new_node)
 
     # Send requests for the new node's master replica to reach
     # Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1 quorumed stashed
     # checkpoints from others
     send_reqs_batches_and_get_suff_replies(looper, txnPoolNodeSet,
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_lagged_checkpoint_completion.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_basic_checkpointing.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,60 +1,38 @@
+from stp_core.loop.eventually import eventually
 from plenum.test import waits
-from plenum.test.checkpoints.helper import check_num_received_checkpoints, \
-    check_received_checkpoint_votes, check_stable_checkpoint, check_num_unstable_checkpoints
-from plenum.test.delayers import cDelay
+from plenum.test.checkpoints.helper import chkChkpoints
 from plenum.test.helper import sdk_send_random_and_check
-from stp_core.loop.eventually import eventually
 
-CHK_FREQ = 5
 
+def test_checkpoint_created(chkFreqPatched, looper, txnPoolNodeSet, sdk_pool_handle,
+                            sdk_wallet_client, reqs_for_checkpoint):
+    """
+    After requests less than `CHK_FREQ`, there should be one checkpoint
+    on each replica. After `CHK_FREQ`, one checkpoint should become stable
+    """
+    # Send one batch less so checkpoint is not created
+    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client,
+                              reqs_for_checkpoint - (chkFreqPatched.Max3PCBatchSize))
+    # Deliberately waiting so as to verify that not more than 1 checkpoint is
+    # created
+    looper.runFor(2)
+    chkChkpoints(txnPoolNodeSet, 1)
+
+    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client,
+                              chkFreqPatched.Max3PCBatchSize)
+
+    timeout = waits.expectedTransactionExecutionTime(len(txnPoolNodeSet))
+    looper.run(eventually(chkChkpoints, txnPoolNodeSet, 1, 0, retryWait=1, timeout=timeout))
 
-def test_lagged_checkpoint_completion(chkFreqPatched, looper, txnPoolNodeSet,
-                                      sdk_wallet_client, sdk_pool_handle):
+
+def test_old_checkpoint_deleted(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, reqs_for_checkpoint):
     """
-    One node in a pool lags to order the last 3PC-batch in a checkpoint so that
-    when it eventually orders this 3PC-batch and thus completes the checkpoint
-    it has already received and stashed the corresponding checkpoint messages
-    from all the other nodes. The test verifies that the node successfully
-    processes the stashed checkpoint messages and stabilizes the checkpoint.
+    Send requests more than twice of `CHK_FREQ`, there should be one new stable
+    checkpoint on each replica. The old stable checkpoint should be removed
     """
-    slow_node = txnPoolNodeSet[-1]
+    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, 2 * reqs_for_checkpoint)
+
+    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, 1)
 
-    # All the nodes in the pool normally orders all the 3PC-batches in a
-    # checkpoint except the last 3PC-batch. The last 3PC-batch in the
-    # checkpoint is ordered by all the nodes except one slow node because this
-    # node lags to receive Commits.
-    sdk_send_random_and_check(looper, txnPoolNodeSet,
-                              sdk_pool_handle, sdk_wallet_client, 4)
-
-    slow_node.nodeIbStasher.delay(cDelay())
-
-    sdk_send_random_and_check(looper, txnPoolNodeSet,
-                              sdk_pool_handle, sdk_wallet_client, 1)
-
-    # All the other nodes complete the checkpoint and send Checkpoint messages
-    # to others. The slow node receives and stashes these messages because it
-    # has not completed the checkpoint.
-    def check():
-        for replica in slow_node.replicas.values():
-            check_stable_checkpoint(replica, 0)
-            check_num_unstable_checkpoints(replica, 0)
-            check_num_received_checkpoints(replica, 1)
-            check_received_checkpoint_votes(replica,
-                                            pp_seq_no=5,
-                                            num_votes=len(txnPoolNodeSet) - 1)
-
-    stabilization_timeout = \
-        waits.expectedTransactionExecutionTime(len(txnPoolNodeSet))
-    looper.run(eventually(check, timeout=stabilization_timeout))
-
-    # Eventually the slow node receives Commits, orders the last 3PC-batch in
-    # the checkpoint and thus completes it, processes the stashed checkpoint
-    # messages and stabilizes the checkpoint.
-    slow_node.nodeIbStasher.reset_delays_and_process_delayeds()
-
-    looper.runFor(waits.expectedOrderingTime(len(txnPoolNodeSet)))
-
-    for replica in slow_node.replicas.values():
-        check_stable_checkpoint(replica, 5)
-        check_num_unstable_checkpoints(replica, 0)
-        check_num_received_checkpoints(replica, 0)
+    timeout = waits.expectedTransactionExecutionTime(len(txnPoolNodeSet))
+    looper.run(eventually(chkChkpoints, txnPoolNodeSet, 2, 0, retryWait=1, timeout=timeout))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_checkpoints_removal_in_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_checkpoints_removal_in_view_change.py`

 * *Files 4% similar despite different names*

```diff
@@ -48,15 +48,15 @@
                           fast_nodes,
                           (0, CHK_FREQ + 1)))
     # check that fast_nodes finalized first checkpoint and slow_nodes are not
     looper.run(eventually(check_checkpoint_finalize,
                           fast_nodes,
                           1, CHK_FREQ))
     for n in slow_nodes:
-        assert not n.master_replica._checkpointer._checkpoint_state[(1, CHK_FREQ)].isStable
+        assert not n.master_replica.checkpoints[(1, CHK_FREQ)].isStable
 
     # View change start emulation for change viewNo and fix last prepare
     # certificate, because if we start a real view change then checkpoints will
     # clean and the first checkpoint would not be need in finalizing.
     for node in txnPoolNodeSet:
         node.viewNo += 1
         node.master_replica.on_view_change_start()
@@ -65,27 +65,27 @@
     reset_delay(slow_nodes, CHECKPOINT)
     # reset view change emulation and start real view change for finish it in
     # a normal mode with catchup
     for node in txnPoolNodeSet:
         node.viewNo -= 1
     ensure_view_change(looper, txnPoolNodeSet)
     for n in slow_nodes:
-        assert not n.master_replica._checkpointer._checkpoint_state[(1, CHK_FREQ)].isStable
+        assert not n.master_replica.checkpoints[(1, CHK_FREQ)].isStable
     # Check ordering the last txn before catchup. Check client reply is enough
     # because slow_nodes contains 3 nodes and without their replies sdk method
     # for get reply will not successfully finish.
     reset_delay(slow_nodes, COMMIT)
     sdk_get_and_check_replies(looper, requests)
     looper.run(eventually(last_ordered_check,
                           txnPoolNodeSet,
                           (0, CHK_FREQ + 1)))
     # check view change finish and checkpoints were cleaned
     ensureElectionsDone(looper, txnPoolNodeSet)
     for n in slow_nodes:
-        assert (1, CHK_FREQ) not in n.master_replica._checkpointer._checkpoint_state
+        assert (1, CHK_FREQ) not in n.master_replica.checkpoints
     # check that all nodes have same data after new txns ordering
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
                               sdk_wallet_client, CHK_FREQ)
     ensure_all_nodes_have_same_data(looper, nodes=txnPoolNodeSet)
 
 
 def delay_msg(nodes, delay_function, types=None):
@@ -104,15 +104,15 @@
 def last_prepared_certificate(nodes, num):
     for n in nodes:
         assert n.master_replica.last_prepared_certificate_in_view() == num
 
 
 def check_checkpoint_finalize(nodes, start_pp_seq_no, end_pp_seq_no):
     for n in nodes:
-        checkpoint = n.master_replica._checkpointer._checkpoint_state[(start_pp_seq_no, end_pp_seq_no)]
+        checkpoint = n.master_replica.checkpoints[(start_pp_seq_no, end_pp_seq_no)]
         assert checkpoint.isStable
 
 
 def last_ordered_check(nodes, last_ordered, instance_id=None):
     for n in nodes:
         last_ordered_3pc = n.master_last_ordered_3PC \
             if instance_id is None \
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_checkpoint_stabilization_after_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_checkpoint_stabilization_after_catchup.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,15 +1,12 @@
 from plenum.test import waits
-from plenum.test.checkpoints.helper import check_stable_checkpoint, check_num_received_checkpoints, \
-    check_num_unstable_checkpoints, check_last_checkpoint
-from plenum.test.helper import sdk_send_random_and_check, assertExp
+from plenum.test.helper import sdk_send_random_and_check
 from plenum.test.node_catchup.helper import waitNodeDataEquality
 from plenum.test.pool_transactions.helper import sdk_add_new_steward_and_node
 from plenum.test.test_node import checkNodesConnected
-from stp_core.loop.eventually import eventually
 
 CHK_FREQ = 5
 LOG_SIZE = 3 * CHK_FREQ
 
 
 def test_second_checkpoint_after_catchup_can_be_stabilized(
         chkFreqPatched, looper, txnPoolNodeSet, sdk_wallet_steward,
@@ -17,56 +14,76 @@
         allPluginsPath):
     _, new_node = sdk_add_new_steward_and_node(
         looper, sdk_pool_handle, sdk_wallet_steward,
         'EpsilonSteward', 'Epsilon', tdir, tconf,
         allPluginsPath=allPluginsPath)
     txnPoolNodeSet.append(new_node)
     looper.run(checkNodesConnected(txnPoolNodeSet))
-    waitNodeDataEquality(looper, new_node, *txnPoolNodeSet[:-1])
+    waitNodeDataEquality(looper, new_node, *txnPoolNodeSet[:-1],
+                         exclude_from_check="check_last_ordered_3pc_backup")
     # Epsilon did not participate in ordering of the batch with EpsilonSteward
     # NYM transaction and the batch with Epsilon NODE transaction.
     # Epsilon got these transactions via catch-up.
 
     master_replica = new_node.replicas._master_replica
 
-    check_stable_checkpoint(master_replica, 0)
-    check_num_received_checkpoints(master_replica, 0)
+    assert len(master_replica.checkpoints) == 0
+
+    assert len(master_replica.stashedRecvdCheckpoints) == 0
 
     assert master_replica.h == 2
     assert master_replica.H == 17
 
     sdk_send_random_and_check(looper, txnPoolNodeSet,
                               sdk_pool_handle, sdk_wallet_client, 1)
 
     for replica in new_node.replicas.values():
+        assert len(replica.checkpoints) == 1
+
+        assert len(replica.stashedRecvdCheckpoints) == 0
+
         assert replica.h == 2
         assert replica.H == 17
 
     sdk_send_random_and_check(looper, txnPoolNodeSet,
                               sdk_pool_handle, sdk_wallet_client, 6)
     stabilization_timeout = \
         waits.expectedTransactionExecutionTime(len(txnPoolNodeSet))
     looper.runFor(stabilization_timeout)
 
     for replica in new_node.replicas.values():
-        check_stable_checkpoint(replica, 5)
-        check_num_unstable_checkpoints(replica, 0)
+        assert len(replica.checkpoints) == 2
+        keys_iter = iter(replica.checkpoints)
+
+        assert next(keys_iter) == (3, 5)
+        assert replica.checkpoints[3, 5].seqNo == 5
+        assert replica.checkpoints[3, 5].digest is None
+        assert replica.checkpoints[3, 5].isStable is False
+
+        assert next(keys_iter) == (6, 10)
+        assert replica.checkpoints[6, 10].seqNo == 9
+        assert replica.checkpoints[6, 10].digest is None
+        assert replica.checkpoints[6, 10].isStable is False
 
         # nothing is stashed since it's ordered during catch-up
-        check_num_received_checkpoints(replica, 0)
+        assert len(replica.stashedRecvdCheckpoints) == 0
 
-        assert replica.h == 5
-        assert replica.H == 20
+        assert replica.h == 2
+        assert replica.H == 17
 
     sdk_send_random_and_check(looper, txnPoolNodeSet,
                               sdk_pool_handle, sdk_wallet_client, 1)
     looper.runFor(stabilization_timeout)
 
     for replica in new_node.replicas.values():
-        check_stable_checkpoint(replica, 10)
-        check_num_unstable_checkpoints(replica, 0)
+        assert len(replica.checkpoints) == 1
+        keys_iter = iter(replica.checkpoints)
 
-        # nothing is stashed since it's ordered during catch-up
-        check_num_received_checkpoints(replica, 0)
+        assert next(keys_iter) == (6, 10)
+        assert replica.checkpoints[6, 10].seqNo == 10
+        assert replica.checkpoints[6, 10].digest is not None
+        assert replica.checkpoints[6, 10].isStable is True
+
+        assert len(replica.stashedRecvdCheckpoints) == 0
 
         assert replica.h == 10
         assert replica.H == 25
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_message_outside_watermark.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_message_outside_watermark.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,36 +1,30 @@
 from plenum.common.constants import PREPREPARE, PREPARE, CHECKPOINT
 from plenum.server.node import Node
-from plenum.server.replica_validator_enums import STASH_WATERMARKS
 from plenum.test import waits
 from plenum.test.delayers import ppDelay, pDelay, chk_delay
 from plenum.test.helper import countDiscarded
 from plenum.test.node_catchup.helper import waitNodeDataEquality, \
     checkNodeDataForInequality
-from plenum.test.test_node import getNonPrimaryReplicas, TestReplica
+from plenum.test.test_node import getNonPrimaryReplicas, TestReplica, TestReplicaStasher
 from plenum.test.helper import sdk_send_random_and_check
 
 CHK_FREQ = 5
 LOG_SIZE = 3 * CHK_FREQ
 
 
-def discardCounts(checkpoint_services, pat):
+def discardCounts(replicas, pat):
     counts = {}
-    for r in checkpoint_services:
-        counts[str(r)] = countDiscarded(r, pat)
+    for r in replicas:
+        counts[r.name] = countDiscarded(r, pat)
     return counts
 
 
 def test_non_primary_recvs_3phase_message_outside_watermarks(
-        chkFreqPatched,
-        reqs_for_logsize,
-        looper,
-        txnPoolNodeSet,
-        sdk_pool_handle,
-        sdk_wallet_client):
+        chkFreqPatched, reqs_for_logsize, looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client):
     """
     A node is slow in receiving PRE-PREPAREs and PREPAREs. A lot of requests
     are sent and the slow node has started receiving COMMITs outside of its
     watermarks and so stashes them. Also this node is slow in receiving
     CHECKPOINTs. So a catch-up does not occur on it.
 
     Then the slow node eventually receives the sent PRE-PREPAREs and PREPAREs
@@ -52,41 +46,41 @@
     slowNode = slowReplica.node
 
     slowNode.nodeIbStasher.delay(ppDelay(300, backupInstId))
     slowNode.nodeIbStasher.delay(pDelay(300, backupInstId))
     slowNode.nodeIbStasher.delay(chk_delay(300))
 
     initialDomainLedgerSize = slowNode.domainLedger.size
-    oldStashCount = slowReplica.stasher.stash_size(STASH_WATERMARKS)
-    slowReplica._checkpointer.set_watermarks(slowReplica.h, LOG_SIZE)
+    oldStashCount = slowReplica.stasher.num_stashed_watermarks
+    slowReplica.H = LOG_SIZE
     # 1. Send requests more than fit between the watermarks on the slow node
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, reqs_for_logsize + 2)
 
     # Verify that the slow node stashes the batches outside of its watermarks
-    newStashCount = slowReplica.stasher.stash_size(STASH_WATERMARKS)
+    newStashCount = slowReplica.stasher.num_stashed_watermarks
     assert newStashCount > oldStashCount
 
-    oldDiscardCounts = discardCounts([n.replicas[backupInstId].stasher for n in txnPoolNodeSet if n != slowNode],
-                                     'marked stable checkpoint')
+    oldDiscardCounts = discardCounts([n.replicas[backupInstId] for n in txnPoolNodeSet if n != slowNode],
+                                     'achieved stable checkpoint')
 
     # 2. Deliver the sent PREPREPAREs and PREPAREs to the slow node
     slowNode.nodeIbStasher.reset_delays_and_process_delayeds(PREPREPARE, PREPARE)
 
     # Verify that the slow node orders the 3PC-batches between its watermarks
     # but no more.
     looper.runFor(waits.expectedTransactionExecutionTime(len(txnPoolNodeSet)))
 
     checkNodeDataForInequality(slowNode, *[n for n in txnPoolNodeSet if n != slowNode])
     assert slowNode.domainLedger.size - initialDomainLedgerSize == reqs_for_logsize
 
     # Also verify that the other nodes discard the COMMITs from the slow node
     # since they have already achieved stable checkpoints for these COMMITs.
     counts = discardCounts(
-        [n.replicas[backupInstId].stasher for n in txnPoolNodeSet if n != slowNode],
-        'marked stable checkpoint')
+        [n.replicas[backupInstId] for n in txnPoolNodeSet if n != slowNode],
+        'achieved stable checkpoint')
     for nm, count in counts.items():
         assert count > oldDiscardCounts[nm]
 
     oldCatchupTimes = slowNode.spylog.count(Node.start_catchup)
 
     # 3. Deliver the sent CHECKPOINTs to the slow node
     slowNode.nodeIbStasher.reset_delays_and_process_delayeds(CHECKPOINT)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_stashed_checkpoint_processing.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_stashed_checkpoint_processing.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,12 +1,9 @@
 from plenum.common.constants import COMMIT, CHECKPOINT
 from plenum.test import waits
-from plenum.test.checkpoints.helper import check_last_checkpoint, check_num_received_checkpoints, \
-    check_last_received_checkpoint, check_received_checkpoint_votes, check_stable_checkpoint, \
-    check_num_unstable_checkpoints
 from plenum.test.delayers import cDelay, chk_delay
 from plenum.test.helper import sdk_send_random_and_check
 from stp_core.loop.eventually import eventually
 
 CHK_FREQ = 5
 
 nodeCount = 5
@@ -35,36 +32,48 @@
                               sdk_pool_handle, sdk_wallet_client, 1)
 
     stabilization_timeout = \
         waits.expectedTransactionExecutionTime(len(txnPoolNodeSet))
     looper.runFor(stabilization_timeout)
 
     for inst_id, replica in epsilon.replicas.items():
-        check_stable_checkpoint(replica, 0)
-        check_num_unstable_checkpoints(replica, 0)
-        check_num_received_checkpoints(replica, 1)
-        check_received_checkpoint_votes(replica, pp_seq_no=5, num_votes=2)
+        assert len(replica.checkpoints) == 1
+        assert (1, 5) in replica.checkpoints
+        assert replica.checkpoints[(1, 5)].seqNo == 4
+        assert replica.checkpoints[(1, 5)].digest is None
+        assert replica.checkpoints[(1, 5)].isStable is False
+
+        assert len(replica.stashedRecvdCheckpoints) == 1
+        assert 0 in replica.stashedRecvdCheckpoints
+        assert len(replica.stashedRecvdCheckpoints[0]) == 1
+        assert (1, 5) in replica.stashedRecvdCheckpoints[0]
+        assert len(replica.stashedRecvdCheckpoints[0][(1, 5)]) == 2
 
     epsilon.nodeIbStasher.reset_delays_and_process_delayeds(COMMIT)
 
     def check():
         for inst_id, replica in epsilon.replicas.items():
-            check_stable_checkpoint(replica, 0)
-            check_num_unstable_checkpoints(replica, 1)
-            check_last_checkpoint(replica, 5)
+            assert len(replica.checkpoints) == 1
+            assert (1, 5) in replica.checkpoints
+            assert replica.checkpoints[(1, 5)].seqNo == 5
+            assert replica.checkpoints[(1, 5)].digest is not None
+            assert replica.checkpoints[(1, 5)].isStable is False
 
-            check_num_received_checkpoints(replica, 1)
-            check_last_received_checkpoint(replica, 5)
+            assert len(replica.stashedRecvdCheckpoints) == 0
 
     looper.run(eventually(check, timeout=waits.expectedOrderingTime(
         len(txnPoolNodeSet))))
 
     epsilon.nodeIbStasher.reset_delays_and_process_delayeds(CHECKPOINT)
 
     stabilization_timeout = \
         waits.expectedTransactionExecutionTime(len(txnPoolNodeSet))
     looper.runFor(stabilization_timeout)
 
     for inst_id, replica in epsilon.replicas.items():
-        check_stable_checkpoint(replica, 5)
-        check_num_unstable_checkpoints(replica, 0)
-        check_num_received_checkpoints(replica, 0)
+        assert len(replica.checkpoints) == 1
+        assert (1, 5) in replica.checkpoints
+        assert replica.checkpoints[(1, 5)].seqNo == 5
+        assert replica.checkpoints[(1, 5)].digest is not None
+        assert replica.checkpoints[(1, 5)].isStable is True
+
+        assert len(replica.stashedRecvdCheckpoints) == 0
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_backup_replica_resumes_ordering_on_lag_in_checkpoints.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_backup_replica_resumes_ordering_on_lag_in_checkpoints.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import pytest
 
 from plenum.common.constants import DOMAIN_LEDGER_ID
 from plenum.server.replica import Replica
 from plenum.test import waits
-from plenum.test.checkpoints.helper import check_num_quorumed_received_checkpoints, check_num_unstable_checkpoints
 from plenum.test.delayers import cDelay, chk_delay
-from plenum.test.helper import sdk_send_random_requests, assertExp, sdk_send_random_and_check, assert_eq
+from plenum.test.helper import sdk_send_random_requests, assertExp
 from stp_core.loop.eventually import eventually
 
 nodeCount = 4
 
 CHK_FREQ = 5
 LOG_SIZE = 3 * CHK_FREQ
 
@@ -23,28 +22,29 @@
     yield tconf
     tconf.Max3PCBatchesInFlight = old
 
 
 def test_backup_replica_resumes_ordering_on_lag_in_checkpoints(
         looper, chkFreqPatched, reqs_for_checkpoint,
         one_replica_and_others_in_backup_instance,
-        sdk_pool_handle, sdk_wallet_client, view_change_done, txnPoolNodeSet):
+        sdk_pool_handle, sdk_wallet_client, view_change_done):
     """
     Verifies resumption of ordering 3PC-batches on a backup replica
     on detection of a lag in checkpoints
     """
 
     slow_replica, other_replicas = one_replica_and_others_in_backup_instance
     view_no = slow_replica.viewNo
 
     # Send a request and ensure that the replica orders the batch for it
     sdk_send_random_requests(looper, sdk_pool_handle, sdk_wallet_client, 1)
 
     looper.run(
-        eventually(lambda: assert_eq(slow_replica.last_ordered_3pc, (view_no, 2)),
+        eventually(lambda *args: assertExp(slow_replica.last_ordered_3pc == (view_no, 2)),
+                   slow_replica,
                    retryWait=1,
                    timeout=waits.expectedTransactionExecutionTime(nodeCount)))
 
     # Don't receive Commits from two replicas
     slow_replica.node.nodeIbStasher.delay(
         cDelay(instId=1, sender_filter=other_replicas[0].node.name))
     slow_replica.node.nodeIbStasher.delay(
@@ -75,28 +75,29 @@
     assert slow_replica.H == LOG_SIZE
 
     # Ensure that the collections related to requests, batches and
     # own checkpoints are not empty.
     # (Note that a primary replica removes requests from requestQueues
     # when creating a batch with them.)
     if slow_replica.isPrimary:
-        assert slow_replica._ordering_service.sentPrePrepares
+        assert slow_replica.sentPrePrepares
     else:
-        assert slow_replica._ordering_service.requestQueues[DOMAIN_LEDGER_ID]
-        assert slow_replica._ordering_service.prePrepares
-    assert slow_replica._ordering_service.prepares
-    assert slow_replica._ordering_service.commits
-    assert slow_replica._ordering_service.batches
+        assert slow_replica.requestQueues[DOMAIN_LEDGER_ID]
+        assert slow_replica.prePrepares
+    assert slow_replica.prepares
+    assert slow_replica.commits
+    assert slow_replica.batches
+    assert slow_replica.checkpoints
 
-    check_num_unstable_checkpoints(slow_replica, 0)
-    check_num_quorumed_received_checkpoints(slow_replica, 1)
+    # Ensure that there are some quorumed stashed checkpoints
+    assert slow_replica.stashed_checkpoints_with_quorum()
 
     # Send more requests to reach catch-up number of checkpoints
-    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
-                              sdk_wallet_client, reqs_for_checkpoint)
+    sdk_send_random_requests(looper, sdk_pool_handle, sdk_wallet_client,
+                             reqs_for_checkpoint)
 
     # Ensure that the replica has adjusted last_ordered_3pc to the end
     # of the last checkpoint
     looper.run(
         eventually(lambda *args: assertExp(slow_replica.last_ordered_3pc == \
                         (view_no, (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ)),
                    slow_replica,
@@ -106,23 +107,24 @@
     # Ensure that the watermarks have been shifted so that the lower watermark
     # has the same value as last_ordered_3pc
     assert slow_replica.h == (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ
     assert slow_replica.H == (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ + LOG_SIZE
 
     # Ensure that the collections related to requests, batches and
     # own checkpoints have been cleared
-    assert not slow_replica._ordering_service.requestQueues[DOMAIN_LEDGER_ID]
-    assert not slow_replica._ordering_service.sentPrePrepares
-    assert not slow_replica._ordering_service.prePrepares
-    assert not slow_replica._ordering_service.prepares
-    assert not slow_replica._ordering_service.commits
-    assert not slow_replica._ordering_service.batches
+    assert not slow_replica.requestQueues[DOMAIN_LEDGER_ID]
+    assert not slow_replica.sentPrePrepares
+    assert not slow_replica.prePrepares
+    assert not slow_replica.prepares
+    assert not slow_replica.commits
+    assert not slow_replica.batches
+    assert not slow_replica.checkpoints
 
-    check_num_unstable_checkpoints(slow_replica, 0)
-    check_num_quorumed_received_checkpoints(slow_replica, 0)
+    # Ensure that now there are no quorumed stashed checkpoints
+    assert not slow_replica.stashed_checkpoints_with_quorum()
 
     # Send a request and ensure that the replica orders the batch for it
     sdk_send_random_requests(looper, sdk_pool_handle, sdk_wallet_client, 1)
 
     looper.run(
         eventually(lambda *args: assertExp(slow_replica.last_ordered_3pc ==
                                      (view_no, (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ + 1)),
@@ -195,33 +197,33 @@
     assert slow_replica.last_ordered_3pc == (view_no, 2)
 
     # Ensure that the watermarks have not been shifted since the view start
     assert slow_replica.h == 0
     assert slow_replica.H == LOG_SIZE
 
     # Ensure that there are some quorumed stashed checkpoints
-    check_num_quorumed_received_checkpoints(slow_replica, 1)
+    assert slow_replica.stashed_checkpoints_with_quorum()
 
     # Receive belated Checkpoints
     slow_replica.node.nodeIbStasher.reset_delays_and_process_delayeds()
 
     # Ensure that the replica has ordered the batch for the last sent request
     looper.run(
         eventually(lambda *args: assertExp(slow_replica.last_ordered_3pc == \
-                                           (view_no, (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ + 2)),
+                        (view_no, (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ + 2)),
                    slow_replica,
                    timeout=waits.expectedTransactionExecutionTime(nodeCount)))
 
     # Ensure that the watermarks have been shifted so that the lower watermark
     # now equals to the end of the last stable checkpoint in the instance
     assert slow_replica.h == (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ
     assert slow_replica.H == (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ + LOG_SIZE
 
     # Ensure that now there are no quorumed stashed checkpoints
-    check_num_quorumed_received_checkpoints(slow_replica, 0)
+    assert not slow_replica.stashed_checkpoints_with_quorum()
 
     # Send a request and ensure that the replica orders the batch for it
     sdk_send_random_requests(looper, sdk_pool_handle, sdk_wallet_client, 1)
 
     looper.run(
         eventually(lambda: assertExp(slow_replica.last_ordered_3pc ==
                                      (view_no, (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ + 3)),
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_ordering_after_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_ordering_after_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_checkpoint_stable_while_unstashing.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_checkpoint_stable_while_unstashing.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from plenum.test.checkpoints.helper import check_for_nodes, check_stable_checkpoint, check_received_checkpoint_votes
+from plenum.test.checkpoints.helper import chkChkpoints, check_stashed_chekpoints
 from plenum.test.delayers import ppDelay, msg_rep_delay
 from plenum.test.helper import sdk_send_random_and_check, assertExp
 from plenum.test.node_catchup.helper import waitNodeDataEquality
 from plenum.test.stasher import delay_rules
 from stp_core.loop.eventually import eventually
 
 CHK_FREQ = 5
@@ -27,31 +27,31 @@
     # 2. delay PrePrepare on 1 node so that prepares and commits will be stashed
     with delay_rules(lagging_node.nodeIbStasher, ppDelay()):
         with delay_rules(lagging_node.nodeIbStasher, msg_rep_delay()):
             sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
                                       sdk_wallet_client, 1)
 
             # all good nodes stabilized checkpoint
-            looper.run(eventually(check_for_nodes, rest_nodes, check_stable_checkpoint, 5))
+            looper.run(eventually(chkChkpoints, rest_nodes, 1, 0))
 
             # bad node received checkpoints from all nodes but didn't stabilize it
-            looper.run(eventually(check_for_nodes, [lagging_node], check_stable_checkpoint, 0))
-            looper.run(eventually(check_for_nodes, [lagging_node], check_received_checkpoint_votes, 5, len(rest_nodes)))
+            looper.run(eventually(check_stashed_chekpoints, lagging_node, len(rest_nodes)))
+            looper.run(eventually(chkChkpoints, [lagging_node], 1, None))
 
             # bad node has all commits and prepares for the last request stashed
             looper.run(eventually(
                 lambda: assertExp(
-                    (0, CHK_FREQ) in lagging_master_replcia._ordering_service.preparesWaitingForPrePrepare and
-                    len(lagging_master_replcia._ordering_service.preparesWaitingForPrePrepare[(0, CHK_FREQ)]) == len(rest_nodes) - 1
+                    (0, CHK_FREQ) in lagging_master_replcia.preparesWaitingForPrePrepare and
+                    len(lagging_master_replcia.preparesWaitingForPrePrepare[(0, CHK_FREQ)]) == len(rest_nodes) - 1
                 )
             ))
             looper.run(eventually(
                 lambda: assertExp(
-                    (0, CHK_FREQ) in lagging_master_replcia._ordering_service.commitsWaitingForPrepare and
-                    len(lagging_master_replcia._ordering_service.commitsWaitingForPrepare[(0, CHK_FREQ)]) == len(rest_nodes)
+                    (0, CHK_FREQ) in lagging_master_replcia.commitsWaitingForPrepare and
+                    len(lagging_master_replcia.commitsWaitingForPrepare[(0, CHK_FREQ)]) == len(rest_nodes)
                 )
             ))
 
     # 3. the delayed PrePrepare is processed, and stashed prepares and commits are unstashed
     # checkpoint will be stabilized during unstashing, and the request will be ordered
-    looper.run(eventually(check_for_nodes, [lagging_node], check_stable_checkpoint, 5))
+    looper.run(eventually(chkChkpoints, [lagging_node], 1, 0))
     waitNodeDataEquality(looper, *txnPoolNodeSet, customTimeout=5)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_view_change_after_checkpoint.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_view_change_after_checkpoint.py`

 * *Files 2% similar despite different names*

```diff
@@ -45,20 +45,20 @@
 
     # Check that after view change, proper clean up is done
     for node in txnPoolNodeSet:
         for r in node.replicas.values():
             # Checkpoint was started after sending audit txn
             # assert not r.checkpoints
             # No stashed checkpoint for previous view
-            assert all(cp.view_no >= r.viewNo for cp in r._checkpointer._received_checkpoints)
-            assert r.h == 0
+            assert not [view_no for view_no in r.stashedRecvdCheckpoints if view_no < r.viewNo]
+            assert r._h == 0
             # from audit txn
-            assert r._ordering_service._lastPrePrepareSeqNo == 1
+            assert r._lastPrePrepareSeqNo == 1
             assert r.h == 0
-            assert r.H == r.h + chkFreqPatched.LOG_SIZE
+            assert r.H == r._h + chkFreqPatched.LOG_SIZE
 
     # All this manipulations because after view change we will send an empty batch for auditing
     checkRequestCounts(txnPoolNodeSet, 0, 1)
     if sent_batches > CHK_FREQ:
         expected_batch_count = sent_batches - CHK_FREQ + 1
         additional_after_vc = 0
     elif sent_batches == CHK_FREQ:
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_checkpoint_bounds_after_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/script/helper.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,42 +1,77 @@
-from plenum.test.checkpoints.helper import check_stable_checkpoint
+import pytest
+from plenum.common.constants import VALIDATOR
+
+from plenum.test.pool_transactions.helper import sdk_send_update_node, sdk_pool_refresh
+from stp_core.common.log import getlogger
+from plenum.common.util import hexToFriendly
+from plenum.test import waits
 from plenum.test.helper import sdk_send_random_and_check
-from plenum.test.node_catchup.helper import waitNodeDataEquality
-from plenum.test.pool_transactions.helper import sdk_add_new_steward_and_node
-from plenum.test.test_node import checkNodesConnected
-
-CHK_FREQ = 5
-
-
-def test_upper_bound_of_checkpoint_after_catchup_is_divisible_by_chk_freq(
-        chkFreqPatched, looper, txnPoolNodeSet,
-        sdk_pool_handle, sdk_wallet_steward, sdk_wallet_client, tdir,
-        tconf, allPluginsPath):
-    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
-                              sdk_wallet_client, 4)
-
-    _, new_node = sdk_add_new_steward_and_node(
-        looper, sdk_pool_handle, sdk_wallet_steward,
-        'EpsilonSteward', 'Epsilon', tdir, tconf,
-        allPluginsPath=allPluginsPath)
-    txnPoolNodeSet.append(new_node)
-    looper.run(checkNodesConnected(txnPoolNodeSet))
-    waitNodeDataEquality(looper, new_node, *txnPoolNodeSet[:-1],
-                         exclude_from_check=['check_last_ordered_3pc_backup'])
-    # Epsilon did not participate in ordering of the batch with EpsilonSteward
-    # NYM transaction and the batch with Epsilon NODE transaction.
-    # Epsilon got these transactions via catch-up.
-
-    sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
-                              sdk_wallet_client, 1)
-
-    for replica in txnPoolNodeSet[0].replicas.values():
-        check_stable_checkpoint(replica, 5)
-
-    # TODO: This is failing now because checkpoints are not created after catchup.
-    #  PBFT paper describes catch-up with per-checkpoint granularity, but otherwise
-    #  quite similar to plenum implementation. Authors of PBFT state that after
-    #  catching up nodes set low watermark to last caught up checkpoint, which is
-    #  actually equivalent to declaring that checkpoint stable. This means that
-    #  most probably we need this functionality in plenum.
-    # for replica in new_node.replicas.values():
-    #    check_stable_checkpoint(replica, 5)
+from plenum.test.test_node import TestNode, checkNodesConnected, \
+    ensureElectionsDone
+from stp_core.network.port_dispenser import genHa
+from plenum.common.config_helper import PNodeConfigHelper
+
+logger = getlogger()
+
+
+@pytest.yield_fixture(scope="module")
+def looper(txnPoolNodesLooper):
+    yield txnPoolNodesLooper
+
+
+def changeNodeHa(looper, txnPoolNodeSet,
+                 tconf, shouldBePrimary, tdir,
+                 sdk_pool_handle, sdk_wallet_stewards,
+                 sdk_wallet_client):
+    # prepare new ha for node and client stack
+    subjectedNode = None
+    node_index = None
+
+    for nodeIndex, n in enumerate(txnPoolNodeSet):
+        if shouldBePrimary == n.has_master_primary:
+            subjectedNode = n
+            node_index = nodeIndex
+            break
+
+    nodeStackNewHA, clientStackNewHA = genHa(2)
+    logger.debug("change HA for node: {} to {}".format(
+        subjectedNode.name, (nodeStackNewHA, clientStackNewHA)))
+
+    # change HA
+    sdk_wallet_steward = sdk_wallet_stewards[node_index]
+    node_dest = hexToFriendly(subjectedNode.nodestack.verhex)
+    sdk_send_update_node(looper, sdk_wallet_steward,
+                         sdk_pool_handle,
+                         node_dest, subjectedNode.name,
+                         nodeStackNewHA[0], nodeStackNewHA[1],
+                         clientStackNewHA[0], clientStackNewHA[1],
+                         services=[VALIDATOR])
+
+    # stop node for which HA will be changed
+    subjectedNode.stop()
+    looper.removeProdable(subjectedNode)
+
+    # start node with new HA
+    config_helper = PNodeConfigHelper(subjectedNode.name, tconf, chroot=tdir)
+    restartedNode = TestNode(subjectedNode.name,
+                             config_helper=config_helper,
+                             config=tconf, ha=nodeStackNewHA,
+                             cliha=clientStackNewHA)
+    looper.add(restartedNode)
+    txnPoolNodeSet[nodeIndex] = restartedNode
+    looper.run(checkNodesConnected(txnPoolNodeSet, customTimeout=70))
+
+
+    electionTimeout = waits.expectedPoolElectionTimeout(
+        nodeCount=len(txnPoolNodeSet),
+        numOfReelections=3)
+    ensureElectionsDone(looper,
+                        txnPoolNodeSet,
+                        retryWait=1,
+                        customTimeout=electionTimeout)
+
+    sdk_pool_refresh(looper, sdk_pool_handle)
+    sdk_send_random_and_check(looper, txnPoolNodeSet,
+                              sdk_pool_handle,
+                              sdk_wallet_client,
+                              8)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_incomplete_short_checkpoint_included_in_lag_for_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_incomplete_short_checkpoint_included_in_lag_for_catchup.py`

 * *Files 4% similar despite different names*

```diff
@@ -53,17 +53,21 @@
     # This checkpoint has a not aligned lower bound on the new node replicas.
     send_reqs_batches_and_get_suff_replies(looper, txnPoolNodeSet,
                                            sdk_pool_handle,
                                            sdk_wallet_client,
                                            reqs_for_checkpoint - 3 * max_batch_size)
 
     # The master replica of the new node stops to receive 3PC-messages
-    new_node.master_replica.external_bus._handlers[PrePrepare] = [lambda *x, **y: (None, None)]
-    new_node.master_replica.external_bus._handlers[Prepare] = [lambda *x, **y: (None, None)]
-    new_node.master_replica.external_bus._handlers[Commit] = [lambda *x, **y: (None, None)]
+    new_node.master_replica.threePhaseRouter.extend(
+        (
+            (PrePrepare, lambda *x, **y: None),
+            (Prepare, lambda *x, **y: None),
+            (Commit, lambda *x, **y: None),
+        )
+    )
 
     completed_catchups_before_reqs = get_number_of_completed_catchups(new_node)
 
     # Send requests for the new node's master replica to reach
     # Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP quorumed stashed
     # checkpoints from others
     send_reqs_batches_and_get_suff_replies(looper, txnPoolNodeSet,
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_stashed_messages_processed_on_backup_replica_ordering_resumption.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_stashed_messages_processed_on_backup_replica_ordering_resumption.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,11 @@
 import pytest
 
 from plenum.server.replica import Replica
-from plenum.server.replica_validator_enums import STASH_WATERMARKS
 from plenum.test import waits
-from plenum.test.checkpoints.helper import check_num_quorumed_received_checkpoints
 from plenum.test.delayers import cDelay, chk_delay
 from plenum.test.helper import sdk_send_random_requests, assertExp, incoming_3pc_msgs_count
 from stp_core.loop.eventually import eventually
 
 nodeCount = 4
 
 CHK_FREQ = 5
@@ -80,34 +78,34 @@
     # Send more requests to reach catch-up number of checkpoints
     sdk_send_random_requests(looper, sdk_pool_handle, sdk_wallet_client,
                              reqs_for_checkpoint)
     looper.runFor(waits.expectedTransactionExecutionTime(nodeCount))
 
     # Ensure that there are no 3PC-messages stashed
     # as laying outside of the watermarks
-    assert slow_replica.stasher.stash_size(STASH_WATERMARKS) == 0
+    assert slow_replica.stasher.num_stashed_watermarks == 0
 
     # Send a request for which the batch will be outside of the watermarks
     sdk_send_random_requests(looper, sdk_pool_handle, sdk_wallet_client, 1)
     looper.runFor(waits.expectedTransactionExecutionTime(nodeCount))
 
     # Ensure that the replica has not ordered any batches
     # after the very first one
     assert slow_replica.last_ordered_3pc == (view_no, 2)
 
     # Ensure that the watermarks have not been shifted since the view start
     assert slow_replica.h == 0
     assert slow_replica.H == LOG_SIZE
 
     # Ensure that there are some quorumed stashed checkpoints
-    check_num_quorumed_received_checkpoints(slow_replica, 1)
+    assert slow_replica.stashed_checkpoints_with_quorum()
 
     # Ensure that now there are 3PC-messages stashed
     # as laying outside of the watermarks
-    assert slow_replica.stasher.stash_size(STASH_WATERMARKS) == incoming_3pc_msgs_count(len(txnPoolNodeSet))
+    assert slow_replica.stasher.num_stashed_watermarks == incoming_3pc_msgs_count(len(txnPoolNodeSet))
 
     # Receive belated Checkpoints
     slow_replica.node.nodeIbStasher.reset_delays_and_process_delayeds()
 
     # Ensure that the replica has ordered the batch for the last sent request
     looper.run(
         eventually(lambda *args: assertExp(slow_replica.last_ordered_3pc ==
@@ -118,19 +116,19 @@
 
     # Ensure that the watermarks have been shifted so that the lower watermark
     # now equals to the end of the last stable checkpoint in the instance
     assert slow_replica.h == (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ
     assert slow_replica.H == (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ + LOG_SIZE
 
     # Ensure that now there are no quorumed stashed checkpoints
-    check_num_quorumed_received_checkpoints(slow_replica, 0)
+    assert not slow_replica.stashed_checkpoints_with_quorum()
 
     # Ensure that now there are no 3PC-messages stashed
     # as laying outside of the watermarks
-    assert slow_replica.stasher.stash_size(STASH_WATERMARKS) == 0
+    assert slow_replica.stasher.num_stashed_watermarks == 0
 
     # Send a request and ensure that the replica orders the batch for it
     sdk_send_random_requests(looper, sdk_pool_handle, sdk_wallet_client, 1)
 
     looper.run(
         eventually(lambda *args: assertExp(slow_replica.last_ordered_3pc ==
                                      (view_no, (Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1) * CHK_FREQ + 2)),
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/checkpoints/test_lag_size_for_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/checkpoints/test_lag_size_for_catchup.py`

 * *Files 5% similar despite different names*

```diff
@@ -27,17 +27,21 @@
     Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP + 1 quorumed stashed received
     checkpoints.
     """
     slow_node = getNonPrimaryReplicas(txnPoolNodeSet, 0)[-1].node
     other_nodes = [n for n in txnPoolNodeSet if n != slow_node]
 
     # The master replica of the slow node stops to receive 3PC-messages
-    slow_node.master_replica.external_bus._handlers[PrePrepare] = [lambda *x, **y: (None, None)]
-    slow_node.master_replica.external_bus._handlers[Prepare] = [lambda *x, **y: (None, None)]
-    slow_node.master_replica.external_bus._handlers[Commit] = [lambda *x, **y: (None, None)]
+    slow_node.master_replica.threePhaseRouter.extend(
+        (
+            (PrePrepare, lambda *x, **y: None),
+            (Prepare, lambda *x, **y: None),
+            (Commit, lambda *x, **y: None),
+        )
+    )
 
     completed_catchups_before_reqs = get_number_of_completed_catchups(slow_node)
 
     # Send requests for the slow node's master replica to get
     # Replica.STASHED_CHECKPOINTS_BEFORE_CATCHUP quorumed stashed checkpoints
     # from others
     send_reqs_batches_and_get_suff_replies(looper, txnPoolNodeSet,
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_moving_average.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_moving_average.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_warn_unordered_log_msg.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_warn_unordered_log_msg.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_post_monitoring_stats.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_post_monitoring_stats.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_safe_start_ema_throughput_measurement.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_safe_start_ema_throughput_measurement.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_stats_publisher.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_stats_publisher.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_backup_throughput_measurement.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_backup_throughput_measurement.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_latency_measurement_class.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_latency_measurement_class.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_throughput_based_master_degradation_detection.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_throughput_based_master_degradation_detection.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_request_time_tracker.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_request_time_tracker.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_system_stats.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_system_stats.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_latency_median_avg.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_latency_median_avg.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_monitoring_params_with_zfn.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_monitoring_params_with_zfn.py`

 * *Files 14% similar despite different names*

```diff
@@ -26,15 +26,15 @@
 def testThroughputThreshold(looper, txnPoolNodeSet, tconf, requests):
     looper.runFor(tconf.throughput_measurement_params['window_size'] *
                   tconf.throughput_measurement_params['min_cnt'])
     for node in txnPoolNodeSet:
         masterThroughput, avgBackupThroughput = node.monitor.getThroughputs(
             node.instances.masterId)
         for r in node.replicas.values():
-            print("{} stats: {}".format(r, repr(r._ordering_service.stats)))
+            print("{} stats: {}".format(r, repr(r.stats)))
         assert masterThroughput / avgBackupThroughput >= node.monitor.Delta
 
 
 def testReqLatencyThreshold(looper, txnPoolNodeSet, requests):
     for node in txnPoolNodeSet:
         for rq in requests:
             key = get_key_from_req(rq)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_acc_monitor_strategy.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_acc_monitor_strategy.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_instance_change_with_Delta.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_instance_change_with_Delta.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_revival_spike_resistant_ema_throughput_measurement.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_revival_spike_resistant_ema_throughput_measurement.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_no_check_if_no_new_requests.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_no_check_if_no_new_requests.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_invalid_reqs_in_monitor.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_invalid_reqs_in_monitor.py`

 * *Files 18% similar despite different names*

```diff
@@ -28,16 +28,15 @@
     """Send 1 valid request and 2 invalid. Then checked, that all 3 requests are stored into monitor."""
     sdk_send_random_and_check(looper,
                               txnPoolNodeSet,
                               sdk_pool_handle,
                               sdk_wallet_steward,
                               COUNT_VALID_REQS)
     for node in txnPoolNodeSet:
-        node.master_replica._ordering_service._do_dynamic_validation = \
-            functools.partial(randomDynamicValidation, node)
+        node.doDynamicValidation = functools.partial(randomDynamicValidation, node)
     with pytest.raises(RequestRejectedException, match='not valid req'):
         sdk_send_random_and_check(looper,
                                   txnPoolNodeSet,
                                   sdk_pool_handle,
                                   sdk_wallet_steward,
                                   COUNT_INVALID_REQS)
     check_count_reqs(txnPoolNodeSet)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_throughput_median_avg.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_throughput_median_avg.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_instance_change_with_req_Lambda.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_instance_change_with_req_Lambda.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_EMALatencyMeasurementForAllClient.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_EMALatencyMeasurementForAllClient.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_throughput.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_throughput.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/monitoring/test_avg_latency.py` & `indy-plenum-1.9.2rc1/plenum/test/monitoring/test_avg_latency.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_node.py` & `indy-plenum-1.9.2rc1/plenum/test/test_node.py`

 * *Files 7% similar despite different names*

```diff
@@ -5,21 +5,18 @@
 from contextlib import ExitStack
 from functools import partial
 from itertools import combinations
 from typing import Iterable, Iterator, Tuple, Sequence, Dict, TypeVar, \
     List, Optional
 
 from crypto.bls.bls_bft import BlsBft
-from plenum.common.event_bus import InternalBus
-from plenum.common.stashing_router import StashingRouter
 from plenum.common.txn_util import get_type
 from plenum.server.client_authn import CoreAuthNr
-from plenum.server.consensus.ordering_service import OrderingService
-from plenum.server.consensus.checkpoint_service import CheckpointService
 from plenum.server.node_bootstrap import NodeBootstrap
+from plenum.server.replica_stasher import ReplicaStasher
 from plenum.test.buy_handler import BuyHandler
 from plenum.test.constants import BUY, GET_BUY, RANDOM_BUY
 from plenum.test.get_buy_handler import GetBuyHandler
 from plenum.test.random_buy_handler import RandomBuyHandler
 from stp_core.crypto.util import randomSeed
 from stp_core.network.port_dispenser import genHa
 
@@ -38,28 +35,29 @@
 from stp_core.common.util import adict
 from plenum.server import replica
 from plenum.server.instances import Instances
 from plenum.server.monitor import Monitor
 from plenum.server.node import Node
 from plenum.server.view_change.node_view_changer import create_view_changer
 from plenum.server.view_change.view_changer import ViewChanger
+from plenum.server.primary_selector import PrimarySelector
 from plenum.test.greek import genNodeNames
 from plenum.test.msgs import TestMsg
 from plenum.test.spy_helpers import getLastMsgReceivedForNode, \
     getAllMsgReceivedForNode, getAllArgs
 from plenum.test.stasher import Stasher
 from plenum.test.test_ledger_manager import TestLedgerManager
 from plenum.test.test_stack import StackedTester, getTestableStack, \
     RemoteState, checkState
 from plenum.test.testable import spyable
 from plenum.test import waits
 from plenum.common.messages.node_message_factory import node_message_factory
 from plenum.server.replicas import Replicas
 from plenum.common.config_helper import PNodeConfigHelper
-from plenum.common.messages.node_messages import Reply, Checkpoint
+from plenum.common.messages.node_messages import Reply
 
 logger = getlogger()
 
 
 @spyable(methods=[CoreAuthNr.authenticate])
 class TestCoreAuthnr(CoreAuthNr):
     pass
@@ -132,21 +130,34 @@
         self.clientIbStasher.process()
         await super().processClientInBox()
 
     def _serviceActions(self):
         self.actionQueueStasher.process()
         return super()._serviceActions()
 
+    def newPrimaryDecider(self):
+        pdCls = self.primaryDecider if self.primaryDecider else \
+            TestPrimarySelector
+        return pdCls(self)
+
     def newViewChanger(self):
         view_changer = self.view_changer if self.view_changer is not None \
             else create_view_changer(self, TestViewChanger)
         # TODO: This is a hack for tests compatibility, do something better
         view_changer.node = self
         return view_changer
 
+    def delaySelfNomination(self, delay: Seconds):
+        if isinstance(self.primaryDecider, PrimarySelector):
+            raise RuntimeError('Does not support nomination since primary is '
+                               'selected deterministically')
+        else:
+            raise RuntimeError('Unknown primary decider encountered {}'.
+                               format(self.primaryDecider))
+
     def delayCheckPerformance(self, delay: Seconds):
         logger.debug("{} delaying check performance".format(self))
         delayerCheckPerf = partial(delayers.delayerMethod,
                                    TestNode.checkPerformance)
         self.actionQueueStasher.delay(delayerCheckPerf(delay))
 
     def resetDelays(self, *names):
@@ -298,22 +309,22 @@
                  Node.transmitToClient,
                  Node.has_ordered_till_last_prepared_certificate,
                  Node.on_inconsistent_3pc_state,
                  Node.sendToViewChanger, ]
 
 class TestNodeBootstrap(NodeBootstrap):
 
-    def _register_domain_req_handlers(self):
-        super()._register_domain_req_handlers()
+    def register_domain_req_handlers(self):
+        super().register_domain_req_handlers()
         self.node.write_manager.register_req_handler(BuyHandler(self.node.db_manager))
         self.node.write_manager.register_req_handler(RandomBuyHandler(self.node.db_manager))
         self.node.read_manager.register_req_handler(GetBuyHandler(self.node.db_manager))
 
-    def _init_common_managers(self):
-        super()._init_common_managers()
+    def init_common_managers(self):
+        super().init_common_managers()
         self.node.ledgerManager = TestLedgerManager(self.node,
                                                     postAllLedgersCaughtUp=self.node.allLedgersCaughtUp,
                                                     preCatchupClbk=self.node.preLedgerCatchUp,
                                                     postCatchupClbk=self.node.postLedgerCatchUp,
                                                     ledger_sync_order=self.node.ledger_ids,
                                                     metrics=self.node.metrics)
 
@@ -367,14 +378,22 @@
     def dump_additional_info(self):
         pass
 
     def restart_clientstack(self):
         self.clientstack.restart()
 
 
+selector_spyables = [PrimarySelector.decidePrimaries]
+
+
+@spyable(methods=selector_spyables)
+class TestPrimarySelector(PrimarySelector):
+    pass
+
+
 view_changer_spyables = [
     ViewChanger.sendInstanceChange,
     ViewChanger._do_view_change_by_future_vcd,
     ViewChanger.process_instance_change_msg,
     ViewChanger.start_view_change,
     ViewChanger.process_future_view_vchd_msg
 ]
@@ -382,117 +401,64 @@
 
 @spyable(methods=view_changer_spyables)
 class TestViewChanger(ViewChanger):
     pass
 
 
 replica_stasher_spyables = [
-    StashingRouter._stash,
-    StashingRouter._process,
-    StashingRouter.discard
+    ReplicaStasher.stash
 ]
 
 
 @spyable(methods=replica_stasher_spyables)
-class TestStashingRouter(StashingRouter):
+class TestReplicaStasher(ReplicaStasher):
     pass
 
 
 replica_spyables = [
+    replica.Replica.sendPrePrepare,
+    replica.Replica._can_process_pre_prepare,
+    replica.Replica.canPrepare,
+    replica.Replica.validatePrepare,
+    replica.Replica.addToPrePrepares,
+    replica.Replica.processPrePrepare,
+    replica.Replica.processPrepare,
+    replica.Replica.processCommit,
+    replica.Replica.process_checkpoint,
+    replica.Replica.doPrepare,
+    replica.Replica.doOrder,
+    replica.Replica.discard,
     replica.Replica.revert_unordered_batches,
+    replica.Replica.revert,
+    replica.Replica.process_three_phase_msg,
+    replica.Replica._request_pre_prepare,
+    replica.Replica._request_pre_prepare_for_prepare,
+    replica.Replica._request_prepare,
+    replica.Replica._request_commit,
     replica.Replica.process_requested_pre_prepare,
     replica.Replica.process_requested_prepare,
-    replica.Replica._send_ordered,
     replica.Replica.process_requested_commit,
+    replica.Replica.is_pre_prepare_time_correct,
+    replica.Replica.is_pre_prepare_time_acceptable,
+    replica.Replica._process_stashed_pre_prepare_for_time_if_possible,
+    replica.Replica.markCheckPointStable,
+    replica.Replica.request_propagates_if_needed,
 ]
 
 
 @spyable(methods=replica_spyables)
 class TestReplica(replica.Replica):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
+        self.stasher = TestReplicaStasher(self)
         # Each TestReplica gets it's own outbox stasher, all of which TestNode
         # processes in its overridden serviceReplicaOutBox
         self.outBoxTestStasher = \
             Stasher(self.outBox, "replicaOutBoxTestStasher~" + self.name)
 
-    def _init_replica_stasher(self):
-        return TestStashingRouter(self.config.REPLICA_STASH_LIMIT,
-                                  buses=[self.internal_bus, self._external_bus],
-                                  unstash_handler=self._add_to_inbox)
-
-    def _init_checkpoint_service(self) -> CheckpointService:
-        return TestCheckpointService(data=self._consensus_data,
-                                     bus=self.internal_bus,
-                                     network=self._external_bus,
-                                     stasher=self.stasher,
-                                     db_manager=self.node.db_manager,
-                                     metrics=self.metrics)
-
-    def _init_ordering_service(self) -> OrderingService:
-        return TestOrderingService(self._consensus_data,
-                                   timer=self.node.timer,
-                                   bus=self.internal_bus,
-                                   network=self._external_bus,
-                                   write_manager=self.node.write_manager,
-                                   bls_bft_replica=self._bls_bft_replica,
-                                   freshness_checker=self._freshness_checker,
-                                   get_current_time=self.get_current_time,
-                                   get_time_for_3pc_batch=self.get_time_for_3pc_batch,
-                                   stasher=self.stasher,
-                                   metrics=self.metrics)
-
-
-checkpointer_spyables = [
-    CheckpointService.set_watermarks,
-    CheckpointService._mark_checkpoint_stable,
-    CheckpointService.process_checkpoint,
-    CheckpointService.discard,
-]
-
-
-@spyable(methods=checkpointer_spyables)
-class TestCheckpointService(CheckpointService):
-    pass
-
-
-ordering_service_spyables = [
-    OrderingService._order_3pc_key,
-    OrderingService._can_prepare,
-    OrderingService._is_pre_prepare_time_correct,
-    OrderingService._is_pre_prepare_time_acceptable,
-    OrderingService._process_stashed_pre_prepare_for_time_if_possible,
-    OrderingService._request_propagates_if_needed,
-    OrderingService.revert_unordered_batches,
-    OrderingService._request_pre_prepare_for_prepare,
-    OrderingService._order_3pc_key,
-    OrderingService._request_pre_prepare,
-    OrderingService._request_prepare,
-    OrderingService._request_commit,
-    OrderingService.send_pre_prepare,
-    OrderingService._can_process_pre_prepare,
-    OrderingService._can_prepare,
-    OrderingService._validate_prepare,
-    OrderingService._add_to_pre_prepares,
-    OrderingService.process_preprepare,
-    OrderingService.process_prepare,
-    OrderingService.process_commit,
-    OrderingService._do_prepare,
-    OrderingService._do_order,
-    OrderingService._revert,
-    OrderingService._validate,
-    OrderingService.post_batch_rejection,
-    OrderingService.post_batch_creation
-]
-
-
-@spyable(methods=ordering_service_spyables)
-class TestOrderingService(OrderingService):
-    pass
-
 
 class TestReplicas(Replicas):
     _replica_class = TestReplica
 
     def _new_replica(self, instance_id: int, is_master: bool, bls_bft: BlsBft):
         return self.__class__._replica_class(self._node, instance_id,
                                              self._config, is_master,
@@ -945,14 +911,15 @@
     :return:
     """
     params = [args for args in getAllArgs(node.view_changer, ViewChanger.start_view_change)]
     assert len(params) > 0
     args = params[-1]
     assert args["proposedViewNo"] == proposedViewNo
     assert node.viewNo == proposedViewNo
+    assert node.elector.viewNo == proposedViewNo
 
 
 def timeThis(func, *args, **kwargs):
     s = time.perf_counter()
     res = func(*args, **kwargs)
     return res, time.perf_counter() - s
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/nodestack/test_resend_stashed_msgs.py` & `indy-plenum-1.9.2rc1/plenum/test/nodestack/test_resend_stashed_msgs.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/run_continuously.py` & `indy-plenum-1.9.2rc1/plenum/test/run_continuously.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_request.py` & `indy-plenum-1.9.2rc1/plenum/test/test_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_adding_stewards.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_adding_stewards.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_start_many_nodes.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_start_many_nodes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_get_txn_request.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_get_txn_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_on_pool_membership_changes.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_on_pool_membership_changes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_add_inactive_node_then_activate.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_add_inactive_node_then_activate.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_add_node_with_invalid_data.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_add_node_with_invalid_data.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_change_ha_persists_post_nodes_restart.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_change_ha_persists_post_nodes_restart.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_nodes_data_changed.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_nodes_data_changed.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_nodes_with_pool_txns.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_nodes_with_pool_txns.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_add_node_with_not_unique_alias.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_add_node_with_not_unique_alias.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_add_stewards_and_client.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_add_stewards_and_client.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_txn_pool_manager.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_txn_pool_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_demote_nonexisted.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_demote_nonexisted.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_suspend_node.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_suspend_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_add_node_with_invalid_key_proof.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_add_node_with_invalid_key_proof.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_z_node_key_changed.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_z_node_key_changed.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_client_with_pool_txns.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_client_with_pool_txns.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/pool_transactions/test_nodes_ha_change_back.py` & `indy-plenum-1.9.2rc1/plenum/test/pool_transactions/test_nodes_ha_change_back.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_action_queue.py` & `indy-plenum-1.9.2rc1/plenum/test/test_action_queue.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_config_req_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/test_config_req_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -44,16 +44,16 @@
         return {f.IDENTIFIER.nm: request.identifier,
                 f.REQ_ID.nm: request.reqId,
                 **{DATA: json.dumps({key: val.decode()})}}
 
 
 class ConfigTestBootstrapClass(TestNodeBootstrap):
 
-    def _register_config_req_handlers(self):
-        super()._register_config_req_handlers()
+    def register_config_req_handlers(self):
+        super().register_config_req_handlers()
         write_rh = WriteConfHandler(self.node.db_manager)
         read_rh = ReadConfHandler(self.node.db_manager)
         self.node.write_manager.register_req_handler(write_rh)
         self.node.read_manager.register_req_handler(read_rh)
 
 
 def write_conf_op(key, value):
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node/test_api.py` & `indy-plenum-1.9.2rc1/plenum/test/node/test_api.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/node/test_quota_control.py` & `indy-plenum-1.9.2rc1/plenum/test/node/test_quota_control.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_node_genesis.py` & `indy-plenum-1.9.2rc1/plenum/test/test_node_genesis.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/greek.py` & `indy-plenum-1.9.2rc1/plenum/test/greek.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/forced_request/test_forced_request_validation.py` & `indy-plenum-1.9.2rc1/plenum/test/forced_request/test_forced_request_validation.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_stack.py` & `indy-plenum-1.9.2rc1/plenum/test/test_stack.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_node_basic.py` & `indy-plenum-1.9.2rc1/plenum/test/test_node_basic.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/helper.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,37 +13,33 @@
 from typing import Tuple, Iterable, Dict, Optional, List, Any, Sequence, Union, Callable
 
 import base58
 import pytest
 from indy.pool import set_protocol_version
 
 from common.serializers.serialization import invalid_index_serializer
-from crypto.bls.bls_factory import BlsFactoryCrypto
 from plenum.common.event_bus import ExternalBus
-from plenum.common.member.member import Member
-from plenum.common.member.steward import Steward
-from plenum.common.signer_did import DidSigner
 from plenum.common.signer_simple import SimpleSigner
 from plenum.common.timer import QueueTimer
 from plenum.config import Max3PCBatchWait
 from psutil import Popen
 import json
 import asyncio
 
 from indy.ledger import sign_and_submit_request, sign_request, submit_request, build_node_request, \
     build_pool_config_request, multi_sign_request
 from indy.error import ErrorCode, IndyError
 
 from ledger.genesis_txn.genesis_txn_file_util import genesis_txn_file
 from plenum.common.constants import DOMAIN_LEDGER_ID, OP_FIELD_NAME, REPLY, REQNACK, REJECT, \
-    CURRENT_PROTOCOL_VERSION, STEWARD, VALIDATOR, TRUSTEE, DATA, BLS_KEY, BLS_KEY_PROOF
+    CURRENT_PROTOCOL_VERSION
 from plenum.common.exceptions import RequestNackedException, RequestRejectedException, CommonSdkIOException, \
     PoolLedgerTimeoutException
 from plenum.common.messages.node_messages import Reply, PrePrepare, Prepare, Commit
-from plenum.common.txn_util import get_req_id, get_from, get_payload_data
+from plenum.common.txn_util import get_req_id, get_from
 from plenum.common.types import f, OPERATION
 from plenum.common.util import getNoInstances, get_utc_epoch
 from plenum.common.config_helper import PNodeConfigHelper
 from plenum.common.request import Request
 from plenum.server.node import Node
 from plenum.server.replica import Replica
 from plenum.test import waits
@@ -309,53 +305,52 @@
     for node, inst_id in itertools.product(txnPoolNodeSet, instances):
         checkRequestNotReturnedToNode(node,
                                       request.key,
                                       inst_id)
 
 
 def checkPrePrepareReqSent(replica: TestReplica, req: Request):
-    prePreparesSent = getAllArgs(replica._ordering_service,
-                                 replica._ordering_service.send_pre_prepare)
+    prePreparesSent = getAllArgs(replica, replica.sendPrePrepare)
     expectedDigest = TestReplica.batchDigest([req])
     assert expectedDigest in [p["ppReq"].digest for p in prePreparesSent]
     assert (req.digest,) in \
            [p["ppReq"].reqIdr for p in prePreparesSent]
 
 
 def checkPrePrepareReqRecvd(replicas: Iterable[TestReplica],
                             expectedRequest: PrePrepare):
     for replica in replicas:
-        params = getAllArgs(replica._ordering_service, replica._ordering_service._can_process_pre_prepare)
+        params = getAllArgs(replica, replica._can_process_pre_prepare)
         assert expectedRequest.reqIdr in [p['pre_prepare'].reqIdr for p in params]
 
 
 def checkPrepareReqSent(replica: TestReplica, key: str,
                         view_no: int):
-    paramsList = getAllArgs(replica._ordering_service, replica._ordering_service._can_prepare)
-    rv = getAllReturnVals(replica._ordering_service,
-                          replica._ordering_service._can_prepare)
+    paramsList = getAllArgs(replica, replica.canPrepare)
+    rv = getAllReturnVals(replica,
+                          replica.canPrepare)
     args = [p["ppReq"].reqIdr for p in paramsList if p["ppReq"].viewNo == view_no]
     assert (key,) in args
     idx = args.index((key,))
     assert rv[idx]
 
 
 def checkSufficientPrepareReqRecvd(replica: TestReplica, viewNo: int,
                                    ppSeqNo: int):
     key = (viewNo, ppSeqNo)
-    assert key in replica._ordering_service.prepares
-    assert len(replica._ordering_service.prepares[key][1]) >= replica.quorums.prepare.value
+    assert key in replica.prepares
+    assert len(replica.prepares[key][1]) >= replica.quorums.prepare.value
 
 
 def checkSufficientCommitReqRecvd(replicas: Iterable[TestReplica], viewNo: int,
                                   ppSeqNo: int):
     for replica in replicas:
         key = (viewNo, ppSeqNo)
-        assert key in replica._ordering_service.commits
-        received = len(replica._ordering_service.commits[key][1])
+        assert key in replica.commits
+        received = len(replica.commits[key][1])
         minimum = replica.quorums.commit.value
         assert received > minimum
 
 
 def checkViewNoForNodes(nodes: Iterable[TestNode], expectedViewNo: int = None):
     """
     Checks if all the given nodes have the expected view no
@@ -1215,15 +1210,15 @@
                               bls_multi_sig=None,
                               view_no=0,
                               pool_state_root=None,
                               pp_seq_no=0,
                               inst_id=0,
                               audit_txn_root=None,
                               reqs=None):
-    digest = Replica.batchDigest(reqs) if reqs is not None else random_string(32)
+    digest = Replica.batchDigest(reqs) if reqs is not None else "random digest"
     req_idrs = [req.key for req in reqs] if reqs is not None else ["random request"]
     params = [inst_id,
               view_no,
               pp_seq_no,
               timestamp or get_utc_epoch(),
               req_idrs,
               init_discarded(0),
@@ -1291,19 +1286,14 @@
               pre_prepare.ppTime,
               pre_prepare.digest,
               pre_prepare.stateRootHash,
               pre_prepare.txnRootHash,
               pre_prepare.auditTxnRootHash]
     return Prepare(*params)
 
-def create_commit_from_pre_prepare(pre_prepare):
-    params = [pre_prepare.instId,
-              pre_prepare.viewNo,
-              pre_prepare.ppSeqNo]
-    return Commit(*params)
 
 def create_prepare(req_key, state_root, inst_id=0):
     view_no, pp_seq_no = req_key
     params = create_prepare_params(view_no, pp_seq_no, state_root, inst_id=inst_id)
     return Prepare(*params)
 
 
@@ -1325,15 +1315,15 @@
     commits = nodes_count - 1  # Messages from all nodes exclude  self node
     # The primary node receives the same number of messages. Doesn't get pre-prepare,
     # but gets one more prepare
     return pre_prepare + prepares + commits
 
 
 def check_missing_pre_prepares(nodes, count):
-    assert all(count <= len(replica._ordering_service.prePreparesPendingPrevPP)
+    assert all(count <= len(replica.prePreparesPendingPrevPP)
                for replica in getNonPrimaryReplicas(nodes, instId=0))
 
 
 class MockTimestamp:
     def __init__(self, value=datetime.utcnow()):
         self.value = value
 
@@ -1416,83 +1406,7 @@
 
 
 def get_handler_by_type_wm(write_manager, h_type):
     for h_l in write_manager.request_handlers.values():
         for h in h_l:
             if isinstance(h, h_type):
                 return h
-
-
-def create_pool_txn_data(node_names: List[str],
-                         crypto_factory: BlsFactoryCrypto,
-                         get_free_port: Callable[[], int],
-                         nodes_with_bls: Optional[int] = None):
-    nodeCount = len(node_names)
-    data = {'txns': [], 'seeds': {}, 'nodesWithBls': {}}
-    for i, node_name in zip(range(1, nodeCount + 1), node_names):
-        data['seeds'][node_name] = node_name + '0' * (32 - len(node_name))
-        steward_name = 'Steward' + str(i)
-        data['seeds'][steward_name] = steward_name + '0' * (32 - len(steward_name))
-
-        n_idr = SimpleSigner(seed=data['seeds'][node_name].encode()).identifier
-        s_idr = DidSigner(seed=data['seeds'][steward_name].encode())
-
-        data['txns'].append(
-                Member.nym_txn(nym=s_idr.identifier,
-                               verkey=s_idr.verkey,
-                               role=STEWARD,
-                               name=steward_name,
-                               seq_no=i)
-        )
-
-        node_txn = Steward.node_txn(steward_nym=s_idr.identifier,
-                                    node_name=node_name,
-                                    nym=n_idr,
-                                    ip='127.0.0.1',
-                                    node_port=get_free_port(),
-                                    client_port=get_free_port(),
-                                    client_ip='127.0.0.1',
-                                    services=[VALIDATOR],
-                                    seq_no=i)
-
-        if nodes_with_bls is None or i <= nodes_with_bls:
-            _, bls_key, bls_key_proof = crypto_factory.generate_bls_keys(
-                seed=data['seeds'][node_name])
-            get_payload_data(node_txn)[DATA][BLS_KEY] = bls_key
-            get_payload_data(node_txn)[DATA][BLS_KEY_PROOF] = bls_key_proof
-            data['nodesWithBls'][node_name] = True
-
-        data['txns'].append(node_txn)
-
-    # Add 4 Trustees
-    for i in range(4):
-        trustee_name = 'Trs' + str(i)
-        data['seeds'][trustee_name] = trustee_name + '0' * (
-                32 - len(trustee_name))
-        t_sgnr = DidSigner(seed=data['seeds'][trustee_name].encode())
-        data['txns'].append(
-            Member.nym_txn(nym=t_sgnr.identifier,
-                           verkey=t_sgnr.verkey,
-                           role=TRUSTEE,
-                           name=trustee_name)
-        )
-
-    more_data_seeds = \
-        {
-            "Alice": "99999999999999999999999999999999",
-            "Jason": "bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb",
-            "John": "dddddddddddddddddddddddddddddddd",
-            "Les": "ffffffffffffffffffffffffffffffff"
-        }
-    more_data_users = []
-    for more_name, more_seed in more_data_seeds.items():
-        signer = DidSigner(seed=more_seed.encode())
-        more_data_users.append(
-            Member.nym_txn(nym=signer.identifier,
-                           verkey=signer.verkey,
-                           name=more_name,
-                           creator="5rArie7XKukPCaEwq5XGQJnM9Fc5aZE3M9HAPVfMU2xC")
-        )
-
-    data['txns'].extend(more_data_users)
-    data['seeds'].update(more_data_seeds)
-    return data
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_node_request.py` & `indy-plenum-1.9.2rc1/plenum/test/test_node_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/testable.py` & `indy-plenum-1.9.2rc1/plenum/test/testable.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_view_change_happens_if_primary_is_slow_to_update_freshness.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_view_change_happens_if_primary_is_slow_to_update_freshness.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_during_pool_ordering.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_during_pool_ordering.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_in_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_in_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_replica_freshness.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_replica_freshness.py`

 * *Files 6% similar despite different names*

```diff
@@ -41,18 +41,18 @@
 @pytest.fixture(scope='function')
 def replica_with_valid_requests(primary_replica):
     requests = {ledger_id: sdk_random_request_objects(1, identifier="did",
                                                       protocol_version=CURRENT_PROTOCOL_VERSION)[0]
                 for ledger_id in LEDGER_IDS}
 
     def patched_consume_req_queue_for_pre_prepare(ledger_id, tm, view_no, pp_seq_no):
-        reqs = [requests[ledger_id]] if len(primary_replica._ordering_service.requestQueues[ledger_id]) > 0 else []
+        reqs = [requests[ledger_id]] if len(primary_replica.requestQueues[ledger_id]) > 0 else []
         return [reqs, [], []]
 
-    primary_replica._ordering_service._consume_req_queue_for_pre_prepare = patched_consume_req_queue_for_pre_prepare
+    primary_replica.consume_req_queue_for_pre_prepare = patched_consume_req_queue_for_pre_prepare
 
     return primary_replica, requests
 
 
 def set_current_time(replica, ts):
     replica.get_current_time.value = OLDEST_TS + ts
     replica.get_time_for_3pc_batch.value = OLDEST_TS + ts
@@ -67,15 +67,15 @@
 
     for ledger_id in ledger_ids:
         msg = replica.outBox.popleft()
         assert isinstance(msg, Ordered)
         assert msg.ledgerId == ledger_id
 
     for ledger_id in ledger_ids:
-        replica._ordering_service.requestQueues[ledger_id].clear()
+        replica.requestQueues[ledger_id].clear()
 
 
 def check_and_pop_freshness_pre_prepare(replica, ledger_id):
     msg = replica.outBox.popleft()
     assert isinstance(msg, PrePrepare)
     assert msg.ledgerId == ledger_id
     assert msg.reqIdr == tuple()
@@ -163,28 +163,28 @@
     ([POOL_LEDGER_ID, DOMAIN_LEDGER_ID, CONFIG_LEDGER_ID], [])
 ])
 def test_freshness_pre_prepare_only_when_no_requests_for_ledger(tconf,
                                                                 replica_with_valid_requests,
                                                                 ordered, refreshed):
     replica, requests = replica_with_valid_requests
     for ordered_ledger_id in ordered:
-        replica._ordering_service.requestQueues[ordered_ledger_id] = OrderedSet([requests[ordered_ledger_id].key])
+        replica.requestQueues[ordered_ledger_id] = OrderedSet([requests[ordered_ledger_id].key])
 
     # send 3PC batch for requests
     assert len(replica.outBox) == 0
     set_current_time(replica, tconf.Max3PCBatchWait + 1)
     replica.send_3pc_batch()
     assert len(replica.outBox) == len(ordered)
 
     # wait for freshness timeout
     set_current_time(replica, FRESHNESS_TIMEOUT + 1)
 
     # order requests
     for i in range(len(ordered)):
-        replica._ordering_service._order_3pc_key((0, i + 1))
+        replica.order_3pc_key((0, i + 1))
     assert len(replica.outBox) == 2 * len(ordered)
     check_and_pop_ordered(replica, ordered)
 
     # refresh state for unordered
     replica.send_3pc_batch()
     assert len(replica.outBox) == len(refreshed)
     for refreshed_ledger_id in refreshed:
@@ -192,15 +192,15 @@
 
 
 def test_order_empty_pre_prepare(looper, tconf, txnPoolNodeSet):
     assert all(node.master_replica.last_ordered_3pc == (0, 0) for node in txnPoolNodeSet)
     assert all(node.spylog.count(node.processOrdered) == 0 for node in txnPoolNodeSet)
 
     replica = getPrimaryReplica([txnPoolNodeSet[0]], instId=0)
-    replica._ordering_service._do_send_3pc_batch(ledger_id=POOL_LEDGER_ID)
+    replica._do_send_3pc_batch(ledger_id=POOL_LEDGER_ID)
 
     looper.run(eventually(
         lambda: assertExp(
             all(
                 node.master_replica.last_ordered_3pc == (0, 1) for node in txnPoolNodeSet
             )
         )
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_view_change.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_batch_updates_last_ordered.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_batch_updates_last_ordered.py`

 * *Files 27% similar despite different names*

```diff
@@ -36,28 +36,28 @@
                           ))
 
     restart_node(looper, txnPoolNodeSet, txnPoolNodeSet[0], tconf, tdir, allPluginsPath)
 
     # no view change happened
     assert txnPoolNodeSet[0].master_replica.isPrimary
 
-    assert txnPoolNodeSet[0].db_manager.get_txn_root_hash(DOMAIN_LEDGER_ID) == \
-           txnPoolNodeSet[1].db_manager.get_txn_root_hash(DOMAIN_LEDGER_ID)
+    assert txnPoolNodeSet[0].master_replica.txnRootHash(DOMAIN_LEDGER_ID) == \
+           txnPoolNodeSet[1].master_replica.txnRootHash(DOMAIN_LEDGER_ID)
     # node caught up till actual last_ordered_3pc
     assert txnPoolNodeSet[0].master_replica.last_ordered_3pc == \
            txnPoolNodeSet[1].master_replica.last_ordered_3pc
 
     old_discard = len(getSpecificDiscardedMsg(txnPoolNodeSet[1], PrePrepare))
 
     # correct ordering
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_steward, 1)
 
     # domain ledger equeal
-    assert txnPoolNodeSet[0].db_manager.get_txn_root_hash(DOMAIN_LEDGER_ID) == \
-           txnPoolNodeSet[1].db_manager.get_txn_root_hash(DOMAIN_LEDGER_ID)
+    assert txnPoolNodeSet[0].master_replica.txnRootHash(DOMAIN_LEDGER_ID) == \
+           txnPoolNodeSet[1].master_replica.txnRootHash(DOMAIN_LEDGER_ID)
 
     # no discard happened
     assert len(getSpecificDiscardedMsg(txnPoolNodeSet[1], PrePrepare)) == old_discard
 
 
 def test_freshness_batch_updates_last_ordered_non_primary(looper, txnPoolNodeSet, sdk_pool_handle,
                                                           sdk_wallet_steward, tconf, tdir, allPluginsPath):
@@ -69,19 +69,19 @@
     looper.run(eventually(check_updated_bls_multi_sig_for_all_ledgers,
                           txnPoolNodeSet, bls_multi_sigs_after_first_update, FRESHNESS_TIMEOUT,
                           timeout=FRESHNESS_TIMEOUT * 2
                           ))
 
     restart_node(looper, txnPoolNodeSet, txnPoolNodeSet[1], tconf, tdir, allPluginsPath)
 
-    assert txnPoolNodeSet[0].db_manager.get_txn_root_hash(DOMAIN_LEDGER_ID) == \
-           txnPoolNodeSet[1].db_manager.get_txn_root_hash(DOMAIN_LEDGER_ID)
+    assert txnPoolNodeSet[0].master_replica.txnRootHash(DOMAIN_LEDGER_ID) == \
+           txnPoolNodeSet[1].master_replica.txnRootHash(DOMAIN_LEDGER_ID)
     # node caught up till actual last_ordered_3pc
     assert txnPoolNodeSet[0].master_replica.last_ordered_3pc == \
            txnPoolNodeSet[1].master_replica.last_ordered_3pc
 
     # correct ordering
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_steward, 1)
 
     # domain ledger equeal
-    assert txnPoolNodeSet[0].db_manager.get_txn_root_hash(DOMAIN_LEDGER_ID) == \
-           txnPoolNodeSet[1].db_manager.get_txn_root_hash(DOMAIN_LEDGER_ID)
+    assert txnPoolNodeSet[0].master_replica.txnRootHash(DOMAIN_LEDGER_ID) == \
+           txnPoolNodeSet[1].master_replica.txnRootHash(DOMAIN_LEDGER_ID)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_replica_freshness_checker.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_replica_freshness_checker.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_after_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_after_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_instance_changes_are_sent_continuosly.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_instance_changes_are_sent_continuosly.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_freshness_during_domain_ordering.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_freshness_during_domain_ordering.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/freshness/test_view_change_happens_if_ordering_is_halted.py` & `indy-plenum-1.9.2rc1/plenum/test/freshness/test_view_change_happens_if_ordering_is_halted.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/ledger/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/ledger/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_commit_txns.py` & `indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_commit_txns.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_discard_txns.py` & `indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_discard_txns.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_append_txns.py` & `indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_append_txns.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_get_by_seqno.py` & `indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_get_by_seqno.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_add_txns.py` & `indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_add_txns.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_append_txns_result.py` & `indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_append_txns_result.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledgers.py` & `indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledgers.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/ledger/test_ledger_get_last_txn.py` & `indy-plenum-1.9.2rc1/plenum/test/ledger/test_ledger_get_last_txn.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/propagate/test_propagate_recvd_after_request.py` & `indy-plenum-1.9.2rc1/plenum/test/propagate/test_propagate_recvd_after_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/propagate/test_propagate_recvd_before_request.py` & `indy-plenum-1.9.2rc1/plenum/test/propagate/test_propagate_recvd_before_request.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/script/test_bootstrap_test_node.py` & `indy-plenum-1.9.2rc1/plenum/test/script/test_bootstrap_test_node.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/script/test_change_primary_node_ha.py` & `indy-plenum-1.9.2rc1/plenum/test/script/test_change_primary_node_ha.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/script/test_change_non_primary_node_ha.py` & `indy-plenum-1.9.2rc1/plenum/test/script/test_change_non_primary_node_ha.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_propagate_primary_after_primary_restart_view_0.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_propagate_primary_after_primary_restart_view_0.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_new_node_accepts_chosen_primary.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_new_node_accepts_chosen_primary.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_promotion_leads_to_primary_inconsistency.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_promotion_leads_to_primary_inconsistency.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection_pool_txn.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection_pool_txn.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_propagate_primary_after_primary_restart_view_1.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_propagate_primary_after_primary_restart_view_1.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_catchup_multiple_rounds.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_catchup_multiple_rounds.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_add_node_with_f_changed.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_add_node_with_f_changed.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_recover_more_than_f_failure.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_recover_more_than_f_failure.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,18 +1,16 @@
-from plenum.test.checkpoints.helper import check_for_nodes, check_stable_checkpoint
 from stp_core.common.log import getlogger
 
 from plenum.test.helper import waitForViewChange, \
     sdk_send_random_and_check
 from plenum.test.node_catchup.helper import ensure_all_nodes_have_same_data
 from plenum.test.pool_transactions.helper import \
     disconnect_node_and_ensure_disconnected
 from plenum.test.test_node import ensureElectionsDone
 from plenum.test.view_change.helper import start_stopped_node
-from stp_core.loop.eventually import eventually
 
 logger = getlogger()
 
 
 def test_recover_stop_primaries(looper, checkpoint_size, txnPoolNodeSet,
                                 allPluginsPath, tdir, tconf, sdk_pool_handle,
                                 sdk_wallet_steward):
@@ -23,57 +21,52 @@
     - restart current master primary (Beta)
     - send txns
     """
 
     active_nodes = list(txnPoolNodeSet)
     assert 4 == len(active_nodes)
     initial_view_no = active_nodes[0].viewNo
-    checkpoint_freq = tconf.CHK_FREQ
 
     logger.info("Stop first node (current Primary)")
     _, active_nodes = stop_primary(looper, active_nodes)
 
     logger.info("Make sure view changed")
     expected_view_no = initial_view_no + 1
     waitForViewChange(looper, active_nodes, expectedViewNo=expected_view_no)
     ensureElectionsDone(looper=looper, nodes=active_nodes, instances_list=range(2))
     ensure_all_nodes_have_same_data(looper, nodes=active_nodes)
 
     logger.info("send at least one checkpoint")
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
                               sdk_wallet_steward, 2 * checkpoint_size - 1)
-    # TODO: When stable checkpoint is not deleted it makes sense to check just our last checkpoint
-    #  and remove eventually. Also for some reason 2 checkpoints generated here
-    looper.run(eventually(check_for_nodes, active_nodes, check_stable_checkpoint, 2 * checkpoint_freq))
+    assert nodes_have_checkpoints(*active_nodes)
     ensure_all_nodes_have_same_data(looper, nodes=active_nodes)
 
     logger.info("Stop second node (current Primary) so the primary looses his state")
     stopped_node, active_nodes = stop_primary(looper, active_nodes)
 
     logger.info("Restart the primary node")
     restarted_node = start_stopped_node(stopped_node, looper, tconf, tdir, allPluginsPath)
-    # TODO: Actually I'm not sure that this is a correct behavior. Can we restore stable
-    #  checkpoint just from audit ledger or node status db?
-    check_for_nodes([restarted_node], check_stable_checkpoint, 0)
-    check_for_nodes(active_nodes, check_stable_checkpoint, 2 * checkpoint_freq)
+    assert nodes_do_not_have_checkpoints(restarted_node)
+    assert nodes_have_checkpoints(*active_nodes)
     active_nodes = active_nodes + [restarted_node]
 
     logger.info("Check that primary selected")
     ensureElectionsDone(looper=looper, nodes=active_nodes,
                         instances_list=range(2), customTimeout=30)
     waitForViewChange(looper, active_nodes, expectedViewNo=expected_view_no)
     ensure_all_nodes_have_same_data(looper, nodes=active_nodes,
                                     exclude_from_check=['check_last_ordered_3pc_backup'])
 
     logger.info("Check if the pool is able to process requests")
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle,
                               sdk_wallet_steward, 10 * checkpoint_size)
     ensure_all_nodes_have_same_data(looper, nodes=active_nodes,
                                     exclude_from_check=['check_last_ordered_3pc_backup'])
-    looper.run(eventually(check_for_nodes, active_nodes, check_stable_checkpoint, 12 * checkpoint_freq))
+    assert nodes_have_checkpoints(*active_nodes)
 
 
 def stop_primary(looper, active_nodes):
     stopped_node = active_nodes[0]
     disconnect_node_and_ensure_disconnected(looper,
                                             active_nodes,
                                             stopped_node,
@@ -84,7 +77,15 @@
 
 
 def primary_replicas_iter(*nodes):
     for node in nodes:
         for replica in node.replicas.values():
             if replica.isPrimary:
                 yield replica
+
+
+def nodes_have_checkpoints(*nodes):
+    return all(replica.checkpoints for replica in primary_replicas_iter(*nodes))
+
+
+def nodes_do_not_have_checkpoints(*nodes):
+    return all(not replica.checkpoints for replica in primary_replicas_iter(*nodes))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_pool_restart.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_pool_restart.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection_after_demoted_node_promotion.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection_after_demoted_node_promotion.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_view_changes.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_view_changes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_add_node_to_pool_with_large_ppseqno.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_add_node_to_pool_with_large_ppseqno.py`

 * *Files 11% similar despite different names*

```diff
@@ -18,15 +18,15 @@
     return min(res)
 
 
 def _set_ppseqno(nodes, new_ppsn):
     for node in nodes:
         for repl in node.replicas.values():
             repl.lastPrePrepareSeqNo = new_ppsn
-            repl._checkpointer.set_watermarks(low_watermark=new_ppsn)
+            repl.h = new_ppsn
             repl.last_ordered_3pc = (repl.viewNo, new_ppsn)
 
 
 @pytest.mark.parametrize('do_view_change', [0, 1])
 def test_add_node_to_pool_with_large_ppseqno_diff_views(do_view_change, looper, txnPoolNodeSet, tconf, sdk_pool_handle,
                                                         sdk_wallet_steward, tdir, allPluginsPath):
     """
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_catchup_needed_check.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_catchup_needed_check.py`

 * *Files 8% similar despite different names*

```diff
@@ -37,14 +37,15 @@
     """
     sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, 3 * Max3PCBatchSize)
     ensure_all_nodes_have_same_data(looper, txnPoolNodeSet)
 
     nprs = getNonPrimaryReplicas(txnPoolNodeSet, 0)
     bad_node = nprs[-1].node
     other_nodes = [n for n in txnPoolNodeSet if n != bad_node]
+    orig_method = bad_node.master_replica.process_three_phase_msg
 
     # Bad node does not process any 3 Commit messages, equivalent to messages
     with delay_rules(bad_node.nodeIbStasher, cDelay()):
 
         # Delay LEDGER_STAUS on slow node, so that only MESSAGE_REQUEST(LEDGER_STATUS) is sent, and the
         # node catch-ups 2 times.
         # Otherwise other nodes may receive multiple LEDGER_STATUSes from slow node, and return Consistency proof for all
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_selection_f_plus_one_quorum.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_selection_f_plus_one_quorum.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_catchup_after_view_change.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_catchup_after_view_change.py`

 * *Files 1% similar despite different names*

```diff
@@ -49,15 +49,15 @@
 
     catchup_reply_counts = {n.name: n.ledgerManager.spylog.count(
         n.ledgerManager.processCatchupRep) for n in txnPoolNodeSet}
     catchup_done_counts = {n.name: n.spylog.count(n.allLedgersCaughtUp)
                            for n in txnPoolNodeSet}
 
     def slow_node_processed_some():
-        assert slow_node.master_replica._ordering_service.batches
+        assert slow_node.master_replica.batches
 
     # The slow node has received some PRE-PREPAREs
     looper.run(
         eventually(
             slow_node_processed_some,
             retryWait=1,
             timeout=delay))
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_recover_after_demoted.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_recover_after_demoted.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_promotion.py` & `indy-plenum-1.9.2rc1/plenum/test/primary_selection/test_primary_selection_after_primary_demotion_and_promotion.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_ledger_manager.py` & `indy-plenum-1.9.2rc1/plenum/test/test_ledger_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_util.py` & `indy-plenum-1.9.2rc1/plenum/test/test_util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/test_notifier_plugin_manager.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/test_notifier_plugin_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/stats_consumer/plugin_stats_consumer.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/stats_consumer/plugin_stats_consumer.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/conftest.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/conftest.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,14 @@
     # The next imports and reloading are needed only in tests, since in
     # production none of these modules would be loaded before plugins are
     # setup (not initialised)
     import plenum.server
     import plenum.common
 
     importlib.reload(plenum.server.replica)
-    importlib.reload(plenum.server.consensus.ordering_service)
     importlib.reload(plenum.server.node)
     importlib.reload(plenum.server.catchup.utils)
     importlib.reload(plenum.server.catchup.catchup_rep_service)
     importlib.reload(plenum.server.catchup.cons_proof_service)
     importlib.reload(plenum.server.catchup.ledger_leecher_service)
     importlib.reload(plenum.server.catchup.node_leecher_service)
     importlib.reload(plenum.server.catchup.seeder_service)
```

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_request_digest.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_request_digest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_plugin_request_handling.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_plugin_request_handling.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_freshness_during_ordering.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_freshness_during_ordering.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/__init__.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/__init__.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_catchup.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_catchup.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/auction_start_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/auction_start_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/abstract_auction_req_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/abstract_auction_req_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/get_bal_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/get_bal_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/place_bid_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/place_bid_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/request_handlers/auction_end_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/request_handlers/auction_end_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/storage.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/storage.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_freshness.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_freshness.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/transactions.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/transactions.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/main.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/main.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/batch_handlers/auction_batch_handler.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/batch_handlers/auction_batch_handler.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/helper.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/plugin/demo_plugin/test_plugin_basic.py` & `indy-plenum-1.9.2rc1/plenum/test/plugin/demo_plugin/test_plugin_basic.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/transactions/test_new_txn_format.py` & `indy-plenum-1.9.2rc1/plenum/test/transactions/test_new_txn_format.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/transactions/test_txn_general_access_utils.py` & `indy-plenum-1.9.2rc1/plenum/test/transactions/test_txn_general_access_utils.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/transactions/test_txn_init_utils.py` & `indy-plenum-1.9.2rc1/plenum/test/transactions/test_txn_init_utils.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/transactions/test_req_to_txn.py` & `indy-plenum-1.9.2rc1/plenum/test/transactions/test_req_to_txn.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_state_regenerated_from_ledger.py` & `indy-plenum-1.9.2rc1/plenum/test/test_state_regenerated_from_ledger.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/waits.py` & `indy-plenum-1.9.2rc1/plenum/test/waits.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/testing_utils.py` & `indy-plenum-1.9.2rc1/plenum/test/testing_utils.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/test/test_crypto.py` & `indy-plenum-1.9.2rc1/plenum/test/test_crypto.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/persistence/req_id_to_txn.py` & `indy-plenum-1.9.2rc1/plenum/persistence/req_id_to_txn.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/persistence/storage.py` & `indy-plenum-1.9.2rc1/plenum/persistence/storage.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/persistence/client_txn_log.py` & `indy-plenum-1.9.2rc1/plenum/persistence/client_txn_log.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/persistence/client_req_rep_store.py` & `indy-plenum-1.9.2rc1/plenum/persistence/client_req_rep_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/plenum/persistence/client_req_rep_store_file.py` & `indy-plenum-1.9.2rc1/plenum/persistence/client_req_rep_store_file.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/crypto/bls/bls_crypto.py` & `indy-plenum-1.9.2rc1/crypto/bls/bls_crypto.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/crypto/bls/bls_bft_replica.py` & `indy-plenum-1.9.2rc1/crypto/bls/bls_bft_replica.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/crypto/bls/bls_key_manager.py` & `indy-plenum-1.9.2rc1/crypto/bls/bls_key_manager.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/crypto/bls/bls_factory.py` & `indy-plenum-1.9.2rc1/crypto/bls/bls_factory.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/crypto/bls/bls_bft.py` & `indy-plenum-1.9.2rc1/crypto/bls/bls_bft.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/crypto/bls/bls_key_register.py` & `indy-plenum-1.9.2rc1/crypto/bls/bls_key_register.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/crypto/bls/bls_multi_signature.py` & `indy-plenum-1.9.2rc1/crypto/bls/bls_multi_signature.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/crypto/bls/indy_crypto/bls_crypto_indy_crypto.py` & `indy-plenum-1.9.2rc1/crypto/bls/indy_crypto/bls_crypto_indy_crypto.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/crypto/test/bls/indy_crypto/test_bls_crypto_indy_crypto.py` & `indy-plenum-1.9.2rc1/crypto/test/bls/indy_crypto/test_bls_crypto_indy_crypto.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/crypto/test/test_multi_signature.py` & `indy-plenum-1.9.2rc1/crypto/test/test_multi_signature.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/pruning_state.py` & `indy-plenum-1.9.2rc1/state/pruning_state.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/db/refcount_db.py` & `indy-plenum-1.9.2rc1/state/db/refcount_db.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/db/persistent_db.py` & `indy-plenum-1.9.2rc1/state/db/persistent_db.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/state.py` & `indy-plenum-1.9.2rc1/state/state.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/test/test_state_proof_verification.py` & `indy-plenum-1.9.2rc1/state/test/test_state_proof_verification.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/test/trie/test_prefix_nodes.py` & `indy-plenum-1.9.2rc1/state/test/trie/test_prefix_nodes.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/test/trie/test_trie_values_at_different_roots.py` & `indy-plenum-1.9.2rc1/state/test/trie/test_trie_values_at_different_roots.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/test/trie/test_proof.py` & `indy-plenum-1.9.2rc1/state/test/trie/test_proof.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/test/test_pruning_state.py` & `indy-plenum-1.9.2rc1/state/test/test_pruning_state.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/trie/pruning_trie.py` & `indy-plenum-1.9.2rc1/state/trie/pruning_trie.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/util/fast_rlp.py` & `indy-plenum-1.9.2rc1/state/util/fast_rlp.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/state/util/utils.py` & `indy-plenum-1.9.2rc1/state/util/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,15 @@
 import os
 import string
 
+import sha3 as _sha3
 
-import hashlib
-if hasattr(hashlib, 'sha3_256'):
-    def sha3_256(x):
-        return hashlib.sha3_256(x).digest()
-else:
-    import sha3 as _sha3
-    def sha3_256(x):
-        return _sha3.sha3_256(x).digest()
+
+def sha3_256(x):
+    return _sha3.sha3_256(x).digest()
 
 
 import rlp
 from rlp.sedes import big_endian_int, BigEndianInt, Binary
 from rlp.utils import decode_hex, encode_hex, ascii_chr, str_to_bytes
 import random
```

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/remote.py` & `indy-plenum-1.9.2rc1/stp_zmq/remote.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/simple_zstack.py` & `indy-plenum-1.9.2rc1/stp_zmq/simple_zstack.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/kit_zstack.py` & `indy-plenum-1.9.2rc1/stp_zmq/kit_zstack.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/util.py` & `indy-plenum-1.9.2rc1/stp_zmq/util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/zstack.py` & `indy-plenum-1.9.2rc1/stp_zmq/zstack.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 import inspect
 
 from plenum.common.metrics_collector import NullMetricsCollector
+from plenum.common.timer import QueueTimer
 from plenum.common.util import z85_to_friendly
 from stp_core.common.config.util import getConfig
 from stp_core.common.constants import CONNECTION_PREFIX, ZMQ_NETWORK_PROTOCOL
 from stp_zmq.client_message_provider import ClientMessageProvider
 
 try:
     import ujson as json
@@ -12,15 +13,15 @@
     import json
 
 import os
 import shutil
 import sys
 import time
 from binascii import hexlify, unhexlify
-from collections import deque
+from collections import deque, OrderedDict
 from typing import Mapping, Tuple, Any, Union, Optional, NamedTuple
 
 from common.exceptions import PlenumTypeError, PlenumValueError
 
 # import stp_zmq.asyncio
 import zmq.auth
 from stp_core.crypto.nacl_wrappers import Signer, Verifier
@@ -369,39 +370,33 @@
         # noinspection PyUnresolvedReferences
         # self.poller.register(self.listener, test.POLLIN)
         public, secret = self.selfEncKeys
         self.listener.curve_secretkey = secret
         self.listener.curve_publickey = public
         self.listener.curve_server = True
         self.listener.identity = self.publicKey
-        logger.info(
+        logger.debug(
             '{} will bind its listener at {}:{}'.format(self, self.ha[0], self.ha[1]))
         set_keepalive(self.listener, self.config)
         set_zmq_internal_queue_size(self.listener, self.queue_size)
         # Cycle to deal with "Address already in use" in case of immediate stack restart.
         bound = False
-
-        sleep_between_bind_retries = 0.2
-        bind_retry_time = 0
+        bind_retries = 0
         while not bound:
             try:
                 self.listener.bind(
                     '{protocol}://{ip}:{port}'.format(ip=self.ha[0], port=self.ha[1],
                                                       protocol=ZMQ_NETWORK_PROTOCOL)
                 )
                 bound = True
             except zmq.error.ZMQError as zmq_err:
-                logger.warning("{} can not bind to {}:{}. Will try in {} secs.".
-                               format(self, self.ha[0], self.ha[1], sleep_between_bind_retries))
-                bind_retry_time += sleep_between_bind_retries
-                if bind_retry_time > self.config.MAX_WAIT_FOR_BIND_SUCCESS:
-                    logger.warning("{} can not bind to {}:{} for {} secs. Going to restart the service.".
-                                   format(self, self.ha[0], self.ha[1], self.config.MAX_WAIT_FOR_BIND_SUCCESS))
+                bind_retries += 1
+                if bind_retries == 50:
                     raise zmq_err
-                time.sleep(sleep_between_bind_retries)
+                time.sleep(0.2)
 
     def close(self):
         if self.listener_monitor is not None:
             self.listener.disable_monitor()
             self.listener_monitor = None
         self.listener.unbind(self.listener.LAST_ENDPOINT)
         self.listener.close(linger=0)
```

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/test_send_to_disconnected.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/test_send_to_disconnected.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/test_kitzstack.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/test_kitzstack.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/conftest.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/test_quotas.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/test_quotas.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/test_large_messages.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/test_large_messages.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/test_stashed_client_messages.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/test_stashed_client_messages.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/test_node_to_node_quota.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/test_node_to_node_quota.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/test_zstack_communication.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/test_zstack_communication.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/test_zstack.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/test_zstack.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/test_stashed_ping_pong.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/test_stashed_ping_pong.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/helper.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/test_heartbeats.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/test_heartbeats.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/test/test_reconnect.py` & `indy-plenum-1.9.2rc1/stp_zmq/test/test_reconnect.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/authenticator.py` & `indy-plenum-1.9.2rc1/stp_zmq/authenticator.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/stp_zmq/client_message_provider.py` & `indy-plenum-1.9.2rc1/stp_zmq/client_message_provider.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,29 +54,29 @@
         return result, error_msg
 
     def _transmit_one_msg_throughlistener(self, msg, ident) -> Tuple[bool, Optional[str], bool]:
 
         def prepare_error_msg(ex):
             err_str = '{}{} got error {} while sending through listener to {}' \
                 .format(CONNECTION_PREFIX, self, ex, ident)
-            logger.debug(err_str)
+            logger.warning(err_str)
             return err_str
 
         need_to_resend = False
         if isinstance(ident, str):
             ident = ident.encode()
         try:
             msg = self._prepare_to_send(msg)
             logger.trace('{} transmitting {} to {} through listener socket'.
                          format(self, msg, ident))
             self.metrics.add_event(self._mt_outgoing_size, len(msg))
             self.listener.send_multipart([ident, msg], flags=zmq.NOBLOCK)
         except InvalidMessageExceedingSizeException as ex:
             err_str = '{}Cannot transmit message. Error {}'.format(CONNECTION_PREFIX, ex)
-            logger.debug(err_str)
+            logger.warning(err_str)
             return False, err_str, need_to_resend
         except zmq.Again as ex:
             need_to_resend = True
             return False, prepare_error_msg(ex), need_to_resend
         except zmq.ZMQError as ex:
             need_to_resend = (ex.errno == 113)
             return False, prepare_error_msg(ex), need_to_resend
```

### Comparing `indy-plenum-1.9.2.dev879/ledger/error.py` & `indy-plenum-1.9.2rc1/ledger/error.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/tree_hasher.py` & `indy-plenum-1.9.2rc1/ledger/tree_hasher.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/util.py` & `indy-plenum-1.9.2rc1/ledger/util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/compact_merkle_tree.py` & `indy-plenum-1.9.2rc1/ledger/compact_merkle_tree.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/immutable_store.py` & `indy-plenum-1.9.2rc1/ledger/immutable_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/hash_stores/hash_store.py` & `indy-plenum-1.9.2rc1/ledger/hash_stores/hash_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/hash_stores/memory_hash_store.py` & `indy-plenum-1.9.2rc1/ledger/hash_stores/memory_hash_store.py`

 * *Files 12% similar despite different names*

```diff
@@ -19,18 +19,18 @@
     def readLeaf(self, pos):
         return self._leafs[pos - 1]
 
     def readNode(self, pos):
         return self._nodes[pos - 1]
 
     def readLeafs(self, startpos, endpos):
-        return [n for n in self._leafs[startpos - 1:endpos]]
+        return (n for n in self._leafs[startpos - 1:endpos - 1])
 
     def readNodes(self, startpos, endpos):
-        return [n for n in self._nodes[startpos - 1:endpos]]
+        return (n for n in self._nodes[startpos - 1:endpos - 1])
 
     @property
     def leafCount(self) -> int:
         return len(self._leafs)
 
     @property
     def nodeCount(self) -> int:
```

### Comparing `indy-plenum-1.9.2.dev879/ledger/genesis_txn/genesis_txn_file_util.py` & `indy-plenum-1.9.2rc1/ledger/genesis_txn/genesis_txn_file_util.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/genesis_txn/genesis_txn_initiator_from_file.py` & `indy-plenum-1.9.2rc1/ledger/genesis_txn/genesis_txn_initiator_from_file.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/test/conftest.py` & `indy-plenum-1.9.2rc1/ledger/test/conftest.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/test/test_init_genesis_txns.py` & `indy-plenum-1.9.2rc1/ledger/test/test_init_genesis_txns.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/test/test_merkle_proof.py` & `indy-plenum-1.9.2rc1/ledger/test/test_merkle_proof.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/test/test_txn_persistence.py` & `indy-plenum-1.9.2rc1/ledger/test/test_txn_persistence.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/test/test_file_hash_store.py` & `indy-plenum-1.9.2rc1/ledger/test/test_file_hash_store.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/test/merkle_test.py` & `indy-plenum-1.9.2rc1/ledger/test/merkle_test.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/test/helper.py` & `indy-plenum-1.9.2rc1/ledger/test/helper.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/test/test_file_store_perf.py` & `indy-plenum-1.9.2rc1/ledger/test/test_file_store_perf.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/test/test_ledger.py` & `indy-plenum-1.9.2rc1/ledger/test/test_ledger.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/merkle_tree.py` & `indy-plenum-1.9.2rc1/ledger/merkle_tree.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/ledger.py` & `indy-plenum-1.9.2rc1/ledger/ledger.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/ledger/merkle_verifier.py` & `indy-plenum-1.9.2rc1/ledger/merkle_verifier.py`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/process_logs/process_logs` & `indy-plenum-1.9.2rc1/scripts/process_logs/process_logs`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/process_logs/process_logs.yml` & `indy-plenum-1.9.2rc1/scripts/process_logs/process_logs.yml`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/init_plenum_keys` & `indy-plenum-1.9.2rc1/scripts/init_plenum_keys`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/log_stats` & `indy-plenum-1.9.2rc1/scripts/log_stats`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/filter_log` & `indy-plenum-1.9.2rc1/scripts/filter_log`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/start_plenum_node` & `indy-plenum-1.9.2rc1/scripts/start_plenum_node`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/gen_node` & `indy-plenum-1.9.2rc1/scripts/gen_node`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/export-gen-txns` & `indy-plenum-1.9.2rc1/scripts/export-gen-txns`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/get_keys` & `indy-plenum-1.9.2rc1/scripts/get_keys`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/init_bls_keys` & `indy-plenum-1.9.2rc1/scripts/init_bls_keys`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/generate_plenum_pool_transactions` & `indy-plenum-1.9.2rc1/scripts/generate_plenum_pool_transactions`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/gen_steward_key` & `indy-plenum-1.9.2rc1/scripts/gen_steward_key`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/udp_receiver` & `indy-plenum-1.9.2rc1/scripts/udp_receiver`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/scripts/udp_sender` & `indy-plenum-1.9.2rc1/scripts/udp_sender`

 * *Files identical despite different names*

### Comparing `indy-plenum-1.9.2.dev879/PKG-INFO` & `indy-plenum-1.9.2rc1/PKG-INFO`

 * *Files 19% similar despite different names*

```diff
@@ -1,17 +1,17 @@
 Metadata-Version: 2.1
 Name: indy-plenum
-Version: 1.9.2.dev879
+Version: 1.9.2rc1
 Summary: Plenum Byzantine Fault Tolerant Protocol
 Home-page: https://github.com/hyperledger/indy-plenum
 Author: Hyperledger
 Author-email: hyperledger-indy@lists.hyperledger.org
 Maintainer: Hyperledger
 Maintainer-email: hyperledger-indy@lists.hyperledger.org
 License: Apache 2.0
-Download-URL: https://github.com/hyperledger/indy-plenum/tarball/1.9.2.dev879
+Download-URL: https://github.com/hyperledger/indy-plenum/tarball/1.9.2rc1
 Description: Plenum Byzantine Fault Tolerant Protocol
 Keywords: Byzantine Fault Tolerant Plenum
 Platform: UNKNOWN
-Provides-Extra: stats
-Provides-Extra: benchmark
 Provides-Extra: tests
+Provides-Extra: benchmark
+Provides-Extra: stats
```

